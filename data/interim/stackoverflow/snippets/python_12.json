[["def filterText(text):\n    return \" \".join(text.split(\" \"))\n\nfilterText(\"Hello \\n\\n\\nWorld\\tI\\n\\nLike money\\t\")\n", "Hello World I Like money\n"], [], [], [], [], ["typing-inspect==0.8.0\ntyping_extensions==4.5.0\n"], [], [], [], ["class Rank(Enum):\n    King = 13\n\nprint(Rank.King.name) # outputs 'King'\nprint(Rank.King.value) # outputs 13\n", "from enum import Enum\nfrom types import DynamicClassAttribute\n\nclass MixedCaseEnum(Enum):\n    @DynamicClassAttribute\n    def name(self):\n        return self._name_.title()\n\nclass Rank(MixedCaseEnum):\n    KING = 13\n\nprint(Rank.KING.name) # outputs 'King'\nprint(Rank.KING.value) # outputs 13\n"], [], [], ["to_save = value.to_string()\nr.set(redis_key, to_save)\n\nvalue = r.get(redis_key)\ndf = pd.read_csv(io.StringIO(value))\n\n"], ["subselect = db.select(Tablename).filter_by(user_id=current_user.id, ...)\n\nselect = db.select(db.func.count()).select_from(subselect)\n\nnumber_of_rows = db.session.execute(select).scalar_one()\n"], ["np.float = float    \nnp.int = int   #module 'numpy' has no attribute 'int'\nnp.object = object    #module 'numpy' has no attribute 'object'\nnp.bool = bool    #module 'numpy' has no attribute 'bool'\n"], ["def updateblog (id:int, title:Optional[str]=None, body:Optional[str]=None, db:Session = Depends(get_db)):\n\n    if title!=None:\n            db.query(models.Blog).filter(models.Blog.id==id).update({'title':title})\n    \n    if body!=None:\n            db.query(models.Blog).filter(models.Blog.id==id).update({'body':body})\n\n    db.commit()\n    \n    return f'Blog #{id} has been updated.'\n"], ["    import os\n    from PIL import Image\n    import pillow_heif\n    \n    \n    def convert_heic_to_jpeg(heic_path, jpeg_path):\n        img = Image.open(heic_path)\n        img.save(jpeg_path, format='JPEG')\n    \n    \n    def convert_all_heic_to_jpeg(input_folder, output_folder):\n        # Register HEIF opener with Pillow\n        pillow_heif.register_heif_opener()\n    \n        # Create output folder if it doesn't exist\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n    \n        # List all files in input folder\n        for filename in os.listdir(input_folder):\n            # Check if file is a HEIC file\n            if filename.endswith(\".heic\") or filename.endswith(\".HEIC\"):\n                # Build full path to input and output file\n                heic_path = os.path.join(input_folder, filename)\n                jpeg_filename = f'{os.path.splitext(filename)[0]}.jpg'\n                jpeg_path = os.path.join(output_folder, jpeg_filename)\n    \n                # Convert HEIC to JPEG\n                convert_heic_to_jpeg(heic_path, jpeg_path)\n    \n    \n    # Example usage\n    input_folder = 'images'\n    output_folder = 'converted'\n    convert_all_heic_to_jpeg(input_folder, output_folder)\n"], ["pip uninstall flask_wtf\npip install flask_wtf\n"], ["\nunique_classes = np.unique(labels)\nclass_counts = np.bincount(labels)\ntotal_samples = len(labels)\n\nclass_weights = {}\nfor cls in unique_classes:\n    class_weight = total_samples / (len(unique_classes) * class_counts[cls])\n    class_weights[cls] = class_weight\n\n\nclass_weights = [class_weights[i] for i in range(len(class_weights))]\nclass_weights = torch.FloatTensor(class_weights).cuda() if torch.cuda.is_available() else torch.FloatTensor(class_weights)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n"], ["      encoding='utf-8'\n", "    encoding='utf-8-sig'\n"], [], ["pygame.Cursor()\n", "cursor_index = 2 # Loading cursor\nit = pygame.Cursor()\nit.data = (cursor_index, )\n"], ["flask==2.0.1\nWerkzeug==2.0.1\n", "pip3 install flask==2.0.1\npip3 install werkzeug==2.0.1\n"], ["\"python.autoComplete.extraPaths\": [\"./path-to-your-code\"],\n"], ["import pywt\nfrom skimage import io, color\n\ndata = io.imread(filename)\n\n# Process your image\ngray = color.rgb2gray(data)\ncoeffs = pywt.dwt2(gray, 'haar')\n\n# Or... process each channel separately\nr, g, b = [c.T for c in data.T]\ncr = pywt.dwt2(r, 'haar')\ncg = pywt.dwt2(g, 'haar')\ncb = pywt.dwt2(b, 'haar')\n\n\n# output: PIL, matplotlib, dump to file...\n"], [], [], [], ["curl -sSL https://install.python-poetry.org | sed 's/symlinks=False/symlinks=True/' | python3 -\n"], [], [], ["from keras.utils.image_utils import img_to_array, load_img \n"], [], ["import pandas_datareader as web\nweb.DataReader('AMZN', 'yahoo', start, end)\n", "import yfinance \nyfinance.download('AMZN', start, end)\n"], ["$ pipenv --rm        # to remove the virtual env\n$ exit               # to exit the virtual env\n$ vim Pipfile        # here change the version to '3.9' by replacing '3.10'\n$ pipenv shell       # this will create a virtual env with 3.9\n$ pipenv install     # to install the requirements\n"], ["try:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\n"], ["from collections import Mapping\n", "from collections.abc import Mapping\n"], [], ["file_id = ''  #right click your file name and paste your link and copy only id\nurl = f'https://drive.google.com/uc?id={file_id}'\n\ndf = pd.read_csv(url)\ndf\n"], ["import pandas as pd\nimport pyarrow as pa\nimport redis\n\n\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\n\ndef save_df_to_redis(r, redis_key, df):\n    buffer = pa.serialize_pandas(df)\n    r.set(redis_key, buffer.to_pybytes())\n\n\ndef load_df_from_redis(r, redis_key):\n    buffer = r.get(redis_key)\n    df = pa.deserialize_pandas(buffer)\n    return df\n\n\n\ndata = {\n    \"Name\": [\"John\", \"Anna\", \"Peter\"],\n    \"Age\": [28, 24, 33],\n}\ndf = pd.DataFrame(data)\n\nsave_df_to_redis(r, \"key\", df)\ndf_redis = load_df_from_redis(r, \"key\")\nprint(df_redis)\n"], [], ["API_KEY=\"xxxxxx\"\nSECRET_KEY=\"xxxxxx\"\n", "from google.colab import drive    \ndrive.mount('/content/drive')\n", "!pip install --quiet python-dotenv\nimport dotenv\nimport os\n\ndotenv.load_dotenv('/content/drive/MyDrive/01 Work \nFile/Credentials/.env')\nsecret_key = os.getenv('SECRET_KEY')\n", "print(secret_key)\n"], [], [], ["metric = 'val_accuracy'\nModelCheckpoint(filepath=r\"C:\\Users\\reda.elhail\\Desktop\\checkpoints\\{}\".format(Name), monitor=metric,\n                    verbose=2, save_best_only=True, mode='max')]\n"], ["pip install boto3 botocore awscli aiobotocore --ignore-installed\n"], ["\"terminal.integrated.env.windows\": { \"PYTHONPATH\": \"${workspaceFolder}\" }\n"], [], ["    # MemorySummary=pd.DataFrame(columns=header)\n    # broken after pandas update?\n    # AttributeError: type object 'object' has no attribute 'dtype'\n    \n    MemorySummary=pd.DataFrame(pd.np.empty((0, 5)))\n    MemorySummary.set_axis( header, axis=1, inplace=True) \n\n"], [], [], ["import pypdf\nfrom pdfminer.high_level import extract_text\n\nfile_path = ['sample-2.pdf', 'image-based-pdf-sample.pdf']\n\nfor doc in file_path:\n    reader = pypdf.PdfReader(doc)\n    for i in range(len(reader.pages)):\n        page = reader.pages[i]\n        text = page.extract_text()\n        total_words = len(text.split())\n    if total_words > 0:\n        print(f\"This document has text: {doc}\")\n    else:\n        print(f\"This document has images: {doc}\")\n", "This document has text: sample-2.pdf\nThis document has images: image-based-pdf-sample.pdf\n"], ["try:\n    nlp = spacy.load(\"en_core_web_trf\")\nexcept:\n    print(\"Downloading spaCy NLP model...\")\n    print(\"This may take a few minutes and it's one time process...\")\n    os.system(\n        \"pip install https://huggingface.co/spacy/en_core_web_trf/resolve/main/en_core_web_trf-any-py3-none-any.whl\")\n    nlp = spacy.load(\"en_core_web_trf\")\n", "import spacy\nimport os\n\ntry:\n    nlp = spacy.load(\"en_core_web_trf\")\nexcept:\n    print(\"Downloading spaCy NLP model...\")\n    print(\"This may take a few minutes and it's one time process...\")\n    os.system(\"pip install https://huggingface.co/spacy/en_core_web_trf/resolve/main/en_core_web_trf-any-py3-none-any.whl\")\n    nlp = spacy.load(\"en_core_web_trf\")\n\n\ndef perform_ner(*args, **kwargs):\n    query = kwargs['query']\n    # Process the input text with spaCy NLP model\n    doc = nlp(query)\n\n    # Extract named entities and categorize them\n    entities = [(entity.text, entity.label_) for entity in doc.ents]\n\n    return entities\n\n\nif __name__ == \"__main__\":\n    # Example input text\n    input_text = \"I want to buy a new iPhone 12 Pro Max from Apple.\"\n\n    # Perform NER on input text\n    entities = perform_ner(query=input_text)\n\n    # Print the extracted entities\n    print(entities)\n"], ["import ast\n\ncode_string = \"\"\"\n# A comment.\ndef foo(a, b):\n  return a + b\nclass Bar(object):\n  def __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\"\"\".strip()\n\ncode_lines = code_string.splitlines(keepends=True)\nfor node in ast.walk(ast.parse(code_string)):\n    if isinstance(node, ast.FunctionDef):\n        lines = code_lines[node.lineno - 1:node.end_lineno]\n        lines[0] = lines[0][node.col_offset:]\n        lines[-1] = lines[-1][:node.end_col_offset]\n        print(''.join(lines))\n", "def foo(a, b):\n  return a + b\ndef __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n"], ["conda create --name python38 python=3.8\n", "conda activate python38\n", "conda install -c conda-forge gdal\n", "pip install pygdal\n"], [], [], [], ["from sqlalchemy.orm import Session,aliased\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, String, select,label,func,text,literal_column\n\n u = aliased(User)\n q = select(u.id).select_from(u).where(u.username == user.username)\n"], [], ["# create OneHotEncoder object\nencoder = OneHotEncoder()\n\n# fit and transform color column\none_hot_array = encoder.fit_transform(df[['color']]).toarray()\n\n# create new dataframe from numpy array\none_hot_df = pd.DataFrame(one_hot_array, columns = encoder.get_feature_names(), index = df.index)\n\n#concat with df\ndata = pd.concat([df, one_hot_df], axis=1).drop(['color'], axis=1)\n"], [], [], ["df.loc['perc'] = df.iloc[2]/df.iloc[1]\n"], ["ex_dict = {\"key1\":5, \"key2\":2, \"key3\": 3 \"key4\": 4, \"key5\": 5}\n\nex_dict.pop('key2')\n\nex_dict\n\nO/P: {\"key1\":5, \"key3\": 3, \"key4\": 4, \"key5\": 5}\n"], ["my_list = [3, 0, 1, 0, 5, 2, 0, 4]\n\n# Sort the list, excluding zeros\nsorted_list = sorted(my_list, key=lambda x: (x == 0, x))\n\nprint(sorted_list)\n"], ["from docx.oxml import OxmlElement, ns\n\ndef create_element(name):\n    return OxmlElement(name)\n\ndef create_attribute(element, name, value):\n    element.set(ns.qn(name), value)\n\n\ndef add_page_number(run):\n    fldChar1 = create_element('w:fldChar')\n    create_attribute(fldChar1, 'w:fldCharType', 'begin')\n\n    instrText = create_element('w:instrText')\n    create_attribute(instrText, 'xml:space', 'preserve')\n    instrText.text = \"PAGE\"\n\n    fldChar2 = create_element('w:fldChar')\n    create_attribute(fldChar2, 'w:fldCharType', 'end')\n\n    run._r.append(fldChar1)\n    run._r.append(instrText)\n    run._r.append(fldChar2)\n\ndoc = Document()\nadd_page_number(doc.sections[0].footer.paragraphs[0].add_run())\ndoc.save(\"your_doc.docx\")\n"], ["import asyncio\n\nimport uvicorn\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello, World!\"}\n\n\nasync def main():\n    config = uvicorn.Config(app, port=5000, log_level=\"info\")\n    server = uvicorn.Server(config)\n    await server.serve()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n"], ["a = int(input('Enter First Number:'))\nb = int(input('Enter Second Number:'))\nc = int(input('Enter Third Number:'))\nmax = 0\nif a > max:\n   max = a\nif b > max:\n   max = b\nif c > max:\n   max = c\nprint (max)\n"], ["check_length = 100000\n\nlist_example = []\n\nfor i in range(check_length):\n    list_example.append({f\"companies_info_{i}\": i})\n", "import pandas as pd\nimport numpy as np\nimport time\nimport math\n\n\n# Method 1\n\ndef get_frame_method_1(l):\n\n    list_example_d = {\"time\": l}\n\n    df_1 = pd.DataFrame.from_dict(data=list_example_d, orient=\"columns\")\n\n    index_list = []\n\n    for count, d in enumerate(df_1.time):\n        index_list.extend(list(d.keys()))\n        df_1.time[count]= list(d.values())[0]\n\n    df_1.index= index_list\n\n    return df_1\n\n\n# Method 2\n\ndef get_frame_method_2(l):\n\n    df_list = []\n\n    for d in l:\n        d_df = pd.DataFrame.from_dict(data=d, orient=\"index\", columns=[\"time\"])\n        df_list.append(d_df)\n\n    df_2 = pd.concat(df_list, axis= 0)\n\n    return df_2\n\n\n# Method 3\n\ndef get_frame_method_3(l):\n\n    df_3 = (pd.concat(map(pd.Series, l))\n            .to_frame('time')\n        )\n    \n    return df_3\n\n\n# Method 4\n\ndef get_frame_method_4(l):\n\n    # build a nested dict from list_example and build df\n    df_4 = pd.DataFrame.from_dict({k: {'time': v} for d in l for k,v in d.items()}, orient='index')\n\n    return df_4\n\n\n# Method 5\n\ndef get_frame_method_5(l):\n\n    df_5 = pd.concat([ pd.Series(d.values(), index=d.keys())\n        for d in l ]).to_frame('time')\n    \n    return df_4\n\n\ncheck_length = 100000\n\nlist_example = []\n\nfor i in range(check_length):\n    list_example.append({f\"companies_info_{i}\": i})\n\n\ntotal_time_1_d = {}\n\nfor i in range(100):\n    t_0 = time.time()\n    df_1 = get_frame_method_1(list_example)\n    t_1 = time.time()\n    df_2 = get_frame_method_2(list_example)\n    t_2 = time.time()\n    df_3 = get_frame_method_3(list_example)\n    t_3 = time.time()\n    df_4 = get_frame_method_4(list_example)\n    t_4 = time.time()\n    df_5= get_frame_method_5(list_example)\n    t_5 = time.time()\n    total_time_1_d[f\"{i}\"] = {\"Method 1\": (t_1-t_0), \"Method 2\": (t_2-t_1), \"Method 3\": (t_3-t_2), \"Method 4\": (t_4-t_3), \"Method 5\": (t_5-t_4)}\n    print(i)\n\n\ntotal_time_df = pd.DataFrame.from_dict(data= total_time_1_d, orient=\"index\")\n\n\nfor i in range(5):\n    print(f\"Method {i+1}: Mean - {total_time_df.describe().iloc[1, i]}, 95% CI ({total_time_df.describe().iloc[1, i]-1.96*(total_time_df.describe().iloc[2, i])/math.sqrt((total_time_df.describe().iloc[0, i]))}, {total_time_df.describe().iloc[1, i]+1.96*(total_time_df.describe().iloc[2, i])/math.sqrt((total_time_df.describe().iloc[0, i]))})\")\n"], [], ["version: '3.8'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  \n  #image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.0}\n  build: .\n  \n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow <2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n    PYTHONPATH: '$PYTHONPATH;/python_extended'\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - ${AIRFLOW_PROJ_DIR:-.}/python:/python_extended\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n"], ["volumes:\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n", "CREATE EXTENSION vector;\n"], ["print(Foo(123) == Foo(123) # Prints 'False'\n"], ["pip install .[dev]\n", "[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = 'my_application_name'\nversion = '0.0.1'\nrequires-python = \">=3.11.0\"\n\ndependencies = [\n    'cryptography==41.0.1',\n]\n\n[project.optional-dependencies]\nlining = [\n  'pylint==2.17.4'\n]\nformatting = [\n  'black[d]==23.3.0'\n]\ndev = ['my_application_name[linting, formatting]']\n"], ["yt-dlp.exe --skip-download --print \"%(duration>%H:%M:%S.%s)s %(creator)s %(uploader)s - %(title)s\" https://youtu.be/bJ9r8LMU9bQ\n"], ["pip uninstall opencv-python-headless -y \n\npip install opencv-python --upgrade\n"], ["pip uninstall opencv-python \n", "pip install opencv-python\n"], ["from PyPDF2 import  PdfReader, PdfWriter\n\npdf_file = \"C:/Users/11359023/Desktop/original.pdf\"\nwatermark = \"C:/Users/11359023/Desktop/watermark.pdf\"\nmerged = \"C:/Users/11359023/Desktop/merged.pdf\"\n\nwith open(pdf_file, \"rb\") as input_file, open(watermark, \"rb\") as watermark_file:\ninput_pdf = PdfReader(input_file) #opens the original file\n\nwatermark_pdf = PdfReader(watermark) #opens the watermarked file\nwatermark_page = watermark_pdf.pages[0] #gets the first page of the watermark\n\noutput = PdfWriter() #this will hold the new pages\n\nfor i in range(len(input_pdf.pages)): #go through each page\n    pdf_page = input_pdf.pages[i]\n    pdf_page.merge_page(watermark_page) #combine the watermark and the current page\n    output.add_page(pdf_page)\n\nwith open(merged, \"wb\") as merged_file:\n    output.write(merged_file)\n"], ["import google.auth\nimport google.auth.transport.requests\ncreds, project = google.auth.default()\n\n# creds.valid is False, and creds.token is None\n# Need to refresh credentials to populate those\n\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Now you can use creds.token\n"], [], [], [], [], ["curl -sSL https://install.python-poetry.org | sed 's/symlinks=False/symlinks=True/' | python3 -\n"], [], ["from dataclasses import dataclass, field\nfrom typing import Callable, List, Union\nfrom dash.dependencies import handle_callback_args\nfrom dash.dependencies import Input, Output, State\n\n\n@dataclass\nclass Callback:\n    func: Callable\n    outputs: Union[Output, List[Output]]\n    inputs: Union[Input, List[Input]]\n    states: Union[State, List[State]] = field(default_factory=list)\n    kwargs: dict = field(default_factory=lambda: {\"prevent_initial_call\": False})\n\n\nclass CallbackManager:\n    def __init__(self):\n        self._callbacks = []\n\n    def callback(self, *args, **kwargs):\n        output, inputs, state, prevent_initial_call = handle_callback_args(\n            args, kwargs\n        )\n\n        def wrapper(func):\n            self._callbacks.append(\n                Callback(\n                    func,\n                    output,\n                    inputs,\n                    state,\n                    {\"prevent_initial_call\": prevent_initial_call}\n                )\n             )\n\n        return wrapper\n\n    def attach_to_app(self, app):\n        for callback in self._callbacks:\n            app.callback(\n                callback.outputs, callback.inputs, callback.states, **callback.kwargs\n            )(callback.func)\n", "import dash\n\nfrom callback_manager import CallbackManager\n\ncallback_manager = CallbackManager()\n\n\n@callback_manager.callback(\n    dash.dependencies.Output('label', 'children'),\n    [dash.dependencies.Input('call_btn', 'n_clicks')])\ndef update_label(n_clicks):\n    if n_clicks > 0:\n        return \"Callback called!\"\n", "import dash\nimport dash_html_components as html\n\nfrom callbacks import callback_manager\n\napp = dash.Dash(__name__)\ncallback_manager.attach_to_app(app)\n\napp.layout = html.Div([\n    html.Div(id=\"label\"),\n    html.Button('Call callback', id='call_btn', n_clicks=0),\n])\nif __name__ == '__main__':\n    app.run_server(debug=True)\n", "from callbacks1 import callback_manager as callback_manager1\nfrom callbacks2 import callback_manager as callback_manager2\n\napp = dash.Dash(__name__)\ncallback_manager1.attach_to_app(app)\ncallback_manager2.attach_to_app(app)\n"], ["sudo apt install python3.8-venv\n", "python3 -m venv env\n", "source env/bin/activate\n", "deactivate\n"], [], [], [], [], ["{   \n\"terminal.integrated.profiles.windows\": {\n    \"Anaconda CMD\": {\n        \"path\": [\n            \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n            \"${env:windir}\\\\System32\\\\cmd.exe\"\n        ],\n        \"args\": [\n            \"/K\",\n            \"C:\\\\Users\\\\${env:USERNAME}\\\\AppData\\\\Local\\\\Anaconda3\\\\Scripts\\\\activate.bat & conda activate base\"\n        ],\n        \"icon\": \"terminal-cmd\"\n    },\n    \"Anaconda PS\": {\n        \"source\": \"PowerShell\",\n        \"args\": [\n            \"powershell\",\n            \"-NoExit\",\n            \"-ExecutionPolicy ByPass\",\n            \"-NoProfile\",\n            \"-Command\",\n            \"'& 'C:\\\\Users\\\\${env:USERNAME}\\\\AppData\\\\Local\\\\Anaconda3\\\\shell\\\\condabin\\\\conda-hook.ps1'; conda activate base'\"\n        ],\n        \"icon\": \"terminal-powershell\"\n    }\n},\n\"terminal.integrated.defaultProfile.windows\": \"Anaconda CMD\"\n"], [], [], ["decomposition = sm.tsa.seasonal_decompose(df, model = 'additive', period=7)\n"], ["model.layers[0].weight # for accessing weights of first layer wrapped in nn.Sequential()\n"], [], ["pip install --upgrade webdriver_manager\n"], [], [], ["python3 -m venv env\nsource env/bin/activate\n", "python3 -m pip install new_module\n"], [], [], ["# This query syntax will result in an error\nreturn session.query(cls).filter(\n    cls.corelation_id == corelation_id,\n    cls.status == cls.ORDER_ACTIVE if not is_cancel else cls.ORDER_CANCEL\n).first()\n", "# Define the status condition based on the 'is_cancel' flag\nstatus = cls.ORDER_ACTIVE if not is_cancel else cls.ORDER_CANCEL\n\n# Use the defined status condition in the SQLAlchemy query\nreturn session.query(cls).filter(\n    cls.corelation_id == corelation_id,\n    cls.status == status\n).first()\n"], [], ["pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n", "pip install pydantic -U\n", "pip install pydantic==1.10.11\n"], [], ["DATABASES = {\"default\": env.db()}\nDATABASE_URL = env(\"DATABASE_URL\")\n", "DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n"], ["st = speedtest.Speedtest(secure = True);\n"], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\n# The Service class is used to start an instance of the Chrome WebDriver\n# The no-argument constructor means it will look for the WebDriver executable in the system's PATH\nservice = Service()\n\n# WebDriver.ChromeOptions() is used to set the preferences for the Chrome browser\noptions = webdriver.ChromeOptions()\n\n# Here, we start an instance of the Chrome WebDriver with the defined options and service\ndriver = webdriver.Chrome(service=service, options=options)\n\n# Your code for interacting with web pages goes here\n\n# In the end, always close or quit the driver to ensure all system resources are freed up\ndriver.quit()\n"], [], [], [], [], ["rm -rf <python_path>/site-packages/OpenSSL\n"], ["driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"], [], [], [], [], ["class TMDB_Category(BaseModel):\n    name: str = Field(validation_alias=\"strCategory\")\n    description: str = Field(validation_alias=\"strCategoryDescription\")\n"], [], [], [], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\noptions = Options()\noptions.add_argument(\"start-maximized\")\ndriver = webdriver.Chrome(options=options)\ndriver.get(\"https://www.google.com/\")\n"], ["cd /tmp\ngit clone --branch v0.4.4 https://github.com/pgvector/pgvector.git\ncd pgvector \nmake\nsudo make install \nCREATE EXTENSION vector;\n"], [], [], [], [], ["from collections.abc import MutableMapping\nfrom typing import Any, Iterator\n\nfrom pydantic import BaseModel\n\n\nclass BaseModelDict(BaseModel, MutableMapping):\n    \"\"\"Goodness of BaseModel and acts like a dictionary.\"\"\"\n\n    def __contains__(self, x: str) -> bool:\n        return True if x in self.__dict__.keys() else False\n\n    def __delitem__(self, x: str) -> None:\n        del self.__dict__[x]\n\n    def __getitem__(self, x: str) -> Any:\n        return self.__dict__[x]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.__dict__)\n\n    def __json__(self) -> dict:\n        return self.__dict__\n\n    def __len__(self) -> int:\n        return len(self.__dict__)\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self.__dict__[key] = value\n"], [], [], [">>> db.session.query(Order).count()\n2111\n", ">>> from sqlalchemy import func\n>>> db.session.scalar(db.select(func.count(Order.order_id)))\n2111\n", ">>> db.session.scalar(db.select(func.count(Order.currency)))\n2111\n", "SELECT count(\"order\".currency) AS count_1 \nFROM \"order\"\n", ">>> to_filter = db.select(Order).where(Order.currency=='GBP')\n>>> db.session.scalar(db.select(func.count()).select_from(to_filter))\n17\n"], [], ["pip uninstall virtualenv\n", "virtualenv my_name\n"], [], [], [], ["echo %PATH%\n"], [], ["ModuleNotFoundError: No module named 'virtualenv.seed.embed.via_app_data\n", "virtualenv             20.4.0\n"], ["In jupyter notebook\n\n!python -m spacy download en.  \nimport spacy. \nnlp = spacy.load('en_core_web_sm')\n"], [], ["from collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom collections.abc import Sequence\n"], [], [], [], [">>float\n\n>>numpy.float64\n\n>>numpy.double\n\n>>numpy.float_\n"], [], ["pygame.mouse.set_visible(False)\ncursor_img_rect = cursor_img.get_rect()\n\nwhile True:\n    # in your main loop update the position every frame and blit the image    \n    cursor_img_rect.center = pygame.mouse.get_pos()  # update position \n    gameDisplay.blit(cursor_img, cursor_img_rect) # draw the cursor\n"], ["pip install \n", "pip uninstall langchain\npip install langchain\n"], ["cur.execute('''\nCREATE TABLE langchain_pg_embedding (\n    uuid UUID NOT NULL,\n    collection_id UUID,\n    embedding VECTOR,\n    document VARCHAR,\n    cmetadata JSON,\n    custom_id VARCHAR,\n    PRIMARY KEY (uuid))\n''')\n", "ALTER DATABASE postgres SET SEARCH_PATH TO postgres_schema;\n"], [], [], ["wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py --user\npython3 -m pip install pyopenssl --upgrade --user\n"], [], ["Poetry (1.5.1) is installed now. Great!\n\nTo get started you need Poetry's bin directory (C:\\Users\\<user>\\AppData\\Roaming\\Python\\Scripts) in your `PATH`\nenvironment variable.\n\nAlternatively, you can call Poetry explicitly with `C:\\Users\\<user>\\AppData\\Roaming\\Python\\Scripts\\poetry`.\n\nYou can test that everything is set up by executing:\n"], ["!pip install pydantic -U\n"], ["\"program\": \"${file}\"\n", "\"module\": \"folder.pythonfilename\"\n"], ["%AppData%\\Programs\\Python\\Python311\n%AppData%\\Programs\\Python\\Python311\\Scripts\n"], [], [], ["pip uninstall pathlib\n"], ["pip3 install pyOpenSSL --upgrade\n"], ["import pandas as pd\nimport redis\nimport pickle\n\nr = redis.Redis(host='localhost', port=6379, db=0)\n\ndata = {\n    \"calories\": [\"v1\", 'v2', 'v3'],\n    \"duration\": [50, 40, 45]\n}\ndf = pd.DataFrame(data, index=[\"day1\", \"day2\", \"day3\"])\n\nr.set(\"key\", pickle.dumps(df))\nprint(pickle.loads(r.get(\"key\")))\n", "import pandas as pd\nfrom direct_redis import DirectRedis\n\nr = DirectRedis(host='localhost', port=6379)\n>>> df =  pd.DataFrame([[1,2,3,'235', '@$$#@'], \n                   ['a', 'b', 'c', 'd', 'e']])\n>>> print(df)\n   0  1  2    3      4\n0  1  2  3  235  @$$#@\n1  a  b  c    d      e   \n\n>>> r.set('df', df)   \n\n>>> r.get('df')\n   0  1  2    3      4\n0  1  2  3  235  @$$#@\n1  a  b  c    d      e   \n\n>>> type(r.get('df'))\n<class 'pandas.core.frame.DataFrame'>\n"], [], [], ["data = pd.get_dummies(data,prefix=['Profession'], columns = ['Profession'], drop_first=True)\n", "transformed = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n#Create a Pandas DataFrame of the hot encoded column\nohe_df = pd.DataFrame(transformed, columns=jobs_encoder.get_feature_names())\n#concat with original data\ndata = pd.concat([data, ohe_df], axis=1).drop(['Profession'], axis=1)\n"], [], [], [], ["From anaconda3\\Library\\bin copy below files and paste them in anaconda3/DLLs\n"], ["From anaconda3\\Library\\bin copy below files and paste them in anaconda3/DLLs\n\n-   libcrypto-1_1-x64.dll\n-   libssl-1_1-x64.dll\n"], [], [], [], [], ["curl -sSL https://install.python-poetry.org | python3.11 -\n"], [], [], [], [], ["---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-2-9b030cf1055e> in <module>()\n----> 1 decomposition = sm.tsa.seasonal_decompose(df, model = 'additive')\n      2 decompose_result.plot()\n\n/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/seasonal.py in seasonal_decompose(x, model, filt, freq, two_sided, extrapolate_trend)\n    125             freq = pfreq\n    126         else:\n--> 127             raise ValueError(\"You must specify a freq or x must be a \"\n    128                              \"pandas object with a timeseries index with \"\n    129                              \"a freq not set to None\")\n\nValueError: You must specify a freq or x must be a pandas object with a time-series index with a freq not set to None\n", "TypeError: seasonal_decompose() got an unexpected keyword argument 'period'\n"], [], [], [], [], ["import speedtest\nwifi = speedtest.Speedtest()\n", "from speedtest import Speedtest\n\ninternet = Speedtest()\n\ndownload_speed = internet.download()\nupload_speed = internet.upload() \n\nprint(\"Download \\t:\", download_speed)\nprint(\"Upload   \\t:\", upload_speed)\n"], [], [], ["{pip install --upgrade openpyxl}\n"], [], [], ["    pip uninstall opencv-python\n    pip install opencv-python==4.7.0.68\n"], ["pip install numpy --upgrade\n"], [], [], ["pyenv install 3.10.1\npyenv global 3.10.1\ncurl -sSL https://install.python-poetry.org | python3.10 -\n"], ["pip install pip --upgrade\npip install pyopenssl --upgrade\n"], [], [], ["curl -sSL https://install.python-poetry.org | python3 -\n", "export PATH=\"/home/tperes/.local/bin:$PATH\"\n"], ["async def _check_channel(self, message, pool):\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\n                \"SELECT ignore_channel_id FROM guild_channel_settings WHERE guild_id = %s\",\n                (message.author.guild.id,),\n            )\n            in_database = await cursor.fetchone()\n\n    if in_database and in_database[0] is not None:\n        channel_list = in_database[0].split(\" \")\n        for channelid in channel_list:\n\n            try:\n                channel_id_int = int(channelid)\n            except ValueError:\n                continue\n\n            if int(message.channel.id) == channel_id_int:\n                return False\n\n\nasync def _get_role_count(self, message, pool):\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\n                \"SELECT ignore_role_id, bonus_role_id FROM guild_role_settings WHERE guild_id = %s\",\n                (message.author.guild.id,),\n            )\n            in_database = await cursor.fetchone()\n    if in_database:\n        first_item, second_item, *_ = in_database\n        if first_item is not None:\n            role_list = first_item.split(\" \")\n            for roleid in role_list:\n                try:\n                    roleid_int = int(roleid)\n                except ValueError:\n                    continue\n\n                role = message.author.guild.get_role(roleid_int)\n                if role is None:\n                    continue\n                if role in message.author.roles:\n                    return False\n\n        if second_item is not None:\n            role_list = second_item.split(\" \")\n            count = 0\n            for roleid in role_list:\n                try:\n                    roleid_int = int(roleid)\n                except ValueError:\n                    continue\n\n                role = message.author.guild.get_role(roleid_int)\n                if role is None:\n                    continue\n                if role in message.author.roles:\n                    count += 1\n            return count\n\n\n@commands.Cog.listener(\"on_message\")\nasync def on_message(self, message):\n    if message.author.bot:\n        return\n    if message.type != discord.MessageType.default:\n        return\n    if isinstance(message.channel, discord.channel.DMChannel):\n        return\n\n    # Cooldown\n\n    self.member_cooldown_list = [\n        i\n        for i in self.member_cooldown_list\n        if i[1] + self.cooldown_val > int(time.time())\n    ]\n    member_index = next(\n        (\n            i\n            for i, v in enumerate(self.member_cooldown_list)\n            if v[0] == message.author.id\n        ),\n        None,\n    )\n    if member_index is not None:\n        if self.member_cooldown_list[member_index][1] + self.cooldown_val > int(\n            time.time()\n        ):\n            return\n\n    self.member_cooldown_list.append((message.author.id, int(time.time())))\n\n    loop = asyncio.get_running_loop()\n    db_pool = await aiomysql.create_pool(\n        minsize=3,\n        host=\"<host>\",\n        port=3306,\n        user=\"<user>\",\n        password=\"<password>\",\n        db=\"<db_name>\",\n        autocommit=False,\n        loop=loop,\n    )\n    count = 1\n\n    check_channel_task = asyncio.create_task(\n        self._check_channel(self, message, db_pool)\n    )\n    role_count_task = asyncio.create_task(self._get_role_count(self, message, db_pool))\n\n    # write to database\n\n    mydb = await db_pool.acquire()\n    mycursor = await mydb.cursor()\n    await mycursor.execute(\n        \"SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s\",\n        (message.author.guild.id, message.author.id),\n    )\n    in_database = await mycursor.fetchone()\n\n    role_count = await role_count_task\n    check_channel = await check_channel_task\n    if False in (role_count, check_channel):\n        await mycursor.close()\n        db_pool.release(mydb)\n        db_pool.close()\n        await db_pool.wait_closed()\n        return\n    if role_count:\n        count += role_count\n    if in_database:\n        await mycursor.execute(\n            \"INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE message_count = message_count + 1\",\n            (message.author.id, count, message.author.guild.id),\n        )\n\n    await mydb.commit()\n    await mycursor.close()\n    db_pool.release(mydb)\n    db_pool.close()\n    await db_pool.wait_closed()\n"], [], [], [], ["       number_1= int(input(\"Enter first number\"))\n       number_2= int(input(\"Enter second number\"))\n       number_3= int(input(\"Enter third number\"))\n       if number_1 > number_2 and number_1 > number_3:\n         print(\"the largest number\", number_1)\n       elif number_2 > number_1 and number_2 > number_3:\n         print(\"The largest number is\", number_1)\n       elif number_3 > number_1 and number_3 > number_2:\n         print(\"The largest number is\",number_1)\n\n\n       Enter first number 70\n       Enter second number60\n       Enter third number50\n\n       the largest number 70\n"], ["conda install gdal\n"], [], [], [], [], ["conda install -n yourEnv yourPackage    \n"], ["pip install seaborn --user\n"], ["{\n    \"python.testing.pytestArgs\": [\n        \"tests\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true\n}\n", "from pathlib import Path\nimport os\nimport sys\n\nmain_folder = Path(__file__).parent.parent\nsys.path.insert(0, str(main_folder))\nsys.path.insert(0, str(main_folder / 'app'))\nsys.path.insert(0, str(main_folder / 'tests'))\nos.chdir(main_folder / 'app')\n\nimport app\n", "@report_execution_time\ndef main_test():\n\n    os.system('python -m pytest ..')\n\n\nif __name__ == \"__main__\":\n\n    main_test()\n"], ["wget https://files.pythonhosted.org/packages/00/3f/ea5cfb789dddb327e6d2cf9377c36d9d8607af85530af0e7001165587ae7/pyOpenSSL-22.1.0-py3-none-any.whl\n", "python3 -m easy_install pyOpenSSL-22.1.0-py3-none-any.whl\n"], [], [], [], ["from pydantic import BaseModel\nfrom typing import Optional\n\nclass Blog(BaseModel):\n    title: str\n    body: str\n\nclass UpdateBlog(BaseModel):\n    title: Optional[str]\n    body: Optional[str]\n", "# Update Blog\n@app.put('/blog/{id}', status_code=status.HTTP_202_ACCEPTED)\ndef update(id, request_body: UpdateBlog, db: Session = Depends(get_db)):\n    blog = db.query(models.Blog).filter(models.Blog.id == id).first()\n\n    if not blog:\n        content={'success': False, 'message': f\"Blog with id {id} don't exists\"}\n        return JSONResponse(status_code=status.HTTP_404_NOT_FOUND, content=content)\n    \n    # exclude_none=True will only update the field which you want to update\n    # If you don't want to update \"body\" then only pass the \"title\" and \"body\" field will stay as it is\n    db.query(models.Blog).filter(models.Blog.id == id).update(request_body.dict(exclude_none=True))\n    db.commit()\n\n    content={'success': True, 'message': f\"Blog with id {id} Updated\"}\n    return JSONResponse(status_code=status.HTTP_200_OK, content=content)\n"], [], [], [], [], ["conda env create -f=\"Python 310.yml\"\n"], [], ["for key in history.history:\n    print(key)\nloss\naccuracy\nauc_4\nprecision_4\nrecall_4\ntrue_positives_4\ntrue_negatives_4\nfalse_positives_4\nfalse_negatives_4\nval_loss\nval_accuracy\nval_auc_4\n", "for something in something_else:\n    tf.keras.backend.clear_session()  # resets the session\n    model = define_model(...)\n    history = train_model(...)\n"], [], [], [], [], ["CMD [\"gunicorn\",\"--workers\", \"3\", \"--timeout\", \"1000\", \"--bind\", \"0.0.0.0:8000\", \"wsgi:app\"]\n"], ["import pandas as pd\nurl=\"https://drive.google.com/file/d/1a7qwzU2mbaJPkFQZMJCkdE37Ne2DbgHA/view?usp=share_link\"\nreconstructed_url='https://drive.google.com/uc?id=' + url.split('/')[-2]\ndf = pd.read_csv(reconstructed_url)\ndf\n"], [], ["sudo apt remove python3-openssl\n"], ["from dash import Dash\n\nfrom layouts import my_layout\nfrom callbacks import my_callback, my_callback_inputs, my_callback_outputs\n\nif __name__ == \"__main__\":\n    app = Dash(__name__)\n\n    app.layout = my_layout\n    app.callback(my_callback_outputs, my_callback_inputs)(my_callback)\n\n    app.run_server()\n", "my_callback_inputs = []\nmy_callback_outputs = []\n\n\ndef my_callback():\n    return\n"], [], ["import yfinance as yf\n\ndf = yf.download(your_ticks_or_a_tick_list, start=start_date, end=end_date)\n"], ["from PIL import Image\nimport pillow_heif\n\n\npillow_heif.register_heif_opener()\n\nimg = Image.open('c:\\image.HEIC')\nimg.save('c:\\image_name.png', format('png'))\n"], ["sudo rm -rf /usr/lib/python3/dist-packages/OpenSSL\nsudo pip3 install pyopenssl\nsudo pip3 install pyopenssl --upgrade\n"], ["pip uninstall opencv-contrib-python\npip uninstall opencv-contrib-python-headless\n", "pip uninstall opencv-python\npip install opencv-python\n"], [], [], ["from sklearn.preprocessing import OneHotEncoder\noh= OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\none_hot_encoded=oh.fit_transform(df[[\"Profession\"]])\ndf = pd.concat([df,one_hot_encoded],axis=1).drop(columns=[\"Profession\"])\n"], ["source ~/.zshrc\n", "poetry --version\n"], [], [], [], [], ["X509_V_FLAG_CB_ISSUER_CHECK\n", "AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'\n", "sudo rm -rf /usr/lib/python3/dist-packages/OpenSSL\n", "sudo pip install pyopenssl\nRequirement already satisfied: pyopenssl in /usr/lib/python3/dist-packages (19.0.0)\n", "pip install pip --upgrade\nSuccessfully installed pip-22.3.1\n\npip install weasyprint\nSuccessfully installed Pyphen-0.13.2 ... weasyprint-57.2 zopfli-0.2.2\n"], [], ["import io\nfrom PIL import Image\nimport pillow_heif\nfrom werkzeug.datastructures import FileStorage\n\nclass Converter:\n\n    def convert_heic_to_jpeg(self, file):\n        # Check if file is a .heic or .heif file\n        if file.filename.endswith(('.heic', '.heif', '.HEIC', '.HEIF')):\n            # Open image using PIL\n            # image = Image.open(file)\n\n            heif_file = pillow_heif.read_heif(file)\n            image = Image.frombytes(\n                heif_file.mode,\n                heif_file.size,\n                heif_file.data,\n                \"raw\",\n            )\n\n            # Convert to JPEG\n            jpeg_image = image.convert('RGB')\n\n            # Save JPEG image to memory temp_img\n            temp_img = io.BytesIO()\n            jpeg_image.save(temp_img, format(\"jpeg\"))\n\n            # Reset file pointer to beginning of temp_img\n            temp_img.seek(0)\n\n            # Create a FileStorage object\n            file_storage = FileStorage(temp_img, filename=f\"{file.filename.split('.')[0]}.jpg\")\n\n            # Set the mimetype to \"image/jpeg\"\n            file_storage.headers['Content-Type'] = 'image/jpeg'\n\n            return file_storage\n        else:\n            raise ValueError(\"File must be of type .heic or .heif\")\n"], [], [], ["import datetime as dt\nimport yfinance as yf\n\ncompany = 'TATAELXSI.NS'\n\n# Define a start date and End Date\nstart = dt.datetime(2020,1,1)\nend =  dt.datetime(2022,1,1)\n\n# Read Stock Price Data \ndata = yf.download(company, start , end)\n\ndata.tail(10)\n"], ["from pathlib import Path\nimport pandas as pd\n\npaths = Path(\"/home/data\").glob(\"*.json\")\ndf = pd.DataFrame([pd.read_json(p, typ=\"series\") for p in paths])\n"], ["conda uninstall pywin32\npip uninstall pywin32\n", "conda uninstall pywin32\nor \npip uninstall pywin32\n"], ["=== LOAD (deserialize)\ndataclass-wizard:  1.742989194\npydantic:          5.31538175\n=== DUMP (serialize)\ndataclass-wizard:  2.300118940\npydantic:          5.582638598\n"], [], ["conda install pycryptodome pycryptodomex\nconda uninstall pandas-datareader\npip install git+https://github.com/raphi6/pandas-datareader.git@ea66d6b981554f9d0262038aef2106dda7138316\n", "! pip install pycryptodome pycryptodomex\n! pip uninstall --yes pandas-datareader\n! pip install git+https://github.com/raphi6/pandas-datareader.git@ea66d6b981554f9d0262038aef2106dda7138316\n", "git clone https://github.com/raphi6/pandas-datareader.git\ncd pandas-datareader\nconda uninstall pandas-datareader\nconda install pycryptodome pycryptodomex\ngit checkout 'Yahoo!_Issue#952'\npython setup.py install --record installed_files.txt\n"], ["pip install --upgrade pywin32==305\n"], [], [">>> import pandas_datareader as dtr\n>>> from datetime import datetime\n>>> initial_portfolio=['AAPL', 'MA', 'F', 'MSFT', '^GSPC']\n>>> startdate = datetime(2022,12,1)\n>>> enddate=datetime(2022,12,10)\n>>> stock_data=dtr.yahoo.daily.YahooDailyReader(initial_portfolio,start=startdate,end=enddate).read()\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"lib/python3.9/site-packages/pandas_datareader/base.py\", line 258, in read\n    df = self._dl_mult_symbols(self.symbols)\n  File \"lib/python3.9/site-packages/pandas_datareader/base.py\", line 268, in _dl_mult_symbols\n    stocks[sym] = self._read_one_data(self.url, self._get_params(sym))\n  File \"lib/python3.9/site-packages/pandas_datareader/yahoo/daily.py\", line 153, in _read_one_data\n    data = j[\"context\"][\"dispatcher\"][\"stores\"][\"HistoricalPriceStore\"]\nTypeError: string indices must be integers\n", "Python 3.9.1 (default, Dec 28 2020, 11:22:14)\n[Clang 11.0.0 (clang-1100.0.33.17)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from pandas_datareader import data as pdr\n>>> import yfinance as yf\n>>> yf.pdr_override()\n>>> y_symbols = ['SCHAND.NS', 'TATAPOWER.NS', 'ITC.NS']\n>>> from datetime import datetime\n>>> startdate = datetime(2022,12,1)\n>>> enddate = datetime(2022,12,15)\n>>> data = pdr.get_data_yahoo(y_symbols, start=startdate, end=enddate)\n[*********************100%***********************]  3 of 3 completed\n>>> data\n             Adj Close                                Close                           ...        Open                             Volume\n                ITC.NS   SCHAND.NS TATAPOWER.NS      ITC.NS   SCHAND.NS TATAPOWER.NS  ...      ITC.NS   SCHAND.NS TATAPOWER.NS    ITC.NS SCHAND.NS TATAPOWER.NS\nDate                                                                                  ...\n2022-12-01  339.549988  195.949997   224.850006  339.549988  195.949997   224.850006  ...  341.700012  191.600006   225.250000  16630417    544485      7833074\n2022-12-02  337.149994  196.600006   225.250000  337.149994  196.600006   225.250000  ...  339.350006  196.000000   225.449997   8388835    122126      7223274\n2022-12-05  336.750000  191.050003   224.199997  336.750000  191.050003   224.199997  ...  337.649994  200.850006   225.250000   9716390    107294     10750610\n2022-12-06  337.299988  196.399994   228.800003  337.299988  196.399994   228.800003  ...  334.100006  191.000000   224.199997   6327430    102911     20071039\n2022-12-07  340.100006  187.350006   225.850006  340.100006  187.350006   225.850006  ...  338.500000  198.000000   228.800003   9813208    122772      7548312\n2022-12-08  338.399994  181.850006   225.050003  338.399994  181.850006   225.050003  ...  340.200012  186.000000   226.000000   6200447    114147      7507975\n2022-12-09  341.399994  176.899994   219.399994  341.399994  176.899994   219.399994  ...  339.750000  183.899994   225.899994   8132228    179660     13087278\n2022-12-12  343.200012  177.350006   217.699997  343.200012  177.350006   217.699997  ...  341.000000  177.750000   219.750000  11214662    133507      8858525\n2022-12-13  345.600006  178.449997   218.850006  345.600006  178.449997   218.850006  ...  344.500000  179.350006   218.800003  10693426     74873      7265105\n2022-12-14  345.399994  179.149994   222.699997  345.399994  179.149994   222.699997  ...  346.000000  180.449997   219.800003   7379878     32085      9179593\n\n[10 rows x 18 columns]\n>>>\n"], ["pipenv update pyOpenSSL\n"], ["    \"settings\": {\n            \"python.defaultInterpreterPath\": \"C:/Users/yyguy/.plotting_test/Scripts/python.exe\",\n            \"code-runner.executorMap\": {\"python\": \"call C:/Users/yyguy/.plotting_test/Scripts/activate.bat && python -u\"}\n    }\n}\n"], [], [], ["pip uninstall spacy\n", "pip install -U pip setuptools wheel\npip install -U spacy\npython -m spacy download en_core_web_sm\n"], [], [], [], ["class MtnPayer(BaseModel):\n  partyIdType: str\n  partyId: str\n\nclass MtnPayment(BaseModel):\n  financialTransactionId: str\n  externalId: str\n  amount: str\n  currency: str\n  payer: MtnPayer\n  payerMessage: str\n  payeeNote: str\n  status: str\n  reason: str\n"], [], [], [], [], [], [], ["sudo apt remove python3-pip \nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\n", "pip install pyopenssl --upgrade\n"], [], [], ["  \"python.terminal.activateEnvironment\": true,\n\n  \"python.venvPath\": \"Add_Venv_DirectoryPath_here\",\n"], ["import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\nclass CategoricalOneHot(BaseEstimator, TransformerMixin):\n    def __init__(self, list_key_words=None):\n        self.oh_dict = {}\n        self.list_key_words = list_key_words\n\n    def fit(self, X, y=None):\n        self.list_cat_col = []\n        for key_word in self.list_key_words:\n            self.list_cat_col += [col for col in X.columns if key_word in col]\n        for col in self.list_cat_col:\n            oh = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n            oh.fit(X[[col]])\n            names = oh.get_feature_names_out()\n            self.oh_dict[col] = (oh, names)\n        return self\n\n    def transform(self, X):\n        _X = X.copy()\n        for col in self.list_cat_col:\n            oh = self.oh_dict[col][0]\n            df_oh = pd.DataFrame(\n                data=oh.transform(_X[[col]]),\n                columns=self.oh_dict[col][1],\n                index=_X.index)\n            _X = pd.concat([_X, df_oh], axis=1)\n            _X.drop(col, axis=1, inplace=True)\n        return _X\n\nif __name__ == \"__main__\":\n    tex = pd.DataFrame({'city': ['a', 'a', 'e', 'b'], 'state': ['f', 'c', 'd', 'd']})\n    coh = CategoricalOneHot(list_key_words=['city', 'state'])\n    print(coh.fit_transform(tex))\n", "  city state\n0    a     f\n1    a     c\n2    e     d\n3    b     d\n", "   city_a  city_b  city_e  state_c  state_d  state_f\n0     1.0     0.0     0.0      0.0      0.0      1.0\n1     1.0     0.0     0.0      1.0      0.0      0.0\n2     0.0     0.0     1.0      0.0      1.0      0.0\n3     0.0     1.0     0.0      0.0      1.0      0.0\n"], [], [], [], [], [], ["$appsFld=\"$env:USERPROFILE\\AppData\\Local\\Microsoft\\WindowsApps\"; \n$pyPath=(Resolve-Path \"$env:USERPROFILE\\AppData\\Local\\Programs\\Python\\Python*\\\")\n$Env:Path = (($Env:Path.Split(';') | Where-Object { $_ -ne \"$appsFld\" }) -join ';'); \n$Env:Path = (($Env:Path.Split(';') | Where-Object { $_ -ne \"$pyPath\" }) -join ';'); \n$Env:Path += \";$pyPath\";\n$Env:Path +=\";$appsFld\";\n[Environment]::SetEnvironmentVariable(\"PATH\", \"$Env:Path\", \"Machine\")\n", "> python\nPython 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> quit\n\n"], ["from dataclasses import dataclass\nfrom functools import cached_property\n\n@dataclass(frozen=True)\nclass Register:\n    subsection: str\n    name: str\n    abbreviation: str\n    address: int\n    n_bits: int\n    _get_method: Callable[[int], int]\n    _set_method: Callable[[int, int], None]\n    _save_method: Callable[[int, int], None]\n\n    @cached_property\n    def bit_mask(self) -> int:\n        # The cache is used to avoid recalculating since this is a static value\n        # (hence max_size = 1)\n        return create_bitmask(\n            n_bits=self.n_bits,\n            start_bit=0,\n            size=self.n_bits,\n            set_val=True\n            )\n\n    def get(self) -> int:\n        raw_value = self._get_method(self.address)\n        return raw_value & self.bit_mask\n\n    def set(self, value: int) -> None:\n        self._set_method(\n            self.address,\n            value & self.bit_mask\n            )\n\n    def save(self, value: int) -> None:\n        self._save_method(\n            self.address,\n            value & self.bit_mask\n            )\n"], ["from typing import Any\n\nfrom pydantic import BaseModel, Field\nfrom pymapme.models.mapping import MappingModel\n\n\nclass Person(BaseModel):\n    name: str\n    surname: str\n\n\nclass Profile(BaseModel):\n    nickname: str\n    person: Person\n\n\nclass User(MappingModel):\n    nickname: str = Field(source='nickname')\n    first_name: str = Field(source='person.name')\n    surname: str = Field(source='person.surname')\n    full_name: str = Field(source_func='_get_full_name')\n\n    @staticmethod\n    def _get_full_name(model: Profile, default: Any):\n        return model.person.name + ' ' + model.person.surname\n\n\nprofile = Profile(nickname='baobab', person=Person(name='John', surname='Smith'))\nuser = User.build_from_model(profile)\nprint(user.dict())  # {'nickname': 'baobab', 'first_name': 'John', 'surname': 'Smith', 'full_name': 'John Smith'}\n", "d = {\n    \"p_id\": 1,\n    \"billing\": {\n        \"first_name\": \"test\"\n    }\n}\n\n\nclass Billing(BaseModel):\n    first_name: str\n\n\nclass Data(BaseModel):\n    p_id: int\n    billing: Billing\n\n\nclass Order(MappingModel):\n    p_id: int\n    pre_name: str = Field(source='billing.first_name')\n\n\norder = Order.build_from_model(Data(**d))\nprint(order.dict())\n"], [">>> TMDB_Category(strCategory=\"name\", strCategoryDescription=\"description\").json()\n'{\"name\": \"name\", \"description\": \"description\"}'\n", ">>> TMDB_Category(name=\"name\", description=\"description\")\nTMDB_Category(strCategory='name', strCategoryDescription='description')\n"], [], [], [], ["pip install Werkzeug~=2.0.0\n", "pip install jinja2~=3.0.3\n"], [], ["which poetry\n\n# $HOME/.local/bin/poetry  # if installed with Brew\n# maybe elsewhere: \"$HOME/.poetry/bin:$PATH\"\n", "export SHELL_RCFILE=\"~/.zshrc\"\necho \"export POETRY_PATH=$HOME/.local/bin/ && export PATH=\"$POETRY_PATH:$PATH\" >> $SHELL_RCFILE\n"], ["for layer in model.children():\n    if isinstance(layer, nn.Linear):\n        print(layer.state_dict())\n", "OrderedDict([\n('weight', tensor([[-0.0039, -0.0045...]])),\n('bias', tensor([[-0.0019, -0.0025...]]))\n])\n", "for layer in model.children():\n    if isinstance(layer, nn.Linear):\n        print('weight:', layer.weight\n        print('bias:', layer.bias\n"], [], ["pyenv global 3.x.x\n"], ["CB_ISSUER_CHECK = _lib.X509_V_FLAG_CB_ISSUER_CHECK\\r\n", "apt-get --reinstall install python-apt\napt-get --reinstall install apt-transport-https\napt-get install build-essential libssl-dev libffi-dev python-dev\n"], [], [], [], [], ["virtualenv --python=\"/YOUR PATH/python3.9\" \"name of your env\"\n"], [], ["from docx import Document\n\nfolder_data = 'C:\\\\Users\\\\...\\\\Data\\\\'\nfolder_output = 'C:\\\\Users\\\\...\\\\Output\\\\'\n\nclient_ = 'Client 1'; price_ = 99.99\n\ndocument_ = Document(f'{folder_data}invoiceTemplate.docx')\ndocument_.paragraphs[3].add_run(f'{price_} EUR')\n\n# ... more code ...\n\ndocument_.save(f'{folder_output}{client_} invoice.docx')\n"], [], [], [], ["pip3 uninstall urllib3\npip3 install urllib3\n"], ["{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${fileDirname}\",\n            \"env\": {\"PYTHONPATH\": \"${workspaceFolder}/mySubdir:${env:PYTHONPATH}\"},\n        }\n    ]\n}\n"], ["brew install poetry\n"], [], ["pip install --upgrade flask_login\n"], ["from keras.preprocessing.image import img_to_array\n", "from keras_preprocessing.image import img_to_array\n"], [], [], ["beyond top level package error\n", "ModuleNotFoundError: No module named 'convenience'\n", "ModuleNotFoundError: No module named 'convenience'\n", "\"terminal.integrated.env.windows\"\n", "\"terminal.integrated.env.linux\"\n"], ["# build a nested dict from list_example and build df\ndf = pd.DataFrame.from_dict({k: {'time': v} for d in list_example for k,v in d.items()}, orient='index')\nprint(df)\n                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], [" python -m pip install --upgrade pip setuptools virtualenv\n"], [], ["  __init__.py file was missing.\n"], ["# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnumeric_X_train = X_train.drop(low_cardinality_cols, axis=1)\nnumeric_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nnew_X_train = pd.concat([numeric_X_train, OH_cols_train], axis=1)\nnew_X_valid = pd.concat([numeric_X_valid, OH_cols_valid], axis=1)\nprint(new_X_train)\n"], ["import pandas as pd\nimport numpy as np\nfrom numba import jit\n\ndf = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], 'b': [1, 3, 5, 7, 9, 11]})\n\n@jit\ndef f(w):\n    # we have access to both columns of the dataframe here\n    return np.max(w), np.min(w)\n\ndf.rolling(3, method='table').apply(f, raw=True, engine='numba')\n"], ["conda env create -n spa --file .\\environment.yml\n", "conda env create -n spa --file environment.yml\n"], [], [], ["C:\\Users\\Default\\AppData\\Local\\Programs\\Python\\Python37\nC:\\Users\\Default\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\n"], [], [">> code ~/.zshrc\n"], [], [], [], ["import yt_dlp # pip install yt_dlp\n\ndef hook(d):\n    if d['status'] == 'finished':\n        filename = d['filename']\n        print(filename) # Here you will see the PATH where was saved.\n\ndef client(video_url, download=False):\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n         return ydl.extract_info(video_url, download=download)\n\nydl_opts = { \n        'format': 'bestaudio/best',\n        'outtmpl': '%(title)s.%(ext)s', # You can change the PATH as you want\n        'download_archive': 'downloaded.txt',\n        'noplaylist': True,   \n        'quiet': True,\n        'no_warnings': True,\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'mp3',\n            'preferredquality': '192',\n        }],\n        'progress_hooks': [hook]\n}\n", "for song in lista:\n    song_details = client(f'ytsearch:{song}', download=False) # get the json details about the song  \n    download     = client(f'ytsearch:{song}', download=True)  # download the song\n\n    song_details['title']\n    song_details['format_note']\n    song_details['audio_ext']\n    song_details['filesize']\n"], [], [], [], [], [], [], [], ["python3 -m virtualenv my_env\n", "sudo apt install virtualenv\n", "virtualenv my_env\n"], [], [], ["import plotly.express as px\ndf = px.data.gapminder().query(\"country=='Canada'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada',color_discrete_sequence=[\"#ff97ff\"])\nfig.show()\n"], [], [], ["ggg = pd.DataFrame({\"a\":[1,2,3,4,5,6,7], \"b\":[7,6,5,4,3,2,1]})\n\ndef my_rolling_apply2(df, fun, window):\n    prepend = [None] * (window - 1)\n    end = len(df) - window\n    mid = map(lambda start: fun(df[start:start + window]), np.arange(0,end))\n    last =  fun(df[end:])\n    return [*prepend, *mid, last]\n\nmy_rolling_apply2(ggg, lambda df: (df[\"a\"].max(), df[\"b\"].min()), 3)\n", "[None, None, (3, 5), (4, 4), (5, 3), (6, 2), (7, 1)]\n"], [], [], [], ["FROM apache/airflow:2.3.2\nCOPY requirements.txt /requirements.txt\nRUN pip install --user --upgrade pip\nRUN pip install --no-cache-dir --user -r /requirements.txt\n", "docker build . --tag pyrequire_airflow:2.3.2\n", "image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.2}\n", "image: ${AIRFLOW_IMAGE_NAME:-pyrequire_airflow:2.3.2}\n"], [], [], ["Remove-Item $env:USERPROFILE\\AppData\\Local\\Microsoft\\WindowsApps\\python*.exe\n"], [], [], [], ["del python.exe\ndel python3.exe\n"], [], [], [], ["def _defrost(cls):\n    cls.stash_setattr = cls.__setattr__\n    cls.stash_delattr = cls.__delattr__\n    cls.__setattr__ = object.__setattr__\n    cls.__delattr__ = object.__delattr__\n\ndef _refreeze(cls):\n    cls.__setattr__ = cls.stash_setattr\n    cls.__delattr__ = cls.stash_delattr\n    del cls.stash_setattr\n    del cls.stash_delattr\n\ndef temp_unfreeze_for_postinit(func):\n    assert func.__name__ == '__post_init__'\n    def wrapper(self, *args, **kwargs):\n        _defrost(self.__class__)\n        func(self, *args, **kwargs)\n        _refreeze(self.__class__)\n    return wrapper\n", "@dataclasses.dataclass(frozen=True)\nclass SimpleClass:\n    a: int\n\n    @temp_unfreeze_for_postinit\n    def __post_init__(self, adder):\n        self.b = self.a + adder\n"], [], ["# using sqlalchemy\n\n  from sqlalchemy import create_engine\n\n    def connect_to_db():\n        \"\"\"Create database connection.\"\"\"\n        url = f\"mysql+pymysql://{user}:{db_pwd}@{host}:{port}/{db_name}\"\n        try:\n            engine = create_engine(url)\n            return engine.connect()\n        except Exception as e:\n            raise e\n    \n    \n    def execute_query(query):\n        connection = connect_to_db()\n        with connection as con:\n            result = con.execute(query)\n            return result\n\n    results = execute_query(\"show tables;\")\n    print(\"count = \",results.rowcount)\n\n    output: count =  1298\n"], [], ["(k := next(iter(d)), d.pop(k))\n", "d.popitem()\n"], [], ["checkpoint = ModelCheckpoint(\"mnist-cnn-keras.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n", "checkpoint = ModelCheckpoint(\"./\", monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')\n"], [], ["sudo pip3 uninstall virtualenv\n\n"], [], [], [], ["    #Added code\n    \n        setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n        \n   \n#Initial code that failed with the error\n        \n        (train_examples, validation_examples), info = tfds.load(\n            'cats_vs_dogs',\n            split=['train[:80%]', 'train[80%:]'],\n            with_info=True,\n            as_supervised=True,\n        )\n\n#Complete code together\n\nsetattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n(train_examples, validation_examples), info = tfds.load(\n    'cats_vs_dogs',\n    split=['train[:80%]', 'train[80%:]'],\n    with_info=True,\n    as_supervised=True,\n)\n"], [], [], ["setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n"], ["pip3 list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip3 install -U \n"], [], ["pd.DataFrame(list_example).stack().droplevel(0).to_frame('time')\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["result = pd.concat([ pd.Series(d.values(), index=d.keys())\n    for d in list_example ]).to_frame('time')\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["df = (pd.concat(map(pd.Series, list_example))\n        .to_frame('time')\n      )\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["from PIL import Image\n\nimport pillow_heif\n\nheif_file = pillow_heif.read(r\"E:\\image\\20210914_150826.heic\")\n\nimage = Image.frombytes(\n    heif_file.mode,\n    heif_file.size,\n    heif_file.data,\n    \"raw\",\n\n)\n\nimage.save(r\"E:\\image\\test.png\", format(\"png\"))\n"], ["pip install snowflake-connector-python\n"], ["import pandas as pd\nimport redis\nimport zlib\nimport pickle\n\ndf=pd.DataFrame({'A':[1,2,3]})\nr = redis.Redis(host='localhost', port=6379, db=0)\nr.set(\"key\", zlib.compress( pickle.dumps(df)))\ndf=pickle.loads(zlib.decompress(r.get(\"key\")))\n"], ["pip uninstall opencv-python\npip uninstall opencv-contrib-python\n\npip install opencv-contrib-python\npip install opencv-python\n"], [], [], ["pygame.mouse.set_cursor(pygame.cursors.Cursor(pygame.SYSTEM_CURSOR_HAND))\n"], ["ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security\n"], [], ["model_history = model.fit(x=aug.flow(X_train, y_train, batch_size=16), epochs=EPOCHS,validation_data=[X_val, y_val], callbacks=[callbacks_list])\n"], ["[tool.pytest.ini_options]\npythonpath = [\n  \".\"\n]\n"], [], [], [], ["pygame.mouse.set_cursor()\n\nPygame Cursor Constant           Description\n--------------------------------------------\npygame.SYSTEM_CURSOR_ARROW       arrow\npygame.SYSTEM_CURSOR_IBEAM       i-beam\npygame.SYSTEM_CURSOR_WAIT        wait\npygame.SYSTEM_CURSOR_CROSSHAIR   crosshair\npygame.SYSTEM_CURSOR_WAITARROW   small wait cursor\n                                 (or wait if not available)\npygame.SYSTEM_CURSOR_SIZENWSE    double arrow pointing\n                                 northwest and southeast\npygame.SYSTEM_CURSOR_SIZENESW    double arrow pointing\n                                 northeast and southwest\npygame.SYSTEM_CURSOR_SIZEWE      double arrow pointing\n                                 west and east\npygame.SYSTEM_CURSOR_SIZENS      double arrow pointing\n                                 north and south\npygame.SYSTEM_CURSOR_SIZEALL     four pointed arrow pointing\n                                 north, south, east, and west\npygame.SYSTEM_CURSOR_NO          slashed circle or crossbones\npygame.SYSTEM_CURSOR_HAND        hand\n\nfor example:\npygame.mouse.set_cursor(pygame.SYSTEM_CURSOR_HAND)\n"], ["conda env create -n myenv-dev --file my_env.yml\n"], [], ["from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_labels), y= train_labels)\n"], [], [], [], ["hist = model.fit(...)\nfor key in hist.history:\nprint(key)\n"], [], ["sudo apt-get install libpq-dev #required for psycop2-binary installation\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n\nsudo apt-get -y install postgresql-13 #or other version number\n\nsudo apt install postgis postgresql-13-postgis-3\n\nsudo -i -u postgres\ncreateuser yourusername\ncreatedb postgis_db -O yourusername #create your db\npsql -d postgis_db\nCREATE EXTENSION postgis;\n\n#make sure these are all installed:\n\nsudo apt-get install binutils libproj-dev \nsudo apt-get install gdal-bin\nsudo apt-get install libgeos++\nsudo apt-get install proj-bin\n"], ["C:\\ProgramData\\Anaconda3>py Scripts\\pywin32_postinstall.py -install\n"], ["import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nloop = asyncio.get_event_loop()\nt1 = loop.create_task(delay(1))\nt2 = loop.create_task(delay(2))\nloop.run_until_complete(t1)\nloop.close()\n", "sleeping for 1 second(s)\nsleeping for 2 second(s)\ndone sleeping for 1 second(s)\nTask was destroyed but it is pending!\ntask: <Task pending name='Task-1' coro=<delay() running at aio.py:10> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7fa794c5b970>()]>\n", "import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nloop = asyncio.get_event_loop()\nt1 = loop.create_task(delay(1))\nt2 = loop.create_task(delay(2))\nloop.run_until_complete(t1)\n\npending = asyncio.all_tasks(loop=loop)\ngroup = asyncio.gather(*pending)\nloop.run_until_complete(group)\n\nloop.close()\n", "import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nasync def main():\n    t1 = asyncio.create_task(delay(1))\n    t2 = asyncio.create_task(delay(2))\n    await t2\n\n\nasyncio.run(main())\n", "import asyncio\n\nasyncio.run(client.start('token'))\n"], [], [], [], [], [], ["class_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(train_classes),\n                                        y = train_classes                                                    \n                                    )\nclass_weights = dict(zip(np.unique(train_classes), class_weights))\nclass_weights\n"], ["await mycursor.execute(\"SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s\", (message.author.guild.id, message.author.id))\n        in_database2 = await mycursor.fetchone()\n        if in_database2:\n            await mycursor.execute(\"UPDATE guild_message_count SET user_id = %s, message_count = message_count + %s WHERE guild_id = %s AND user_id = %s\", (message.author.id, count, message.author.guild.id, message.author.id))\n        else:\n            await mycursor.execute(\"INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s)\", (message.author.id, count, message.author.guild.id))\n", "INSERT INTO guild_message_count\n        (user_id, message_count, guild_id)\n        VALUES \n        (%s, %s, %s)\n    ON DUPLICATE KEY UPDATE\n        message_count = message_count + 1\n"], [], ["addopts= --cov <path> -ra\n", "\"python.testing.pytestArgs\": [\"--no-cov\"],\n", "    \"python.testing.pytestArgs\": [\n        \"--rootdir\",\"${workspaceFolder}/<path-to-directory-with-tests>\"\n    ],\n"], [], [], ["apt list --installed | grep virtualenvwrapper \napt list --installed | grep virtualenvwrapper \n", "pip install virtualenvwrapper virtualenvwrapper \n", "export WORKON_HOME=$HOME/.virtualenvs\nexport PROJECT_HOME=$HOME/amd\nexport VIRTUALENVWRAPPER_SCRIPT=/home/robot/.local/bin/virtualenvwrapper.sh\nexport VIRTUALENVWRAPPER_PYTHON=$(which python3)\nsource /home/robot/.local/bin/virtualenvwrapper.sh\n"], ["from PIL import Image\nimport pillow_heif\n\n    heif_file = pillow_heif.read_heif(\"HEIC_file.HEIC\")\n    image = Image.frombytes(\n        heif_file.mode,\n        heif_file.size,\n        heif_file.data,\n        \"raw\",\n    \n    )\n\n    image.save(\"./picture_name.png\", format(\"png\"))\n"], [], [], ["set PATH=c:\\...\\Lib\\site-packages\\pywin32_system32;%PATH%\n", "import os\nprint(os.environ[\"PATH\"])\n", "os.environ[\"PATH\"] = r\"c:\\...\\pywin32_system32;\" + os.environ[\"PATH\"]\n", "os.environ[\"PATH\"] = r\"/.../pywin32_system32:\" + os.environ[\"PATH\"]\n"], ["def fn_cat_onehot(df):\n\n    \"\"\"Generate onehoteencoded features for all categorical columns in df\"\"\"\n\n    printmd(f\"df shape: {df.shape}\")\n\n    # NaN handing\n    nan_count = df.isna().sum().sum()\n    if nan_count > 0:\n        printmd(f\"NaN = **{nan_count}** will be categorized under feature_nan columns\")\n\n    # generation\n    from sklearn.preprocessing import OneHotEncoder\n\n    model_oh = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    for c in df.select_dtypes(\"category\").columns:\n        printmd(f\"Encoding **{c}**\")  # which column\n        matrix = model_oh.fit_transform(\n            df[[c]]\n        )  # get a matrix of new features and values\n        names = model_oh.get_feature_names_out()  # get names for these features\n        df_oh = pd.DataFrame(\n            data=matrix, columns=names, index=df.index\n        )  # create df of these new features\n        display(df_oh.plot.hist())\n        df = pd.concat([df, df_oh], axis=1)  # concat with existing df\n        df.drop(\n            c, axis=1, inplace=True\n        )  # drop categorical column so that it is all numerical for modelling\n\n    printmd(f\"#### New df shape: **{df.shape}**\")\n    return df\n"], [], ["pip list | grep opencv\n", "pip install opencv-python==4.5.4.60\n"], ["python3 -m venv venv\n"], [], ["pip show pywin32\n", "pip install pywin32==300 --upgrade\n"], [], ["import speedtest\n\ntest = speedtest.Speedtest()\n\nprint(\"Loading server list...\")\ntest.get_servers()\nprint(\"Choosing best server...\")\nbest = test.get_best_server()\n\nprint(f\"Found: {best['host']} located in {best['country']}\")\n\nprint(\"Performing download test...\")\ndownload_result = test.download()\nprint(\"Performing upload test...\")\nupload_result = test.upload()\nping_result = test.results.ping\n\nprint(f\"Download speed: {download_result / 1024 / 1024:.2f}Mbit/s\")\nprint(f\"Upload speed: {upload_result / 1024 / 1024:.2f}Mbit/s\")\nprint(f\"Ping: {ping_result}ms\")\n"], [], ["import pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nimport multiprocessing\nfrom uvicorn import Config, Server\n\n\nclass UvicornServer(multiprocessing.Process):\n\n    def __init__(self, config: Config):\n        super().__init__()\n        self.server = Server(config=config)\n        self.config = config\n\n    def stop(self):\n        self.terminate()\n\n    def run(self, *args, **kwargs):\n        self.server.run()\n\n\n\n\n@pytest.fixture(scope=\"session\")\ndef server():\n    config = Config(\"app.main:app\", host=\"127.0.0.1\", port=5000, log_level=\"debug\")\n    instance = UvicornServer(config=config)\n    instance.start()\n    yield instance\n    instance.stop()\n\n@pytest.fixture(scope=\"module\")\ndef mock_app(server):\n    client = TestClient(app)\n    yield client\n", "def test_root(mock_app):\n    response = mock_app.get(\"\")\n    assert response.status_code == 200\n"], ["pip3 install fitz\npip3 install PyMuPDF==1.16.14\n"], ["pip install --force-reinstall --no-binary :all: cffi\n"], [], ["pip install --upgrade cffi xcffib\n"], [], [], ["import sys\nprint(sys.executable)\n", "/path/to/python/used/by/vs/code/python -m pip install pillow\n"], ["pip install Pillow\n"], ["class Person:\n    def __init__(self, name):\n        self.name = name\n    def greeting(self):\n        # Should return \"hi, my name is \" followed by the name of the Person.\n        return name\n\n# Create a new instance with a name of your choice\nsome_person =  Person(\"XYZ\")\n# Call the greeting method\nprint(f\"hi, my name is {some_person.name}\")\n"], [], ["workbook.Close(SaveChanges=True)\nxl.Quit()\n"], ["import fitz\n\nmy_pdf = r\"C:\\Users\\Test\\FileName.pdf\"\ndoc = fitz.open(my_pdf) \ndef pdftype(doc):\n    i=0\n    for page in doc:\n        if len(page.getText())>0: #for scanned page it will be 0\n            i+=1\n    if i>0:\n        print('full/partial text PDF file')\n    else:\n        print('only scanned images in PDF file')\npdftype(doc)\n"], [], ["line_color=\"#0000ff\"\n", " fig['data'][0]['line']['color']=\"#00ff00\"\n", " fig.data[0].line.color = \"#00ff00\"\n", "import plotly.graph_objects as go\nimport numpy as np\n\nt = np.linspace(0, 10, 100)\ny = np.cos(t)\ny2= np.sin(t)\nfig = go.Figure(data=go.Scatter(x=t, y=y,mode='lines+markers', line_color='#ffe476'))\nfig.add_trace(go.Scatter(x=t, y=y2,mode='lines+markers', line=dict(color=\"#0000ff\")))\nfig.show()\n", "fig['data'][0]['line']['color']=\"#00ff00\"\nfig.show()\n"], [], ["from django.test import TestCase\n\nclass views(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        import django\n        django.setup()\n\n    def test_something(self,):\n        from user.model import something\n        ...\n"], [], [], ["import pandas as pd\nurl='https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\ndf = pd.read_csv(url)\n"], ["     workspace\n       .env\n       ./src\n            __init__.py\n            code1.py\n            code2.py\n       ./tests\n            __init__.py\n            test_code1.py\n            test_code2.py\n", "    {\n        \"workbench.colorTheme\": \"Visual Studio Dark\",\n        \"editor.fontFamily\": \" monospace, Menlo, Monaco, Courier New\",\n        \"python.testing.unittestEnabled\": false,\n        \"python.testing.cwd\": \".\",\n        \"terminal.integrated.inheritEnv\": true,\n        \"python.envFile\": \"${workspaceFolder}/.env\",\n        \"python.defaultInterpreterPath\": \n        \"~/anaconda3/envs/mycurrentenv/bin/python\",\n        \"pythonTestExplorer.testFramework\": \"pytest\",\n        \"python.testing.pytestEnabled\": true,\n        \"python.testing.pytestArgs\": [\n           \"tests\"\n        ],\n        \"python.terminal.activateEnvironment\": true,\n        \"python.terminal.activateEnvInCurrentTerminal\": true,\n        \"terminal.integrated.env.osx\": {\n            \"PYTHONPATH\": \"${workspaceFolder}/src:${env:PYTHONPATH}\"\n          }\n    }\n"], ["from pydantic import BaseModel, Field\n\n\nclass TMDB_Category(BaseModel):\n    name: str = Field(alias=\"strCategory\")\n    description: str = Field(alias=\"strCategoryDescription\")\n\n\ndata = {\n    \"strCategory\": \"Beef\",\n    \"strCategoryDescription\": \"Beef is ...\"\n}\n\n\nobj = TMDB_Category.parse_obj(data)\n\n# {'name': 'Beef', 'description': 'Beef is ...'}\nprint(obj.dict())\n"], ["count_query = query.with_only_columns(func.count(Foo.id))\ncount = session.execute(count_query).scalar_one()\n"], [], [], ["pip install black \"black[jupyter]\"\nblack {source_file_or_directory}\n"], [], [], ["from copy import deepcopy\n\n@dataclass(frozen=True)\nclass A:\n    a: str = ''\n    b: int = 0\n\n    def mutate(self, **options):\n        new_config = deepcopy(self.__dict__)\n        # some validation here\n        new_config.update(options)\n        return self.__class__(**new_config)\n", "from dataclasses import dataclass, InitVar\n\n\n@dataclass(frozen=True)\nclass A:\n    a: str = ''\n    b: int = 0\n    config: InitVar[dict] = None\n\n    def __post_init__(self, config: dict):\n        if config:\n            self.__init__(**config)\n", "A(config={'a':'a', 'b':1})\n", "A(a='a', b=1)\n", "A(config={'a':'a', 'b':1, 'config':{'a':'b'}})\n", "A(a='b', b=1)\n"], [], ["!python -m spacy download en\n", "spacy.load('en_core_web_sm')\n"], [], [], [], ["python -m spacy download en_core_web_lg\npython -m spacy download en_core_web_sm\n", "python -m spacy download en\n"], [], ["python -m pip install <module>\n", "python3 -m pip install <module>\n"], [], [], [], ["decompose_result = seasonal_decompose(df.Sales, model='multiplicative', period=1)\ndecompose_result.plot();\n", "**Signature:**\nseasonal_decompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n)\n"], [], [], ["brew install --cask miniforge\nconda init zsh\nconda activate\nconda install numpy scipy scikit-learn\n"], ["class Person:\ndef __init__(self, name):\n    self.name = name\ndef greeting(self):\n    # Should return \"hi, my name is \" followed by the name of the Person.\n    return name\n\n# Create a new instance with a name of your choice\nsome_person = Person(\"Bob\")\n# Call the greeting method\nprint(f\"hi, my name is {some_person.name}\")\n"], [], ["export PATH=\"$HOME/.poetry/bin:$PATH\"\n"], [], [], ["pip install cryptography==3.1.1\n", "python -m pip install --upgrade pip\n\nsudo pip install -U pip setuptools\n"], [], [], [], [], ["import os\n\nos.add_dll_directory(r\"C:\\Program Files\\GTK3-Runtime Win64\\bin\")\n\nfrom weasyprint import HTML\n\nHTML('https://weasyprint.org/').write_pdf('weasyprint-website.pdf')\n"], ["{\n    \"python.defaultInterpreterPath\": \"/path/to/your/venv/bin/python\",\n}\n"], [], ["DELETE FROM public.django_migrations\nWHERE public.django_migrations.app = 'target_app_name';\n"], ["import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\n"], ["_PIP_ADDITIONAL_REQUIREMENTS\n"], [], [], ["\"python.analysis.extraPaths\": [\n        \"./directory_name\"\n    ]\n"], [], ["winpty python3\n"], ["C:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\n", "nlp = spacy.load(r'C:\\Users\\name\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0')\n"], [], ["from spacy.lang.en import English\nnlp=English()\n"], ["python -m spacy download en\npython -m spacy link en_core_web_sm en_core_web_sm\n", "import spacy\nspacy.load('en_core_web_sm')\n"], [], ["\"terminal.integrated.profiles.windows\":{\n    \"PowerShell\": {\n        \"source\": \"PowerShell\",\n        \"icon\": \"terminal-powershell\"\n    },\n    \"Command Prompt\": {\n        \"path\": [\n            \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n            \"${env:windir}\\\\System32\\\\cmd.exe\"\n        ],\n        \"args\": [],\n        \"icon\": \"terminal-cmd\"\n    },\n    \"Git Bash\": {\n        \"source\": \"Git Bash\"\n    },\n    \"Anaconda Prompt\": {\n        \"source\": \"PowerShell\",\n        \"args\": [\n            \"powershell\",\n            \"-NoExit\",\n            \"-ExecutionPolicy ByPass\",\n            \"-NoProfile\",\n            \"-File C:\\\\path\\\\to\\\\Miniconda3\\\\shell\\\\condabin\\\\conda-hook.ps1\"\n        ],\n        \"icon\": \"smiley\"\n    }\n}\n"], ["    df = pd.read_csv('/content/drive/MyDrive/Dataset/dataset.csv')\n    df.head()\n"], [], [], [], ["#Use str method instead of greeting() method\ndef __str__(self):\n    # Should return \"hi, my name is \" followed by the name of the Person.\n    return \"hi, my name is {}\".format(self.name) \nsome_person = Person(\"xyz\")  \n# Call the __str__ method\nprint(some_person)\n"], [], ["df = pd.read_csv('/content/drive/MyDrive/.../your_file.csv')\n"], ["[default]\naws_access_key_id=AKIAIOSFODNN7EXAMPLE\naws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n", "!mkdir -p ~/.aws\n!cp /content/drive/MyDrive/aws_config ~/.aws/config\n"], [], ["conda install jupyter notebook\n"], [], [], ["extracted_text = ''.join([page.getText() for page in fitz.open(path)])\ndoc_type = \"text\" if extracted_text else \"scan\"\n"], [], ["# install\n!pip install pycodestyle pycodestyle_magic\n\n\n# load\n%load_ext pycodestyle_magic\n\n\n# use\n%%pycodestyle\ndef square_of_number(\n     num1, num2, num3, \n     num4):\n    return num1**2, num2**2, num3*\n\n# Output\n2:1: E302 expected 2 blank lines, found 0\n3:23: W291 trailing whitespace\n\n"], [], ["<class 'blog.schemas.Blog'>\n"], [], [], [], [], ["C:\\Users\\<user>\\AppData\\Local\\Programs\\Python\\Python39\nC:\\Users\\<user>\\AppData\\Local\\Programs\\Python\\Python39\\Scripts\n"], [], [], [], [], [], [], ["import PyPDF2\nimport pathlib\n\npath = 'D:\\Python\\PDF processing' #add your folder path here\nfolder = pathlib.Path(path).resolve()\n\nfor item in folder.iterdir():\n    if item.is_file() and item.suffix == '.pdf' and not item.name == 'wtr.pdf':\n        with open(f'{item}', 'rb') as file, open('wtr.pdf', 'rb') as wtr:\n            input_file_reader = PyPDF2.PdfFileReader(file)  # input file reader\n            wtr_reader = PyPDF2.PdfFileReader(wtr)  # watermark reader\n            wtr_page = wtr_reader.getPage(0)  # getting the page with the watermark\n            output = PyPDF2.PdfFileWriter()  # the output writer\n\n            for i in range(input_file_reader.numPages):  # looping the input reader pages\n                page = input_file_reader.getPage(i) # getting pages one by one\n                page.mergePage(wtr_page)  # merging each page with the watermark\n                output.addPage(page)  # adding the merged page to the output writer\n\n            with open(f'merged_{item.name}', 'wb') as merged_file:\n                output.write(merged_file)  # saving to a file from the output writer\n"], ["conda init\n", "~\\Path\\to\\my\\virtual\\environment\\python.exe\n"], ["  command: -c \"pip3 install apache-airflow-providers-sftp  apache-airflow-providers-ssh --user\"\n", "docker-compose up airflow-init\ndocker-compose up\n"], ["pip list | grep opencv\n"], ["def get_pdf_searchable_pages(fname):\n    \"\"\" intentifying a digitally created pdf or a scanned pdf\"\"\"    \n    from pdfminer.pdfpage import PDFPage\n    searchable_pages = []\n    non_searchable_pages = []\n    page_num = 0\n    with open(fname, 'rb') as infile:\n\n        for page in PDFPage.get_pages(infile):\n            page_num += 1\n            if 'Font' in page.resources.keys():\n                searchable_pages.append(page_num)\n            else:\n                non_searchable_pages.append(page_num)\n    if page_num == len(searchable_pages):\n        return(\"searchable_pages\")\n    elif page_num != len(searchable_pages):\n        return(\"non_searchable_pages\")\n    else:\n        return(\"Not a valid document\")\n"], ["from datetime import date, timedelta\nimport pandas as pd\n\n#Start date and end_date\nstart_date = pd.to_datetime(\"2019-06-01\")\nend_date = pd.to_datetime(\"2021-08-20\") - timedelta(days=1) #Excluding last\n\n#List of all dates\nall_date = pd.date_range(start_date, end_date, freq='d')\n\n#Left join your main data on dates data\nall_date_df = pd.DataFrame({'date':all_date})\ntdf = df.groupby('date', as_index=False)['session_count'].sum()\ntdf = pd.merge(all_date_df, tdf, on='date', how=\"left\")\ntdf.fillna(0, inplace=True)\n"], [], [], ["pip3 install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n", "pip install /Users/yourpath/Downloads/en_core_web_sm-3.1.0.tar.gz;\n"], [], ["c:\\>activate <conda environment name>\n", "nlp = en_core_web_sm.load()\n"], ["    pip install pdfplumber\n\n    with pdfplumber.open(file_name) as pdf:\n        page = pdf.pages[0]\n        text = page.extract_text()\n        print(text)\n"], ["!pip install seaborn\n"], ["pip install --upgrade jupyter_client    \npip install --upgrade pywin32==224 --force-reinstall\n"], [], [], [], ["df.loc['Perc.'] = (\n    df.loc['>48h'] / df.loc['<48h']\n).apply(lambda x: f\"{x:.0%}\")\n", "df.loc['Perc.'] = (\n    df.loc['>48h'] / df.loc['<48h']\n)\n\ndisplay_str = df.T.to_string(formatters={\"Perc.\": lambda x: f\"{x:.0%}\"})\nprint(display_str)\n", "   Para  <48h  >48h Perc.\n0  1.21  22.0   0.0    0%\n1  2.21  25.0   1.0    4%\n2  3.21  38.0   3.0    8%\n", "df.T.style.format({\"Perc.\": \"{:.0%}\"})\n"], [], ["#If first column is not index create it\n#df = df.set_index('Para')\n\n\ndf.loc['Perc'] = df.loc['>48h'].div(df.loc['<48h']).mul(100).round()\nprint (df)\n      01.21  02.21  03.21\nPara                     \n<48h   22.0   25.0   38.0\n>48h    0.0    1.0    3.0\nPerc    0.0    4.0    8.0\n", "#If first column is not index create it\n#df = df.set_index('Para')\n\ndf = df.T\n\ndf['Perc'] = df['>48h'].div(df['<48h']).mul(100).round()\nprint (df)\nPara   <48h  >48h  Perc\n01.21    22     0   0.0\n02.21    25     1   4.0\n03.21    38     3   8.0\n"], ["para = ['01.21', '02.21', '03.21']\na = np.array(([22, 25, 38]))\nb = np.array(([0, 1, 3]))\ndf = pd.DataFrame([para, a, b], index=['Para', '<48h', '>48h'], columns=['col0', 'col1', 'col2'])\n\nperc = np.array(((b/a)*100))\n\ndf2 = pd.DataFrame([perc], index=['Perc.'], columns=['col0', 'col1', 'col2'])\ndf = df.append(df2)\nprint(df)\n", "        col0   col1     col2\nPara   01.21  02.21    03.21\n<48h      22     25       38\n>48h       0      1        3\nPerc.      0      4  7.89474\n"], ["pip uninstall pywin32\npip install pywin32==228\n"], [], ["pip install speedtest-cli\n"], ["conda install pywin32\n"], [], ["`# Build a feed-forward network\n class FFN(nn.Module):\n     def __init__(self):\n         super().__init__()\n         self.layer1 = nn.Linear(input_size, hidden_sizes[0])\n         self.layer2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n         self.layer3 = nn.Linear(hidden_sizes[1], output_size)\n         self.relu = nn.ReLU()\n         self.softmax = nn.Softmax(dim=1)\n     def forward(self, x):\n         x = self.relu(self.layer1(x))\n         x = self.relu(self.layer2(x))\n         x = self.softmax(self.layer3(x))\n         return x\n\nmodel = FFN()\nprint(model.parameters())`\n"], [], [], ["df_test = pd.DataFrame({'timestamp': [1462352000000000000, 1462352100000000000, 1462352200000000000, 1462352300000000000],\n                'listData': [[1,2,1,9], [2,2,3,0], [1,3,3,0], [1,1,3,9]],\n                'duration_sec': [3.0, 3.0, 3.0, 3.0]})\ntdi = pd.DatetimeIndex(df_test.timestamp)\ndf_test.set_index(tdi, inplace=True)\ndf_test.drop(columns='timestamp', inplace=True)\ndf_test.index.name = 'datetimeindex'\n\ndf_test = df_test.explode('listData') \nsizes = df_test.groupby(level=0)['listData'].transform('size').sub(1)\nduration = df_test['duration_sec'].div(sizes)\ndf_test.index += pd.to_timedelta(df_test.groupby(level=0).cumcount() * duration, unit='s') \n", "2016-05-04 08:53:20    1\n2016-05-04 08:53:21    2\n2016-05-04 08:53:22    1\n2016-05-04 08:53:23    9\n2016-05-04 08:55:00    2\n2016-05-04 08:55:01    2\n2016-05-04 08:55:02    3\n2016-05-04 08:55:03    0\n2016-05-04 08:56:40    1\n2016-05-04 08:56:41    3\n2016-05-04 08:56:42    3\n2016-05-04 08:56:43    0\n2016-05-04 08:58:20    1\n2016-05-04 08:58:21    1\n2016-05-04 08:58:22    3\n2016-05-04 08:58:23    9\n", "result_add = seasonal_decompose(x=df_test['listData'], model='additive', extrapolate_trend='freq', period=1)\nplt.rcParams.update({'figure.figsize': (5,5)})\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()\n", "result_add = seasonal_decompose(x=df_test['listData'], model='additive', extrapolate_trend='freq', period=2)\nplt.rcParams.update({'figure.figsize': (5,5)})\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()\n", "result_add = seasonal_decompose(x=df_test['listData'], model='additive', extrapolate_trend='freq', period=int(len(df_test)/4))\nplt.rcParams.update({'figure.figsize': (5,5)})\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()\n", "result_add = seasonal_decompose(x=df_test['listData'], model='additive', extrapolate_trend='freq', period=int(len(df_test)/2))\nplt.rcParams.update({'figure.figsize': (5,5)})\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()\n", "sm.tsa.seasonal_decompose(df, model = 'additive', period = int(len(df)/2))\n", "sm.tsa.seasonal_decompose(df.asfreq('MS'), model = 'additive')\n", "2016-05-04 08:53:20      1\n2016-05-04 08:53:21      2\n2016-05-04 08:53:22      1\n2016-05-04 08:53:23      9\n2016-05-04 08:53:24    NaN\n                      ...\n2016-05-04 08:58:19    NaN\n2016-05-04 08:58:20      1\n2016-05-04 08:58:21      1\n2016-05-04 08:58:22      3\n2016-05-04 08:58:23      9\nFreq: S, Name: listData, Length: 304, dtype: object\n", "2016-05-04 08:53:20      1\n2016-05-04 08:54:20    NaN\n2016-05-04 08:55:20    NaN\n2016-05-04 08:56:20    NaN\n2016-05-04 08:57:20    NaN\n2016-05-04 08:58:20      1\n", "2016-05-04 08:53:20    1\n"], ["pip install -U nbqa pylint\nnbqa pylint notebook.ipynb\n"], ["import pandas as pd\nimport glob\n\ndef readFiles(path):\n    files = glob.glob(path)\n    dfs = [] # an empty list to store the data frames\n    for file in files:\n        data = pd.read_json(file, lines=True) # read data frame from json file\n        dfs.append(data) # append the data frame to the list\n\n    df = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.\n    return df\n"], [], [], ["...\n\n\"terminal.integrated.profiles.windows\": {\n  \"Conda\": {\n    \"path\": \"C:\\\\windows\\\\System32\\\\cmd.exe\",\n    \"args\": [\n      \"/K\", \"C:\\\\ProgramData\\\\Anaconda3\\\\Scripts\\\\activate.bat C:\\\\ProgramData\\\\Anaconda3\"\n    ],\n    \"icon\": \"squirrel\"\n  }\n},\n\"terminal.integrated.defaultProfile.windows\": \"Conda\", \n...\n"], ["pip install pywin32==225\n"], ["pip uninstall pywin32\n\npip install pywin32\n", "pip uninstall win32api\npip install win32api\n"], ["--init-hook=\"from pylint.config import find_pylintrc; import os, sys; sys.path.append(os.path.dirname(find_pylintrc()))\"\n"], [], [], ["def get_callbacks(app):\n    @app.callback([Output(\"figure1\", \"figure\")],\n                  [Input(\"child1\", \"value\")])\n    def callback1(figure):\n        return\n\n    @app.callback([Output(\"figure2\", \"figure\")],\n                  [Input(\"child2\", \"value\")])\n    def callback2(figure):\n        return\n", "import dash\nfrom callbacks import get_callbacks\nimport layout\n\napp = dash.Dash(__name__)\napp.layout = layout.layout\n\nget_callbacks(app)\n"], [], [], [], ["@app.put(\"/blog/update/{id}\")\ndef updateBlog(id, request:schemas.Blog, db:Session=Depends(get_db)):\n   blog= db.query(model.Blog).filter(model.Blog.id == id).first()\n   \n   if not blog:\n             raise HTTPException(status_code=status.HTTP_404_NOT_FOUND,detail=f'blog with id {id} not found')\n   else:\n  db.query(model.Blog).filter(model.Blog.id == id).update(request.dict())\n\ndb.commit()\ndb.refresh(blog)\nreturn blog\n"], [], ["pip install --upgrade pip\n"], ["sudo -H pip3 install --upgrade pip\n"], [], ["FROM apache/airflow:2.1.0\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n", "---\nversion: '3'\nx-airflow-common:\n  &airflow-common\n  build: .\n  # REPLACED # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.1.0}\n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__API__AUTH_BACKEND: 'airflow.api.auth.backend.basic_auth'\n  volumes:\n    - ./dags:/opt/airflow/dags\n    - ./logs:/opt/airflow/logs\n    - ./plugins:/opt/airflow/plugins\n  user: \"${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}\"\n  depends_on:\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n\n# ...\n", "airflow-project\n|docker-compose.yaml\n|Dockerfile\n|requirements.txt\n"], [], ["python --version\n"], ["# loading and transforming\nimport joblib\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = joblib.load('scaler')\nassert isinstance(scaler, MinMaxScaler)\nscaler.clip = False  # add this line\ndata = scaler.transform(data)  # throws exceptio\n", "scaler.clip = False\n"], ["youtube-dl --write-annotations --write-info-json --skip-download www.youtube.com/yourvideo\n"], ["\"terminal.integrated.profiles.windows\": {\n\n    \"PowerShell\": {\n        \"source\": \"PowerShell\",\n        \"icon\": \"terminal-powershell\"\n    },\n\n    \"Miniconda3\": {\n        \"source\": \"PowerShell\",\n        \"args\": \"-ExecutionPolicy ByPass -NoExit -Command \\\"& 'C:\\\\ProgramData\\\\Miniconda3\\\\shell\\\\condabin\\\\conda-hook.ps1' ; conda activate 'C:\\\\ProgramData\\\\Miniconda3' \\\"\",\n        \"icon\": \"C:\\\\ProgramData\\\\Miniconda3\\\\Lib\\\\site-packages\\\\conda\\\\shell\\\\conda_icon.ico\"\n    },\n\n    \"Command Prompt\": {\n        \"path\": [\n            \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n            \"${env:windir}\\\\System32\\\\cmd.exe\"\n        ],\n        \"args\": [],\n        \"icon\": \"terminal-cmd\"\n    },\n\n    \"Git Bash\": {\n        \"source\": \"Git Bash\"\n    }\n\n}\n"], ["import numpy as np\nimport pandas as pd\nfrom numpy_ext import rolling_apply as rolling_apply_ext\n\ndef box_sum(a,b):\n    return np.sum(a) + np.sum(b)\n\ndf = pd.DataFrame({\"x\": [1,2,3,4], \"y\": [1,2,3,4]})\n\nwindow = 2\ndf[\"sum\"] = rolling_apply_ext(box_sum, window , df.x.values, df.y.values)\n", "print(df.to_string(index=False))\n x  y  sum\n 1  1  NaN\n 2  2  6.0\n 3  3 10.0\n 4  4 14.0\n"], ["[WARNING] Worker with pid 71 was terminated due to signal 9\n", "Out of memory: Killed process 776660 (gunicorn)\n"], [], ["python3 -m virtualenv --help\n", "python3 -m virtualenv my_env\n"], [], ["import PyPDF2\nimport sys\n\npdf_file_list = sys.argv[1:]\nwatermark = 'wtr.pdf'\nmerger = PyPDF2.PdfFileMerger()\nwriter = PyPDF2.PdfFileWriter()\n\n\nfor file in pdf_file_list:\n    merger.append(file)\n\nmerger.write('super.pdf')\n\n\nwith open('super.pdf', 'rb') as fileinput:\n    pdf_file = PyPDF2.PdfFileReader(fileinput)\n\n    with open(watermark, 'rb') as filewatermark:\n        watermark_pdf = PyPDF2.PdfFileReader(filewatermark)\n\n        for page in range(pdf_file.getNumPages()):\n            current_pdf_page = pdf_file.getPage(page)\n            first_page_watermark = watermark_pdf.getPage(0)\n            current_pdf_page.mergePage(first_page_watermark)\n            writer.addPage(current_pdf_page)\n\n            with open(\"watermarked.pdf\", 'wb')as file_output:\n                writer.write(file_output)\n"], ["blog.update({'title': request.title, 'body': request.body})\n", "blog.update(request.dict())\n"], ["num_owls_zooA + num_owls_zooB + num_owls_zooC / 3\n", "num_owls_zooA + num_owls_zooB + (num_owls_zooC / 3)\n", "(num_owls_zooA + num_owls_zooB + num_owls_zooC) / 3\n"], ["avg_owls = 0.0\n\nnum_owls_zooA = float(input())\nnum_owls_zooB = float(input())\nnum_owls_zooC = float(input())\n\navg_owls = (num_owls_zooA + num_owls_zooB + num_owls_zooC) / 3\n\nprint(f'Average owls per zoo: {avg_owls} ')\n"], ["import pandas as pd\n\nurl='https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\nfile_id=url.split('/')[-2]\ndwn_url='https://drive.google.com/uc?id=' + file_id\ndf = pd.read_csv(dwn_url)\nprint(df.head())\n", "import pandas as pd\nimport requests\nfrom io import StringIO\n\nurl='https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\n\nfile_id = url.split('/')[-2]\ndwn_url='https://drive.google.com/uc?export=download&id=' + file_id\nurl2 = requests.get(dwn_url).text\ncsv_raw = StringIO(url2)\ndf = pd.read_csv(csv_raw)\nprint(df.head())\n", "      sex   age state  cheq_balance  savings_balance  credit_score  special_offer\n0  Female  10.0    FL       7342.26          5482.87           774           True\n1  Female  14.0    CA        870.39         11823.74           770           True\n2    Male   0.0    TX       3282.34          8564.79           605           True\n3  Female  37.0    TX       4645.99         12826.76           608           True\n4    Male   NaN    FL           NaN          3493.08           551          False\n"], ["library(tidyverse)\n\nurl='https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\nfile_id=nth(strsplit(url, split = \"/\")[[1]], -2)\ndwn_url=paste('https://drive.google.com/uc?id=',file_id,sep = \"\")\ndf = read_csv(dwn_url)\n\nhead(df)\n"], [], ["import numpy as np\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n"], ["ocrmypdf.ocr(file_path, save_path, rotate_pages=True, remove_background=False, language=language, deskew=False, force_ocr=False, skip_text=True)\n"], [], [], ["url = 'https://drive.google.com/uc?id=0B6GhBwm5vaB2ekdlZW5WZnppb28'\ndfs = pd.read_csv(url)\n"], [], ["column_names = [\"a\", \"b\", \"c\"]\ndf = pd.DataFrame(columns = column_names, dtype=object)\n"], [], [], ["def openRedisCon():\n   pool = redis.ConnectionPool(host=REDIS_HOST, port=REDIS_PORT, db=0)\n   r = redis.Redis(connection_pool=pool)\n   return r\n\ndef storeDFInRedis(alias, df):\n    \"\"\"Store the dataframe object in Redis\n    \"\"\"\n\n    buffer = io.BytesIO()\n    df.to_parquet(buffer, compression='gzip')\n    buffer.seek(0) # re-set the pointer to the beginning after reading\n    r = openRedisCon()\n    res = r.set(alias,buffer.read())\n\ndef loadDFFromRedis(alias, useStale: bool = False):\n    \"\"\"Load the named key from Redis into a DataFrame and return the DF object\n    \"\"\"\n\n    r = openRedisCon()\n\n    try:\n        buffer = io.BytesIO(r.get(alias))\n        buffer.seek(0)\n        df = pd.read_parquet(buffer)\n        return df\n    except:\n        return None\n\n\n"], [], ["--no-cov\n"], [], [], ["pip install --upgrade pywin32==224\n"], [], [], [], ["nlpObject = spacy.load('/path_to_models/en_core_web_lg-3.0.0/en_core_web_lg/en_core_web_lg-3.0.0')\n"], [], [], ["from pydantic import BaseModel\n\nd = {\n    \"p_id\": 1,\n    \"billing\": {\n        \"first_name\": \"test\"\n    }\n}\n\n\nclass Order(BaseModel):\n    p_id: int\n    pre_name: str\n\n    def __init__(self, **kwargs):\n        kwargs[\"pre_name\"] = kwargs[\"billing\"][\"first_name\"]\n        super().__init__(**kwargs)\n\n\nprint(Order.parse_obj(d))  # p_id=1 pre_name='test'\n"], [], [], [], [], ["from chatterbot import ChatBot\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nChatBot(\"hello\")\n"], [], [], ["python3 -m http.server 8080\n", "python -m http.server 8080\n"], ["  generating cffi module 'build/temp.linux-x86_64-3.7/_openssl.c'\n  running build_rust\n  \n      =============================DEBUG ASSISTANCE=============================\n      If you are seeing a compilation error please try the following steps to\n      successfully install cryptography:\n      1) Upgrade to the latest pip and try again. This will fix errors for most\n         users. See: https://pip.pypa.io/en/stable/installing/#upgrading-pip\n      2) Read https://cryptography.io/en/latest/installation.html for specific\n         instructions for your platform.\n      3) Check our frequently asked questions for more information:\n         https://cryptography.io/en/latest/faq.html\n      4) Ensure you have a recent Rust toolchain installed:\n         https://cryptography.io/en/latest/installation.html#rust\n      5) If you are experiencing issues with Rust for *this release only* you may\n         set the environment variable `CRYPTOGRAPHY_DONT_BUILD_RUST=1`.\n      =============================DEBUG ASSISTANCE=============================\n", "pip install -U pip\n"], [], [], ["python3 -m pip install --no-use-pep517 cryptography\n"], ["my_footer_run = footer.paragraphs[0].add_run()\nmy_footer_run.text = \"Copyright MyCompany  All Rights Reserved.\\t\\t\"\nadd_page_number(my_footer_run)\n"], ["import pip\npip.main(['install','seaborn'])\n"], [], [], [], [], ["import pandas as pd\n\ndfs = []\nfor file in file_list:\n    with open(file) as f:\n        json_data = pd.json_normalize(json.loads(f.read()))\n    dfs.append(json_data)\ndf = pd.concat(dfs, sort=False) # or sort=True depending on your needs\n", "import pandas as pd\n\ndfs = []\nfor file in file_list:\n    with open(file) as f:\n        for line in f.readlines():\n            json_data = pd.json_normalize(json.loads(line))\n            dfs.append(json_data)\ndf = pd.concat(dfs, sort=False) # or sort=True depending on your needs\n"], [], [], ["!python -m spacy download en\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n", "import en_core_web_md\nnlp = en_core_web_md.load()\n"], [], [], [], ["import PyPDF2\n\ntemplate = PyPDF2.PdfFileReader(open('C:/Users/11359023/Desktop/deepfake_vee.pdf', 'rb'))\nwatermark = PyPDF2.PdfFileReader(open('C:/Users/11359023/Desktop/simple.pdf', 'rb'))\noutput = PyPDF2.PdfFileWriter()\n\nfor i in range(template.getNumPages()):\n  page = template.getPage(i)\n  page.mergePage(watermark.getPage(0))\n  output.addPage(page)\n\n  with open('C:/Users/11359023/Desktop/merged.pdf', 'wb') as file:\n    output.write(file)\n"], [], ["apt remove python3-virtualenvwrapper  # not purge, I want no changes in ~/.virtualenvs/\napt purge python3-virtualenv\n/usr/bin/python3.8 -m pip install --force-reinstall virtualenvwrapper\n/usr/bin/python3.8 -m pip install --force-reinstall virtualenv==20.0.23\n"], [], ["Project_dir:\n    .vscode/settings.json\n    dir_1\n        > a\n        > b\n        > c\n    dir_2\n        > x\n        > y\n        > z\n", "Project_dir\n    dir_3\n        import a\n        import y\n", "Import \"dir_1.a\" could not be resolvedPylancereportMissingImports\nImport \"dir_2.y\" could not be resolvedPylancereportMissingImports\n", ".vscode/settings.json\n", "\"python.analysis.extraPaths\": [dir_1, dir_2]\n"], ["from multiprocessing import set_start_method\nfrom multiprocessing import Process, Manager\ntry:\n    set_start_method('spawn')\nexcept RuntimeError:\n    pass\n@app.get(\"/article_classify\")\ndef classification(text:str):\n    \"\"\"function to classify article using a deep learning model.\n    Returns:\n        [type]: [description]\n    \"\"\"\n    manager = Manager()\n\n    return_result = manager.dict()\n    # as the inference is failing \n    p = Process(target = inference,args=(text,return_result,))\n    p.start()\n    p.join()\n    # print(return_result)\n    result = return_result['all_tags']\n    return result\n"], [], [], [], [], [], [], [], [], ["python setup.py install\n"], [], ["$ sudo apt-get install -y python3-seaborn\n"], [], [], [], [], [], ["import google\nimport os\nimport requests\n\nGOOGLE_APPLICATION_CREDENTIALS = \"GOOGLE_APPLICATION_CREDENTIALS\"\nGCS_OAUTH_TOKEN = \"GCS_OAUTH_TOKEN\"\nSCOPE = \"https://www.googleapis.com/auth/cloud-platform\"\nURL = \"https://www.googleapis.com/oauth2/v1/tokeninfo\"\nPAYLOAD = \"access_token={}\"\nHEADERS = {\"content-type\": \"application/x-www-form-urlencoded\"}\nOK = \"OK\"\n\n\ndef get_gcs_token():\n    \"\"\"\n    Returns gcs access token.\n    Ideally, this function generates a new token, requries that GOOGLE_APPLICATION_CREDENTIALS be set in the environment\n    (os.environ).\n    Alternatively, environment variable GCS_OAUTH_TOKEN could be set if a token already exists\n    \"\"\"\n    if GOOGLE_APPLICATION_CREDENTIALS in os.environ:\n        # getting the credentials and project details for gcp project\n        credentials, your_project_id = google.auth.default(scopes=[SCOPE])\n\n        # getting request object\n        auth_req = google.auth.transport.requests.Request()\n        credentials.refresh(auth_req)  # refresh token\n        token = credentials.token\n    elif GCS_OAUTH_TOKEN in os.environ:\n        token = os.environ[GCS_OAUTH_TOKEN]\n    else:\n        raise ValueError(\n            f\"\"\"Could not generate gcs token because {GOOGLE_APPLICATION_CREDENTIALS} is not set in the environment.\nAlternatively, environment variable {GCS_OAUTH_TOKEN} could be set if a token already exists, but it was not\"\"\"\n        )\n\n    r = requests.post(URL, data=PAYLOAD.format(token), headers=HEADERS)\n    if not r.reason == OK:\n        raise ValueError(\n            f\"Could not verify token {token}\\n\\nResponse from server:\\n{r.text}\"\n        )\n    if not r.json()[\"expires_in\"] > 0:\n        raise ValueError(f\"token {token} expired\")\n    return token\n"], ["def get_user_choice(choice):\nprint(\"Network Toolkit Menu\")\n"], ["def main(choice):\n    ...\n\nmain() #<-here\n", "main(1)\n", "def main():\n    ...\n"], ["def main(choice):\n    ...\nmain()\n", "def main():\n    counter=False\n    while counter==False:\n        choice = get_user_choice()\n...\n", "def get_user_choice():\n    ...\n"], ["import socket\nimport uuid\nimport os\nimport re\n\nHNAME=1\nIP=2\nMAC=3\nARP=4\nROUT=5\nQUIT=6\n\n\n\ndef get_user_choice():\n    print(\"Network Toolkit Menu\")\n    print(\"_____________________________________\")\n    print(\"1. Display the Host Name\")\n    print(\"2. Display the IP Address\")\n    print(\"3. Display the MAC Address\")\n    print(\"4. Display the ARP Table\")\n    print(\"5. Display the Routing Table\")\n    print(\"6. Quit\")\n    print()\n    \n    choice = int(input(\"Enter your choice: \"))\n    \n    return choice\n\ndef choicefun(choice):\n   \n    while choice > QUIT or choice < HNAME:\n        \n        choice = int(input(\"Please enter a valid number: \"))\n        print()\n        \n    return choice\n\ndef get_hostname(host):\n    host=socket.gethostname()\n    print(\"\\n The host name is: \", host)\n    #return host\n\ndef get_ipaddr(ipv4):\n    ipv4=socket.gethostbyname()\n    print(\"\\n The IPv4 address of the system is: \", ipv4)\n    #return ipv4\n\ndef get_mac(ele):\n    print (\"The MAC address is : \", end=\"\") \n    print (':'.join(['{:02x}'.format((uuid.getnode() >> ele) & 0xff) \n    for ele in range(0,8*6,8)][::-1]))\n\ndef get_arp(line):\n    print(\"ARP Table\")\n    with os.popen('arp -a') as f:\n        data=f.read()\n    for line in re.findall('([-.0-9]+)\\s+([-0-9a-f]{17})\\s+(\\w+)',data):\n        print(line)\n    return line\n\ndef __pyroute2_get_host_by_ip(ip):\n    print(\"Routing table\\n: \")\n    table=os.popen('route table')\n    print(table)\n\ndef main():\n    counter=False\n    while counter==False:\n        choice = get_user_choice()\n        choicefun(choice)         \n        if str(choice) == \"6\":            \n            counter==True\n        elif str(choice) == \"1\":\n            get_hostname()\n        elif str(choice) == \"2\":\n            get_ipaddr()\n        elif str(choice) == \"3\":\n           get_mac() \n        elif str(choice)== \"4\":\n            get_arp()\n        elif str(choice) == \"5\":\n            __pyroute2_get_host_by_ip()\n\nmain()\n"], ["        get_user_choice(choice)\n"], ["import en_core_web_sm\nnlp = en_core_web_sm.load()\n"], ["python -m spacy download en_core_web_sm\n\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\n"], [], [], ["#!/usr/bin/env python\n"], ["path = \".../your_path\"\nimg = pd.read_csv(path + \"h3.6m/dataset/S1/directions_1.txt\") #read the image\n\n#if you want to apply DWT you can continue with dataframe    \ncoeffs2 = dwt(image,  'bior1.3')\ntitles = ['Approximation', ' Horizontal detail',\n              'Vertical detail', 'Diagonal detail']\n\nLL, LH = coeffs2\n"], [], [], ["import pyarrow as pa\nimport redis\n\npool = redis.ConnectionPool(host='localhost', port=6379, db=0)\nr = redis.Redis(connection_pool=pool)\n\ndef storeInRedis(alias, df):\n    df_compressed = pa.serialize(df).to_buffer().to_pybytes()\n    res = r.set(alias,df_compressed)\n    if res == True:\n        print(f'{alias} cached')\n\ndef loadFromRedis(alias):\n    data = r.get(alias)\n    try:\n        return pa.deserialize(data)\n    except:\n        print(\"No data\")\n\n\nstoreInRedis('locations', locdf)\n\nloadFromRedis('locations')\n"], [], ["  pip install -U --user spacy    \n  python -m spacy download en\n", "import spacy\nspacy.load('en')\n"], [], ["python -m spacy download en_core_web_sm\npip install .tar.gz archive from path or URL\npip install /Users/you/en_core_web_sm-2.2.0.tar.gz\n", "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n", "nlp = spacy.load('en_core_web_sm') \n", "name: root\nchannels:\n  - defaults\n  - conda-forge\n  - anaconda\ndependencies:\n  - python=3.8.3\n  - pip\n  - spacy=2.3.2\n  - scikit-learn=0.23.2\n  - pip:\n    - https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm\n"], ["cryptography==2.8\n", "cryptography\n", "pip install -r requirements.txt\n", "pip freeze > requirements.txt\n"], ["sudo apt install python3-virtualenv\n"], [], ["print (os.environ.get(\"EMAIL_USER\")) \n", "app.config['MAIL_SERVER'] = 'smtp.gmail.com'\napp.config['MAIL_PORT'] = 465\napp.config['MAIL_USE_SSL'] = True\n"], ["\"python.pythonPath\": \"...\\\\your_path\\\\.venv\\\\Scripts\\\\python.exe\",\n\"python.linting.pylintPath\": \"...\\\\your_path\\\\.venv\\\\Scripts\\\\pylint.exe\",\n\"python.formatting.autopep8Path\": \"...\\\\your_path\\\\.venv\\\\Scripts\\\\autopep8.exe\",\n"], ["import pandas as pd\nimport snowflake as sf\nfrom snowflake import connector\n"], ["Traceback (most recent call last):\n  File \"/usr/local/bin/pipenv\", line 8, in <module>\n    sys.exit(cli())\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/decorators.py\", line 73, in new_func\n    return ctx.invoke(f, obj, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/vendor/click/decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/cli/command.py\", line 252, in install\n    site_packages=state.site_packages\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 1928, in do_install\n    site_packages=site_packages,\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 580, in ensure_project\n    pypi_mirror=pypi_mirror,\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 512, in ensure_virtualenv\n    python=python, site_packages=site_packages, pypi_mirror=pypi_mirror\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 999, in do_create_virtualenv\n    project._environment.add_dist(\"pipenv\")\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 135, in add_dist\n    self.extend_dists(dist)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 127, in extend_dists\n    extras = self.resolve_dist(dist, self.base_working_set)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 122, in resolve_dist\n    deps |= cls.resolve_dist(dist, working_set)\n  File \"/usr/local/lib/python3.6/site-packages/pipenv/environment.py\", line 121, in resolve_dist\n    dist = working_set.find(req)\n  File \"/usr/local/lib/python3.6/site-packages/pkg_resources/__init__.py\", line 642, in find\n    raise VersionConflict(dist, req)\npkg_resources.VersionConflict: (importlib-metadata 2.0.0 (/usr/local/lib/python3.6/site-packages), Requirement.parse('importlib-metadata<2,>=0.12; python_version < \"3.8\"'))\n", "Installing collected packages: importlib-metadata\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 2.0.0\n    Uninstalling importlib-metadata-2.0.0:\n      Successfully uninstalled importlib-metadata-2.0.0\nSuccessfully installed importlib-metadata-1.7.0\n", "virtualenv = \"==20.0.31\"\nimportlib-metadata = \"==1.7.0\"\n"], [], [], ["# Install the module and import it :\n!pip install google-cloud-secret-manager\nfrom google.cloud import secretmanager\n\n# Create a Client:\nclient = secretmanager.SecretManagerServiceClient()\nsecret_name = \"my-secret\" # => To be replaced with your secret name\nproject_id = 'my-project' # => To be replaced with your GCP Project\n\n# Forge the path to the latest version of your secret with an F-string:\nresource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\" \n\n# Get your secret :\nresponse = client.access_secret_version(request={\"name\": resource_name})\nsecret_string = response.payload.data.decode('UTF-8')\n\n# Tada ! you secret is in the secret_string variable!\n"], [], ["(ProcMCD43A1) C:\\Users\\justincase>conda compare environment.yml --json\n[\n  \"argon2-cffi found but mismatch. Specification pkg: argon2-cffi==20.1.0=py37he774522_1, Running pkg: argon2-cffi==20.1.0=py37h4ab8f01_1\",\n  \"attrs found but mismatch. Specification pkg: attrs==19.3.0=py_0, Running pkg: attrs==20.2.0=pyh9f0ad1d_0\",\n  \"backcall found but mismatch. Specification pkg: backcall==0.2.0=py_0, Running pkg: backcall==0.2.0=pyh9f0ad1d_0\",\n  \"bleach found but mismatch. Specification pkg: bleach==3.1.5=py_0, Running pkg: bleach==3.2.0=pyh9f0ad1d_0\",\n  \"brotlipy found but mismatch. Specification pkg: brotlipy==0.7.0=py37he774522_1000, Running pkg: brotlipy==0.7.0=py37h4ab8f01_1000\",\n  \"bzip2 found but mismatch. Specification pkg: bzip2==1.0.8=he774522_0, Running pkg: bzip2==1.0.8=he774522_3\",\n  \"cffi found but mismatch. Specification pkg: cffi==1.14.0=py37h7a1dbc1_0, Running pkg: cffi==1.14.3=py37h26f1ce3_0\",\n  \"cfitsio found but mismatch. Specification pkg: cfitsio==3.470=he774522_5, Running pkg: cfitsio==3.470=hbbe6aef_6\",\n  \"cftime found but mismatch. Specification pkg: cftime==1.2.1=py37h2a96729_0, Running pkg: cftime==1.2.1=py37h44b1f71_0\",\n  \"chardet found but mismatch. Specification pkg: chardet==3.0.4=py37_1003, Running pkg: chardet==3.0.4=py37hc8dfbb8_1006\",\n  \"click not found\",\n  \"click-plugins not found\",\n  \"cligj not found\",\n  \"cryptography found but mismatch. Specification pkg: cryptography==2.9.2=py37h7a1dbc1_0, Running pkg: cryptography==3.1=py37h26f1ce3_0\",\n  \"curl found but mismatch. Specification pkg: curl==7.67.0=h2a8f88b_0, Running pkg: curl==7.71.1=h4b64cdc_5\",\n  \"cycler found but mismatch. Specification pkg: cycler==0.10.0=py37_0, Running pkg: cycler==0.10.0=py_2\",\n  \"descartes not found\",\n  \"entrypoints found but mismatch. Specification pkg: entrypoints==0.3=py37_0, Running pkg: entrypoints==0.3=py37hc8dfbb8_1001\",\n  \"fiona not found\",\n  \"freexl found but mismatch. Specification pkg: freexl==1.0.5=hfa6e2cd_0, Running pkg: freexl==1.0.5=hd288d7e_1002\",\n  \"gdal found but mismatch. Specification pkg: gdal==3.0.2=py37hdf43c64_0, Running pkg: gdal==3.1.2=py37h6ddc196_1\",\n  \"geopandas not found\",\n  \"geos found but mismatch. Specification pkg: geos==3.8.0=h33f27b4_0, Running pkg: geos==3.8.1=he025d50_0\",\n  \"geotiff found but mismatch. Specification pkg: geotiff==1.5.1=h5770a2b_1, Running pkg: geotiff==1.6.0=h09e6dc1_1\",\n  \"h5py not found\",\n  \"hdf4 found but mismatch. Specification pkg: hdf4==4.2.13=h712560f_2, Running pkg: hdf4==4.2.13=hf8e6fe8_1003\",\n  \"hdf5 found but mismatch. Specification pkg: hdf5==1.10.4=h7ebc959_0, Running pkg: hdf5==1.10.6=nompi_he0bbb20_101\",\n   ...\n]\n"], ["awesome_code.py\n__init__.py\n    src/\n        __init__.py\n        stuff1.py\n        stuff2.py\n", "awesome_code.py\n    src/\n        __init__.py\n        stuff1.py\n        stuff2.py\n"], ["import spacy\n\nnlp = spacy.load('/opt/anaconda3/envs/NLPENV/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\n"], ["from matplotlib.image import imread\nimport numpy as np\nimport pywt\n   \nA = imread(\"1.jpg\")\noriginal = np.mean(A, -1)\n#rest of your codes\n"], [], [], ["pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n"], ["   {\n        \"python.autoComplete.extraPaths\": [\n            \"./**\",\n        ],\n        \"python.pythonPath\": \"env\\\\Scripts\\\\python.exe\",\n        \"python.languageServer\": \"Microsoft\"\n   }\n"], ["    root/\n    __init__.py  # Empty\n\n        folder/\n            __init__.py # Empty\n\n            sub_folder_b/\n                my_code.py\n            sub_folder_c/\n                another_code.py\n\n", "from folder.sub_folder_b import my_code.py\n", "from root.folder.sub_folder_b import my_code.py\n"], [], ["\"python.languageServer\": \"Jedi\"\n"], ["python3 -m venv my_env\n"], [], ["pip install pywin32\n", "python path\\to\\python\\Scripts\\pywin32_postinstall.py -install\n"], ["temp = temp.append(data, ignore_index = True)\n"], ["if event.type == pygame.MOUSEMOTION:\n    x, y = event.pos\n    if ( x in range(100,100)) and (y in range(200,200)):\n        print(\"Hovering over the item!\")\n        new_cursor()\n    else: default_cursor()\n"], ["import google.auth\nimport google.auth.transport.requests\n\n\n# getting the credentials and project details for gcp project\ncredentials, your_project_id = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n\n#getting request object\nauth_req = google.auth.transport.requests.Request()\n\nprint(credentials.valid) # prints False\ncredentials.refresh(auth_req) #refresh token\n#cehck for valid credentials\nprint(credentials.valid)  # prints True\nprint(credentials.token) # prints token\n"], ["{\n    \"python.pythonPath\": \n           \"/Users/username/.local/share/virtualenvs/Your-Virual-Env/bin/python\"\n}\n"], [], ["class calculator:\n    \n    def addition(x,y):\n        added = x + y\n        print(added)\n        \n    def subtraction(x,y):\n        sub = x - y\n        print(sub)\n\n    def multiplication(x,y):\n        mult = x * y\n        print(mult)\n\n    def division(x,y):\n        div = x / y\n        print(div)\n\ncalculator.addition(2,3)\n"], [], ["calculator_instance = calculator()\n"], ["\"python.languageServer\": \"Jedi\"\n"], ["import pandas as pd\ndef turn_into_csv(data, csver):\n    ids = []\n    texts = []\n    for each in data:\n        texts.append(each[\"full_text\"])\n        ids.append(str(each[\"id\"]))\n\n    df = pd.DataFrame({'ID': ids, 'FULL_TEXT': texts})\n    writer = pd.ExcelWriter(csver + '.xlsx', engine='xlsxwriter')\n    df.to_excel(writer, sheet_name='Sheet1', encoding=\"utf-8-sig\")\n\n    # Close the Pandas Excel writer and output the Excel file.\n    writer.save()\n"], ["ex_dict = {\"milk\":5, \"eggs\":2, \"flour\": 3}\n\ndef sum_rec(it):\n    if isinstance(it, dict):\n        it = iter(it.values())\n    try:\n        v = next(it)\n    except StopIteration:\n        return 0\n    return v + sum_rec(it)\n\nsum_rec(ex_dict)\n# 10\n", "ex_dict = {\"milk\":5, \"eggs\":2, \"flour\": 3}\n\ndef sum_rec(d):\n    try:\n        k,v = d.popitem()\n    except KeyError:\n        return 0\n    return v + sum_rec(d)\n\nsum_rec(ex_dict)\n# 10\n"], ["sum([v for v in ex_dict.values()])\n"], [], ["youtube-dl --download-archive archive.txt \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n"], [], ["{\n    \"python.pythonPath\": \"/usr/local/bin/python3\",\n}\n", "{\n    \"python.pythonPath\": \"/usr/local/bin/python\",\n}\n"], [], ["    {\n        \"python.pythonPath\": \"apis/bin/python\"\n    }\n", "    from app.models import setup_db\n"], ["    \"code-runner.executorMap\": {\n        \"python\": \"python3 -u\",\n    }\n"], ["{\n    \"python.pythonPath\": \"/usr/local/bin/python3\",\n    \"git.ignoreLimitWarning\": true\n}\n"], [], [], [], [], [], [], ["{\n    \"python.pythonPath\": \"/usr/bin/\"\n}\n"], ["!python -m spacy download en_core_web_lg \n", "import spacy\nnlp = spacy.load(\"en_core_web_lg\")\n"], ["{\n    ... # any other settings you have already added (remove this line)\n\n    \"terminal.integrated.shell.windows\": \"C:\\\\WINDOWS\\\\System32\\\\cmd.exe\",\n    \"terminal.integrated.shellArgs.windows\": [\"/K\", \"C:\\\\Anaconda3\\\\Scripts\\\\activate.bat C:\\\\Anaconda3\"],\n    \"python.condaPath\": \"C:\\\\Anaconda3\\\\Scripts\\\\conda.exe\"\n}\n"], [], [], [], ["pipenv install pylint-django\n", "load-plugins=pylint-django\n"], ["\"python.analysis.disabled\": [\n    \"unresolved-import\"\n],\n\"python.linting.pylintArgs\": [\"--load-plugin\",\"pylint_protobuf\"]\n"], [], [], [], ["PYTHONPATH=YOUR/MODULES/PATH\n", "\"python.envFile\": \".env\"\n"], ["{\n    \"python.jediEnabled\": false\n}\n"], ["import pandas as pd\n\nurl = 'https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\npath = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\ndf = pd.read_csv(path)\n"], ["doc = Document()\nadd_page_number(doc.sections[0].footer.paragraphs[0].add_run())\ndoc.sections[0].footer.paragraphs[0].alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\ndoc.save(\"your_doc.docx\")\n", "def create_element(name):\n    return OxmlElement(name)\n\n\ndef create_attribute(element, name, value):\n    element.set(nsqn(name), value)\n\n\ndef add_page_number(paragraph):\n    paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n\n    page_run = paragraph.add_run()\n    t1 = create_element('w:t')\n    create_attribute(t1, 'xml:space', 'preserve')\n    t1.text = 'Page '\n    page_run._r.append(t1)\n\n    page_num_run = paragraph.add_run()\n\n    fldChar1 = create_element('w:fldChar')\n    create_attribute(fldChar1, 'w:fldCharType', 'begin')\n\n    instrText = create_element('w:instrText')\n    create_attribute(instrText, 'xml:space', 'preserve')\n    instrText.text = \"PAGE\"\n\n    fldChar2 = create_element('w:fldChar')\n    create_attribute(fldChar2, 'w:fldCharType', 'end')\n\n    page_num_run._r.append(fldChar1)\n    page_num_run._r.append(instrText)\n    page_num_run._r.append(fldChar2)\n\n    of_run = paragraph.add_run()\n    t2 = create_element('w:t')\n    create_attribute(t2, 'xml:space', 'preserve')\n    t2.text = ' of '\n    of_run._r.append(t2)\n\n    fldChar3 = create_element('w:fldChar')\n    create_attribute(fldChar3, 'w:fldCharType', 'begin')\n\n    instrText2 = create_element('w:instrText')\n    create_attribute(instrText2, 'xml:space', 'preserve')\n    instrText2.text = \"NUMPAGES\"\n\n    fldChar4 = create_element('w:fldChar')\n    create_attribute(fldChar4, 'w:fldCharType', 'end')\n\n    num_pages_run = paragraph.add_run()\n    num_pages_run._r.append(fldChar3)\n    num_pages_run._r.append(instrText2)\n    num_pages_run._r.append(fldChar4)\n\ndoc = Document()\nadd_page_number(doc.sections[0].footer.paragraphs[0])\ndoc.save(\"your_doc.docx\")\n"], [], [], ["from PyPDF2 import PdfFileMerger, PdfFileReader, PdfFileWriter\n\npdf_file = \"C:/Users/11359023/Desktop/deepfake_vee.pdf\"\nwatermark = \"C:/Users/11359023/Desktop/simple.pdf\"\nmerged = \"C:/Users/11359023/Desktop/merged.pdf\"\n\nwith open(pdf_file, \"rb\") as input_file, open(watermark, \"rb\") as watermark_file:\n    input_pdf = PdfFileReader(input_file)\n    watermark_pdf = PdfFileReader(watermark_file)\n    watermark_page = watermark_pdf.getPage(0)\n\n    output = PdfFileWriter()\n\n    for i in range(input_pdf.getNumPages()):\n        pdf_page = input_pdf.getPage(i)\n        pdf_page.mergePage(watermark_page)\n        output.addPage(pdf_page)\n\n    with open(merged, \"wb\") as merged_file:\n        output.write(merged_file)\n"], [], [], [], ["import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n"], [], [], ["import en_core_web_sm\nnlp = en_core_web_sm.load()\n"], [], [], [], ["some_person = Person(\"xyz\")\n"], [], ["from subprocess import PIPE, Popen\n\n\ndef cmdline(command):\n    process = Popen(\n        args=command,\n        stdout=PIPE,\n        shell=True\n    )\n    return process.communicate()[0]\n\n\ntoken = cmdline(\"gcloud auth application-default print-access-token\")\nprint(\"Token:\"+token)\n"], ["def except_zero(items: list) -> Iterable: \n    ans=[]\n    ind=[]\n    for i in range(len(items)): \n        if (items[i] != 0): \n            ans.append(items[i]) \n            ind.append(i)\n\n    ans = list(sorted(ans))\n    for i in range(len(ind)): \n            items[ind[i]] = ans[i] \n    return items\n", "def except_zero(items: list) -> Iterable: \n    # get and sort non zero values\n    non_zero = list(sorted([x for x in items if x!=0]))\n    j = 0 \n    # place them in correct position\n    for i in range(len(items)): \n        if (items[i] != 0): \n            items[i] = non_zero[j]  # get next sorted non_zero value for this position\n            j += 1 # update index\n\n    return items\n"], ["zero_positions = [i for i, this in enum(mylist) if this == 0]\n", "ordered = [this for this in mylist if this != 0].sorted()\n", "for i in zero_positions:\n    ordered.insert(i, 0)\n"], ["def except_zero(items):\n    zeroes = {i for i, v in enumerate(items) if v == 0}\n    others = iter(sorted(v for v in items if v != 0))\n    return [0 if i in zeroes else next(others) for i in range(len(items))]\n", "except_zero([5,3,0,0,4,1,4,0,7])\n", "[1, 3, 0, 0, 4, 4, 5, 0, 7]\n"], ["def sort_and_extract_non_zero_items(items: list) -> Iterable:\n  return sorted([n for n in l if n != 0])\n\ndef replace_non_zero_items(original: list, non_zeroes: list) -> Iterable:\n  return [non_zeroes.pop(0) if n != 0 else 0 for n in original]\n\nnon_zero_list = sort_and_extract_non_zero_items(items)\n\nitems = replace_non_zero_items(items, non_zero_list)\n"], ["orig_list = [5,3,0,0,4,1,4,0,7]\nresult = sorted([i for i in orig_list if i!=0])\nfor inx, elem in enumerate(orig_list):\n    if elem == 0:\n        result.insert(inx, 0)\nprint(result)  # [1,3,0,0,4,4,5,0,7]\n"], [], ["\"terminal.integrated.shellArgs.windows\": [\n    \"-ExecutionPolicy\",\n    \"ByPass\",\n    \"-NoExit\",\n    \"-Command\",\n    \"your-path-to-\\\\conda-hook.ps1\",\n    \";conda activate 'your-path-to-the-base-conda-environment'\"\n]\n"], [], ["num1 = 5\nnum2 = 1\nnum3 = 0\nreturn num1 > num2 and num2 > num3 # returns True because all the conditions are true\nreturn num1 > num2 and num1: # returns True because num1>num2 and num1 is not 0\nreturn num1 > num2 and num3: # returns False because one of the conditions is False becuase num3 is 0\n", "if num1 > num2 and num3: # if num1 > num2 and num3 is not 0 \n\nelif num2 > num1 and num3: # if num2 > num1 and num3 is not 0\n\nelif num3 > num1 and num2: # if num3 > num1 and num2 is not 0\n", "def max_num(num1, num2, num3):\n  if num1 > num2 and num1 > num3:\n    return num1\n  elif num2 > num1 and num2 > num3:\n    return num2\n  return num3\n\nprint(max_num(-10, 0, 10)) # returns 10\nprint(max_num(-10, 5, -30)) # returns 5\nprint(max_num(-5, -10, -10)) # returns -5\n"], ["def max_num(num1, num2, num3):\n  if num1 > num2 and num1 > num3:\n    return num1\n  elif num2 > num1 and num2 > num3:\n    return num2\n  elif num3 > num1 and num3 > num2:\n    return num3\n\nprint(max_num(-10, 0, 10)) # returns 10\nprint(max_num(-10, 5, -30)) # returns 5\nprint(max_num(-5, -10, -10)) # returns -5\n", "def max_num(num1, num2, num3):\n    if num1 > num2:\n        if num1 > num3:\n            return num1\n        else:\n            return num3\n    else:\n        if num2 > num3:\n            return num2\n        else:\n            return num3\n\nprint(max_num(-10, 0, 10)) # returns 10\nprint(max_num(-10, 5, -30)) # returns 5\nprint(max_num(-5, -10, -10)) # returns -5\n"], [], [], [], ["{\n    \"python.pythonPath\": \".venv/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.pylintEnabled\": true,\n    \"python.linting.pycodestyleEnabled\": false,\n    \"python.linting.flake8Enabled\": false,\n    \"python.linting.pylintPath\": \".venv/bin/pylint\",\n    \"python.linting.pylintArgs\": [\n        \"--load-plugins=pylint_django\",\n    ],\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackArgs\": [\n        \"--line-length\",\n        \"100\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.nosetestsEnabled\": false,\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestPath\": \".venv/bin/pytest\",\n    \"python.envFile\": \"${workspaceFolder}/.env\",\n    \"python.testing.pytestArgs\": [\n        \"--no-cov\"\n    ],\n}\n"], [], [], ["# function to find an index corresponding\n# to current value minus offset value\ndef prevInd(series, offset, date):\n    offset = to_offset(offset)\n    end_date = date - offset\n    end = series.index.searchsorted(end_date, side=\"left\")\n    return end\n\n# function to find an index corresponding\n# to the first value greater than current\n# it is useful when one has timeseries with non-unique\n# but monotonically increasing values\ndef nextInd(series, date):\n    end = series.index.searchsorted(date, side=\"right\")\n    return end\n\ndef twoColumnsRoll(dFrame, offset, usecols, fn, columnName = 'twoColRol'):\n    # find all unique indices\n    uniqueIndices = dFrame.index.unique()\n    numOfPoints = len(uniqueIndices)\n    # prepare an output array\n    moving = np.zeros(numOfPoints)\n    # nameholders\n    price = dFrame[usecols[0]]\n    qty   = dFrame[usecols[1]]\n\n    # iterate over unique indices\n    for ii in range(numOfPoints):\n        # nameholder\n        pp = uniqueIndices[ii]\n        # right index - value greater than current\n        rInd = afta.nextInd(dFrame,pp)\n        # left index - the least value that \n        # is bigger or equal than (pp - offset)\n        lInd = afta.prevInd(dFrame,offset,pp)\n        # call the actual calcuating function over two arrays\n        moving[ii] = fn(price[lInd:rInd], qty[lInd:rInd])\n    # construct and return DataFrame\n    return pd.DataFrame(data=moving,index=uniqueIndices,columns=[columnName])\n"], ["import numpy as np\nimport pandas as pd\nfrom numpy_ext import rolling_apply\n\n\ndef masscenter(price, nQty):\n    return np.sum(price * nQty) / np.sum(nQty)\n\n\ndf = pd.DataFrame( [['02:59:47.000282', 87.60, 739],\n                    ['03:00:01.042391', 87.51, 10],\n                    ['03:00:01.630182', 87.51, 10],\n                    ['03:00:01.635150', 88.00, 792],\n                    ['03:00:01.914104', 88.00, 10]], \n                   columns=['stamp', 'price','nQty'])\ndf['stamp'] = pd.to_datetime(df['stamp'], format='%H:%M:%S.%f')\ndf.set_index('stamp', inplace=True, drop=True)\n\nwindow = 2\ndf['y'] = rolling_apply(masscenter, window, df.price.values, df.nQty.values)\nprint(df)\n\n                            price  nQty          y\nstamp                                             \n1900-01-01 02:59:47.000282  87.60   739        NaN\n1900-01-01 03:00:01.042391  87.51    10  87.598798\n1900-01-01 03:00:01.630182  87.51    10  87.510000\n1900-01-01 03:00:01.635150  88.00   792  87.993890\n1900-01-01 03:00:01.914104  88.00    10  88.000000\n"], ["conda list -n python3 --export > python3-packages.txt\nconda list -n pytorch_p36 --export > pytorch_p36-packages.txt\nFC python3-packages.txt pytorch_p36-packages.txt\n"], ["python $HOME/.vscode/extensions/ms-python.python-$VERSION/pythonFiles/testing_tools/run_adapter.py discover pytest -- --rootdir $ROOT_DIR_OF_YOUR_PROJECT -s --cache-clear\nTest Discovery failed: \nError: ============================= test session starts ==============================\n<SETTINGS RELATED TO YOUR MACHINE, PYTHON VERSION, PYTEST VERSION,...>\ncollected N items / M errors\n...\n"], [], [], ["status = models.ForeignKey(Status, on_delete=models.PROTECT,db_constraint=False)\n\n"], ["!conda install -c anaconda seaborn -y\n"], [], [], [], ["import en_core_web_sm\n\nnlp = en_core_web_sm.load()\n"], ["\"terminal.integrated.shellArgs.windows\": [\n       \"PowerShell -NoExit -File C:\\\\ProgramFiles\\\\Anaconda\\\\shell\\\\condabin\\\\conda-hook.ps1\"\n    ]\n"], [], ["df.sort_index(inplace= True)\n"], [], [], ["DISM.exe /Online /Add-Capability /CapabilityName:Microsoft.WebDriver~~~~0.0.1.0\n"], ["sudo pip install awscli --ignore-installed six\n"], [], [], ["python -m pip install scrapy --user\n"], ["def get_pdf_searchable_pages(fname):\n    # pip install pdfminer\n    from pdfminer.pdfpage import PDFPage\n    searchable_pages = []\n    non_searchable_pages = []\n    page_num = 0\n    with open(fname, 'rb') as infile:\n\n        for page in PDFPage.get_pages(infile):\n            page_num += 1\n            if 'Font' in page.resources.keys():\n                searchable_pages.append(page_num)\n            else:\n                non_searchable_pages.append(page_num)\n    if page_num > 0:\n        if len(searchable_pages) == 0:\n            print(f\"Document '{fname}' has {page_num} page(s). \"\n                  f\"Complete document is non-searchable\")\n        elif len(non_searchable_pages) == 0:\n            print(f\"Document '{fname}' has {page_num} page(s). \"\n                  f\"Complete document is searchable\")\n        else:\n            print(f\"searchable_pages : {searchable_pages}\")\n            print(f\"non_searchable_pages : {non_searchable_pages}\")\n    else:\n        print(f\"Not a valid document\")\n\n\nif __name__ == '__main__':\n    get_pdf_searchable_pages(\"1.pdf\")\n    get_pdf_searchable_pages(\"1Scanned.pdf\")\n\n", "Document '1.pdf' has 1 page(s). Complete document is searchable\nDocument '1Scanned.pdf' has 1 page(s). Complete document is non-searchable\n"], [], ["%%bash\n\npip install seaborn\n"], ["    \"terminal.integrated.shellArgs.windows\": [\n        \"/K\", \n        \"C:\\\\Programs\\\\Anaconda3\\\\Scripts\\\\activate.bat & conda activate py37\"\n    ]\n"], ["import subprocess as sp\nimport re\n\noutput = sp.getoutput(\"ocrmypdf input.pdf output.pdf\")\nif not re.search(\"PriorOcrFoundError: page already has text!\",output):\n   print(\"Uploaded scanned pdf\")\nelse:\n   print(\"Uploaded digital pdf\")\n"], [], [], ["pip uninstall pypiwin32\npip install pywin32\n"], ["from sqlalchemy.sql import text\n...\ncursor.execute(text(<whatever_needed_to_be_casted>))\n"], ["!pip install -U spacy download en_core_web_sm\n!pip install -U spacy download en_core_web_sm\n"], [], [], [], [], [], ["apt-get install python3 python3-pip redis-server\npip3 install pandas pyarrow redis\n", "import pandas as pd\nimport pyarrow as pa\nimport redis\n\ndf=pd.DataFrame({'A':[1,2,3]})\nr = redis.Redis(host='localhost', port=6379, db=0)\n\ncontext = pa.default_serialization_context()\nr.set(\"key\", context.serialize(df).to_buffer().to_pybytes())\ncontext.deserialize(r.get(\"key\"))\n   A\n0  1\n1  2\n2  3\n"], [], [], ["export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/legacy_credentials/<me>/adc.json\n"], [], [], [], ["\"python.jediEnabled\": true\n"], ["Rank.KING.value[0]\nRank.KING.value[1]\n"], [], [], ["import numpy as np\nimport matplotlib.pyplot as plt\nimport pywt\nimport pywt.data\n# Load image\noriginal = pywt.data.camera()\n# Wavelet transform of image, and plot approximation and details\ntitles = ['Approximation', ' Horizontal detail', 'Vertical detail', 'Diagonal detail']\ncoeffs2 = pywt.dwt2(original, 'bior1.3')\nLL, (LH, HL, HH) = coeffs2\nfig = plt.figure(figsize=(12, 3))\nfor i, a in enumerate([LL, LH, HL, HH]):\nax = fig.add_subplot(1, 4, i + 1)\nax.imshow(a, interpolation=\"nearest\", cmap=plt.cm.gray)\nax.set_title(titles[i], fontsize=10)\nax.set_xticks([])\nax.set_yticks([])\nfig.tight_layout()\nplt.show()\n"], ["import cv2\nimport numpy as np\nimport pywt\n\nimage = cv2.imread('1.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Convert to float for more resolution for use with pywt\nimage = np.float32(image)\nimage /= 255\n\n# ...\n# Do your processing\n# ...\n\n# Convert back to uint8 OpenCV format\nimage *= 255\nimage = np.uint8(image)\n\ncv2.imshow('image', image)\ncv2.waitKey(0)\n"], [], [], ["document = Document(\"my-template.docx\")\n"], [], [], ["import pandas as pd\nimport requests\nfrom io import StringIO\n\nurl = requests.get('https://doc-0g-78-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/5otus4mg51j69f99n47jgs0t374r46u3/1560607200000/09837260612050622056/*/0B6GhBwm5vaB2ekdlZW5WZnppb28?e=download')\ncsv_raw = StringIO(url.text)\ndfs = pd.read_csv(csv_raw)\n"], ["from collections import OrderedDict\nmodel = nn.Sequential(OrderedDict([\n                  ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n                  ('relu1', nn.ReLU()),\n                  ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n                  ('relu2', nn.ReLU()),\n                  ('output', nn.Linear(hidden_sizes[1], output_size)),\n                  ('softmax', nn.Softmax(dim=1))]))\n", "Parameter containing:\ntensor([[-7.3584e-03, -2.3753e-02, -2.2565e-02,  ...,  2.1965e-02,\n      1.0699e-02, -2.8968e-02],\n    [ 2.2930e-02, -2.4317e-02,  2.9939e-02,  ...,  1.1536e-02,\n      1.9830e-02, -1.4294e-02],\n    [ 3.0891e-02,  2.5781e-02, -2.5248e-02,  ..., -1.5813e-02,\n      6.1708e-03, -1.8673e-02],\n    ...,\n    [-1.2596e-03, -1.2320e-05,  1.9106e-02,  ...,  2.1987e-02,\n     -3.3817e-02, -9.4880e-03],\n    [ 1.4234e-02,  2.1246e-02, -1.0369e-02,  ..., -1.2366e-02,\n     -4.7024e-04, -2.5259e-02],\n    [ 7.5356e-03,  3.4400e-02, -1.0673e-02,  ...,  2.8880e-02,\n     -1.0365e-02, -1.2916e-02]], requires_grad=True)\n"], ["conda list -n python3 --export > python3-packages.txt\nconda list -n pytorch_p36 --export > pytorch_p36-packages.txt\ndiff python3-packages.txt pytorch_p36-packages.txt\n"], [], [], ["import enum\n\nclass TestMeta(enum.EnumMeta):\n    def __call__(cls, value, names=None, *, module=None, qualname=None, type=None, start=1):\n        if names is not None:\n            # the enum is being constructed via the functional syntax\n            return super().__call__(value, names=names, module=module, qualname=qualname, type=type, start=start)\n\n        try:\n            # attempt to get an enum member\n            return super().__call__(value, names=names, module=module, qualname=qualname, type=type, start=start)\n        except ValueError:\n            # no such member exists, but we don't care\n            return value\n\nclass Test(enum.Enum, metaclass=TestMeta):\n    A = 5\n    B = 6    \n\nprint(Test(5), Test(6), Test(7))\n", "class TestMeta(enum.EnumMeta):\n    def __call__(cls, value, names=None, module=None, type=None, start=1):\n        if names is not None:\n            return enum.EnumMeta.__call__(cls, value, names, module, type, start)\n\n        try:    \n            return enum.EnumMeta.__call__(cls, value, names, module, type, start)\n        except ValueError:\n            return value\n"], ["from aenum import Enum, extend_enum\n\nclass Foo(Enum):\n    _order_ = 'A B C'   # because Python 2 does not keep order\n    A = 1\n    B = 2\n    C = 3\n\n    def __str__(self):\n        return self.name\n\n    @classmethod\n    def _missing_(cls, value):\n        extend_enum(cls, str(value), (value, ))\n        return list(cls)[-1]\n"], ["known_values = {'a':1,'b':2,'c':1}    # only one of 'a' and 'c' will be used\n                                      # 'a' guaranteed in 3.7+, implementation-specific before\n                                      # (https://stackoverflow.com/q/39980323)\nextracted_values = (1,2,3,4,5)\n\nknown_values_reverse = {}\nfor k,v in known_values.items():\n    if v not in known_values_reverse:\n        known_values_reverse[v]=k\n    del k,v    #if placed outside the loop, will error if known_values is empty\n\nfor v in extracted_values:\n    if v not in known_values_reverse:\n        known_values_reverse[v]=str(v)\n    del v\n\nAutoFooEnum = enum.Enum(value='AutoFooEnum',names=(k,v for v,k in known_values_reverse.items()))\n"], [], [], [], [], ["[default]\naws_access_key_id=AKIAIOSFODNN7EXAMPLE\naws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n", "!pip install -U -q PyDrive\n", "# Imports\nimport os\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\nfrom google.colab import auth\nfrom oauth2client.client import GoogleCredentials\n\n# Google drive authentication\nauth.authenticate_user()\ngauth = GoogleAuth()\ngauth.credentials = GoogleCredentials.get_application_default()\ndrive = GoogleDrive(gauth)\n\n# File params\nlocal_save_dir = \"/root/.aws\"\nfilename = \"credentials\"\nsave_path = \"{0}/{1}\".format(local_save_dir, filename)\n\n# Choose/create a local (colab) directory to store the data.\nlocal_download_path = os.path.expanduser(local_save_dir)\ntry:\n  os.makedirs(local_download_path)\nexcept: pass\n\ndrive_list = drive.ListFile().GetList()\nf = [x for x in drive_list if x[\"title\"] == filename][0]\n\nprint('title: %s, id: %s' % (f['title'], f['id']))\nfname = os.path.join(local_download_path, f['title'])\nprint('downloading to {}'.format(fname))\nf_ = drive.CreateFile({'id': f['id']})\nf_.GetContentFile(fname)\n\nwith open(save_path) as creds:\n    for i, line in enumerate(creds):\n        if i == 1:\n            access_token_key = line.replace(\"aws_access_key_id=\", \"\").replace(\"\\n\", \"\")\n        if i == 2:\n            access_token_secret = line.replace(\"aws_secret_access_key=\", \"\").replace(\"\\n\", \"\")\n"], [], [], [">>> KING = 13, 'King'\n>>> KING\n(13, 'King')\n>>> KING[0]\n13\n>>> KING[1]\n'King'\n>>> \n"], ["order = Order.objects.create(user=user, customer_name=name, customer_phone=phone, status_id=1)\n"], ["from getpass import getpass\nsecret = getpass('Enter the secret value: ')\n"], [], ["import tokenize\nfrom io import BytesIO\nfrom collections import deque\ncode_string = \"\"\"\n# A comment.\ndef foo(a, b):\n  return a + b\n\nclass Bar(object):\n  def __init__(self):\n\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\n  def test(self): pass\n  def abc(self):\n    '''multi-\n    line token'''\n\ndef baz():\n  return [\n1,\n  ]\n\nclass Baz(object):\n  def hello(self, x):\n    a = \\\n1\n    return self.hello(\nx - 1)\n\ndef my_type_annotated_function(\n  my_long_argument_name: SomeLongArgumentTypeName\n) -> SomeLongReturnTypeName:\n  pass\n  # unmatched parenthesis: (\n\"\"\".strip()\nfile = BytesIO(code_string.encode())\ntokens = deque(tokenize.tokenize(file.readline))\nlines = []\nwhile tokens:\n    token = tokens.popleft()\n    if token.type == tokenize.NAME and token.string == 'def':\n        start_line, _ = token.start\n        last_token = token\n        while tokens:\n            token = tokens.popleft()\n            if token.type == tokenize.NEWLINE:\n                break\n            last_token = token\n        if last_token.type == tokenize.OP and last_token.string == ':':\n            indents = 0\n            while tokens:\n                token = tokens.popleft()\n                if token.type == tokenize.NL:\n                    continue\n                if token.type == tokenize.INDENT:\n                    indents += 1\n                elif token.type == tokenize.DEDENT:\n                    indents -= 1\n                    if not indents:\n                        break\n                else:\n                    last_token = token\n        lines.append((start_line, last_token.end[0]))\nprint(lines)\n", "[(2, 3), (6, 11), (13, 13), (14, 16), (18, 21), (24, 27), (29, 33)]\n", "a = \\\n1\n", "TokenInfo(type=53 (OP), string=':', start=(24, 20), end=(24, 21), line='  def hello(self, x):\\n')\nTokenInfo(type=4 (NEWLINE), string='\\n', start=(24, 21), end=(24, 22), line='  def hello(self, x):\\n')\nTokenInfo(type=5 (INDENT), string='    ', start=(25, 0), end=(25, 4), line='    a = 1\\n')\nTokenInfo(type=1 (NAME), string='a', start=(25, 4), end=(25, 5), line='    a = 1\\n')\nTokenInfo(type=53 (OP), string='=', start=(25, 6), end=(25, 7), line='    a = 1\\n')\nTokenInfo(type=2 (NUMBER), string='1', start=(25, 8), end=(25, 9), line='    a = 1\\n')\nTokenInfo(type=4 (NEWLINE), string='\\n', start=(25, 9), end=(25, 10), line='    a = 1\\n')\nTokenInfo(type=1 (NAME), string='return', start=(26, 4), end=(26, 10), line='    return self.hello(\\n')\n"], ["import re\n\ncode_string = \"\"\"\n# A comment.\ndef foo(a, b):\n  return a + b\nclass Bar(object):\n  def __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\ndef baz():\n  return [\n1,\n  ]\n\nclass Baz(object):\n  def hello(self, x):\n    return self.hello(\nx - 1)\n\ndef my_type_annotated_function(\n  my_long_argument_name: SomeLongArgumentTypeName\n) -> SomeLongReturnTypeName:\n  # This function's indentation isn't unusual at all.\n  pass\n\ndef test_multiline():\n    \\\"\"\"\n    asdasdada\nsdadd\n    \\\"\"\"\n    pass\n\ndef test_comment(\n    a #)\n):\n    return [a,\n    # ]\na]\n\ndef test_escaped_endline():\n    return \"asdad \\\nasdsad \\\nasdas\"\n\ndef test_nested():\n    return {():[[],\n{\n}\n]\n}\n\ndef test_strings():\n    return '\\\"\"\" asdasd' + \\\"\"\"\n12asd\n12312\n\"asd2\" [\n\\\"\"\"\n\n\\\"\"\"\ndef test_fake_def_in_multiline()\n\\\"\"\"\n    print(123)\na = \"def in_string():\"\n  def after().\n    print(\"NOPE\")\n\n\\\"\"\"Phew this ain't valid syntax\\\"\"\" def something(): pass\n\n\"\"\".strip()\n\ncode_string += '\\n'\n\n\nfunc_list=[]\nfunc = ''\ntab  = ''\nbrackets = {'(':0, '[':0, '{':0}\nclose = {')':'(', ']':'[', '}':'{'}\nstring=''\ntab_f=''\nc1=''\nmultiline=False\ncheck=False\nfor line in code_string.split('\\n'):\n    tab = re.findall(r'^\\s*',line)[0]\n    if re.findall(r'^\\s*def', line) and not string and not multiline:\n        func += line + '\\n'\n        tab_f = tab\n        check=True\n    if func:\n        if not check:\n            if sum(brackets.values()) == 0 and not string and not multiline:\n                if len(tab) <= len(tab_f):\n                    func_list.append(func)\n                    func=''\n                    c1=''\n                    c2=''\n                    continue\n            func += line + '\\n'\n        check = False\n    for c0 in line:\n        if c0 == '#' and not string and not multiline:\n            break\n        if c1 != '\\\\':\n            if c0 in ['\"', \"'\"]:\n                if c2 == c1 == c0 == '\"' and string != \"'\":\n                    multiline = not multiline\n                    string = ''\n                    continue\n                if not multiline:\n                    if c0 in string:\n                        string = ''\n                    else:\n                        if not string:\n                            string = c0\n            if not string and not multiline:\n                if c0 in brackets:\n                    brackets[c0] += 1\n                if c0 in close:\n                    b = close[c0]\n                    brackets[b] -= 1\n        c2=c1\n        c1=c0\n\nfor f in func_list:\n    print('-'*40)\n    print(f)\n", "----------------------------------------\ndef foo(a, b):\n  return a + b\n\n----------------------------------------\n  def __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\n----------------------------------------\ndef baz():\n  return [\n1,\n  ]\n\n----------------------------------------\n  def hello(self, x):\n    return self.hello(\nx - 1)\n\n----------------------------------------\ndef my_type_annotated_function(\n  my_long_argument_name: SomeLongArgumentTypeName\n) -> SomeLongReturnTypeName:\n  # This function's indentation isn't unusual at all.\n  pass\n\n----------------------------------------\ndef test_multiline():\n    \"\"\"\n    asdasdada\nsdadd\n    \"\"\"\n    pass\n\n----------------------------------------\ndef test_comment(\n    a #)\n):\n    return [a,\n    # ]\na]\n\n----------------------------------------\ndef test_escaped_endline():\n    return \"asdad asdsad asdas\"\n\n----------------------------------------\ndef test_nested():\n    return {():[[],\n{\n}\n]\n}\n\n----------------------------------------\ndef test_strings():\n    return '\"\"\" asdasd' + \"\"\"\n12asd\n12312\n\"asd2\" [\n\"\"\"\n\n----------------------------------------\n  def after():\n    print(\"NOPE\")\n"], ["code_string = \"\"\"\n#A comment\ndef foo(a, b):\n  return a + b\n\ndef bir(a, b):\n  c = a + b\n  return c\n\nclass Bar(object):\n  def __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\ndef baz():\n  return [\n1,\n  ]\n\n\"\"\".strip()\n\nlines = code_string.split('\\n')\n\n#looking for lines with 'def' keywords\ndefidxs = [e[0] for e in enumerate(lines) if 'def' in e[1]]\n\n#getting the indentation of each 'def'\nindents = {}\nfor i in defidxs:\n    ll = lines[i].split('def')\n    indents[i] = len(ll[0])\n\n#extracting the strings\nend = len(lines)-1\nwhile end > 0:\n    if end < defidxs[-1]:\n        defidxs.pop()\n    try:\n        start = defidxs[-1]\n    except IndexError: #break if there are no more 'def'\n        break\n\n    #empty lines between functions will cause an error, let's remove them\n    if len(lines[end].strip()) == 0:\n        end = end -1\n        continue\n\n    try:\n        #fix lines removing indentation or compile will not compile\n        fixlines = [ll[indents[start]:] for ll in lines[start:end+1]] #remove indentation\n        body = '\\n'.join(fixlines)\n        compile(body, '<string>', 'exec') #if it fails, throws an exception\n        print(body)\n        end = start #no need to parse less line if it succeed.\n    except:\n        pass\n\n    end = end -1\n", "def baz():\n  return [\n1,\n  ]\ndef __init__(self):\n  self.my_list = [\n      'a',\n      'b',\n  ]\ndef bir(a, b):\n  c = a + b\n  return c\ndef foo(a, b):\n  return a + b\n"], [], [], [], ["def __post_init__(self):\n    object.__setattr__(self, 'value', RANKS.index(self.rank) + 1)\n"], [], [], [], [], [], [], [">>> s = \"Hello world\\n\\n\"\n>>> s.rstrip(\"\\n\")\n'Hello world'\n", ">>> s = \"Hello world\\n\\n\"\n>>> len(s) - len(s.rstrip(\"\\n\"))\n2\n"], ["s = \"Hello world\\n\\n\"\ns.count(\"\\n\")\n", "s.strip(\"\\n\")\n"], ["s = \"Hello world\\n\\n\" \ns.count('\\n')\n", "s.strip('\\n')\n"], ["count = s.count(\"\\n\");\n"], ["{\n  \"alg\": \"RS256\",\n  \"typ\": \"JWT\",\n  \"kid\": \"42ba1e234ac91ffca687a5b5b3d0ca2d7ce0fc0a\"\n}\n", "{\n  \"iss\": \"myservice@myproject.iam.gserviceaccount.com\",\n  \"iat\": 1493833746,\n  \"aud\": \"myservice.appspot.com\",\n  \"exp\": 1493837346,\n  \"sub\": \"myservice@myproject.iam.gserviceaccount.com\"\n}\n", "curl -v -X GET -H \"Authorization: Bearer <access_token_here>\" ^\nhttps://www.googleapis.com/compute/v1/projects/%PROJECT_ID%/zones/%ZONE%/instances/%INSTANCE_NAME%/start\n", "from google.auth import compute_engine\n\ncred = compute_engine.Credentials()\n", "SCOPES = ['https://www.googleapis.com/auth/sqlservice.admin']\nSERVICE_ACCOUNT_FILE = 'service-account-credentials.json'\n\nfrom google.oauth2 import service_account\n\ncred = service_account.Credentials.from_service_account_file(\n            SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n", "from google_auth_oauthlib.flow import InstalledAppFlow\n\nflow = InstalledAppFlow.from_client_secrets_file(\n    'client_secrets.json',\n    scopes=scope)\n\ncred = flow.run_local_server(\n    host='localhost',\n    port=8088,\n    authorization_prompt_message='Please visit this URL: {url}',\n    success_message='The auth flow is complete; you may close this window.',\n    open_browser=True)\n", "from requests_oauthlib import OAuth2Session\n\ngcp = OAuth2Session(\n        app.config['gcp_client_id'],\n        scope=scope,\n        redirect_uri=redirect_uri)\n\n# print('Requesting authorization url:', authorization_base_url)\n\nauthorization_url, state = gcp.authorization_url(\n                        authorization_base_url,\n                        access_type=\"offline\",\n                        prompt=\"consent\",\n                        include_granted_scopes='true')\n\nsession['oauth_state'] = state\n\nreturn redirect(authorization_url)\n\n\n# Next section of code after the browser approves the request\n\ntoken = gcp.fetch_token(\n            token_url,\n            client_secret=app.config['gcp_client_secret'],\n            authorization_response=request.url)\n"]]