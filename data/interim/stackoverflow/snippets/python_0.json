[["labels[:, -1] = -100  # Typically, -100 is used to ignore the loss calculation at specific positions\n"], ["from selenium import webdriver\nimport undetected_chromedriver as uc\n\nmy_options = webdriver.ChromeOptions()\nmy_options.add_argument( '--log-level=3' )\nmy_options.add_argument( '--no-sandbox' )\nmy_options.add_argument( '--disable-dev-shm-usage' )\nmy_options.add_argument( '--disable-blink-features=AutomationControlled' )\nmy_options.add_argument( 'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36' )\nmy_options.add_argument( '--no-first-run' ) # this might be specific to undetected_chromedriver.v2 only\nmy_options.add_argument( '--no-service-autorun' ) # this might be specific to undetected_chromedriver.v2 only\nmy_options.add_argument( '--password-store=basic' ) # this might be specific to undetected_chromedriver.v2 only\n#my_options.add_experimental_option( 'useAutomationExtension', False )\n#my_options.add_experimental_option( 'excludeSwitches', ( 'enable-automation', ) )\nmy_options.add_argument( '--start-maximized' )\nmy_options.add_argument( '--blink-settings=imagesEnabled=false' )\nmy_options.headless = False\nmy_options.page_load_strategy = 'normal'\n\nmy_driver = uc.Chrome( options = my_options, version_main = 109 )\nmy_driver.get( 'about:blank' )\nmy_driver.execute_script( \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\" )\nmy_driver.get( ..............\n"], ["pip install pipwin \npipwin install pyaudio\n"], [], [], ["model_checkpoint = ModelCheckpoint(\n    filepath=\"tmp_file.keras\",\n    options=None\n)\n"], ["    cd /usr/.local/lib/python3.5/site-packages/virtualenv-16.1.0.dist-info/\n    \n    touch METADATA\n"], ["class ModelBase(pydantic.BaseModel):\n  a: int\n  b: str\n\n\nclass ModelCreate(ModelBase):\n  pass\n\n# Make all fields optional\n@make_optional()\nclass ModelUpdate(ModelBase):\n  pass\n"], ["from typing_extensions import Annotated\nfrom datetime import datetime, timezone\nfrom pydantic import BaseModel, PlainSerializer, BeforeValidator\n\n\nCustomDatetime = Annotated[\n    datetime,\n    BeforeValidator(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M%z').astimezone(tz=timezone.utc)),\n    PlainSerializer(lambda x: x.strftime('%Y-%m-%dT%H:%M:%SZ'))\n]\n\n\nclass MyModel(BaseModel):\n    datetime_in_utc_with_z_suffix: CustomDatetime\n\n\nif __name__ == \"__main__\":\n    special_datetime = MyModel(datetime_in_utc_with_z_suffix=\"2042-3-15T12:45+01:00\")  # note the different timezone\n\n    # input conversion\n    print(special_datetime.datetime_in_utc_with_z_suffix)  # 2042-03-15 11:45:00+00:00\n\n    # output conversion\n    print(special_datetime.model_dump_json())  # {\"datetime_in_utc_with_z_suffix\": \"2042-03-15T11:45:00Z\"}\n"], [], ["conda install -c conda-forge prophet\n"], ["def add_translatable_field_to_model(model, field_name, field):\n    \"\"\"\n    This is functionally identical, except usage syntax.\n\n    Example usage:\n\n        # models.py\n        class Acme(models.Model):\n            code = models.CharField()\n\n        add_translatable_field_to_model(Acme, \"name\", models.CharField(blank=True, verbose_name=_(\"common name\")))\n        add_translatable_field_to_model(Acme, \"description\", models.TextField())\n    \"\"\"\n    for code, name in settings.LANGUAGES:\n        field.clone().contribute_to_class(model, f\"{field_name}_{code}\")\n\n    def local_translation_getter(self):\n        language_code = get_language_code()\n        return getattr(self, f\"{field_name}_{language_code}\", \"\")\n\n    setattr(model, field_name, property(local_translation_getter))\n", "def get_language_code():\n    \"\"\"\n    Gets the two letter code of the currently selected language, or the default \n   language if none specified, set by Django LocaleMiddleware.\n    \"\"\"\n    language = translation.get_language()\n    if language is None:\n        return settings.DEFAULT_LANGUAGE\n    available_language_codes = set(code for code, _ in settings.LANGUAGES)\n    if language not in available_language_codes and \"-\" in language:\n        language = language.split(\"-\")[0]\n    if language in available_language_codes:\n        return language\n    return settings.DEFAULT_LANGUAGE\n"], ["libdevice not found at ./libdevice.10.bc\n         [[{{node Adam/StatefulPartitionedCall_88}}]] [Op:__inference_train_function_10134]\n"], [], [], ["user_grade = int(input())\n\nif 9 <= user_grade <= 12:\n\n    print('in high school')\n\nelse:\n\n    print('not in high school')\n"], ["conda install -n base conda-libmamba-solver\n", "conda install tensorflow --solver=libmamba\n", "conda config --set solver libmamba\n"], [], [], ["import urllib.request\nimport ssl\n\n# Create a secure SSL context\nssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS)  # Use the appropriate protocol\n\nurl = \"https://example.com\"  # Replace with your URL\nresponse = urllib.request.urlopen(url, context=ssl_context)\n\n# Read and process the response\ndata = response.read().decode(\"utf-8\")\n"], [], ["custom_ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\ncustom_ssl_context.options |= 0x00040000 # OP flag SSL_OP_ALLOW_UNSAFE_LEGACY_RENEGOTIATION\n\nconnector = aiohttp.TCPConnector(ssl=custom_ssl_context)\n\nasync with aiohttp.ClientSession(connector=connector) as session:\n    async with session.get(url) as response:\n        return await response.text()\n", "custom_ssl_context.check_hostname = False\ncustom_ssl_context.verify_mode = ssl.CERT_NONE\n"], ["documents = SimpleDirectoryReader('../news').load_data()\nindex = GPTVectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine()\nr = query_engine.query(\"how did the pandemic effect business\")\nprint(r)\n"], ["RuntimeError: cannot reuse already awaited coroutine\n", "def auth_required(func):\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        return func(*args, **kwargs) #DO NOT WAIT\n    return wrapper\n\n@app.post(\"/\")\n@auth_required # Custom decorator\ndef root(payload: SampleModel): #NOT ASYNC\n    return {\"message\": \"Hello World\", \"payload\": payload}\n"], ["\"python.pythonPath\": \"python3\",\n\"code-runner.executorMap\": {\n    \"python3\": \"/usr/bin/python3\"\n}\n", "\"python.pythonPath\": \"python3\",\n\"code-runner.executorMap\": {\n    \"python\": \"/usr/bin/python3\"\n}\n"], ["def fix_field_types(self):\n    for key, value in self.asdict().items():\n        field = self.__dataclass_fields__[key]\n        if not field.type == type(value):\n            new_value = field.type.__call__(value)\n            self.__setattr__(field.name, new_value)\n"], ["pip uninstall imblearn --yes\n", "conda install -c conda-forge imbalanced-learn\n"], ["FOR /F \"usebackq delims=\" %G IN (requirements.txt) DO poetry add --lock %G\n"], [], ["from selenium import webdriver \nfrom selenium.webdriver.chrome.options import Options\n\nchrome_options = Options()\n# chrome_options.add_argument(\"--disable-extensions\")\n# chrome_options.add_argument(\"--disable-gpu\")\n# chrome_options.add_argument(\"--no-sandbox\") # linux only\nchrome_options.add_argument(\"--headless=new\") # for Chrome >= 109\n# chrome_options.add_argument(\"--headless\")\n# chrome_options.headless = True # also works\ndriver = webdriver.Chrome(options=chrome_options)\nstart_url = \"https://duckgo.com\"\ndriver.get(start_url)\nprint(driver.page_source.encode(\"utf-8\"))\n# b'<!DOCTYPE html><html xmlns=\"http://www....\ndriver.quit()\n"], [], ["list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflattened_list = [element for sublist in list_of_lists for element in sublist]\nprint(flattened_list)\n", "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflattened_list = list(itertools.chain(*list_of_lists))\nprint(flattened_list)\n", "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflattened_list = sum(list_of_lists, [])\nprint(flattened_list)\n", "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"], ["df['ab_weighted'] = df.groupby('c').transform(pd.Series.sum, axis=0).apply(lambda x: x.a/x.b, axis=1)\n\n   a   b  c  ab_weighted\n0  1   7  q    0.294118\n1  2   8  q    0.294118\n2  3   9  q    0.294118\n3  4  10  q    0.294118\n4  5  11  w    0.478261\n5  6  12  w    0.478261\n", "df.groupby('c')[['a','b']].sum().assign(ab_weighted = lambda x: x.a/x.b)\n\n    a   b  ab_weighted\nc                     \nq  10  34     0.294118\nw  11  23     0.478261\n"], [], [], [], ["pip uninstall decouple\n", "pip install python-decouple\n"], [], [], ["kosinkie_l@Fedora ~/project/build $ python -c \"import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))\"\n\n2022-08-09 15:31:03.414926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\ntensorflow version 2.10.0-rc0\nhello, Tensor(\"MatMul:0\", shape=(1, 1), dtype=float32)\nkosinkie_l@Fedora ~/project/build $\n", "131 #ifndef __AVX__\n132     CheckIfFeatureUnused(CPUFeature::AVX, \"AVX\", missing_instructions);\n133 #endif  // __AVX__\n134 #ifndef __AVX2__\n135     CheckIfFeatureUnused(CPUFeature::AVX2, \"AVX2\", missing_instructions);\n136 #endif  // __AVX2__\n...\n192     if (!missing_instructions.empty()) {\n193       LOG(INFO) << \"This TensorFlow binary is optimized with \"\n194                 << \"oneAPI Deep Neural Network Library (oneDNN) \"\n195                 << \"to use the following CPU instructions in performance-\"\n196                 << \"critical operations: \" << missing_instructions << std::endl\n197                 << \"To enable them in other operations, rebuild TensorFlow \"\n198                 << \"with the appropriate compiler flags.\";\n199     }\n"], [], ["##++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Function to check all elements in a df column is same\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\ndef check_all_are_equal_in_a_column_df(data_frame, column_name):\n    data_list = data_frame[column_name].unique()\n    data_set = set(data_list)\n    if((len(data_set)) == 1):\n        return_data = 1\n    else:\n        return_data = 0\n    return return_data\n"], [], [], [], [], [], ["sudo apt-get install python3-dev default-libmysqlclient-dev build-essential pkg-config\n"], [], [], [], ["n = int(input())\n\nfor x in range(1,n+1,1):\n    y = 3 * x + 2\n    if y % 4 != 0:\n       print(y)\n"], [], [], ["    str1 := \"bash\"\n    str2 := \"-c\"\n    str3 := \"pip install --no-cache -r requirements.txt -t /asset-output && cp -au . /asset-output\"\n    command := []*string{&str1, &str2, &str3}\n\n    lambdaFn := awslambda.NewFunction(stack, jsii.String(\"foo\"), &awslambda.FunctionProps{\n        Runtime: awslambda.Runtime_PYTHON_3_9(),\n        Handler: jsii.String(\"index.lambda_handler\"),\n        Timeout: awscdk.Duration_Seconds(jsii.Number(900)),\n        Code: awslambda.Code_FromAsset(jsii.String(\"lambda/foo/blah\"), &awss3assets.AssetOptions{\n            Bundling: &awscdk.BundlingOptions{\n                Image:   awslambda.Runtime_PYTHON_3_9().BundlingImage(),\n                Command: &command,\n            },\n        }),\n    })\n"], [], ["from selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.get(\"https://www.selenium.dev/selenium/web/web-form.html\")\n", "from selenium import webdriver\nchrome_driver_path = 'C:/Users/Morteza/Documents/Dev/chromedriver.exe'\ndriver = webdriver.Chrome(executable_path=chrome_driver_path)\n\nurl = \"https://www.google.com\"\ndriver.get(url)\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\ns=Service('C:/Users/Morteza/Documents/Dev/chromedriver.exe')\nbrowser = webdriver.Chrome(service=s)\nurl='https://www.google.com'\nbrowser.get(url)\n"], ["    options = ChromeOptions()\n    options.add_argument(\"start-maximized\")\n    options.binary_location = \"/opt/brave.com/brave/brave\"\n\n    browser = Chrome(options=options)\n"], [], ["import plotly.io as pio\n\npio.write_image(fig, \"figname.png\") \n"], ["ax.set_xticks(ax.get_xticks())\nax.set_yticks(ax.get_yticks())\n"], [], ["import pandas as pd\ndf = pd.DataFrame(df_list, columns=['{column_name1}','{column_name2}'])\n"], [], ["pip3 install tensorflow[and-cuda]\n"], ["if not files_present:\n    pd.to_csv(filename)\nelse:\n    print 'WARNING: This file already exists!'\n"], [], ["py -m ensurepip --upgrade\n"], [], [], [], ["def solution(A, K):\n    n = len(A)\n    return [A[(i-K)%n] for i in range(n)]\n"], ["{\n  .\n  .\n  \"autopep8.args\": [\"--max-line-length\", \"180\"]\n  .\n  .\n}\n"], ["    def push_pd_to_bq(self, df: pd.DataFrame, tabel: str, project: str, mode: str = 'append'):\n    df = df.copy()\n    df['commit_timestamp'] = pd.Timestamp('now', tz='Europe/Helsinki')\n    table_schema = [] # [{'name': 'col1', 'type': 'STRING'},...]\n    for col in df.columns:\n        if 'object' in str(df[col].dtype):\n             table_schema.append({'name': col, 'type': 'STRING'})\n        elif 'float' in str(df[col].dtype):\n            table_schema.append({'name': col, 'type': 'FLOAT64'})\n        elif 'datetime' in str(df[col].dtype):\n            table_schema.append({'name': col, 'type': 'TIMESTAMP'})\n        elif 'date' in col.lower():\n             table_schema.append({'name': col, 'type': 'DATE'})\n    df.to_gbq(destination_table=tabel, \n              if_exists=mode, \n              project_id=project, \n              table_schema=table_schema, \n              location='europe-north1')\n"], ["color_1 = int(input())\ncolor_2 = int(input())\ncolor_3 = int(input())\ngray = 50\nmini = 0\n\nmini= (min(color_1, color_2, color_3))\n\ncolor_1 = color_1 - mini\ncolor_2 = color_2 - mini\ncolor_3 = color_3 - mini\n\nprint(color_1, color_2, color_3)\n"], ["from setuptools import setup, find_packages\n\nsetup(\n    ...\n    packages=find_packages('src', exclude=['test']),\n    package_dir = {\"\": \"src\"},\n    ...\n)\n"], [], [], [], [], ["sudo pkill 'uvicorn'\n"], ["from werkzeug.contrib.fixers import ProxyFix\n", "from werkzeug.middleware.proxy_fix import ProxyFix\n"], [], ["from playwright.sync_api import Playwright, sync_playwright\nimport time\n\n\ndef run(playwright: Playwright) -> None:\n    browser = playwright.chromium.launch(headless=False)\n    context = browser.new_context()\n\n    # Open new page\n    page = context.new_page()\n\n    page.goto('https://www.youtube.com/')\n\n    # page.mouse.wheel(horizontally, vertically(positive is \n    # scrolling down, negative is scrolling up)\n    for i in range(5): #make the range as long as needed\n        page.mouse.wheel(0, 15000)\n        time.sleep(2)\n        \n    \n    time.sleep(15)\n    # ---------------------\n    context.close()\n    browser.close()\n\n\nwith sync_playwright() as playwright:\n    run(playwright)\n"], ["embed = discord.Embed(title=\"Title here\", description=\"\",\n                              timestamp=datetime.utcnow(),\n                              color=0x26ad00)\n\nfile = discord.File(f\"images/{msg.id}.png\")\nembed.set_image(url=f\"attachment://{msg.id}.png\")\nawait msg.edit(embed=embed, attachments=[file])\n"], ["sudo apt update\nsudo apt install python3.X-dev\n"], ["from sklearn.preprocessing import LabelEncoder, StandardScaler\n\nX = lbl_encoder_1.fit_transform(X)\n", "sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n", "model.fit(X_train, y_train, batch_size = 10, epochs = 100)\n", "x_train = np.asarray(x_train).astype(np.float32)\n"], [], ["from . import widgets \nFile \"C:\\Users\\my_folder\\flask_env\\lib\\site-packages\\flask_wtf\\recaptcha\\widgets.py\", line 6, in <module>\nJSONEncoder = json.JSONEncoder\nAttributeError: module 'flask.json' has no attribute 'JSONEncoder'\n"], [], [], ["\"pylint.args\": [\n\"\\\"--generated-members\\\", \\\"torch.*\\\"\"\n"], ["pip install setuptools==58.2.0\n"], [], ["pip install keras --upgrade \n"], ["brew install mariadb\npip install mariadb\n"], [], ["    client.execute(\n        'SELECT * FROM test',\n        settings={'allow_experimental_object_type': 1}\n )\n[({'b': 1}, {'x': 2})]\n"], [], [], [], ["write_videofile(result_path, bitrate=bitrate, threads=64, verbose=False, preset='ultrafast', ffmpeg_params=['-loglevel', 'panic'], codec=\"libx264\")\n"], [], [], [], [], [], ["pip install aiohttp==3.9.0b0\n"], ["connexion[swagger-ui]<3\nflask>=2.0  \nWerkzeug>=2.0\ngunicorn>=20.0\n", "Successfully installed Werkzeug-2.2.3 connexion-2.14.2 flask-2.2.5\n"], [], ["torch.cuda.empty_cache()\ngc.collect()\n", "import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n", "def __getitem__(self, i_dex, resize_=(320,480)):\n        transforms_ = transforms.Compose([\n                                        transforms.PILToTensor(),\n                        transforms.ConvertImageDtype(torch.float32),\n                                        ])\n        im_ = Image_.open(self.data_paths[i_dex])\n        if im_.mode !='RGB':\n            im_ = im_.convert('RGB')\n        im_ = im_.resize(resize_)\n      \n        return transforms_(im_), labels[i_dex]\n"], [], ["pip install aiohttp==3.9.0b0\n"], [], ["list_of_dfs = [df1, df2, dfx]\n# now remove all columns from the dataframes which are empty or have all-NA \ncleaned_list_of_dfs = [df.dropna(axis=1, how='all') for df in list_of_dfs]\noutput_df = pd.concat(cleaned_list_of_dfs)\n", "output_df= pd.concat(df.dropna(axis=1, how='all') for df in [df1, df2])\n"], [], [], ["for /f %i in (requirements.txt) do (poetry add %i)\n"], [], ["\"python.condaPath\": \"C:\\\\apps\\\\Anaconda3\\\\Scripts\\\\\",\n\"python.venvPath\": \"C:\\\\apps\\\\Anaconda3\\\\envs\",\n\"terminal.integrated.defaultProfile.windows\": \"Command Prompt\",\n", "\"python.condaPath\": \"C:\\\\apps\\\\Anaconda3\\\\Scripts\\\\\",\n\"python.venvPath\": \"C:\\\\apps\\\\Anaconda3\\\\envs\",\n"], ["pip install setuptools\n"], [], [], [], ["callbacks = [\nkeras.callbacks.ModelCheckpoint(\n    filepath=\"convnet_from_scratch.tf\",\n    save_best_only=True,\n    monitor=\"val_loss\",\n    save_format=\"tf\")\n]\nhistory = model.fit(\n    train_dataset,\n    epochs=30,\n    validation_data=validation_dataset,\n    callbacks=callbacks)\n", "test_model = keras.models.load_model(\"convnet_from_scratch.tf\")\ntest_loss, test_acc = test_model.evaluate(test_dataset)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n"], [], [], ["\"[python]\": {\n        \"editor.defaultFormatter\": \"ms-python.autopep8\",\n        \"autopep8.args\": [\n            \"--max-lin-length\",\n            \"120\",\n            \"--experimental\"\n        ]\n  }\n"], ["aiohttp==3.9.0b0\n"], [], [], [], [], ["# FixedFormatter should only be used together with FixedLocator. \n# Otherwise, one cannot be sure where the labels will end up.\n", "from matplotlib import ticker\n\npositions = [0, 1, 2, 3, 4, 5]\nlabels = ['A', 'B', 'C', 'D', 'E', 'F']\nax.xaxis.set_major_locator(ticker.FixedLocator(positions))\nax.xaxis.set_major_formatter(ticker.FixedFormatter(labels))\n", "# FuncFormatter can be used as a decorator\n@ticker.FuncFormatter\ndef major_formatter(x, pos):\n    return f'{x:.2f}'\n", "ax.xaxis.set_major_locator(ticker.LogLocator(base=10, numticks=5))\nax.xaxis.set_major_formatter(major_formatter)\n"], [], ["\"jupyter.interactiveWindow.textEditor.executeSelection\": true\n"], ["#region conda initialize\n# !! Contents within this block are managed by 'conda init' !!\n(& \"C:\\Users\\USER\\anaconda3\\Scripts\\conda.exe\" \"shell.powershell\" \"hook\") | Out-String | Invoke-Expression\n#endregion\n"], ["df1 = pd.DataFrame({\"A\": [.1, .2, .3]})\ndf2 = pd.DataFrame(columns=[\"A\"], dtype=\"object\")\n\nout = pd.concat([df1, df2]) ; print(out)\n\n     A\n0  0.1\n1  0.2\n2  0.3\n", "out = (df1.copy() if df2.empty else df2.copy() if df1.empty\n       else pd.concat([df1, df2]) # if both DataFrames non empty\n      )\n", "out = pd.concat([df1.astype(df2.dtypes), df2.astype(df1.dtypes)])\n"], ["query_enginge = index.as_query_engine()\nresponse = query_engine.query(\"My query\")\n"], ["from PIL import Image as pil\nfrom pkg_resources import parse_version\nif parse_version(pil.__version__)>=parse_version('10.0.0'):\n    Image.ANTIALIAS=Image.LANCZOS\n"], [], [], ["user_grade = int(input())\n\nif 9 <= user_grade and user_grade <= 12:\n    print('in high school')\nelse:\n    print('not in high school')\n"], [], [], ["import threading\n\nclass RunThread(threading.Thread):\n    def __init__(self, func, args, kwargs):\n        self.func = func\n        self.args = args\n        self.kwargs = kwargs\n        self.result = None\n        super().__init__()\n\n    def run(self):\n        self.result = asyncio.run(self.func(*self.args, **self.kwargs))\n\ndef run_async(func, *args, **kwargs):\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        loop = None\n    if loop and loop.is_running():\n        thread = RunThread(func, args, kwargs)\n        thread.start()\n        thread.join()\n        return thread.result\n    else:\n        return asyncio.run(func(*args, **kwargs))\n", "async def test(name):\n    await asyncio.sleep(5)\n    return f\"hello {name}\"\n\nrun_async(test, \"user\")  # blocks for 5 seconds and returns \"hello user\"\n"], ["mkdir layer\ncp requirements.txt layer/requirements.txt\ndocker run -ti -v $(pwd)/layer:/app -w /app --entrypoint /bin/bash public.ecr.aws/lambda/python:3.11 -c \"pip3 install --target ./python -r requirements.txt\"\n"], ["def as_form(cls):\n    new_params = [\n        inspect.Parameter(\n            field_name,\n            inspect.Parameter.POSITIONAL_ONLY,\n            default=model_field.default,\n            annotation=Annotated[model_field.annotation, *model_field.metadata, Form()],\n        )\n        for field_name, model_field in cls.model_fields.items()\n    ]\n\n    cls.__signature__ = cls.__signature__.replace(parameters=new_params)\n\n    return cls\n", "def before_validate_int(value: int) -> int:\n    raise ValueError('before int')\n\n\nMyInt = Annotated[int, BeforeValidator(before_validate_int)]\n\n\n@as_form\nclass User(BaseModel):\n    age: MyInt\n\n\n@app.post(\"/postdata\")\ndef postdata(user: User = Depends()):\n    return {\"age\": user.age}\n", "{\n  \"detail\": [\n    {\n      \"type\": \"value_error\",\n      \"loc\": [\n        \"body\",\n        \"age\"\n      ],\n      \"msg\": \"Value error, before int\",\n      \"input\": \"12\",\n      \"ctx\": {\n        \"error\": {}\n      },\n      \"url\": \"https://errors.pydantic.dev/2.3/v/value_error\"\n    }\n  ]\n}\n\n"], [], [], ["python3 --version\npython3 -m pip install ipykernel\npython3 -m ipykernel install --user\n", "python3 -m pip install <package_name>\n"], [], ["from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss\n"], [], [], [], [], ["packages=find_packages(\n    where='src',\n    include=['mypackage'],\n),\n", "[tool.setuptools.packages.find]\nwhere = [\"src\"]\ninclude = [\"mypackage*\"]\n"], ["    data_types = {'program_type':'string','program_number':'int64','program_isan':'string','program_title':'string'}\n    df = pd.read_csv(local_file_path, dtype=data_types)\n"], ["FROM python:alpine\napk update && apk upgrade; \\\napk add pkgconfig; \\\napk add --no-cache gcc musl-dev mariadb-dev mariadb-connector-c-dev; \\\n"], ["same_val = np.amax(x) == np.amin(x) and len(x) > 1\n"], ["pip uninstall scikit-learn\n", "pip install scikit-learn==1.2.2\n"], [], ["class AllOptional(ModelMetaclass):\ndef __new__(self, name, bases, namespaces, **kwargs):\n    annotations = namespaces.get('__annotations__', {})\n    for base in bases:\n        optionals = {\n            key: Optional[value] if not key.startswith('__') else value for key, value in base.__annotations__.items()\n        }\n        annotations.update(optionals)\n\n    namespaces['__annotations__'] = annotations\n    return super().__new__(self, name, bases, namespaces, **kwargs)\n"], [], ["class AllOptional(pydantic.main.ModelMetaclass):\n    def __new__(mcls, name, bases, namespaces, **kwargs):\n        cls = super().__new__(mcls, name, bases, namespaces, **kwargs)\n        for field in cls.__fields__.values():\n            field.required=False\n        return cls\n"], ["pip install python-dotenv\n", "apt install python3-dotenv\n"], [], [], ["d0: 2021-01-01 00:00:00+00:00, timezone: UTC\n\nd1: 2021-01-01 00:00:00+00:00, timezone: UTC\n\nd2: 2021-01-01 00:00:00+00:00, timezone: UTC\n\nd3: 1 validation error for Datapoint\ntimestamp\n  Input should have timezone info [type=timezone_aware, input_value='2021-01-01T00:00:00', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.3/v/timezone_aware\n\nd4: 1 validation error for Datapoint\ntimestamp\n  Value error, Timezone must be UTC [type=value_error, input_value='2021-01-01T00:00:00+02:00', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.3/v/value_error\n"], ["from pydantic import BaseModel, create_model\n\nclass Item(BaseModel):\n    name: str\n    description: str\n    price: float\n    tax: float\n\nUpdateItem = create_model(\n    'UpdateItem',\n    __base__=Item,\n    **{k: (v.annotation, None) for k, v in Item.model_fields.items()}\n)\n", "In [410]: Item.model_fields\nOut[410]: \n{'name': FieldInfo(annotation=str, required=True),\n 'description': FieldInfo(annotation=str, required=True),\n 'price': FieldInfo(annotation=float, required=True),\n 'tax': FieldInfo(annotation=float, required=True)}\n\nIn [411]: UpdateItem.model_fields\nOut[411]: \n{'name': FieldInfo(annotation=str, required=False),\n 'description': FieldInfo(annotation=str, required=False),\n 'price': FieldInfo(annotation=float, required=False),\n 'tax': FieldInfo(annotation=float, required=False)}\n\nIn [412]: UpdateItem()\nOut[412]: UpdateItem(name=None, description=None, price=None, tax=None)\n"], ["{\n    \"key\": \"ctrl+enter\",\n    \"command\": \"extension.pycmd\",\n    \"when\": \"editorTextFocus && editorLangId == 'python'\"\n}\n"], [" ax2.xaxis.set_major_locator(mdates.DayLocator(bymonthday=ttick_solar))\n ax2.set_xticklabels(label_solar)\n"], [], [], ["!sudo apt-get install python<your-python-version>-distutils\n", "sudo apt-get install python3.8-distutils\n"], [], [], [], [], [], [], ["pip uninstall urllib3\npip install 'urllib3<2.0'\n"], [], [], [], [], [], ["python -m build  # builds both sdist and wheel\n"], [], [], [], [], [], ["brew uninstall youtube-dl\nbrew install --HEAD youtube-dl    \n"], ["docker ps # to get the CONTAINER ID\ndocker stop <CONTAINER ID>\n"], [], [], [], [], ["import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n\nimport tensorflow as tf\n"], [], ["sudo apt update && sudo apt install pkg-config\n"], [], [], ["def save_model_checkpoint(epoch, logs):\n    if logs['val_loss'] < save_model_checkpoint.best_val_loss:\n        save_model_checkpoint.best_val_loss = logs['val_loss']\n        model.save_weights(new_base_dir / 'model_checkpoint')\n        print('Model checkpoint saved.')\n\nsave_model_checkpoint.best_val_loss = float('inf')\n\n# Initialize your model here\n\nmodel.fit(\n    # Other model.fit arguments...\n    callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=save_model_checkpoint)]\n )\n"], [], ["FROM python:3.11-alpine\nWORKDIR /usr/src/app\nCOPY requirements.txt .\nRUN apk add --no-cache --virtual build-deps gcc musl-dev libffi-dev2 pkgconf mariadb-dev && \\\n    apk add --no-cache mariadb-connector-c-dev && \\\n    pip install --no-cache-dir -r requirements.txt && \\\n    apk del build-deps\nCOPY . .\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n", "mysqlclient==2.2.0\nDjango~=4.2.0\n"], ["img = cv2.resize(img,(int(model_height*ratio),model_height),interpolation=Image.ANTIALIAS)AttributeError: module 'PIL.Image' has no attribute 'ANTIALIAS'\n", "pip uninstall Pillow\npip install Pillow==9.5.0\n"], ["keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch.keras\",\n        ..)\n", "keras.callbacks.ModelCheckpoint(\n        filepath=\"convnet_from_scratch.x\",\n        ..)\n"], [], ["FROM python:3.11.3-slim-bullseye\n\nRUN apt-get update \\\n    && apt-get upgrade -y \\\n    && apt-get install -y gcc default-libmysqlclient-dev pkg-config \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /usr/src/app\nCOPY . .\n\nRUN pip install --upgrade pip \\\n    && pip install mysqlclient \\\n    && pip install -r requirements.txt\n\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n"], [], [], [], ["import PIL\nimport numpy as np\n\n# Gradient image with a sharp color boundary across the diagonal\nlarge_arr = np.fromfunction(lambda x, y, z: (x+y)//(z+1),\n                            (256, 256, 3)).astype(np.uint8)\nlarge_img = PIL.Image.fromarray(large_arr)\n\n# Resize it: PIL.Image.LANCZOS also works here\nsmall_img = large_img.resize((128, 128), PIL.Image.Resampling.LANCZOS)\nprint(small_img.size)\n\nlarge_img.show()\nsmall_img.show()\n\n"], [], ["from llama_index import GPTVectorStoreIndex, ..\n"], [], [], ["    # Latex/Mactex needed \n    # To install on macOS:\n    # brew install --cask mactex\n    # eval \"$(/usr/libexec/path_helper)\"\n    jupyter nbconvert --to pdf my_notebook.ipynb\n"], ["python -m venv venv\n"], [], ["urllib3<2\n"], [], ["import os\n\nimage_width = 1280\nimage_height = 720\n\ndef yolo_to_voc_convertion(input_file, output_file):\n    with open(input_file, 'r') as f:\n        lines = f.readlines()\n\n    new_lines = list()\n    for line in lines:\n        data = line.strip().split(' ')\n\n        class_id = int(data[0])\n        x_center = float(data[1])\n        y_center = float(data[2])\n        width = float(data[3])\n        height = float(data[4])\n\n        x_min = int((x_center - (width / 2)) * image_width)\n        y_min = int((y_center - (height / 2)) * image_height)\n        x_max = int((x_center + (width / 2)) * image_width)\n        y_max = int((y_center + (height / 2)) * image_height)\n\n        new_data = f'{class_id} {x_min} {y_min} {x_max} {y_max}\\n'\n        new_lines.append(new_data)\n\n    with open(output_file, 'w') as f:\n        f.writelines(new_lines)\n\ninput_folder = '..' # folder that includes .txt files\noutput_folder = '..' # output folder that will be included new format ann files\n\nfor filename in os.listdir(input_folder):\n    if filename.endswith('.txt'):\n        input_file = os.path.join(input_folder, filename)\n        output_file = os.path.join(output_folder, filename)\n\n        yolo_to_voc_convertion(input_file, output_file)\n"], ["conda create -n <environment_name> python=3.8.3\n\nconda activate <environment_name>\n", "code\n"], [], ["sudo apt install git\n", "sudo apt install pip\n", "sudo pip install --upgrade --force-reinstall \"git+https://github.com/ytdl-org/youtube-dl.git\"\n"], ["driver = webdriver.Chrome(ChromeDriverManager().install())\n\n# Or\ns = Service('C:/Users/Downloads/chromedriver/chromedriver.exe')\ndriver = webdriver.Chrome(service=s)\n\n# Or\ndriver = webdriver.Chrome('/path/to/chromedriver')\n", "from selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://www.google.com\")\n"], [], [], ["model.resize_token_embeddings(len(tokenizer))\n", "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n"], ["from typing import Optional, Type, Any, Tuple\nfrom copy import deepcopy\n\nfrom pydantic import BaseModel, create_model\nfrom pydantic.fields import FieldInfo\n\n\ndef partial_model(model: Type[BaseModel]):\n    def make_field_optional(field: FieldInfo, default: Any = None) -> Tuple[Any, FieldInfo]:\n        new = deepcopy(field)\n        new.default = default\n        new.annotation = Optional[field.annotation]  # type: ignore\n        return new.annotation, new\n    return create_model(\n        f'Partial{model.__name__}',\n        __base__=model,\n        __module__=model.__module__,\n        **{\n            field_name: make_field_optional(field_info)\n            for field_name, field_info in model.__fields__.items()\n        }\n    )\n", "@partial_model\nclass Model(BaseModel):\n    i: int\n    f: float\n    s: str\n\n\nModel(i=1)\n"], ["FROM python:3.11-alpine\nWORKDIR /usr/src/app\nCOPY requirements.txt .\nRUN apk update\nRUN apk add pkgconfig\nRUN apk add --no-cache gcc musl-dev mariadb-connector-c-dev \nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\nCMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"]\n"], [], [], ["requests==2.28.2\n"], ["conda activate venv\n"], ["# Set OP_LEGACY_SERVER_CONNECT option\nOpenSSL::SSL::SSLContext::DEFAULT_PARAMS[:options] |= OpenSSL::SSL::OP_LEGACY_SERVER_CONNECT\n\n# Make a request\nuri = URI('https://example.com')\nres = Net::HTTP.post(uri, {}.to_json)\n\n# Unset OP_LEGACY_SERVER_CONNECT option\nOpenSSL::SSL::SSLContext::DEFAULT_PARAMS[:options] &= ~OpenSSL::SSL::OP_LEGACY_SERVER_CONNECT\n"], [], ["pip uninstall Flask  \n", "pip install Flask==2.2.3 \n"], ["pip install .\n"], ["def solution(A): \n     \n    num = 0\n\n    for i in range(len(A)): \n        num = num ^ A[i]        \n         \n    return num\n"], [], [], [">>> PatchPoll()\nPatchPoll(id=UUID('dcd80011-e81e-41fb-872b-4f82839a2a76'), subject='', description='')\n>>> PatchPoll().__fields_set__\nset()\n>>> PatchPoll(subject=\"jskdlfjk\").__fields_set__\n{'subject'}\n", "def remove_defaults(baseclass: Type[T]) -> Type[T]:\n    validators = {\"__validators__\": baseclass.__validators__}\n    fields = baseclass.__fields__\n\n    def remove_default(item: pydantic.fields.ModelField) -> pydantic.fields.FieldInfo:\n        info = item.field_info\n        if info.default == pydantic.fields.Undefined and not info.default_factory:\n            raise RuntimeError(\"Field has no default\")\n\n        # Funny enough, if we don't keep the default for Optional types,\n        # openapi-generator will not make it optional at all.\n        if item.allow_none:\n            return copy.copy(item.field_info)\n\n        return pydantic.Field(\n            alias=item.field_info.alias,\n            title=item.field_info.title,\n            description=item.field_info.description,\n            exclude=item.field_info.exclude,\n            include=item.field_info.include,\n            const=item.field_info.const,\n            gt=item.field_info.gt,\n            ge=item.field_info.ge,\n            lt=item.field_info.lt,\n            le=item.field_info.le,\n            multiple_of=item.field_info.multiple_of,\n            allow_inf_nan=item.field_info.allow_inf_nan,\n            max_digits=item.field_info.max_digits,\n            decimal_places=item.field_info.decimal_places,\n            min_items=item.field_info.min_items,\n            max_items=item.field_info.max_items,\n            unique_items=item.field_info.unique_items,\n            min_length=item.field_info.min_length,\n            max_length=item.field_info.max_length,\n            allow_mutation=item.field_info.allow_mutation,\n            regex=item.field_info.regex,\n            discriminator=item.field_info.discriminator,\n            repr=item.field_info.repr,\n        )\n\n    nondefault_fields = {\n        key: (item.type_, remove_default(item)) for key, item in fields.items()\n    }\n\n    return pydantic.create_model(\n        __model_name=f\"{baseclass.__name__}Optional\",\n        __base__=baseclass,\n        __validators__=validators,\n        **nondefault_fields,\n    )\n\n\nclass PatchPoll(pydantic.BaseModel):\n    id: UUID = pydantic.Field(default_factory=uuid4)\n    subject: str = pydantic.Field(max_length=1024, default=\"\")\n    description: Optional[str] = pydantic.Field(max_length=1024 * 1024, default=\"\")\n\n\nclass Poll(remove_defaults(PatchPoll)):\n    ...\n"], ["def solution(A, K):\n\n        n = len(A)`enter code here`\n        K = K % n # Ensure K is within the range of array size \n        rotated_arr = A[K:] + A[:K] # Rotating from end to the starting\n        return rotated_arr\n    A = [1, 2, 3, 4]\n    K = int(input())\n    rotated_arr = solution(A, K)\n    print(rotated_arr)\n"], [], ["async scrollIntoView (locator : Locator) {\n        let i = 0;\n        while(await locator.isHidden()) {\n            await this.page.locator('your locator goes here').click();\n            await this.page.mouse.wheel(0, 300);\n            i++;\n            if (await locator.isVisible()) { return; }\n            else if (i >= 5) { return; }\n        }\n    }\n"], ["sudo apt-get install python3-dev\n"], [], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(r\"C:\\chromedriver.exe\")\noptions = webdriver.ChromeOptions()\ndriver = webdriver.Chrome(service=service, options=options)\n"], ["from json import JSONEncoder\n"], ["from flask_wtf import FlaskForm\n", "Flask-WTF~=1.1.1\n"], ["python3 -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz\n", "python3 /Library/Frameworks/Python.framework/Versions/3.7/bin/yt-dlp --no-check-certificate \"https://www.youtube.com/watch?v=QvkQ1B3FBqA\"\n"], [], [], [], [], [], [], [], ["nvidia-smi\n", ">>> import torch\n>>> torch.zeros(1).cuda()\n"], ["sudo apt install cuda-11-8\n"], ["$ conda install pytorch torchvision torchaudio cudatoolkit=11.1\n", "$ conda list pytorch\npytorch                   2.0.0               py3.9_cpu_0    pytorch\npytorch-mutex             1.0                         cpu    pytorch\ncudatoolkit               11.1.1              heb2d755_10    conda-forge\n", "$ conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n$ conda install -c anaconda cudatoolkit\n", "$ conda list pytorch\npytorch                   2.0.0           py3.9_cuda11.8_cudnn8_0    pytorch\npytorch-cuda              11.8                 h24eeafa_3    pytorch\npytorch-mutex             1.0                        cuda    pytorch\ncudatoolkit               11.3.1               h59b6b97_2    anaconda\n"], [], ["for /f \"tokens=*\" %%i in (requirements.txt) do (\n    poetry add %%i\n)\n"], [], [], ["import ssl\nimport urllib.request\n\nurl = 'http://....'\n\n# Set up SSL context to allow legacy TLS versions\nctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\nctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\n\n# Use urllib to open the URL and read the content\nresponse = urllib.request.urlopen(url, context=ctx)\n"], [], [], [], [], ["sudo apt-get install cuda-toolkit\n"], [], [], ["python3 -m venv --system-site-packages /c/python/python39/python39_venv\ncd /c/python/python39/python39_venv\nsource /c/python/python39/python39_venv/source/bin/activate\npip install {module of choice}=={version of choice}\n", "/c/Python/Python39/Scripts/python.exe -m venv --system-site-packages /c/python/python39/python39_venv\n", "View > Command Palette > type: \"Python: Select Interpreter\" > type \"C:/Python/Python39/python39_venv/Scripts/python.exe\"\n"], ["openssl_conf = openssl_init\n\n[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nOptions = UnsafeLegacyRenegotiation\n", "OPENSSL_CONF=/path/to/custom/openssl.cnf python your_scraper.py\n", "export OPENSSL_CONF=/path/to/custom/openssl.cnf\npython your_scraper.py\n", "OPENSSL_CONF=/path/to/custom/openssl.cnf\n"], [], ["def render_template(template):\n    \"\"\"decorator to render a template with a context\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n\n            # access request object\n            request = kwargs.get('request')\n\n            context = func(*args, **kwargs)\n            if context is None:\n                context = {}\n            return templates.TemplateResponse(template, {**context, 'request': request})\n        return wrapper\n    return decorator\n"], ["#!/usr/bin/python3\n"], [], ["    ydl = youtube_dl.YoutubeDL({'outtmpl': '%(id)s%(ext)s'})\n    from __future__ import unicode_literals\n    import youtube_dl\n    ydl_opts = {\n         'format': 'bestaudio/best',\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'mp3',\n            'preferredquality': '192'\n        }],\n        'postprocessor_args': [\n            '-ar', '16000'\n        ],\n        'prefer_ffmpeg': True,\n        'keepvideo': True\n    }\n    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n        ydl.download(['link'])\n\nuse this:\nfrom pytube import YouTube\nimport os\n\nyt = YouTube('link')\n\nvideo = yt.streams.filter(only_audio=True).first()\n\nout_file = video.download(output_path=\".\")\n\nbase, ext = os.path.splitext(out_file)\nnew_file = base + '.mp3'\nos.rename(out_file, new_file)\n"], ["df = df.astype(str)\n"], [], [], ["import torch\nimport time\nimport gc\nfrom pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n\ndef clear_gpu_memory():\n    torch.cuda.empty_cache()\n    gc.collect()\n    del variables\n\ndef wait_until_enough_gpu_memory(min_memory_available, max_retries=10, sleep_time=5):\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(torch.cuda.current_device())\n\n    for _ in range(max_retries):\n        info = nvmlDeviceGetMemoryInfo(handle)\n        if info.free >= min_memory_available:\n            break\n        print(f\"Waiting for {min_memory_available} bytes of free GPU memory. Retrying in {sleep_time} seconds...\")\n        time.sleep(sleep_time)\n    else:\n        raise RuntimeError(f\"Failed to acquire {min_memory_available} bytes of free GPU memory after {max_retries} retries.\")\n\n# Usage example\nmin_memory_available = 2 * 1024 * 1024 * 1024  # 2GB\nclear_gpu_memory()\nwait_until_enough_gpu_memory(min_memory_available)\n"], [], [">>> tf.sysconfig.get_build_info() \nOrderedDict([('cpu_compiler', '/usr/bin/x86_64-linux-gnu-gcc-11'), \n('cuda_compute_capabilities', ['compute_86']), \n('cuda_version', '12.0'), ('cudnn_version', '8'), \n('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])\n"], [], [], [], ["# Create conda environment\nconda create --name cuda_venv\nconda activate cuda_venv\n\n# Install pytorch following commands from https://pytorch.org/get-started/locally/ \nconda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia\n", "import torch\nprint(torch.cuda.is_available()) \n"], [], [], [], ["file_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ndf.to_csv(file_name + '_' + file_timestamp + '.csv')\n", "file_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\ndf.to_csv(file_name + '_' + file_timestamp + '.csv')\n"], [], [], ["options.add_argument(\"--headless=new\")\n"], ["   Requires-Dist: aiohttp (==3.8.1)\n", "   Requires-Dist: aiohttp (==3.8.4)\n", " python -pip instal (file_location)/alpaca_trade_api-2.3.0-py3-none-any.whl \n"], [], ["python3 -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz\n", "yt-dlp URL\n", "import yt_dlp as youtube_dl\n"], [], ["kill -9 $(ps -ef | grep uvicorn  | awk '{print $2}')\n", "alias uvicornprocess=\"kill -9 $(ps -ef | grep uvicorn  | awk '{print $2}')\" \n"], [], ["from werkzeug.middleware.profiler import ProfilerMiddleware\napp = ProfilerMiddleware(app)\n"], [], ["!pip install cython\n", "!pip install --force-reinstall virtualenv\n"], ["docker pull selenium/standalone-chrome\ndocker run --rm -d -p 4444:4444 --shm-size=2g selenium/standalone-chrome\n", "driver = webdriver.Remote('http://localhost:4444/wd/hub', webdriver.DesiredCapabilities.CHROME)\ndriver.set_window_size(1280, 1024)\ndriver.get('https://www.google.com')\n"], ["def make_partial_model(model: Type[BaseModel], optional_fields: Optional[list[str]] = None) -> Type[BaseModel]:\n    class NewModel(model):\n        ...\n\n    for field in NewModel.__fields__.values():\n        if not optional_fields or field in optional_fields:\n            field.required = False\n\n    NewModel.__name__ = f'Partial{model.__name__}'\n    return NewModel\n\nPartialRequest = cast(Type[RequestModel], make_partial_model(RequestModel))\n"], [], [], ["pip wheel --no-deps -w dist .\n", "python setup.py bdist_wheel\n"], [], [], [], ["import itertools\n\ndef all_equal(iterable):\n    \"Returns True if all elements are equal\"\n    g = itertools.groupby(iterable)\n    next(g, None)\n    try:\n        return not next(g, False)\n    except TypeError:  # pd.NA next to a different value?\n        return False\n\nall_equal(df.counts)\n", "constant_columns = df.columns[df.apply(all_equal)]\n", "df.counts.min() == df.counts.max()\n"], ["options.add_argument(\"--headless\")\n", "options.add_argument(\"--no-sandbox\");\noptions.add_argument(\"--disable-dev-shm-usage\");\noptions.add_argument(\"--disable-renderer-backgrounding\");\noptions.add_argument(\"--disable-background-timer-throttling\");\noptions.add_argument(\"--disable-backgrounding-occluded-windows\");\noptions.add_argument(\"--disable-client-side-phishing-detection\");\noptions.add_argument(\"--disable-crash-reporter\");\noptions.add_argument(\"--disable-oopr-debug-crash-dump\");\noptions.add_argument(\"--no-crash-upload\");\noptions.add_argument(\"--disable-gpu\");\noptions.add_argument(\"--disable-extensions\");\noptions.add_argument(\"--disable-low-res-tiling\");\noptions.add_argument(\"--log-level=3\");\noptions.add_argument(\"--silent\");\n"], ["{\n\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"Build Python Env\",\n            \"type\": \"shell\",\n            \"group\": {\n                \"kind\": \"build\",\n                \"isDefault\": true\n            },\n            \"linux\": {\n                \"options\": {\n                    \"cwd\": \"${workspaceFolder}\"\n                },\n                \"command\": \"python3 -m venv py_venv && source py_venv/bin/activate && python3 -m pip install --upgrade pip && python3 -m pip install -r requirements.txt && deactivate py_venv\"\n            },\n            \"osx\": {\n                \"options\": {\n                    \"cwd\": \"${workspaceFolder}\"\n                },\n                \"command\": \"python3 -m venv py_venv && source py_venv/bin/activate && python3 -m pip install --upgrade pip && python3 -m pip install -r requirements.txt && deactivate py_venv\"\n            },\n            \"windows\": {\n                \"options\": {\n                    \"shell\": {\n                        \"executable\": \"C:\\\\Windows\\\\system32\\\\cmd.exe\",\n                        \"args\": [\n                            \"/d\",\n                            \"/c\"\n                        ]\n                    },\n                    \"cwd\": \"${workspaceFolder}\"\n                },\n                \"command\": \"(if not exist py_venv py -m venv py_venv) && .\\\\py_venv\\\\Scripts\\\\activate.bat && py -m pip install --upgrade pip && py -m pip install -r requirements.txt && deactivate py_venv\"\n            },\n            \"problemMatcher\": []\n        }\n    ]\n}\n"], [], ["from pydantic import BaseModel, create_model\nfrom typing import Optional\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None) # avoids creating many classes with same name\ndef make_optional(baseclass: Type[BaseModel]) -> Type[BaseModel]:\n    # Extracts the fields and validators from the baseclass and make fields optional\n    fields = baseclass.__fields__\n    validators = {'__validators__': baseclass.__validators__}\n    optional_fields = {key: (Optional[item.type_], None)\n                       for key, item in fields.items()}\n    return create_model(f'{baseclass.__name__}Optional', **optional_fields,\n                        __validators__=validators)\n\nclass Item(BaseModel):\n    name: str\n    description: str\n    price: float\n    tax: float\n\nItemOptional = make_optional(Item)\n", "> Item.__fields__\n\n{'name': ModelField(name='name', type=str, required=True),\n 'description': ModelField(name='description', type=str, required=True),\n 'price': ModelField(name='price', type=float, required=True),\n 'tax': ModelField(name='tax', type=float, required=True)}\n\n> ItemOptional.__fields__\n\n{'name': ModelField(name='name', type=Optional[str], required=False, default=None),\n 'description': ModelField(name='description', type=Optional[str], required=False, default=None),\n 'price': ModelField(name='price', type=Optional[float], required=False, default=None),\n 'tax': ModelField(name='tax', type=Optional[float], required=False, default=None)}\n", "@app.post(\"/items\", response_model=Item)\nasync def post_item(item: Item = Depends()):\n    ...\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: str, item: make_optional(Item) = Depends()):\n    ...\n", "def make_optional_no_id(baseclass):\n    ... # same as make optional\n    optional_fields = {key: (Optional[item.type_], None) \n                       for key, item in fields.items() if key != 'ID'} # take out here ID\n    ... # you can also take out also validators of ID\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item: make_optional_no_id(Item) = Depends()):\n"], ["data = my_series.to_numpy()\n", "data = my_series.to_list()\n", "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n"], [], [], [], ["pip install numpy==1.24.1\n"], ["sudo apt-get install python3.10-distutils\n"], [], ["\"code-runner.executorMap\": {\n    ...\n    \"python\": \"python -u\"\n    ...\n}\n", "\"code-runner.executorMap\": {\n    ...\n    \"python\": \"python3 -u\"\n    ...\n}\n"], [], ["import requests\nimport urllib3\nimport ssl\n\n\nclass CustomHttpAdapter (requests.adapters.HTTPAdapter):\n    # \"Transport adapter\" that allows us to use custom ssl_context.\n\n    def __init__(self, ssl_context=None, **kwargs):\n        self.ssl_context = ssl_context\n        super().__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = urllib3.poolmanager.PoolManager(\n            num_pools=connections, maxsize=maxsize,\n            block=block, ssl_context=self.ssl_context)\n\n\ndef get_legacy_session():\n    ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n    ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT\n    session = requests.session()\n    session.mount('https://', CustomHttpAdapter(ctx))\n    return session\n", "get_legacy_session().get(\"some-url\")\n"], [], ["pip install setuptools==58.2.0\n"], ["sudo apt-get install python3.9-distutils\n"], [], ["heatmap = sb.heatmap(pd.DataFrame(full_dict).T.fillna(0), annot=True, linewidths=1, xticklabels=1, yticklabels=1, annot_kws={'rotation': 90})\ncbar = heatmap.collections[0].colorbar\ncbar.ax.set_yticklabels(cbar.ax.get_yticklabels(), rotation=90, va='center')\n", "heatmap = sb.heatmap(pd.DataFrame(full_dict).T.fillna(0), annot=True, linewidths=1, xticklabels=1, yticklabels=1, annot_kws={'rotation': 90})\ncbar = heatmap.collections[0].colorbar\ncbar.ax.tick_params(axis='y', labelrotation=90)\n"], [], [], [], [">> nvidia-smi -mig 0\n"], [], [], ["    ```` conda uninstall pytorch \n     conda install pytorch torchvision cudatoolkit=11.3 -c pytorch -c conda-forge ````\n"], ["aiohttp==3.8.1\nyarl==1.4.2\nfrozenlist==1.3.0\n", "aiohttp==3.8.2\nyarl==1.8.1\nfrozenlist==1.3.1\n"], [], [], ["import aiohttp\nconn = aiohttp.TCPConnector(limit_per_host=5)\n\nasync with aiohttp.ClientSession(connector=conn) as session:\n"], [], ["from dataclasses import dataclass, fields\n\n@dataclass()\nclass Test:\n    value: int\n\n    def __post_init__(self):\n        for field in fields(self):\n            setattr(self, field.name, field.type(getattr(self, field.name)))\n", ">>> test = Test('1')\n>>> type(test.value)\n<class 'int'>\n"], ["message = await message.channel.send('Live') \nwhile True:\n    mss().shot(output=\"foo.png\") #mss is used to make a screenshot but the basic concept is you have to update the image file but keep the filename\n    await message.add_files(discord.File(fp=\"foo.png\"))\n    \n\n"], ["import os\nimport pybboxes as pbx\nimport cv2\n\nDATA_PATH = \"<data_path>\"\n                                                  \nfor i in sorted(os.listdir(DATA_PATH)):\n    print(i)\n    if i[-1]==\"g\":\n        img = cv2.imread(os.path.join(DATA_PATH, i))\n        print(os.path.join(DATA_PATH, i))\n\n        fl = open(os.path.join(DATA_PATH, f\"{i[:-3]}txt\"), 'r')\n        data = fl.readlines()\n        fl.close()\n\n        H, W = img.shape[:2]\n\n        for dt in data:\n            _, x, y, w, h = map(float, dt.split(' '))\n            box_voc = pbx.convert_bbox((x,y,w,h), from_type=\"yolo\", to_type=\"voc\", image_size=(W,H))\n\n            cv2.rectangle(img, (box_voc[0], box_voc[1]), (box_voc[2], box_voc[3]), (0, 0, 255), 3)\n        cv2.imshow(i, img)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n\n\n"], ["--max-line-length\n", "120\n", "--experimental\n"], [], ["## pip install pybboxes \nimport pybboxes as pbx\n\nyolo_normalized = (0.048765432089567184, 0.6583333611488342, 0.09753086417913437, 0.29814815521240234) \n\nH, W = img.shape[:2]\n\nbox_voc = pbx.convert_bbox(yolo_normalized, from_type=\"yolo\", to_type=\"voc\", image_size=(W,H))\n\nprint(box_voc)\n\n# [Out]: (0, 153, 29, 242)\n\n## for plotting:\n\ncv2.rectangle(img, (box_voc[0], box_voc[1]), (box_voc[2], box_voc[3]), (0, 0, 255), 1)\n"], [], ["###############################\n# 1. Install extension \"macros\" in Visual Code\n#\n# Hit View on top menu\n# Search for extension named \"macros\" (by geddski)\n# Install \"macros\" extension\n#\n###############################\n\n\n###############################\n# 2. Add code below to keybindings.json\n#\n# Hit <Ctrl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open keyboard shortcuts\n#\n###############################\n\n{\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.pythonExecSelectionAndCursorDown\",\n        \"when\": \"editorTextFocus && editorLangId == 'python'\"\n    }\n\n\n###############################\n# 3. Add code below to settings.json\n#\n# Hit <Ctrl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open settings \n#\n###############################\n\n\"macros\": {  // Note: this requires macros extension by publisher:\"geddski\" \n        \"pythonExecSelectionAndCursorDown\": [\n            \"python.execSelectionInTerminal\", \n            \"cursorDown\" \n        ]\n    }\n"], [], ["sudo apt install python-is-python3\n"], [], [], [], [], ["pipenv install\n"], [], [], [], [], ["Test(value=1)\nTest(value=12)\ninvalid literal for int() with base 10: '3.21'\n"], ["class IntConversionDescriptor:\n\n    def __set_name__(self, owner, name):\n        self._name = \"_\" + name\n\n    def __get__(self, instance, owner):\n        return getattr(instance, self._name)\n\n    def __set__(self, instance, value):\n        setattr(instance, self._name, int(value))\n\n\n@dataclass\nclass Test:\n    value: IntConversionDescriptor = IntConversionDescriptor()\n", ">>> test = Test(value=1)\n>>> type(test.value)\n<class 'int'>\n\n>>> test = Test(value=\"12\")\n>>> type(test.value)\n<class 'int'>\n\ntest.value = \"145\"\n>>> type(test.value)\n<class 'int'>\n\ntest.value = 45.12\n>>> type(test.value)\n<class 'int'>\n"], ["$env:XLA_FLAGS=\"--xla_gpu_cuda_data_dir='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7'\"\n"], ["from io import StringIO\n# temporarily store the dataframe as a csv in a string variable\ntemp_csv_string = df.to_csv(sep=\";\", index=False)\ntemp_csv_string_IO = StringIO(temp_csv_string)\n# create new dataframe from string variable\nnew_df = pd.read_csv(temp_csv_string_IO, sep=\";\")\n# this new df can be uploaded to BQ with no issues\nnew_df.to_gbq(table_id, project_id, if_exists=\"append\")\n"], ["poetry add redis --group=extras\n", "[tool.poetry.group.extras.dependencies]\n"], ["    while page.locator(\"span\",has_text=\"End of results\").is_visible() is False:\n        page.mouse.wheel(0,100)\n        #page.keyboard.down(PageDown) also works\n"], [], [], [], [], [], [], [], ["class AnyForm(BaseModel):\n    any_param: str\n    any_other_param: int = 1\n\n    @classmethod\n    def as_form(\n        cls,\n        any_param: str = Form(...),\n        any_other_param: int = Form(1)\n    ) -> AnyForm:\n        return cls(any_param=any_param, any_other_param=any_other_param)\n\n@router.post('')\nasync def any_view(form_data: AnyForm = Depends(AnyForm.as_form)):\n        ...\n", "import inspect\nfrom typing import Type\n\nfrom fastapi import Form\nfrom pydantic import BaseModel\nfrom pydantic.fields import ModelField\n\ndef as_form(cls: Type[BaseModel]):\n    new_parameters = []\n\n    for field_name, model_field in cls.__fields__.items():\n        model_field: ModelField  # type: ignore\n\n        new_parameters.append(\n             inspect.Parameter(\n                 model_field.alias,\n                 inspect.Parameter.POSITIONAL_ONLY,\n                 default=Form(...) if model_field.required else Form(model_field.default),\n                 annotation=model_field.outer_type_,\n             )\n         )\n\n    async def as_form_func(**data):\n        return cls(**data)\n\n    sig = inspect.signature(as_form_func)\n    sig = sig.replace(parameters=new_parameters)\n    as_form_func.__signature__ = sig  # type: ignore\n    setattr(cls, 'as_form', as_form_func)\n    return cls\n", "@as_form\nclass Test(BaseModel):\n    param: str\n    a: int = 1\n    b: str = '2342'\n    c: bool = False\n    d: Optional[float] = None\n\n\n@router.post('/me', response_model=Test)\nasync def me(request: Request, form: Test = Depends(Test.as_form)):\n    return form\n"], ["n= int(input())\ncount=0\nx=1\nwhile(count<n):\n    y=3*x+2\n    if(y%4!=0):\n        print(y, end=' ')\n        count=count+1\n    x=x+1\n"], ["\"jupyter.sendSelectionToInteractiveWindow\": true\n"], [], [], ["def solution(A):    \n    for i in set(A):\n    if A.count(i) == 1:\n        return i\n", "from collections import Counter\ndef solution(A): \n    c = Counter(A)\n    final = [k for k, v in c.items() if v == 1]\n    return final[0]\n", "def solution(A):\n    A.sort()\n    A.append(-1) #Just to make list even and run till last but not match with existing integers\n    for i in range(0,len(A),2):\n        if A[i]!=A[i+1]:\n            return A[I]\n"], ["async def edit_attachments(message: discord.Message, files: File):\n    await message.remove_attachments(message.attachments)\n    await message.add_files(files)\n"], ["from functools import reduce\n\nmy_list = [[1, 2], [10, 5], [-4, 5]]\n\nreduce(lambda a, b: a + b, my_list)\n"], ["blankIndex=[''] * len(df)\ndf.index=blankIndex\ndf\n\n"], ["import plotly.express as px\n\n# a sample scatter plot figure created\nfig = px.scatter(x=range(10), y=range(10))\nfig.write_html(\"path/to/file.html\")\n"], ["page.evaluate(\n    \"\"\"\n    var intervalID = setInterval(function () {\n        var scrollingElement = (document.scrollingElement || document.body);\n        scrollingElement.scrollTop = scrollingElement.scrollHeight;\n    }, 200);\n\n    \"\"\"\n)\nprev_height = None\nwhile True:\n    curr_height = page.evaluate('(window.innerHeight + window.scrollY)')\n    if not prev_height:\n        prev_height = curr_height\n        time.sleep(1)\n    elif prev_height == curr_height:\n        page.evaluate('clearInterval(intervalID)')\n        break\n    else:\n        prev_height = curr_height\n        time.sleep(1)\n"], ["# Note that this dockerfile is only used to build the lambda asset - the\n# lambda still just runs with a zip source, not a docker image.\n# See the docstring for aws_lambda.Code.from_docker_build\nFROM public.ecr.aws/lambda/python:3.9.2022.04.27.10-x86_64\n\nCOPY index.py /asset/\nCOPY requirements.txt /tmp/\nRUN pip3 install -r /tmp/requirements.txt -t /asset\n"], [], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")\ndriver = webdriver.Chrome(executable_path=r\"C:\\Program \nFiles\\Google\\Chrome\\Application\\chromedriver.exe\", options=chrome_options)\n"], ["pip install prophet --no-cache-dir\n"], ["@dataclasses.dataclass\nclass Test:\n    value: int\n\n    def __post_init__(self):\n        self.value = int(self.value)\n", ">>> test = Test(\"42\")\n>>> type(test.value)\n<class 'int'>\n", "@attr.define\nclass Test:\n    value: int = attr.field(converter=int)\n", ">>> test = Test(\"42\")\n>>> type(test.value)\n<class 'int'>\n", "@dataclasses.dataclass\nclass Test:\n    value: int\n", ">>> test = cattrs.structure({\"value\": \"42\"}, Test)\n>>> type(test.value)\n<class 'int'>\n", "class Test(pydantic.BaseModel):\n    value: int\n", ">>> test = Test(value=\"42\")\n>>> type(test.value)\n<class 'int'>\n"], [], [], [], ["ax.set_xticks([1,2,3])\nax.set_xtickslabels(['Label1', 'Label2', 'Label3'])\n"], [], ["with torch.no_grad():\n  for m in self.children():\n    m.cuda()\n    m.eval()\n    x = m(x)\n    m.cpu()\n    torch.cuda.empty_cache()\n"], [], ["{\n    \"key\": \"ctrl+enter\",\n    \"command\": \"macros.pythonExecSelectionAndCursorDown\",\n    \"when\": \"editorTextFocus && editorLangId == 'python' && resourceExtname == '.py'\"\n}\n"], [], ["from pathlib import Path\n\nwith Path(\"myfile.html\").open(\"w\") as f:\n    f.write(fig.to_html())\n"], ["unique = df.apply(lambda row: len(row.unique()) == 1, axis=1)\n"], ["df.set_index('CLASS').isna().groupby(level=0).sum()\n", "# Will be deprecated soon.. do not use. You should use above statement instead.\ndf.set_index('CLASS').isna().sum(level=0)\n", "       FEATURE1  FEATURE2  FEATURE3\nCLASS                              \nX           1.0       1.0       2.0\nB           0.0       0.0       0.0\n"], [], ["if resp.status_code == 200:\n    print ('OK!')\nelse:\n    print ('Boo!')\n", "if resp.ok:\n    print ('OK!')\nelse:\n    print ('Boo!')\n"], ["! LaTeX Error: File `ucharcat.sty' not found\n"], [], [], ["pip install --upgrade pip setuptools wheel\n"], [], [], ["from selenium.webdriver.chrome.service import Service\n\nchrome_executable = Service(executable_path='chromedriver.exe', log_path='NUL')\ndriver = webdriver.Chrome(service=chrome_executable)\n"], ["if 200 <= resp.status_code <= 299:\n    print ('OK!') \nelse:\n    print ('Boo!')\n"], [], [], [], ["function solution($a){\n      $result  = array_count_values($a);\n     $result = array_filter($result , function($a){\n           return ($a  % 2) && 1;\n     });\n    return key($result);   \n}\n"], [], [], ["sudo apt-get install --reinstall python3.7-distutils\n"], [], [], [], ["$ poeareq --help\n\nusage: poeareq [-h] [-D] [requirements.txt files ...]\n\nAdd dependencies specified in requirements.txt to your Poetry project\n\npositional arguments:\n  requirements.txt file(s)\n                        Path(s) to your requirements.txt file(s) (default: requirements.txt)\n\noptions:\n  -h, --help            show this help message and exit\n  -D, --dev             Add to development dependencies (default: False)\n"], [], [], ["from fastapi.testclient import TestClient\nfrom fastapi import FastAPI, Depends, Form\nfrom pydantic import BaseModel\n\n\napp = FastAPI()\n\n\ndef form_body(cls):\n    cls.__signature__ = cls.__signature__.replace(\n        parameters=[\n            arg.replace(default=Form(...))\n            for arg in cls.__signature__.parameters.values()\n        ]\n    )\n    return cls\n\n\n@form_body\nclass Item(BaseModel):\n    name: str\n    another: str\n\n\n@app.post('/test', response_model=Item)\ndef endpoint(item: Item = Depends(Item)):\n    return item\n\n\ntc = TestClient(app)\n\n\nr = tc.post('/test', data={'name': 'name', 'another': 'another'})\n\nassert r.status_code == 200\nassert r.json() == {'name': 'name', 'another': 'another'}\n"], [], ["conda install -c anaconda cudatoolkit\n"], [], ["X_train =t ensorflow.convert_to_tensor(X_train, dtype=tensorflow.float32)\ny_train = tensorflow.convert_to_tensor(y_train, dtype=tensorflow.float32)\nX_test = tensorflow.convert_to_tensor(X_test, dtype=tensorflow.float32)\ny_test = tensorflow.convert_to_tensor(y_test, dtype=tensorflow.float32)\n"], [], ["error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice\n", "$ find / -type d -name nvvm 2>/dev/null\n/usr/lib/cuda/nvvm\n$ cd /usr/lib/cuda/nvvm\n/usr/lib/cuda/nvvm$ ls\nlibdevice\n/usr/lib/cuda/nvvm$ cd libdevice\n/usr/lib/cuda/nvvm/libdevice$ ls\nlibdevice.10.bc\n", "export XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda\n"], [], ["ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nax.xaxis.set_major_locator(dates.DayLocator())\n", "ax.tick_params(axis='x', labelrotation = 45)\n"], ["{\n\"workbench.colorTheme\": \"Default Dark+\",\n\"code-runner.executorMap\": {\n\n    \"javascript\": \"node\",\n    \"java\": \"cd $dir && javac $fileName && java $fileNameWithoutExt\",\n    \"c\": \"cd $dir && gcc $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"cpp\": \"cd $dir && g++ $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"objective-c\": \"cd $dir && gcc -framework Cocoa $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"php\": \"php\",\n    \"python\": \"python -u\",\n    \"perl\": \"perl\",\n    \"perl6\": \"perl6\",\n    \"ruby\": \"ruby\",\n    \"go\": \"go run\",\n    \"lua\": \"lua\",\n    \"groovy\": \"groovy\",\n    \"powershell\": \"powershell -ExecutionPolicy ByPass -File\",\n    \"bat\": \"cmd /c\",\n    \"shellscript\": \"bash\",\n    \"fsharp\": \"fsi\",\n    \"csharp\": \"scriptcs\",\n    \"vbscript\": \"cscript //Nologo\",\n    \"typescript\": \"ts-node\",\n    \"coffeescript\": \"coffee\",\n    \"scala\": \"scala\",\n    \"swift\": \"swift\",\n    \"julia\": \"julia\",\n    \"crystal\": \"crystal\",\n    \"ocaml\": \"ocaml\",\n    \"r\": \"Rscript\",\n    \"applescript\": \"osascript\",\n    \"clojure\": \"lein exec\",\n    \"haxe\": \"haxe --cwd $dirWithoutTrailingSlash --run $fileNameWithoutExt\",\n    \"rust\": \"cd $dir && rustc $fileName && $dir$fileNameWithoutExt\",\n    \"racket\": \"racket\",\n    \"scheme\": \"csi -script\",\n    \"ahk\": \"autohotkey\",\n    \"autoit\": \"autoit3\",\n    \"dart\": \"dart\",\n    \"pascal\": \"cd $dir && fpc $fileName && $dir$fileNameWithoutExt\",\n    \"d\": \"cd $dir && dmd $fileName && $dir$fileNameWithoutExt\",\n    \"haskell\": \"runhaskell\",\n    \"nim\": \"nim compile --verbosity:0 --hints:off --run\",\n    \"lisp\": \"sbcl --script\",\n    \"kit\": \"kitc --run\",\n    \"v\": \"v run\",\n    \"sass\": \"sass --style expanded\",\n    \"scss\": \"scss --style expanded\",\n    \"less\": \"cd $dir && lessc $fileName $fileNameWithoutExt.css\",\n    \"FortranFreeForm\": \"cd $dir && gfortran $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"fortran-modern\": \"cd $dir && gfortran $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"fortran_fixed-form\": \"cd $dir && gfortran $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\",\n    \"fortran\": \"cd $dir && gfortran $fileName -o $fileNameWithoutExt && $dir$fileNameWithoutExt\"\n}\n", "\"python\": \"python -u\",\n", "\"python\": \"python3\",\n"], [], [], ["@Before\npublic void setup() {\n    WebDriverManager.chromedriver().setup();\n    ChromeOptions options = new ChromeOptions();\n    // Bypass Cloudflare checks\n    options.setExperimentalOption(\"useAutomationExtension\", false);\n    options.addArguments(\"--disable-blink-features=AutomationControlled\");\n    driver = new ChromeDriver(options);\n    driver.manage().window().maximize();\n}\n"], ["export SYSTEM_VERSION_COMPAT=1\n"], [], ["client.query_dataframe(sql).to_json(orient='records',default_handler=str)\n"], ["DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n", "pip3 install -U selenium\n", "pip3 install webdriver-manager\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\ndriver.get(\"https://www.google.com\")\n", "[WDM] - ====== WebDriver manager ======\n[WDM] - Current google-chrome version is 96.0.4664\n[WDM] - Get LATEST driver version for 96.0.4664\n[WDM] - Driver [C:\\Users\\Admin\\.wdm\\drivers\\chromedriver\\win32\\96.0.4664.45\\chromedriver.exe] found in cache\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\noptions = Options()\noptions.add_argument(\"start-maximized\")\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\ndriver.get(\"https://www.google.com\")\n"], [], ["import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n"], [], ["import asyncio\nfrom aiohttp import ClientSession\n\n\nasync def main():\n    url = \"https://stackoverflow.com/\"\n\n    async with ClientSession() as session:\n        async with session.get(url, ssl=False) as resp:\n            print(resp.status)\n\nasyncio.run(main())\n"], [], [], [], ["XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4\n"], ["import torch\ntorch.cuda.empty_cache()\n", "import gc\ndel variables\ngc.collect()\n", "torch.cuda.memory_summary(device=None, abbreviated=False)\n"], [], [], ["from selenium.webdriver import Firefox\nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service\nfrom selenium.webdriver.firefox.options import Options\n\nprofile_path = r'C:\\Users\\Admin\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\s8543x41.default-release'\noptions=Options()\noptions.set_preference('profile', profile_path)\noptions.set_preference('network.proxy.type', 1)\noptions.set_preference('network.proxy.socks', '127.0.0.1')\noptions.set_preference('network.proxy.socks_port', 9050)\noptions.set_preference('network.proxy.socks_remote_dns', False)\nservice = Service('C:\\\\BrowserDrivers\\\\geckodriver.exe')\ndriver = Firefox(service=service, options=options)\ndriver.get(\"https://www.google.com\")\ndriver.quit()\n", "from selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\nfrom selenium.webdriver.chrome.service import Service\n\noptions = Options()\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\noptions.add_experimental_option('useAutomationExtension', False)\noptions.add_argument('--disable-blink-features=AutomationControlled')\ns = Service('C:\\\\BrowserDrivers\\\\geckodriver.exe')\ndriver = webdriver.Chrome(service=s, options=options)\n", "from selenium.webdriver import Firefox  \nfrom selenium import webdriver\nfrom selenium.webdriver.firefox.service import Service\nfrom selenium.webdriver.firefox.options import Options\nimport os\n\ntorexe = os.popen(r'C:\\Users\\username\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Tor\\tor.exe')\nprofile_path = r'C:\\Users\\username\\Desktop\\Tor Browser\\Browser\\TorBrowser\\Data\\Browser\\profile.default'\nfirefox_options=Options()\nfirefox_options.set_preference('profile', profile_path)\nfirefox_options.set_preference('network.proxy.type', 1)\nfirefox_options.set_preference('network.proxy.socks', '127.0.0.1')\nfirefox_options.set_preference('network.proxy.socks_port', 9050)\nfirefox_options.set_preference(\"network.proxy.socks_remote_dns\", False)\nfirefox_options.binary_location = r'C:\\Users\\username\\Desktop\\Tor Browser\\Browser\\firefox.exe'\nservice = Service('C:\\\\BrowserDrivers\\\\geckodriver.exe')\ndriver = webdriver.Firefox(service=service, options=firefox_options)\ndriver.get(\"https://www.tiktok.com/\")\n"], ["ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\nctx.options |= 0x4\nsession.mount('https://', CustomHttpAdapter(ctx))\n"], ["git clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg integrate install\n./vcpkg install sentencepiece:x64-windows-static\n"], ["pip install webdriver-manager\n", "from selenium.webdriver.firefox.service import Service\nfrom webdriver_manager.firefox import GeckoDriverManager\ndriver = webdriver.Firefox(service=Service(executable_path=GeckoDriverManager().install()))\n"], ["def solution(A,K):        \n    for k in np.arange(K):\n        B=[]\n        for i in range(len(A)):\n            B.append(A[i-1])\n        A=B\n    return B\n"], ["vs_buildtools.exe --norestart --passive --downloadThenInstall --includeRecommended --add Microsoft.VisualStudio.Workload.NativeDesktop --add Microsoft.VisualStudio.Workload.VCTools --add Microsoft.VisualStudio.Workload.MSBuildTools\n"], [], ["from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(800, 800))  \ndisplay.start()\n", "    #Display in order to avoid CloudFare bot detection\n    display = Display(visible=0, size=(800, 800))  \n    display.start()\n  \n    options = webdriver.ChromeOptions()\n    options.add_argument('--no-sandbox')\n    options.add_argument('start-maximized')\n    options.add_argument('enable-automation')\n    options.add_argument('--disable-infobars')\n    options.add_argument('--disable-dev-shm-usage')\n    options.add_argument('--disable-browser-side-navigation')\n    options.add_argument(\"--remote-debugging-port=9222\")\n    # options.add_argument(\"--headless\")\n    options.add_argument('--disable-gpu')\n    options.add_argument(\"--log-level=3\")\n    driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n"], [], [], [], ["def solution(A):\n    cloned = []\n    A.sort()\n    if len(A) > 1:\n       for itr in A:\n          if itr in cloned:\n             itrindex = cloned.index(itr)\n             cloned.pop(itrindex)\n          else:\n             cloned.append(itr)\n    else:\n        return A[0]\n\n    return cloned[0]\n"], [">>> import pystan\n>>> model_code = 'parameters {real y;} model {y ~ normal(0,1);}'\n>>> model = pystan.StanModel(model_code=model_code)\n>>> y = model.sampling().extract()['y']\n>>> y.mean()  # with luck the result will be near 0\n"], [], ["conda activate <NAME_OF_VENV>\npip install notebook\nconda install nbconvert\nconda install pandoc\nconda deactivate\n", "sudo apt install texlive-xetex\n"], ["K = range(2,10)\n\nfor k in K:\n\n  kmeanModel = KMeans(n_clusters=k)\n\n  kmeanModel.fit(data)\n\n  distortions.append(kmeanModel.inertia_)\n"], [], ["from itertools import islice, count\n\nN = 10\nl = list(islice((x for i in count(start=1) if (x:=3*i+2)%4), N))\n"], ["limit = 10\ncounter = 0\nn = 0\n\nwhile counter != limit:\n    n += 1\n    statement = (3*n)+2\n    if (statement % 4 != 0) and (statement > 4):\n        print(statement)\n    counter += 1\n"], ["n = int (input())\ncounter = 0\nfor x in range (1, n + 1, 1):\n    for y in range (1, 100, 1):\n        z = 3 * y + 2\n        if counter >= 10:\n            break\n        if z % 4 != 0:\n            print(z, end=' ')\n            counter += 1\n"], [], [], [], ["{\n   {\n     \"python.pythonPath\": \"VirtualEnPath/bin/python3.6\"\n   }\n}\n"], ["$ cat requirements.txt | xargs poetry add\n"], [], [], ["from datetime import datetime\n\ntime_expected = datetime.now()\ntime_actual = datetime.strptime(time_actual.isoformat(), \"%Y-%m-%dT%H:%M:%S.%f\")\nassert time_actual == time_expected\n", "from datetime import datetime\n\ntime_expected = datetime.now()\ntime_actual = datetime.fromisoformat(time_expected.isoformat())\nassert time_actual == time_expected\n"], [], [], [], [], [], [], [], [], [], [], [], ["def slice(A):\n    B = []\n    for i  in range(0,len(A)) :\n        B.append(A[-1+i])\n    return B \n\ndef solution(A, K):\n    for i in range (1,K+1):\n        A = slice(A)\n    return A\n"], [], [], ["conda create --name py36 python==3.6.13\nconda install tensorflow\nconda install keras\nconda install tensorflow-gpu\nconda install tensorflow-estimator==2.1.0\n"], [], ["poetry add $(sed -e 's/#.*//' -e '/^$/ d' < requirements.txt)\n"], [], [], [], [], ["poetry add $( cat requirements.txt )\n"], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\ndriver_path = \"C:/Users/johnm/PycharmProjects/chromedriver.exe\"\nbrave_path = \"C:/Program Files/BraveSoftware/Brave-Browser/Application/brave.exe\"\n\ns=Service(driver_path)\noption = webdriver.ChromeOptions()\noption.binary_location = brave_path\nbrowser = webdriver.Chrome(service=s, options=option)\nbrowser.get(\"https://www.google.es\")\n"], ["from selenium import webdriver    \nimport chromedriver_autoinstaller\nfrom selenium.webdriver.chrome.service import Service\n\nchromedriver_autoinstaller.install()\ndriver = webdriver.Chrome(service=Service())\n"], [], [], ["from datetime import datetime, timezone\n\nfrom pydantic.datetime_parse import parse_datetime\n\n\nclass utc_datetime(datetime):\n    @classmethod\n    def __get_validators__(cls):\n        yield parse_datetime  # default pydantic behavior\n        yield cls.ensure_tzinfo\n\n    @classmethod\n    def ensure_tzinfo(cls, v):\n        # if TZ isn't provided, we assume UTC, but you can do w/e you need\n        if v.tzinfo is None:\n            return v.replace(tzinfo=timezone.utc)\n        # else we convert to utc\n        return v.astimezone(timezone.utc)\n    \n    @staticmethod\n    def to_str(dt:datetime) -> str:\n        return dt.isoformat() # replace with w/e format you want\n", "from pydantic import BaseModel\n\nclass SomeObject(BaseModel):\n    some_datetime_in_utc: utc_datetime\n\n    class Config:\n        json_encoders = {\n            utc_datetime: utc_datetime.to_str\n        }\n"], [], ["8 < user_grade < 13\n"], ["\"python.terminal.activateEnvironment\": true,\n"], ["pip install koila\n", "from koila import lazy\ninput = lazy(input, batch=0)\n"], ["from collections import deque\ndef solution(A, K):\n    m=deque(A)\n    m.rotate(K)\n    return list(m)\n"], ["pip install -U kaleido\n", "conda install -c conda-forge python-kaleido\n", "fig.write_image(\"yourfile.png\") \n", "# imports\nimport plotly\nimport plotly.express as px\n\n# data\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\n\n# plotly express bar chart\nfig = px.line(df, x=\"year\", y=\"lifeExp\", color='country')\n\n# html file\nplotly.offline.plot(fig, filename='C:/plotlyplots/lifeExp.html')\n", "npm install -g electron@1.8.4 orca\n\npip install psutil requests\n"], ["conda activate <name of your env> && which python\n", "{\n    \"python.defaultInterpreterPath\": \"<Path to your env>\",\n}\n"], [], [], ["curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n", "python get-pip.py\n"], [], [], ["jeremy@jeremy-Blade:~$ sudo apt-get install python3.10-distutils \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\n...\nSetting up python3.10-lib2to3 (3.10.0-1+focal1) ...\nSetting up python3.10-distutils (3.10.0-1+focal1) ...\n\njeremy@jeremy-Blade:~$ python3.10 -m pip install opencv-python \n"], ["https://pytorch.org/\n", "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n", "https://www.nvidia.com/Download/index.aspx\n"], ["[\n    {\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.pythonExecSelectionAndCursorDown\",\n        \"when\": \"editorTextFocus && editorLangId == 'python'\"\n    },\n    {\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.jupyterExeSelThenCursorDown\",\n        \"when\": \"editorTextFocus && isWorkspaceTrusted && jupyter.ownsSelection && !findInputFocussed && !notebookEditorFocused && !replaceInputFocussed && editorLangId == 'python'\"\n    },\n    {\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.pythonExecSelectionAndCursorDown\",\n        \"when\": \"editorTextFocus && !findInputFocussed && !jupyter.ownsSelection && !notebookEditorFocused && !replaceInputFocussed && editorLangId == 'python'\"\n    },\n    {\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.jupyterRunCellThenCursorDown\",\n        \"when\": \"editorTextFocus && isWorkspaceTrusted && jupyter.hascodecells && !editorHasSelection && !notebookEditorFocused\"\n    },\n    {\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.interactiveExe\",\n        \"when\": \"resourceScheme == 'vscode-interactive'\"\n    }\n]\n", "\"macros\": {  \n        \"pythonExecSelectionAndCursorDown\": [\n            \"python.execSelectionInTerminal\",\n            \"cursorDown\"\n        ],\n        \"jupyterExeSelThenCursorDown\":[\n            \"jupyter.execSelectionInteractive\",\n            \"cursorDown\"\n        ],\n        \"jupyterRunCellThenCursorDown\":[\n            \"jupyter.runcurrentcelladvance\",\n            \"cursorDown\"\n        ],\n        \"interactiveExe\":[\n            \"interactive.execute\",\n            \"cursorDown\"\n        ]\n    }\n"], [], ["ChromeOptions options = new ChromeOptions();\n\noptions.addExtensions(new File(\"/path/to/extension.crx\"));\n\nChromeDriver driver = new ChromeDriver(options);\n", "ChromeOptions options = new ChromeOptions();\n\n// Add the WebDriver proxy capability.\n\nProxy proxy = new Proxy();\n\nproxy.setHttpProxy(\"myhttpproxy:3337\");\n\noptions.setCapability(\"proxy\", proxy);\n\n// Add a ChromeDriver-specific capability.\n\noptions.addExtensions(new File(\"/path/to/extension.crx\"));\n\nChromeDriver driver = new ChromeDriver(options);\n"], ["import aiohttp\nconn = aiohttp.TCPConnector()\n\nasync with aiohttp.ClientSession(connector=conn) as session:\n    await session.get('https://example.com', ssl=False)\n"], [" Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy Remotesigned\n"], [], [], [], [], [], ["df.apply(lambda x: np.exp(np.mean(np.log(x))), axis = 1)\n", "2010-07-01         NaN\n2010-10-01    0.968237\n2011-01-01         NaN\n2011-04-01         NaN\n2011-07-01    0.994817\n2011-10-01    1.007370\n2012-01-01    1.057660\n2012-04-01    0.979955\n2012-07-01    1.017180\n2012-10-01    0.947547\ndtype: float64\n"], ["2021-08-05 08:38:52.889213: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n2021-08-05 08:38:52.896033: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74] Searched for CUDA in the following directories:\n2021-08-05 08:38:52.899128: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   C:/Users/Julian/anaconda3/envs/TF250_PY395_xeus/Library/bin\n2021-08-05 08:38:52.902510: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2\n2021-08-05 08:38:52.905815: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   .\n"], [], ["from scipy.stats import gmean\ndf[\"gmean\"] = df.apply(gmean, axis=1)\n"], ["import warnings\nwarnings.filterwarnings(\"ignore\")\n"], [], [], [], [], [], ["{\n        \"python.defaultInterpreterPath\": \n    \"C:\\\\tproj\\\\tproj_env\\\\Scripts\\\\python\"\n    }\n"], ["OOM when allocating tensor with shape[800000,32,30,62]\n"], [], [], ["In [1]: from datetime import datetime                                                                                                                                                          \n\nIn [2]: datetime.strptime(\"2021-08-08\", \"%Y-%m-%d\")                                                                                                                                           \nOut[2]: datetime.datetime(2021, 8, 8, 0, 0)\n"], [], [], [], [], [], [], ["conda update --all --yes\n"], [], ["import tensorflow as tf\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)\n\n"], [], [], [".\\venv\\Scripts\\activate\n"], ["options = webdriver.ChromeOptions()\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('useAutomationExtension', False)\noptions.add_argument(\"--disable-blink-features=AutomationControlled\")\ndriver = webdriver.Chrome(options=options, executable_path=r\"webdriver\\chromedriver.exe\")\n"], [], [], [], ["[tool.poetry.extras]\ncaching = [\"redis\"]\n"], ["vs_buildtools__370953915.1537938681.exe --quiet --add Microsoft.VisualStudio.Workload.VCTools\n", "VC_redist.x64.exe  /q /norestart\n"], [], [], [], [], [], ["$reqs = @(cat requirements.txt)\nfor($i = 0; $i -lt $reqs.length; $i++){poetry add $reqs[i]}\n"], [], ["from marshmallow import Schema, fields\n\n\ndef camelcase(s):\n    parts = iter(s.split(\"_\"))\n    return next(parts) + \"\".join(i.title() for i in parts)\n\n\nclass CamelCaseSchema(Schema):\n    \"\"\"Schema that uses camel-case for its external representation\n    and snake-case for its internal representation.\n    \"\"\"\n\n    def on_bind_field(self, field_name, field_obj):\n        field_obj.data_key = camelcase(field_obj.data_key or field_name)\n\n\n# -----------------------------------------------------------------------------\n\n\nclass UserSchema(CamelCaseSchema):\n    first_name = fields.Str(required=True)\n    last_name = fields.Str(required=True)\n\n\nschema = UserSchema()\nloaded = schema.load({\"firstName\": \"David\", \"lastName\": \"Bowie\"})\nprint(loaded)  # => {'last_name': 'Bowie', 'first_name': 'David'}\ndumped = schema.dump(loaded)\nprint(dumped)  # => {'lastName': 'Bowie', 'firstName': 'David'}\n"], ["ffmpeg -encoders\n", "final.write_videofile(\n        filename,\n        threads=5,\n        bitrate=\"2000k\",\n        audio_codec=\"aac\",\n        codec=\"h264_videotoolbox\",\n    )\n"], [], ["$ cat test_empty.json\n{\n}\n$ curl -i -H'Content-Type: application/json' --data @test_empty.json --request POST localhost:8000/items\nHTTP/1.1 400 Bad Request\ncontent-type: application/json\n\n{\"detail\":\"Missing some fields..\"}\n\n$ cat test_incomplete.json \n{\n    \"name\": \"test-name\",\n    \"tax\": 0.44\n}\n$ curl -i -H'Content-Type: application/json' --data @test_incomplete.json --request POST localhost:8000/items\nHTTP/1.1 400 Bad Request\ncontent-type: application/json\n\n{\"detail\":\"Missing some fields..\"}\n\n$ cat test_ok.json\n{\n    \"name\": \"test-name\",\n    \"description\": \"test-description\",\n    \"price\": 123.456,\n    \"tax\": 0.44\n}\n$ curl -i -H'Content-Type: application/json' --data @test_ok.json --request POST localhost:8000/items\nHTTP/1.1 200 OK\ncontent-type: application/json\n\n{\"name\":\"test-name\",\"description\":\"test-description\",\"price\":123.456,\"tax\":0.44}\n", "@app.patch('/items/{item_id}', response_model=Item)\nasync def update_item(item_id: str, item: Item):\n    update_item_values = item.dict(exclude_defaults=True, exclude_none=True)\n\n    # Get intersection of keys/fields\n    # Must have at least 1 common\n    if not (set(update_item_values.keys()) & set(Item.__fields__)):\n        raise HTTPException(status_code=400, detail='No common fields')\n\n    update_item = Item(**update_item_values)\n\n    return update_item\n", "$ cat test2.json\n{\n    \"asda\": \"1923\"\n}\n$ curl -i -s -H'Content-Type: application/json' --data @test2.json --request PATCH localhost:8000/items/1\nHTTP/1.1 400 Bad Request\ncontent-type: application/json\n\n{\"detail\":\"No common fields\"}\n\n$ cat test2.json\n{\n    \"description\": \"test-description\"\n}\n$ curl -i -s -H'Content-Type: application/json' --data @test2.json --request PATCH localhost:8000/items/1\nHTTP/1.1 200 OK\ncontent-type: application/json\n\n{\"name\":null,\"description\":\"test-description\",\"price\":null,\"tax\":null}\n"], [], [], [], [], [], [], [], [], [], [], [], ["code .\n"], [], [], [], [], [], ["from tensorflow.keras.layers import Input\n"], ["import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,2,3,4,5,6],\n               'b':[7,8,9,10,11,12],\n               'c':['q', 'q', 'q', 'q', 'w', 'w']\n              })\n\ndef groupby_transform(df: pd.DataFrame, group_by_column: str, lambda_to_apply) -> np.array:\n    \"\"\"\n    Groupby and transform. Returns a column for the original dataframe.\n    :param df: Dataframe.\n    :param group_by_column: Column(s) to group by.\n    :param lambda_to_apply: Lambda.\n    :return: Column to append to original dataframe.\n    \"\"\"\n    df = df.reset_index(drop=True)  # Dataframe index is now strictly in order of the rows in the original dataframe.\n    values = df.groupby(group_by_column).apply(lambda_to_apply)\n    values.sort_index(level=1, inplace=True)  # Sorts result into order of original rows in dataframe (as groupby will undo that order when it groups).\n    result = np.array(values)  # Sort rows into same order as original dataframe.\n    if result.shape[0] == 1:  # e.g. if shape is (1,1003), make it (1003,).\n        result = result[0]\n    return result  # Return column.\n\n\ndf[\"result\"] = groupby_transform(df, \"c\", lambda x: x[\"a\"].shift(1) + x[\"b\"].shift(1))\n", "   a   b  c  result\n0  1   7  q     NaN\n1  2   8  q     8.0\n2  3   9  q    10.0\n3  4  10  q    12.0\n4  5  11  w     NaN\n5  6  12  w    16.0\n", "@pd.api.extensions.register_dataframe_accessor(\"ex\")\nclass GroupbyTransform:\n    \"\"\"\n    Groupby and transform. Returns a column for the original dataframe.\n    \"\"\"\n    def __init__(self, pandas_obj):\n        self._validate(pandas_obj)\n        self._obj = pandas_obj\n\n    @staticmethod\n    def _validate(obj):\n        # TODO: Check that dataframe is sorted, throw if not.\n        pass\n\n    def groupby_transform(self, group_by_column: str, lambda_to_apply):\n        \"\"\"\n        Groupby and transform. Returns a column for the original dataframe.\n        :param df: Dataframe.\n        :param group_by_column: Column(s) to group by.\n        :param lambda_to_apply: Lambda.\n        :return: Column to append to original dataframe.\n        \"\"\"\n        df = self._obj.reset_index(drop=True)  # Dataframe index is now strictly in order of the rows in the original dataframe.\n        values = df.groupby(group_by_column).apply(lambda_to_apply)\n        values.sort_index(level=1, inplace=True)  # Sorts result into order of original rows in dataframe (as groupby will undo that order when it groups).\n        result = np.array(values)\n        if result.shape[0] == 1:  # e.g. if shape is (1,1003), make it (1003,).\n            result = result[0]\n        return result\n", "df[\"result\"] = df.ex.groupby_transform(\"c\", lambda x: x[\"a\"].shift(1) + x[\"b\"].shift(1))\n"], ["df = pd.DataFrame({'a':[1,2,3,4,5,6],\n               'b':[7,8,9,10,11,12],\n               'c':['q', 'q', 'q', 'q', 'w', 'w']\n              })\n\ndf.reset_index(drop=True, inplace=True)\n\nvalues = df.groupby(['c']).apply(lambda x: sum(x['a'])/sum(x['b']))\n# Convert result to dataframe.\ndf_to_join = values.to_frame()\n\n# Ensure indexes have common names.\ndf_to_join.index.set_names([\"index\"], inplace=True)\ndf.set_index(\"c\", inplace=True)\ndf.index.set_names([\"index\"], inplace=True)\n\n# Set column name of result we want.\ndf_to_join.rename(columns={0: \"ab_weighted\"}, inplace=True, errors='raise')\n\n# Join result of groupby to original dataframe.\ndf_result = df.merge(df_to_join, on=[\"index\"])\nprint(df_result)\n\n# output \n       a   b  ab_weighted\nindex                    \nq      1   7     0.294118\nq      2   8     0.294118\nq      3   9     0.294118\nq      4  10     0.294118\nw      5  11     0.478261\nw      6  12     0.478261\n", "df_result.reset_index(inplace=True)\ndf_result.rename(columns={\"index\": \"c\"}, inplace=True)\n"], [], ["from datetime import datetime, date\n\nfrom pydantic import BaseModel, validator\n\n\nclass OddDate(BaseModel):\n    birthdate: date\n\n    @validator(\"birthdate\", pre=True)\n    def parse_birthdate(cls, value):\n        return datetime.strptime(\n            value,\n            \"%d/%m/%Y\"\n        ).date()\n\n\nif __name__ == \"__main__\":\n    odd_date = OddDate(birthdate=\"12/04/1992\")\n    print(odd_date.json()) #{\"birthdate\": \"1992-04-12\"}\n"], ["###############################\n# 1. Install extension \"macros\" in Visual Code\n#\n# Hit View on top menu\n# Search for extension named \"macros\" (by geddski)\n# Install \"macros\" extension\n#\n###############################\n\n\n###############################\n# 2. Add code below to keybindings.json\n#\n# Hit <Crtl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open keyboard shortcuts\n#\n###############################\n\n{\n        \"key\": \"ctrl+enter\",\n        \"command\": \"macros.ExecSelectionAndCursorDown\",\n    }\n\n\n###############################\n# 3. Add code below to settings.json\n#\n# Hit <Crtl> + <Shift> + <P>\n# Enter in search bar: JSON\n# Select Open settings \n#\n###############################\n\n\"macros\": {  // Note: this requires macros extension by publisher:\"geddski\" \n            \"ExecSelectionAndCursorDown\": [\n                \"workbench.action.terminal.runSelectedText\", \n                \"cursorDown\" \n            ]\n        }\n"], [], ["!pip uninstall tensorflow \n!pip install tensorflow==1.14\n"], ["red = int(input())\ngreen = int(input())\nblue = int(input())\ngrey = 0\n\n\nif (red <= green) and ( red <= blue):\n    grey = red\n    \nif (green <= red) and ( green <= blue):\n    grey = green\n    \nif (blue <= green) and ( blue <= red):\n    grey = blue\n    \nred = red - grey\ngreen = green - grey\nblue = blue - grey\n    \nprint(red, green, blue)\n"], ["\"python.pythonPath\": \"python3\",\n\"code-runner.executorMap\": {\n    \"python\": \"$pythonPath -u $fullFileName\"\n},\n"], ["[tool.poetry]\nname = \"yolo\"\nversion = \"1.0.0\"\ndescription = \"\"\nauthors = []\n\n[tool.poetry.dependencies]\npython = \"2.7\"\nDjango = \"*\"\n\n[tool.poetry.dev-dependencies]\npytest = \"*\"\nipdb = {version = \"*\", optional = true}\n\n[tool.poetry.extras]\ndev_tools = [\"ipdb\"]\n", "[tool.poetry]\nname = \"yolo\"\nversion = \"1.0.0\"\ndescription = \"\"\nauthors = []\n\n[tool.poetry.dependencies]\npython = \"2.7\"\nDjango = \"*\"\nipdb = {version = \"*\", optional = true}\n\n[tool.poetry.dev-dependencies]\npytest = \"*\"\n\n[tool.poetry.extras]\ndev_tools = [\"ipdb\"]\n"], ["from dataclasses import dataclass\nfrom pydantic import validate_arguments\n\n\n@validate_arguments\n@dataclass\nclass Test:\n    value: int\n\n", ">>> test = Test('1')\n>>> type(test.value)\n<class 'int'>\n", ">>> test = Test('apple')\nTraceback (most recent call last):\n...\npydantic.error_wrappers.ValidationError: 1 validation error for Test\nvalue\n  value is not a valid integer (type=type_error.integer)\n"], ["def user_grade (input):\n    if input >= 9 and input <= 12:\n        print('in high school')\n    else:\n        print('not in high school')\n    \nuser_grade(13)\n"], ["if  9 <= user_grade <= 12:\n"], ["if 9 <= user_grade <= 12:\n    print('in high school')\n"], ["features, labels in batch:\n   features, labels = features.to(device), labels.to(device)\n"], [], ["def is_unique(s):\n    a = s.to_numpy() # s.values (pandas<0.24)\n    return (a[0] == a).all()\n\nis_unique(df['counts'])\n# False\n", "def unique_cols(df):\n    a = df.to_numpy() # df.values (pandas<0.24)\n    return (a[0] == a).all(0)\n", "unique_cols(df)\n# array([False, False])\n", "s_num = pd.Series(np.random.randint(0, 1_000, 1_100_000))\n\nperfplot.show(\n    setup=lambda n: s_num.iloc[:int(n)], \n\n    kernels=[\n        lambda s: s.nunique() == 1,\n        lambda s: is_unique(s)\n    ],\n\n    labels=['nunique', 'first_vs_rest'],\n    n_range=[2**k for k in range(0, 20)],\n    xlabel='N'\n)\n", "from numba import njit\n\n@njit\ndef unique_cols_nb(a):\n    n_cols = a.shape[1]\n    out = np.zeros(n_cols, dtype=np.int32)\n    for i in range(n_cols):\n        init = a[0, i]\n        for j in a[1:, i]:\n            if j != init:\n                break\n        else:\n            out[i] = 1\n    return out\n", "df = pd.DataFrame(np.concatenate([np.random.randint(0, 1_000, (500_000, 200)), \n                                  np.zeros((500_000, 10))], axis=1))\n\nperfplot.show(\n    setup=lambda n: df.iloc[:int(n),:], \n\n    kernels=[\n        lambda df: (df.nunique(0) == 1).values,\n        lambda df: unique_cols_nb(df.values).astype(bool),\n        lambda df: unique_cols(df) \n    ],\n\n    labels=['nunique', 'unique_cols_nb', 'unique_cols'],\n    n_range=[2**k for k in range(0, 20)],\n    xlabel='N'\n)\n"], ["def solution(A, K):\n    for i in range(0, K): # will perform K iterations of the below code\n        if A == []: # check if list is empty\n            return A # return A if A is the empty list\n        A.insert(0, A.pop()) # inserts at the first index of A the last element of A\n    return A # will return the list A\n"], ["def solution(A, K):\n    K = K % len(A)\n    return A[-K:] + A[:-K]\n"], [], ["conda activate tom\n", "code\n"], [], [], [], ["import pandas as pd\n\ndf = pd.read_csv('sample.csv') // read data from csv\nresult = df.groupby('sex').size() // use .size() to get the row counts\n", "sex\nf    2\nm    2\ndtype: int64\n"], ["import collections\nimport pandas as pd\ndf = pd.DataFrame({'CODE':['E101','E102','E103','E104'],'SEX':['M','F','M','F']})\nfor key, value in collections.Counter(df['SEX']).items():\n    print(key,\":\",value)\n", "M : 2\nF : 2\n"], ["import pandas as pd\n\ndata = pd.read_csv('sample.csv')\n\nnum_males = sum(data['SEX'] == 'M')\nnum_females = len(data['SEX']) - num_males\n"], ["import pandas as pd\n\ncsv_path_file = '' # your csv path file\nseparator = ';'\n\ndf = pd.read_csv(csv_path_file, sep = separator)\ndf['SEX'].value_counts()\n\n"], ["import pandas as pd\ndf = pd.read_csv(\"sample.csv\")\n\nprint(f\"M : {len(df[df['SEX'] == 'M'])}\")\nprint(f\"F : {len(df[df['SEX'] == 'F'])}\")\n"], [">>> import csv\n>>> M,F = 0,0\n>>> with open('file.csv') as csvfile:\n...     data = csv.reader(csvfile)\n...     for row in data:\n...         M += 1 if row[2] == \"M\" else F += 1\n"], [], ["numeric_list = df.select_dtypes(include=[np.number]).columns\ndf[numeric_list] = df[numeric_list].astype(np.float32)\n"], [], ["awk -F '==' '{print $1}' requirements.txt | xargs -n1 poetry add\n", "awk '{print $1}' requirements.txt | xargs -n1 poetry add\n"], [], [], ["df['counts'].eq(df['counts'].iloc[0]).all()\n"], [], ["from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy\n"], ["{ \"key\": \"shift+enter\",           \"command\": \"jupyter.execSelectionInteractive\", \"when\": \"editorTextFocus\"\n},\n"], [], [], [], ["conda install --quiet --yes conda=4.7.11 \npython -m pip install --upgrade pip==19.2.2\n"], [], ["from selenium import webdriver\n\nchromedriver_path = '/usr/bin/chromedriver'\nbrave_path = '/usr/bin/brave-browser'\noption = webdriver.ChromeOptions()\noption.binary_location = brave_path\nbrowser = webdriver.Chrome(executable_path=driver_path, options=option)\nbrowser.get(\"https://www.google.es\")\n"], [], [], [], ["pip install --upgrade pip setuptools wheel\n", "sudo apt-get install libhdf5-dev\n"], ["import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mticker\n\nmpl.rcParams['font.size'] = 6.5\n\nx = np.array(range(1000, 5000, 500))\ny = 37*x\n\nfig, [ax1, ax2, ax3] = plt.subplots(1,3)\n\nax1.plot(x,y, linewidth=5, color='green')\nax2.plot(x,y, linewidth=5, color='red')\nax3.plot(x,y, linewidth=5, color='blue')\n\nlabel_format = '{:,.0f}'\n\n# nothing done to ax1 as it is a \"control chart.\"\n\n# fixing yticks with \"set_yticks\"\nticks_loc = ax2.get_yticks().tolist()\nax2.set_yticks(ax1.get_yticks().tolist())\nax2.set_yticklabels([label_format.format(x) for x in ticks_loc])\n\n# fixing yticks with matplotlib.ticker \"FixedLocator\"\nticks_loc = ax3.get_yticks().tolist()\nax3.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\nax3.set_yticklabels([label_format.format(x) for x in ticks_loc])\n\n# fixing xticks with FixedLocator but also using MaxNLocator to avoid cramped x-labels\nax3.xaxis.set_major_locator(mticker.MaxNLocator(3))\nticks_loc = ax3.get_xticks().tolist()\nax3.xaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\nax3.set_xticklabels([label_format.format(x) for x in ticks_loc])\n\nfig.tight_layout()\nplt.show()\n", "conda install matplotlib=3.2.2\n"], ["tf.get_default_graph()\n", "tf.compat.v1.get_default_graph()\n"], ["# pip install webdriver-manager\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ns=Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=s)\ndriver.maximize_window()\ndriver.get('https://www.google.com')\ndriver.find_element(By.NAME, 'q').send_keys('Yasser Khalil')\n"], [], ["<b>from functools import wraps</b>\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n\nclass SampleModel(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\n\n\n<b>def auth_required(func):\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        return await func(*args, **kwargs)\n\n    return wrapper</b>\n\n\n@app.post(\"/\")\n<b>@auth_required # Custom decorator</b>\nasync def root(payload: SampleModel):\n    return {\"message\": \"Hello World\", \"payload\": payload}", "<b>from fastapi import Request</b>\n\n\n@app.post(\"/\")\n<b>@auth_required  # Custom decorator</b>\nasync def root(<b>request: Request,</b> payload: SampleModel):\n    return {\"message\": \"Hello World\", \"payload\": payload}"], ["from tensorflow.keras.models import Sequential\n", "from keras.models import Sequential\n", "$conda update python\n$conda update keras\n$conda update tensorflow\n", "pip install --upgrade tensorflow\npip install --upgrade keras\npip install --upgrade python\n", "pip uninstall keras\npip install keras --upgrade\n"], ["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential, load_model\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.Dense(32, input_dim=784))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.LSTM(17))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n"], ["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n", "model = keras.Sequential(\n    [\n        layers.Dense(layers.Dense(32, input_dim=784)),\n        layers.Dense(activation=\"relu\"),\n        layers.Dense(LSTM(17))\n\n    ]\n)\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n"], ["x_train = np.asarray(x_train).astype(np.float32)\ny_train = np.asarray(y_train).astype(np.float32)\n"], ["loss =  self.criterion(pred, label)\n\ntotal_loss += loss\n", "loss =  self.criterion(pred, label)\n\ntotal_loss += loss.item()\n"], ["      File \"C:\\Users\\bencu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\", line 96, in convert_to_eager_tensor\n        return ops.EagerTensor(value, ctx.device_name, \n"], [], ["conda update --prefix C:\\apps\\anaconda3 anaconda\n"], [], [], [], ["sudo apt-get install libmariadb3 libmariadb-dev\n", "pip3 install mariadb\n"], [], ["\"Could not build wheels for ____ which use PEP 517 and cannot be installed directly\" \n", "sudo pip3 install _____ --no-binary :all:\n"], ["import java.util.Scanner;\n\npublic class LabProgram {\n   public static void main(String[] args) {\n      Scanner scnr = new Scanner(System.in);\n      int a  = scnr.nextInt();\n      int b = scnr.nextInt();\n      int c = scnr.nextInt();\n      int gray = (a<b)?((a<c)?a:c):((b<c)?b:c);\n      \n      System.out.print(a-gray);\n      System.out.print(\" \");\n      System.out.print(b-gray);\n      System.out.print(\" \");\n      System.out.println(c-gray);  \n   }\n}\n"], ["-vcodec [accelerator_type]\n# h264_nvenc\n# hevc\n# hevc_nvenc\n# libx265\n", "input = 'a.mkv'\noutput = 'b.mp4'\n\ncall = \"echo y|ffmpeg -r 25 -i \\\"%s\\\" -vcodec h264_nvenc  \\\"%s\\\"\" % (input, output)\ncall\n\nimport os\nos.system(call)\n\n# subprocess.call or os.popen can get the call's return, \n# but if you want get the return at the same time,\n# you should use this way:\nimport subprocess\npi= subprocess.Popen(call,shell=True,stdout=subprocess.PIPE)\nfor i in iter(pi.stdout.readline,'b'):\n    print(i)\n\n", "# concat_ffmpeg.bat\necho y|ffmpeg -i 1.mkv  -qscale 4 1.mpg\necho y|ffmpeg -i 2.mkv  -qscale 4 2.mpg\necho y|ffmpeg -i \"concat:1.mpg|2.mpg\" -c copy output.mp4\n\n## sometimes can't use the [-c copy], u can try this and use GPU: \n# echo y|ffmpeg -i \"concat:1.mpg|2.mpg\" -vcodec h264_nvenc output.mp4\n"], ["from werkzeug.contrib.cache import FileSystemCache\n", "from cachelib import FileSystemCache\n"], ["py -3.8 <command>\n"], [], ["import keras\nfrom keras.models import load_model\nfrom keras.models import Sequential\n"], ["profile = webdriver.FirefoxProfile('C:\\\\Users\\\\You\\\\AppData\\\\Roaming\\\\Mozilla\\\\Firefox\\\\Profiles\\\\something.default-release')\n\nPROXY_HOST = \"12.12.12.123\"\nPROXY_PORT = \"1234\"\nprofile.set_preference(\"network.proxy.type\", 1)\nprofile.set_preference(\"network.proxy.http\", PROXY_HOST)\nprofile.set_preference(\"network.proxy.http_port\", int(PROXY_PORT))\nprofile.set_preference(\"dom.webdriver.enabled\", False)\nprofile.set_preference('useAutomationExtension', False)\nprofile.update_preferences()\ndesired = DesiredCapabilities.FIREFOX\n\ndriver = webdriver.Firefox(firefox_profile=profile, desired_capabilities=desired)\n"], [], [], ["from tensorflow.python.keras.models import Sequential\n\nfrom tensorflow.python.keras.layers.core import Dense, Activation\n"], ["import nest_asyncio\nnest_asyncio.apply()\n"], ["python -m pip install --upgrade pip\n", "pip install --upgrade google-cloud\npip install --upgrade google-cloud-bigquery\npip install --upgrade google-cloud-storage\n"], ["\"python.formatting.provider\": \"autopep8\",\n\"python.formatting.autopep8Args\": [\n    \"--max-line-length\",\n    \"120\",\n    \"--experimental\"\n]\n"], ["\"python.formatting.autopep8Args\": [\"--max-line-length\", \"120\", \"--experimental\"]\n"], [], [], ["result , columns = client.execute('SELECT * FROM myTbl LIMIT 5',with_column_types=True)\ndf=pandas.DataFrame(result,columns=[tuple[0] for tuple in columns])\ndfJson=df.to_json(orient='records')\n"], [], ["pipenv install -r requiremnts.txt\npipenv shell\npython manage.py runserver\n", "python -m venv myEnv\nmyEnv/Scripts/activate\npip install -r requirements.txt\npython manage.py runserver\n"], [], ["train_X[:2, :].view()\n#array([[4.6, 3.1, 1.5, 0.2],\n#       [5.9, 3.0, 5.1, 1.8]], dtype=object)\ntrain_X = train_X.astype(np.float32)\n#array([[4.6, 3.1, 1.5, 0.2],\n#       [5.9, 3. , 5.1, 1.8]], dtype=float32)\n"], ["r = int(input())\ng = int(input())\nb = int(input())\nsmallest_num = 0\n\nif (r < g) and (r < b):     \n    smallest_num = r\nelif (g < r) and (g < b):       #This condiditional finds the smallest number\n    smallest_num = g            #out of the 3 inputs\nelse:\n    smallest_num = b\n\nno_grey_r = r - smallest_num    #New variables without the 'grey' value,\nno_grey_g = g - smallest_num    #which is the 'equal part' i.e.\nno_grey_b = b - smallest_num    #the smallest number in each color\n\nprint(f'{no_grey_r} {no_grey_g} {no_grey_b}')\n"], ["pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html\n"], [], ["pip install --user keras==2.3.1\n"], [], [], ["df.fillna(value='', inplace=True)\n", "cols = df.select_dtypes(include=['object'])\nfor col in cols.columns.values:\n    df[col] = df[col].fillna('')\n"], [], [], [], ["import tensorflow as tf\nfrom tensorflow.keras import backend as k\n"], ["from keras import backend as K\nX_train1 = K.cast_to_floatx(X_train)\ny_train1 = K.cast_to_floatx(y_train)\n"], ["SET GLOBAL READ_ONLY = OFF;\n"], ["ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/root/anaconda3/lib/python3.6/site-packages/tornado-6.0.4.dist-info/METADATA\n", "DESCRIPTION.rst  LICENSE.txt  metadata.json\n"], ["from selenium import webdriver\ndriver_path = \"C:\\\\Users\\\\5150s\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\chromedriver.exe\"\nbrave_path = \"C:\\\\Program Files (x86)\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\noption = webdriver.ChromeOptions()\noption.binary_location = brave_path\nbrowser = webdriver.Chrome(executable_path=driver_path, options=option)\nbrowser.get(\"https://www.google.es\")\n"], [], [], ["\"python.linting.pylintArgs\": [\n    \"--generated-members\", \"torch.*\"\n]\n", "\"python.linting.pylintArgs\": [\n    \"--generated-members\", \"torch.* other_module.* next_module.*\"\n]\n"], ["from fastapi import Form, Depends\n\nclass AnyForm:\n    def __init__(self, any_param: str = Form(...), any_other_param: int = Form(1)):\n        self.any_param = any_param\n        self.any_other_param = any_other_param\n\n    def __str__(self):\n        return \"AnyForm \" + str(self.__dict__)\n\n@app.post('/me')\nasync def me(form: AnyForm = Depends()):\n    print(form)\n    return form\n", "from uuid import UUID, uuid4\nfrom fastapi import Form, Depends\nfrom pydantic import BaseModel\n\nclass AnyForm(BaseModel):\n    id: UUID\n    any_param: str\n    any_other_param: int\n\n    def __init__(self, any_param: str = Form(...), any_other_param: int = Form(1)):\n        id = uuid4()\n        super().__init__(id, any_param, any_other_param)\n\n@app.post('/me')\nasync def me(form: AnyForm = Depends()):\n    print(form)\n    return form\n"], [], ["class ResolveThread(threading.Thread):\n            def __init__(self,result1,fun,url):\n                self.result1= result1\n                self.fun = fun\n                self.url = url\n                threading.Thread.__init__(self)\n            def run(self):\n                result1[0] = asyncio.run(self.fun(self.url))\n\n\nresult1 = [None]\nsp = ResolveThread(result1)\nsp.start()\nsp.join() # connect main thread\nresult = result1[0]\n"], ["python3 -m pip uninstall werkzeug\npython3 -m pip install werkzeug\npython3 -m pip install flask-session\n"], ["(base) [localhost ~]$ conda --version\nconda 4.8.2\n(base) [localhost ~]$ conda install -c anaconda requests-kerberos\nCollecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n", "conda config --set allow_conda_downgrades true\nconda install conda=4.6.14\n", "conda create --name myenv_conda\n", "conda activate myenv_conda\n", "eg: conda install -c conda requests-kerberos\n\noutput:\n(myenv_conda) [localhost ~]$ conda install -c anaconda requests-kerberos\nCollecting package metadata: done\nSolving environment: done\n....\n....\n....\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n"], [], [], [], ["from selenium import webdriver\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--headless\")\noptions.add_argument(\"--disable-extensions\")\noptions.add_argument(\"--disable-dev-shm-usage\")\noptions.add_argument(\"--no-sandbox\")\noptions.add_experimental_option(\"prefs\",{\"download.default_directory\":\"/databricks/driver\"})\ndriver = webdriver.Chrome(chrome_options=options)\n"], [], ["conda install -c conda-forge/label/cf201901 fbprophet \n"], [], ["pip install Werkzeug<1\n"], ["pip3 install Werkzeug<1\n"], [], ["df['ab_weighted'] = \\\ndf.groupby('c', group_keys = False)['a', 'b'].apply(\n    lambda x: pd.Series(x.a.sum()/x.b.sum(), \n                        index = x.index).to_frame()\n).iloc[:,0]\nprint(df)\n\n# output \n#    a   b  c  ab_weighted\n# 0  1   7  q     0.294118\n# 1  2   8  q     0.294118\n# 2  3   9  q     0.294118\n# 3  4  10  q     0.294118\n# 4  5  11  w     0.478261\n# 5  6  12  w     0.478261\n"], [], [], ["python -m pip install --upgrade google-cloud-storage\n"], [">>> from datetime import date, datetime, time\n>>> from backports.datetime_fromisoformat import MonkeyPatch\n>>> MonkeyPatch.patch_fromisoformat()\n\n>>> datetime.fromisoformat(\"2014-01-09T21:48:00-05:30\")\ndatetime.datetime(2014, 1, 9, 21, 48, tzinfo=-05:30)\n\n>>> date.fromisoformat(\"2014-01-09\")\ndatetime.date(2014, 1, 9)\n\n>>> time.fromisoformat(\"21:48:00-05:30\")\ndatetime.time(21, 48, tzinfo=-05:30)\n"], ["import numpy as np\n\nX = np.asarray(X).astype(np.float32)\n"], ["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n", "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n"], [], ["    if resp.ok :\n        print ('OK!')\n    else:\n        print ('Boo!')\n"], ["from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, LSTM\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\nmodel.add(LSTM(17))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=tensorflow.keras.losses.binary_crossentropy, optimizer=tensorflow.keras.optimizers.Adam(), metrics=['accuracy'])\n", "from __future__ import print_function\nimport tensorflow\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\n\nbatch_size = 1024\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\ny_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n             activation='relu',\n             input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=tensorflow.keras.losses.categorical_crossentropy,\n          optimizer=tensorflow.keras.optimizers.Adadelta(),\n          metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n      batch_size=batch_size,\n      epochs=epochs,\n      verbose=1,\n      validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n"], [], ["Return a date corresponding to a date_string given in the format YYYY-MM-DD:\n>>>\n\n>>> from datetime import date\n>>> date.fromisoformat('2019-12-04')\ndatetime.date(2019, 12, 4)\n\nThis is the inverse of date.isoformat(). It only supports the format YYYY-MM-DD.\n\nNew in version 3.7.\n"], ["@app.post(\"/form\", response_model=SimpleModel)\ndef form_post(no: int = Form(...),nm: str = Form(...)):\n    return SimpleModel(no=no,nm=nm)\n"], [], ["import tensorflow as tf\nprint(tf.__version__)\nprint(\"Num GPUs Available: \", \n       len(tf.config.experimental.list_physical_devices('GPU')))\n# Checking the version for incompatibilities and GPU list devices \n# for a fast check on GPU drivers installation. \n\nmodel_filepath = './your_model_path.h5'\n\nmodel = tf.keras.models.load_model(\n    model_filepath,\n    custom_objects=None,\n    compile=False\n)\n"], ["tf.compat.v1.disable_eager_execution()\nprint(tf.compat.v1.get_default_graph())\n"], [], ["import h5py\ndata_p = f.attrs['training_config']\ndata_p = data_p.decode().replace(\"learning_rate\",\"lr\").encode()\nf.attrs['training_config'] = data_p\nf.close()\n"], ["history = model.fit([trainimage, train_product_embd],train_label, validation_data=([validimage,valid_product_embd],valid_label), epochs=10, steps_per_epoch=100, validation_steps=10, batch_size=32)\n"], [], ["pip3 install PyVmomi\nRequirement already satisfied: PyVmomi in /usr/local/lib/python3.7/site-packages (6.7.3)\nRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/site-packages (from PyVmomi) (2.22.0)\nRequirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/site-packages (from PyVmomi) (1.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.3.0->PyVmomi) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.3.0->PyVmomi) (1.25.7)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.3.0->PyVmomi) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.3.0->PyVmomi) (2.8)\n\npip3 install pyVim\nRequirement already satisfied: pyVim in /usr/local/lib/python3.7/site-packages (3.0.2)\nRequirement already satisfied: docopt in /usr/local/lib/python3.7/site-packages (from pyVim) (0.6.2)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from pyVim) (3.0.2)\nRequirement already satisfied: pyflakes in /usr/local/lib/python3.7/site-packages (from pyVim) (2.1.1)\nRequirement already satisfied: pygments in /usr/local/lib/python3.7/site-packages (from pyVim) (2.5.2)\nRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from pyVim) (1.13.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from prompt-toolkit<3.1.0,>=2.0.0->pyVim) (0.1.7)\n"], [], [], [], ["import numpy as np\nnp.ndarray.flatten(np.array([[\"14\"],[\"2\"],[\"75\"],[\"15\"]])).astype(int).tolist()\n# Out[6]: [14, 2, 75, 15]\n"], ["newlist = [int(str) for sublist in mylist for str in sublist]\n"], [], [], ["from itertools import chain\n\nmylist = [[\"14\"],[\"2\"],[\"75\"],[\"15\"]]\nnewest = list(map(int, chain.from_iterable(mylist)))\n\n# newest is => [14, 2, 75, 15]\n"], [], [], ["import requests\n\nresponse = requests.get(url)\nif not response:\n    #handle error here\nelse:\n    #handle normal response\n"], [], ["webdriver\n    Returns true if webdriver-active flag is set, false otherwise.\n", "navigator.webdriver\n    Defines a standard way for co-operating user agents to inform the document that it is controlled by WebDriver, for example so that alternate code paths can be triggered during automation.\n"], [], [], ["conda config --set allow_conda_downgrades true\nconda install conda=4.6.14\n"], ["from selenium import webdriver\n\ndriver_path = \"C:/Users/username/PycharmProjects/chromedriver.exe\"\nbrave_path = \"C:/Program Files (x86)/BraveSoftware/Brave-Browser/Application/brave.exe\"\n\noption = webdriver.ChromeOptions()\noption.binary_location = brave_path\n# option.add_argument(\"--incognito\") OPTIONAL\n# option.add_argument(\"--headless\") OPTIONAL\n\n# Create new Instance of Chrome\nbrowser = webdriver.Chrome(executable_path=driver_path, chrome_options=option)\n\nbrowser.get(\"https://www.google.es\")\n"], [], ["pip uninstall tensorflow-gpu\npip install tensorflow-gpu==1.15\n"], ["# install chrome\ncurl -sS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\necho \"deb [arch=amd64]  http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list\napt-get -y update\napt-get -y install google-chrome-stable\n\n# install chrome driver\nwget https://chromedriver.storage.googleapis.com/77.0.3865.40/chromedriver_linux64.zip\nunzip chromedriver_linux64.zip\nmv chromedriver /usr/bin/chromedriver\nchown root:root /usr/bin/chromedriver\nchmod +x /usr/bin/chromedriver\n", "pip install selenium\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\noptions = Options()\noptions.add_argument(\"no-sandbox\")\noptions.add_argument(\"headless\")\noptions.add_argument(\"start-maximized\")\noptions.add_argument(\"window-size=1900,1080\"); \ndriver = webdriver.Chrome(chrome_options=options, executable_path=\"/usr/bin/chromedriver\")\ndriver.get(\"https://www.example.com\")\nhtml = driver.page_source\nprint(html)\n"], ["history = model.fit(X, y,validation_split=0.1, \\\n                epochs=20, \\\n                batch_size=1000, \\\n                class_weight = cw)\n"], [], [], ["    {\n        \"name\": \"Run\",\n        \"etc\": \"etc\",\n        \"envFile\": \"${workspaceFolder}/venv\"\n    }\n"], ["color_1 = int(input())\ncolor_2 = int(input())\ncolor_3 = int(input())\n\nif color_1 < color_2:\n  if color_1 < color_3:\n    color_2 = color_2 - color_1\n    color_3 = color_3 - color_1\n    color_1 = (color_1 - color_1)\n\nif color_2 < color_1:\n  if color_2 < color_3:\n    color_1 = color_1 - color_2\n    color_3 = color_3 - color_2\n    color_2 = (color_2 - color_2)\n\nif color_3 < color_2:\n  if color_3 < color_1:\n    color_2 = color_2 - color_3\n    color_1 = color_1 - color_3\n    color_3 = (color_3 - color_3)\n\nif color_1 == color_2:\n   color_1 = color_1 - color_1\n   color_2 = color_2 - color_2\n   color_3 = color_3 - color_3\n\nprint(color_1, color_2, color_3)\n"], ["model = load_model('my_model_name.h5', custom_objects={\n    'Adam': lambda **kwargs: hvd.DistributedOptimizer(keras.optimizers.Adam(**kwargs))\n})\n"], ["//Headless chrome browser and configure\n            WebDriverManager.chromedriver().setup();\n            ChromeOptions chromeOptions = new ChromeOptions();\n            chromeOptions.addArguments(\"--no-sandbox\");\n            chromeOptions.addArguments(\"--headless\");\n            chromeOptions.addArguments(\"disable-gpu\");\n//          chromeOptions.addArguments(\"window-size=1400,2100\"); // Linux should be activate\n            driver = new ChromeDriver(chromeOptions);\n"], ["python -m venv Code\\Python\\Project1\\venv\n"], ["if color_1 == 255:\n    color_1 = 0\n    if color_1 == 50:\n        color_1 = 0\n            if (color_1 >= 51) and  not (color_1 >= 255):\n            color_1 = color_2 - gray\n"], ["color_1 = int(input())\ncolor_2 = int(input())\ncolor_3 = int(input())\ngray = 50\n\nif color_1 >= 50 and not color_1 >= 255:\n    color_1 = color_1 - gray\n\nif color_2 >= 50 and not color_2 >= 255:\n    color_2 = color_2 - gray\n\nif color_3 >= 50 and not color_3 >= 255:\n    color_3 = color_3 - gray\n\nprint(color_1, color_2, color_3)\n"], [], ["import sys\nsys.path.append('src/package1')\nimport script1\n"], [], [], ["git clone --recursive https://github.com/stan-dev/pystan.git\ncd pystan\npython setup.py install\n"], ["conda create -n stan python=<your_version> numpy cython\n", "conda activate stan   \n", "source activate stan\n(stan)  pip install pystan\n(stan)  pip install gcc\n", "gcc --version\ngcc (GCC) 4.8.5\n"], ["pip uninstall fbprophet pystan\npip --no-cache-dir install pystan==2.17  #any version\npip --no-cache-dir install fbprophet==0.2 #any version\nconda install Cython --force\n\npip install pystan\nconda install pystan -c conda-forge\nconda install -c conda-forge fbprophet\n", "import pystan\nmodel_code = 'parameters {real y;} model {y ~ normal(0,1);}'\nmodel = pystan.StanModel(model_code=model_code)\ny = model.sampling().extract()['y']\ny.mean()  # with luck the result will be near 0\n"], ["import numpy as np\nvalues= your_df.loc[your_id]\nindexer= ~values.isna()\navg_log=values[indexer].map(np.log).mean()\nnp.exp(avg_log)\n"], ["np.exp(np.log(df.prod(axis=1))/df.notna().sum(1))\n", "2010-07-01         NaN\n2010-10-01    0.968237\n2011-01-01         NaN\n2011-04-01         NaN\n2011-07-01    0.994817\n2011-10-01    1.007370\n2012-01-01    1.057660\n2012-04-01    0.979955\n2012-07-01    1.017180\n2012-10-01    0.947547\ndtype: float64\n"], ["len(np.unique(df.counts))==1\nFalse\n", "len(set(df.counts.tolist()))==1\n", "df.counts.eq(df.counts.iloc[0]).all()\nFalse\n", "df.counts.std()==0\nFalse\n"], [], ["$ python3 -m pip install PyMySQL[rsa]\n"], ["#!/usr/bin/env bash\n\nLAYER_NAME=$1 # input layer, retrived as arg\nZIP_ARTIFACT=${LAYER_NAME}.zip\nLAYER_BUILD_DIR=\"python\"\n\n# note: put the libraries in a folder supported by the runtime, means that should by python\n\nrm -rf ${LAYER_BUILD_DIR} && mkdir -p ${LAYER_BUILD_DIR}\n\ndocker run --rm -v `pwd`:/var/task:z lambci/lambda:build-python3.6 python3.6 -m pip --isolated install -t ${LAYER_BUILD_DIR} -r requirements.txt\n\nzip -r ${ZIP_ARTIFACT} .\n\necho \"Publishing layer to AWS...\"\naws lambda publish-layer-version --layer-name ${LAYER_NAME} --zip-file fileb://${ZIP_ARTIFACT} --compatible-runtimes python3.6\n\n# clean up\nrm -rf ${LAYER_BUILD_DIR}\nrm -r ${ZIP_ARTIFACT}\n"], [], [], [], ["from django.db import models\n\n\ndef dynamic_fieldname_model_factory(fields_prefix):\n    class AbstractModel(models.Model):\n\n        class Meta:\n            abstract = True\n\n    AbstractModel.add_to_class(\n        fields_prefix + '_title',\n        models.CharField(max_length=255, blank=True, default=''),\n    )\n    return AbstractModel\n\n\nclass ModelOne(dynamic_fieldname_model_factory('someprefix1')):\n    id = models.AutoField(primary_key=True)\n\n\nclass ModelTwo(dynamic_fieldname_model_factory('someprefix2')):\n    id = models.AutoField(primary_key=True)\n", "# Generated by Django 2.1.7 on 2019-03-07 19:53\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='ModelOne',\n            fields=[\n                ('someprefix1_title', models.CharField(blank=True, default='', max_length=255)),\n                ('id', models.AutoField(primary_key=True, serialize=False)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='ModelTwo',\n            fields=[\n                ('someprefix2_title', models.CharField(blank=True, default='', max_length=255)),\n                ('id', models.AutoField(primary_key=True, serialize=False)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n    ]\n"], ["class Animal(models.Model):\n    name = models.CharField(max_length=32)\n", "attrs = {\n    'name': models.CharField(max_length=32),\n    '__module__': 'myapp.models'\n}\nAnimal = type(\"Animal\", (models.Model,), attrs)\n", "from south.db import db\nmodel_class = generate_my_model_class()\nfields = [(f.name, f) for f in model_class._meta.local_fields]\ntable_name = model_class._meta.db_table\ndb.create_table(table_name, fields)\n# some fields (eg GeoDjango) require additional SQL to be executed\ndb.execute_deferred_sql()\n"], ["from django.contrib.postgres.fields import HStoreField\nfrom django.db import models\n\nclass Dog(models.Model):\n    name = models.CharField(max_length=200)\n    data = HStoreField()\n\n    def __str__(self):\n        return self.name\n", ">>> Dog.objects.create(name='Rufus', data={'breed': 'labrador'})\n>>> Dog.objects.create(name='Meg', data={'breed': 'collie'})\n\n>>> Dog.objects.filter(data__breed='collie')\n<QuerySet [<Dog: Meg>]>\n"], ["ModelOne.add_to_class(\n    '%s_title' % field_prefix, \n    models.CharField(max_length=255, blank=True, default='')\n)\n"], ["@dataclasses.dataclass\nclass Test:\n    value: int\n\n    def __post_init__(self):\n        if not isinstance(self.value, int):\n            raise ValueError('value not an int')\n            # or self.value = int(self.value)\n", "def __post_init__(self):\n    for field in dataclasses.fields(self):\n        value = getattr(self, field.name)\n        if not isinstance(value, field.type):\n            raise ValueError(f'Expected {field.name} to be {field.type}, '\n                             f'got {repr(value)}')\n            # or setattr(self, field.name, field.type(value))\n"], ["import dataclasses\n\n@dataclasses.dataclass\nclass Test:\n    value : int\n\n    def __post_init__(self):\n        self.value = int(self.value)\n"], [], ["Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/home/vagrant/.local/lib/python3.7/site-packages/pip-18.1.dist-info/RECORD'\n"], ["Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '~/.local/lib/python3.7/site-packages/pip-19.0.1.dist-info/METADATA'\n", "~/.local/lib/python3.7/site-packages/pip-19.0.1.dist-info/\n", "cd ~/.local/lib/python3.7/site-packages/pip-19.0.1.dist-info/\ncp -r ./pip-19.0.1.dist-info/* ./\nrm -r ./pip-19.0.1.dist-info\n", "entry_points.txt  INSTALLER  LICENSE.txt  METADATA  RECORD  top_level.txt  WHEEL\n"], ["pip install cryptography\n"], ["cd /path/to/new/virtual/environment\nsource env/bin/activate\n"], ["import mysql.connector\ndef connection():\n    conn = mysql.connector.connect(host = \"XXXXX\",\n                  user = 'XXXXX',\n                  password = 'XXXXX',\n                  database = 'login_page',\n                  auth_plugin='mysql_native_password')\n\n    c = conn.cursor()\n    return c , conn\n"], ["if resp.status_code == 200:\n    print ('OK!')\nelse:\n    print ('Boo!)\n"], [], ["g=df.groupby('CLASS')\n\n-g.count().sub(g.size(),0)\n\n          FEATURE1  FEATURE2  FEATURE3\nCLASS                              \nB             0         0         0\nX             1         1         2\n", "pd.DataFrame({x: y.isna().sum()for x , y in g }).T.drop('CLASS',1)\nOut[468]: \n   FEATURE1  FEATURE2  FEATURE3\nB         0         0         0\nX         1         1         2\n"], ["df.drop('CLASS', 1).isna().groupby(df.CLASS, sort=False).sum().reset_index()\n\n  CLASS  FEATURE1  FEATURE2  FEATURE3\n0     X       1.0       1.0       2.0\n1     B       0.0       0.0       0.0\n", "df.groupby('CLASS').count().rsub(df.groupby('CLASS').size(), axis=0)\n", "g = df.groupby('CLASS')\ng.count().rsub(g.size(), axis=0)\n", "       FEATURE1  FEATURE2  FEATURE3\nCLASS                              \nB             0         0         0\nX             1         1         2\n"], ["v = df.groupby('c')[['a', 'b']].transform('sum')\ndf['ab_weighted'] = v.a / v.b\n\ndf\n   a   b  c  ab_weighted\n0  1   7  q     0.294118\n1  2   8  q     0.294118\n2  3   9  q     0.294118\n3  4  10  q     0.294118\n4  5  11  w     0.478261\n5  6  12  w     0.478261\n"], ["df.c.map(df.groupby(['c'])['a', 'b'].apply(lambda x: sum(x['a'])/sum(x['b'])))\nOut[67]: \n0    0.294118\n1    0.294118\n2    0.294118\n3    0.294118\n4    0.478261\n5    0.478261\nName: c, dtype: float64\n"], ["C:Users\\bcollins\\UHD_PY\\uhd\\host\\build\\python\\uhd\n"]]