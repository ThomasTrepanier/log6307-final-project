[["def filterText(text):\n    return \" \".join(text.split(\" \"))\n\nfilterText(\"Hello \\n\\n\\nWorld\\tI\\n\\nLike money\\t\")\n", "Hello World I Like money\n"], [], [], [], [], ["typing-inspect==0.8.0\ntyping_extensions==4.5.0\n"], [], [], [], ["class Rank(Enum):\n    King = 13\n\nprint(Rank.King.name) # outputs 'King'\nprint(Rank.King.value) # outputs 13\n", "from enum import Enum\nfrom types import DynamicClassAttribute\n\nclass MixedCaseEnum(Enum):\n    @DynamicClassAttribute\n    def name(self):\n        return self._name_.title()\n\nclass Rank(MixedCaseEnum):\n    KING = 13\n\nprint(Rank.KING.name) # outputs 'King'\nprint(Rank.KING.value) # outputs 13\n"], [], [], ["to_save = value.to_string()\nr.set(redis_key, to_save)\n\nvalue = r.get(redis_key)\ndf = pd.read_csv(io.StringIO(value))\n\n"], ["subselect = db.select(Tablename).filter_by(user_id=current_user.id, ...)\n\nselect = db.select(db.func.count()).select_from(subselect)\n\nnumber_of_rows = db.session.execute(select).scalar_one()\n"], ["np.float = float    \nnp.int = int   #module 'numpy' has no attribute 'int'\nnp.object = object    #module 'numpy' has no attribute 'object'\nnp.bool = bool    #module 'numpy' has no attribute 'bool'\n"], ["def updateblog (id:int, title:Optional[str]=None, body:Optional[str]=None, db:Session = Depends(get_db)):\n\n    if title!=None:\n            db.query(models.Blog).filter(models.Blog.id==id).update({'title':title})\n    \n    if body!=None:\n            db.query(models.Blog).filter(models.Blog.id==id).update({'body':body})\n\n    db.commit()\n    \n    return f'Blog #{id} has been updated.'\n"], ["    import os\n    from PIL import Image\n    import pillow_heif\n    \n    \n    def convert_heic_to_jpeg(heic_path, jpeg_path):\n        img = Image.open(heic_path)\n        img.save(jpeg_path, format='JPEG')\n    \n    \n    def convert_all_heic_to_jpeg(input_folder, output_folder):\n        # Register HEIF opener with Pillow\n        pillow_heif.register_heif_opener()\n    \n        # Create output folder if it doesn't exist\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n    \n        # List all files in input folder\n        for filename in os.listdir(input_folder):\n            # Check if file is a HEIC file\n            if filename.endswith(\".heic\") or filename.endswith(\".HEIC\"):\n                # Build full path to input and output file\n                heic_path = os.path.join(input_folder, filename)\n                jpeg_filename = f'{os.path.splitext(filename)[0]}.jpg'\n                jpeg_path = os.path.join(output_folder, jpeg_filename)\n    \n                # Convert HEIC to JPEG\n                convert_heic_to_jpeg(heic_path, jpeg_path)\n    \n    \n    # Example usage\n    input_folder = 'images'\n    output_folder = 'converted'\n    convert_all_heic_to_jpeg(input_folder, output_folder)\n"], ["pip uninstall flask_wtf\npip install flask_wtf\n"], ["\nunique_classes = np.unique(labels)\nclass_counts = np.bincount(labels)\ntotal_samples = len(labels)\n\nclass_weights = {}\nfor cls in unique_classes:\n    class_weight = total_samples / (len(unique_classes) * class_counts[cls])\n    class_weights[cls] = class_weight\n\n\nclass_weights = [class_weights[i] for i in range(len(class_weights))]\nclass_weights = torch.FloatTensor(class_weights).cuda() if torch.cuda.is_available() else torch.FloatTensor(class_weights)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n"], ["      encoding='utf-8'\n", "    encoding='utf-8-sig'\n"], [], ["pygame.Cursor()\n", "cursor_index = 2 # Loading cursor\nit = pygame.Cursor()\nit.data = (cursor_index, )\n"], ["flask==2.0.1\nWerkzeug==2.0.1\n", "pip3 install flask==2.0.1\npip3 install werkzeug==2.0.1\n"], ["\"python.autoComplete.extraPaths\": [\"./path-to-your-code\"],\n"], ["import pywt\nfrom skimage import io, color\n\ndata = io.imread(filename)\n\n# Process your image\ngray = color.rgb2gray(data)\ncoeffs = pywt.dwt2(gray, 'haar')\n\n# Or... process each channel separately\nr, g, b = [c.T for c in data.T]\ncr = pywt.dwt2(r, 'haar')\ncg = pywt.dwt2(g, 'haar')\ncb = pywt.dwt2(b, 'haar')\n\n\n# output: PIL, matplotlib, dump to file...\n"], [], [], [], ["curl -sSL https://install.python-poetry.org | sed 's/symlinks=False/symlinks=True/' | python3 -\n"], [], [], ["from keras.utils.image_utils import img_to_array, load_img \n"], [], ["import pandas_datareader as web\nweb.DataReader('AMZN', 'yahoo', start, end)\n", "import yfinance \nyfinance.download('AMZN', start, end)\n"], ["$ pipenv --rm        # to remove the virtual env\n$ exit               # to exit the virtual env\n$ vim Pipfile        # here change the version to '3.9' by replacing '3.10'\n$ pipenv shell       # this will create a virtual env with 3.9\n$ pipenv install     # to install the requirements\n"], ["try:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\n"], ["from collections import Mapping\n", "from collections.abc import Mapping\n"], [], ["file_id = ''  #right click your file name and paste your link and copy only id\nurl = f'https://drive.google.com/uc?id={file_id}'\n\ndf = pd.read_csv(url)\ndf\n"], ["import pandas as pd\nimport pyarrow as pa\nimport redis\n\n\nr = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\n\ndef save_df_to_redis(r, redis_key, df):\n    buffer = pa.serialize_pandas(df)\n    r.set(redis_key, buffer.to_pybytes())\n\n\ndef load_df_from_redis(r, redis_key):\n    buffer = r.get(redis_key)\n    df = pa.deserialize_pandas(buffer)\n    return df\n\n\n\ndata = {\n    \"Name\": [\"John\", \"Anna\", \"Peter\"],\n    \"Age\": [28, 24, 33],\n}\ndf = pd.DataFrame(data)\n\nsave_df_to_redis(r, \"key\", df)\ndf_redis = load_df_from_redis(r, \"key\")\nprint(df_redis)\n"], [], ["API_KEY=\"xxxxxx\"\nSECRET_KEY=\"xxxxxx\"\n", "from google.colab import drive    \ndrive.mount('/content/drive')\n", "!pip install --quiet python-dotenv\nimport dotenv\nimport os\n\ndotenv.load_dotenv('/content/drive/MyDrive/01 Work \nFile/Credentials/.env')\nsecret_key = os.getenv('SECRET_KEY')\n", "print(secret_key)\n"], [], [], ["metric = 'val_accuracy'\nModelCheckpoint(filepath=r\"C:\\Users\\reda.elhail\\Desktop\\checkpoints\\{}\".format(Name), monitor=metric,\n                    verbose=2, save_best_only=True, mode='max')]\n"], ["pip install boto3 botocore awscli aiobotocore --ignore-installed\n"], ["\"terminal.integrated.env.windows\": { \"PYTHONPATH\": \"${workspaceFolder}\" }\n"], [], ["    # MemorySummary=pd.DataFrame(columns=header)\n    # broken after pandas update?\n    # AttributeError: type object 'object' has no attribute 'dtype'\n    \n    MemorySummary=pd.DataFrame(pd.np.empty((0, 5)))\n    MemorySummary.set_axis( header, axis=1, inplace=True) \n\n"], [], [], ["import pypdf\nfrom pdfminer.high_level import extract_text\n\nfile_path = ['sample-2.pdf', 'image-based-pdf-sample.pdf']\n\nfor doc in file_path:\n    reader = pypdf.PdfReader(doc)\n    for i in range(len(reader.pages)):\n        page = reader.pages[i]\n        text = page.extract_text()\n        total_words = len(text.split())\n    if total_words > 0:\n        print(f\"This document has text: {doc}\")\n    else:\n        print(f\"This document has images: {doc}\")\n", "This document has text: sample-2.pdf\nThis document has images: image-based-pdf-sample.pdf\n"], ["try:\n    nlp = spacy.load(\"en_core_web_trf\")\nexcept:\n    print(\"Downloading spaCy NLP model...\")\n    print(\"This may take a few minutes and it's one time process...\")\n    os.system(\n        \"pip install https://huggingface.co/spacy/en_core_web_trf/resolve/main/en_core_web_trf-any-py3-none-any.whl\")\n    nlp = spacy.load(\"en_core_web_trf\")\n", "import spacy\nimport os\n\ntry:\n    nlp = spacy.load(\"en_core_web_trf\")\nexcept:\n    print(\"Downloading spaCy NLP model...\")\n    print(\"This may take a few minutes and it's one time process...\")\n    os.system(\"pip install https://huggingface.co/spacy/en_core_web_trf/resolve/main/en_core_web_trf-any-py3-none-any.whl\")\n    nlp = spacy.load(\"en_core_web_trf\")\n\n\ndef perform_ner(*args, **kwargs):\n    query = kwargs['query']\n    # Process the input text with spaCy NLP model\n    doc = nlp(query)\n\n    # Extract named entities and categorize them\n    entities = [(entity.text, entity.label_) for entity in doc.ents]\n\n    return entities\n\n\nif __name__ == \"__main__\":\n    # Example input text\n    input_text = \"I want to buy a new iPhone 12 Pro Max from Apple.\"\n\n    # Perform NER on input text\n    entities = perform_ner(query=input_text)\n\n    # Print the extracted entities\n    print(entities)\n"], ["import ast\n\ncode_string = \"\"\"\n# A comment.\ndef foo(a, b):\n  return a + b\nclass Bar(object):\n  def __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n\"\"\".strip()\n\ncode_lines = code_string.splitlines(keepends=True)\nfor node in ast.walk(ast.parse(code_string)):\n    if isinstance(node, ast.FunctionDef):\n        lines = code_lines[node.lineno - 1:node.end_lineno]\n        lines[0] = lines[0][node.col_offset:]\n        lines[-1] = lines[-1][:node.end_col_offset]\n        print(''.join(lines))\n", "def foo(a, b):\n  return a + b\ndef __init__(self):\n    self.my_list = [\n        'a',\n        'b',\n    ]\n"], ["conda create --name python38 python=3.8\n", "conda activate python38\n", "conda install -c conda-forge gdal\n", "pip install pygdal\n"], [], [], [], ["from sqlalchemy.orm import Session,aliased\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, String, select,label,func,text,literal_column\n\n u = aliased(User)\n q = select(u.id).select_from(u).where(u.username == user.username)\n"], [], ["# create OneHotEncoder object\nencoder = OneHotEncoder()\n\n# fit and transform color column\none_hot_array = encoder.fit_transform(df[['color']]).toarray()\n\n# create new dataframe from numpy array\none_hot_df = pd.DataFrame(one_hot_array, columns = encoder.get_feature_names(), index = df.index)\n\n#concat with df\ndata = pd.concat([df, one_hot_df], axis=1).drop(['color'], axis=1)\n"], [], [], ["df.loc['perc'] = df.iloc[2]/df.iloc[1]\n"], ["ex_dict = {\"key1\":5, \"key2\":2, \"key3\": 3 \"key4\": 4, \"key5\": 5}\n\nex_dict.pop('key2')\n\nex_dict\n\nO/P: {\"key1\":5, \"key3\": 3, \"key4\": 4, \"key5\": 5}\n"], ["my_list = [3, 0, 1, 0, 5, 2, 0, 4]\n\n# Sort the list, excluding zeros\nsorted_list = sorted(my_list, key=lambda x: (x == 0, x))\n\nprint(sorted_list)\n"], ["from docx.oxml import OxmlElement, ns\n\ndef create_element(name):\n    return OxmlElement(name)\n\ndef create_attribute(element, name, value):\n    element.set(ns.qn(name), value)\n\n\ndef add_page_number(run):\n    fldChar1 = create_element('w:fldChar')\n    create_attribute(fldChar1, 'w:fldCharType', 'begin')\n\n    instrText = create_element('w:instrText')\n    create_attribute(instrText, 'xml:space', 'preserve')\n    instrText.text = \"PAGE\"\n\n    fldChar2 = create_element('w:fldChar')\n    create_attribute(fldChar2, 'w:fldCharType', 'end')\n\n    run._r.append(fldChar1)\n    run._r.append(instrText)\n    run._r.append(fldChar2)\n\ndoc = Document()\nadd_page_number(doc.sections[0].footer.paragraphs[0].add_run())\ndoc.save(\"your_doc.docx\")\n"], ["import asyncio\n\nimport uvicorn\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello, World!\"}\n\n\nasync def main():\n    config = uvicorn.Config(app, port=5000, log_level=\"info\")\n    server = uvicorn.Server(config)\n    await server.serve()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n"], ["a = int(input('Enter First Number:'))\nb = int(input('Enter Second Number:'))\nc = int(input('Enter Third Number:'))\nmax = 0\nif a > max:\n   max = a\nif b > max:\n   max = b\nif c > max:\n   max = c\nprint (max)\n"], ["check_length = 100000\n\nlist_example = []\n\nfor i in range(check_length):\n    list_example.append({f\"companies_info_{i}\": i})\n", "import pandas as pd\nimport numpy as np\nimport time\nimport math\n\n\n# Method 1\n\ndef get_frame_method_1(l):\n\n    list_example_d = {\"time\": l}\n\n    df_1 = pd.DataFrame.from_dict(data=list_example_d, orient=\"columns\")\n\n    index_list = []\n\n    for count, d in enumerate(df_1.time):\n        index_list.extend(list(d.keys()))\n        df_1.time[count]= list(d.values())[0]\n\n    df_1.index= index_list\n\n    return df_1\n\n\n# Method 2\n\ndef get_frame_method_2(l):\n\n    df_list = []\n\n    for d in l:\n        d_df = pd.DataFrame.from_dict(data=d, orient=\"index\", columns=[\"time\"])\n        df_list.append(d_df)\n\n    df_2 = pd.concat(df_list, axis= 0)\n\n    return df_2\n\n\n# Method 3\n\ndef get_frame_method_3(l):\n\n    df_3 = (pd.concat(map(pd.Series, l))\n            .to_frame('time')\n        )\n    \n    return df_3\n\n\n# Method 4\n\ndef get_frame_method_4(l):\n\n    # build a nested dict from list_example and build df\n    df_4 = pd.DataFrame.from_dict({k: {'time': v} for d in l for k,v in d.items()}, orient='index')\n\n    return df_4\n\n\n# Method 5\n\ndef get_frame_method_5(l):\n\n    df_5 = pd.concat([ pd.Series(d.values(), index=d.keys())\n        for d in l ]).to_frame('time')\n    \n    return df_4\n\n\ncheck_length = 100000\n\nlist_example = []\n\nfor i in range(check_length):\n    list_example.append({f\"companies_info_{i}\": i})\n\n\ntotal_time_1_d = {}\n\nfor i in range(100):\n    t_0 = time.time()\n    df_1 = get_frame_method_1(list_example)\n    t_1 = time.time()\n    df_2 = get_frame_method_2(list_example)\n    t_2 = time.time()\n    df_3 = get_frame_method_3(list_example)\n    t_3 = time.time()\n    df_4 = get_frame_method_4(list_example)\n    t_4 = time.time()\n    df_5= get_frame_method_5(list_example)\n    t_5 = time.time()\n    total_time_1_d[f\"{i}\"] = {\"Method 1\": (t_1-t_0), \"Method 2\": (t_2-t_1), \"Method 3\": (t_3-t_2), \"Method 4\": (t_4-t_3), \"Method 5\": (t_5-t_4)}\n    print(i)\n\n\ntotal_time_df = pd.DataFrame.from_dict(data= total_time_1_d, orient=\"index\")\n\n\nfor i in range(5):\n    print(f\"Method {i+1}: Mean - {total_time_df.describe().iloc[1, i]}, 95% CI ({total_time_df.describe().iloc[1, i]-1.96*(total_time_df.describe().iloc[2, i])/math.sqrt((total_time_df.describe().iloc[0, i]))}, {total_time_df.describe().iloc[1, i]+1.96*(total_time_df.describe().iloc[2, i])/math.sqrt((total_time_df.describe().iloc[0, i]))})\")\n"], [], ["version: '3.8'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  \n  #image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.0}\n  build: .\n  \n  environment:\n    &airflow-common-env\n    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    # For backward compatibility, with Airflow <2.3\n    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n    AIRFLOW__CORE__FERNET_KEY: ''\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n    # yamllint disable rule:line-length\n    # Use simple http server on scheduler for health checks\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\n    # yamllint enable rule:line-length\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n    PYTHONPATH: '$PYTHONPATH;/python_extended'\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - ${AIRFLOW_PROJ_DIR:-.}/python:/python_extended\n  user: \"${AIRFLOW_UID:-50000}:0\"\n  depends_on:\n    &airflow-common-depends-on\n    redis:\n      condition: service_healthy\n    postgres:\n      condition: service_healthy\n"], ["volumes:\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n", "CREATE EXTENSION vector;\n"], ["print(Foo(123) == Foo(123) # Prints 'False'\n"], ["pip install .[dev]\n", "[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = 'my_application_name'\nversion = '0.0.1'\nrequires-python = \">=3.11.0\"\n\ndependencies = [\n    'cryptography==41.0.1',\n]\n\n[project.optional-dependencies]\nlining = [\n  'pylint==2.17.4'\n]\nformatting = [\n  'black[d]==23.3.0'\n]\ndev = ['my_application_name[linting, formatting]']\n"], ["yt-dlp.exe --skip-download --print \"%(duration>%H:%M:%S.%s)s %(creator)s %(uploader)s - %(title)s\" https://youtu.be/bJ9r8LMU9bQ\n"], ["pip uninstall opencv-python-headless -y \n\npip install opencv-python --upgrade\n"], ["pip uninstall opencv-python \n", "pip install opencv-python\n"], ["from PyPDF2 import  PdfReader, PdfWriter\n\npdf_file = \"C:/Users/11359023/Desktop/original.pdf\"\nwatermark = \"C:/Users/11359023/Desktop/watermark.pdf\"\nmerged = \"C:/Users/11359023/Desktop/merged.pdf\"\n\nwith open(pdf_file, \"rb\") as input_file, open(watermark, \"rb\") as watermark_file:\ninput_pdf = PdfReader(input_file) #opens the original file\n\nwatermark_pdf = PdfReader(watermark) #opens the watermarked file\nwatermark_page = watermark_pdf.pages[0] #gets the first page of the watermark\n\noutput = PdfWriter() #this will hold the new pages\n\nfor i in range(len(input_pdf.pages)): #go through each page\n    pdf_page = input_pdf.pages[i]\n    pdf_page.merge_page(watermark_page) #combine the watermark and the current page\n    output.add_page(pdf_page)\n\nwith open(merged, \"wb\") as merged_file:\n    output.write(merged_file)\n"], ["import google.auth\nimport google.auth.transport.requests\ncreds, project = google.auth.default()\n\n# creds.valid is False, and creds.token is None\n# Need to refresh credentials to populate those\n\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Now you can use creds.token\n"], [], [], [], [], ["curl -sSL https://install.python-poetry.org | sed 's/symlinks=False/symlinks=True/' | python3 -\n"], [], ["from dataclasses import dataclass, field\nfrom typing import Callable, List, Union\nfrom dash.dependencies import handle_callback_args\nfrom dash.dependencies import Input, Output, State\n\n\n@dataclass\nclass Callback:\n    func: Callable\n    outputs: Union[Output, List[Output]]\n    inputs: Union[Input, List[Input]]\n    states: Union[State, List[State]] = field(default_factory=list)\n    kwargs: dict = field(default_factory=lambda: {\"prevent_initial_call\": False})\n\n\nclass CallbackManager:\n    def __init__(self):\n        self._callbacks = []\n\n    def callback(self, *args, **kwargs):\n        output, inputs, state, prevent_initial_call = handle_callback_args(\n            args, kwargs\n        )\n\n        def wrapper(func):\n            self._callbacks.append(\n                Callback(\n                    func,\n                    output,\n                    inputs,\n                    state,\n                    {\"prevent_initial_call\": prevent_initial_call}\n                )\n             )\n\n        return wrapper\n\n    def attach_to_app(self, app):\n        for callback in self._callbacks:\n            app.callback(\n                callback.outputs, callback.inputs, callback.states, **callback.kwargs\n            )(callback.func)\n", "import dash\n\nfrom callback_manager import CallbackManager\n\ncallback_manager = CallbackManager()\n\n\n@callback_manager.callback(\n    dash.dependencies.Output('label', 'children'),\n    [dash.dependencies.Input('call_btn', 'n_clicks')])\ndef update_label(n_clicks):\n    if n_clicks > 0:\n        return \"Callback called!\"\n", "import dash\nimport dash_html_components as html\n\nfrom callbacks import callback_manager\n\napp = dash.Dash(__name__)\ncallback_manager.attach_to_app(app)\n\napp.layout = html.Div([\n    html.Div(id=\"label\"),\n    html.Button('Call callback', id='call_btn', n_clicks=0),\n])\nif __name__ == '__main__':\n    app.run_server(debug=True)\n", "from callbacks1 import callback_manager as callback_manager1\nfrom callbacks2 import callback_manager as callback_manager2\n\napp = dash.Dash(__name__)\ncallback_manager1.attach_to_app(app)\ncallback_manager2.attach_to_app(app)\n"], ["sudo apt install python3.8-venv\n", "python3 -m venv env\n", "source env/bin/activate\n", "deactivate\n"], [], [], [], [], ["{   \n\"terminal.integrated.profiles.windows\": {\n    \"Anaconda CMD\": {\n        \"path\": [\n            \"${env:windir}\\\\Sysnative\\\\cmd.exe\",\n            \"${env:windir}\\\\System32\\\\cmd.exe\"\n        ],\n        \"args\": [\n            \"/K\",\n            \"C:\\\\Users\\\\${env:USERNAME}\\\\AppData\\\\Local\\\\Anaconda3\\\\Scripts\\\\activate.bat & conda activate base\"\n        ],\n        \"icon\": \"terminal-cmd\"\n    },\n    \"Anaconda PS\": {\n        \"source\": \"PowerShell\",\n        \"args\": [\n            \"powershell\",\n            \"-NoExit\",\n            \"-ExecutionPolicy ByPass\",\n            \"-NoProfile\",\n            \"-Command\",\n            \"'& 'C:\\\\Users\\\\${env:USERNAME}\\\\AppData\\\\Local\\\\Anaconda3\\\\shell\\\\condabin\\\\conda-hook.ps1'; conda activate base'\"\n        ],\n        \"icon\": \"terminal-powershell\"\n    }\n},\n\"terminal.integrated.defaultProfile.windows\": \"Anaconda CMD\"\n"], [], [], ["decomposition = sm.tsa.seasonal_decompose(df, model = 'additive', period=7)\n"], ["model.layers[0].weight # for accessing weights of first layer wrapped in nn.Sequential()\n"], [], ["pip install --upgrade webdriver_manager\n"], [], [], ["python3 -m venv env\nsource env/bin/activate\n", "python3 -m pip install new_module\n"], ["import os\nos.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = r'path/to/qt/plugins/platforms'\n", "import os\nimport PyQt5\nprint(os.path.dirname(PyQt5.__file__))\n"], [], [], ["# This query syntax will result in an error\nreturn session.query(cls).filter(\n    cls.corelation_id == corelation_id,\n    cls.status == cls.ORDER_ACTIVE if not is_cancel else cls.ORDER_CANCEL\n).first()\n", "# Define the status condition based on the 'is_cancel' flag\nstatus = cls.ORDER_ACTIVE if not is_cancel else cls.ORDER_CANCEL\n\n# Use the defined status condition in the SQLAlchemy query\nreturn session.query(cls).filter(\n    cls.corelation_id == corelation_id,\n    cls.status == status\n).first()\n"], [], ["pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n", "pip install pydantic -U\n", "pip install pydantic==1.10.11\n"], [], ["DATABASES = {\"default\": env.db()}\nDATABASE_URL = env(\"DATABASE_URL\")\n", "DATABASES = {\n    \"default\": {\n        \"ENGINE\": \"django.db.backends.sqlite3\",\n        \"NAME\": BASE_DIR / \"db.sqlite3\",\n    }\n}\n"], ["st = speedtest.Speedtest(secure = True);\n"], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\n# The Service class is used to start an instance of the Chrome WebDriver\n# The no-argument constructor means it will look for the WebDriver executable in the system's PATH\nservice = Service()\n\n# WebDriver.ChromeOptions() is used to set the preferences for the Chrome browser\noptions = webdriver.ChromeOptions()\n\n# Here, we start an instance of the Chrome WebDriver with the defined options and service\ndriver = webdriver.Chrome(service=service, options=options)\n\n# Your code for interacting with web pages goes here\n\n# In the end, always close or quit the driver to ensure all system resources are freed up\ndriver.quit()\n"], [], [], [], [], ["rm -rf <python_path>/site-packages/OpenSSL\n"], ["driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n"], [], [], [], [], ["class TMDB_Category(BaseModel):\n    name: str = Field(validation_alias=\"strCategory\")\n    description: str = Field(validation_alias=\"strCategoryDescription\")\n"], [], [], [], ["from selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\noptions = Options()\noptions.add_argument(\"start-maximized\")\ndriver = webdriver.Chrome(options=options)\ndriver.get(\"https://www.google.com/\")\n"], ["cd /tmp\ngit clone --branch v0.4.4 https://github.com/pgvector/pgvector.git\ncd pgvector \nmake\nsudo make install \nCREATE EXTENSION vector;\n"], [], [], [], [], ["from collections.abc import MutableMapping\nfrom typing import Any, Iterator\n\nfrom pydantic import BaseModel\n\n\nclass BaseModelDict(BaseModel, MutableMapping):\n    \"\"\"Goodness of BaseModel and acts like a dictionary.\"\"\"\n\n    def __contains__(self, x: str) -> bool:\n        return True if x in self.__dict__.keys() else False\n\n    def __delitem__(self, x: str) -> None:\n        del self.__dict__[x]\n\n    def __getitem__(self, x: str) -> Any:\n        return self.__dict__[x]\n\n    def __iter__(self) -> Iterator:\n        return iter(self.__dict__)\n\n    def __json__(self) -> dict:\n        return self.__dict__\n\n    def __len__(self) -> int:\n        return len(self.__dict__)\n\n    def __setitem__(self, key: str, value: Any) -> None:\n        self.__dict__[key] = value\n"], [], [], [">>> db.session.query(Order).count()\n2111\n", ">>> from sqlalchemy import func\n>>> db.session.scalar(db.select(func.count(Order.order_id)))\n2111\n", ">>> db.session.scalar(db.select(func.count(Order.currency)))\n2111\n", "SELECT count(\"order\".currency) AS count_1 \nFROM \"order\"\n", ">>> to_filter = db.select(Order).where(Order.currency=='GBP')\n>>> db.session.scalar(db.select(func.count()).select_from(to_filter))\n17\n"], [], ["pip uninstall virtualenv\n", "virtualenv my_name\n"], [], [], [], ["echo %PATH%\n"], [], ["ModuleNotFoundError: No module named 'virtualenv.seed.embed.via_app_data\n", "virtualenv             20.4.0\n"], ["In jupyter notebook\n\n!python -m spacy download en.  \nimport spacy. \nnlp = spacy.load('en_core_web_sm')\n"], [], ["from collections.abc import Mapping\nfrom collections.abc import MutableMapping\nfrom collections.abc import Sequence\n"], [], [], [], [">>float\n\n>>numpy.float64\n\n>>numpy.double\n\n>>numpy.float_\n"], [], ["pygame.mouse.set_visible(False)\ncursor_img_rect = cursor_img.get_rect()\n\nwhile True:\n    # in your main loop update the position every frame and blit the image    \n    cursor_img_rect.center = pygame.mouse.get_pos()  # update position \n    gameDisplay.blit(cursor_img, cursor_img_rect) # draw the cursor\n"], ["pip install \n", "pip uninstall langchain\npip install langchain\n"], ["cur.execute('''\nCREATE TABLE langchain_pg_embedding (\n    uuid UUID NOT NULL,\n    collection_id UUID,\n    embedding VECTOR,\n    document VARCHAR,\n    cmetadata JSON,\n    custom_id VARCHAR,\n    PRIMARY KEY (uuid))\n''')\n", "ALTER DATABASE postgres SET SEARCH_PATH TO postgres_schema;\n"], [], [], ["wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py --user\npython3 -m pip install pyopenssl --upgrade --user\n"], [], [" export QT_QPA_PLATFORM=offscreen \n"], ["Poetry (1.5.1) is installed now. Great!\n\nTo get started you need Poetry's bin directory (C:\\Users\\<user>\\AppData\\Roaming\\Python\\Scripts) in your `PATH`\nenvironment variable.\n\nAlternatively, you can call Poetry explicitly with `C:\\Users\\<user>\\AppData\\Roaming\\Python\\Scripts\\poetry`.\n\nYou can test that everything is set up by executing:\n"], ["!pip install pydantic -U\n"], ["\"program\": \"${file}\"\n", "\"module\": \"folder.pythonfilename\"\n"], ["%AppData%\\Programs\\Python\\Python311\n%AppData%\\Programs\\Python\\Python311\\Scripts\n"], [], [], ["pip uninstall pathlib\n"], ["pip3 install pyOpenSSL --upgrade\n"], ["import pandas as pd\nimport redis\nimport pickle\n\nr = redis.Redis(host='localhost', port=6379, db=0)\n\ndata = {\n    \"calories\": [\"v1\", 'v2', 'v3'],\n    \"duration\": [50, 40, 45]\n}\ndf = pd.DataFrame(data, index=[\"day1\", \"day2\", \"day3\"])\n\nr.set(\"key\", pickle.dumps(df))\nprint(pickle.loads(r.get(\"key\")))\n", "import pandas as pd\nfrom direct_redis import DirectRedis\n\nr = DirectRedis(host='localhost', port=6379)\n>>> df =  pd.DataFrame([[1,2,3,'235', '@$$#@'], \n                   ['a', 'b', 'c', 'd', 'e']])\n>>> print(df)\n   0  1  2    3      4\n0  1  2  3  235  @$$#@\n1  a  b  c    d      e   \n\n>>> r.set('df', df)   \n\n>>> r.get('df')\n   0  1  2    3      4\n0  1  2  3  235  @$$#@\n1  a  b  c    d      e   \n\n>>> type(r.get('df'))\n<class 'pandas.core.frame.DataFrame'>\n"], [], [], ["data = pd.get_dummies(data,prefix=['Profession'], columns = ['Profession'], drop_first=True)\n", "transformed = jobs_encoder.transform(data['Profession'].to_numpy().reshape(-1, 1))\n#Create a Pandas DataFrame of the hot encoded column\nohe_df = pd.DataFrame(transformed, columns=jobs_encoder.get_feature_names())\n#concat with original data\ndata = pd.concat([data, ohe_df], axis=1).drop(['Profession'], axis=1)\n"], [], [], [], ["From anaconda3\\Library\\bin copy below files and paste them in anaconda3/DLLs\n"], ["From anaconda3\\Library\\bin copy below files and paste them in anaconda3/DLLs\n\n-   libcrypto-1_1-x64.dll\n-   libssl-1_1-x64.dll\n"], [], [], [], [], ["curl -sSL https://install.python-poetry.org | python3.11 -\n"], [], [], [], [], ["---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-2-9b030cf1055e> in <module>()\n----> 1 decomposition = sm.tsa.seasonal_decompose(df, model = 'additive')\n      2 decompose_result.plot()\n\n/usr/local/lib/python3.7/dist-packages/statsmodels/tsa/seasonal.py in seasonal_decompose(x, model, filt, freq, two_sided, extrapolate_trend)\n    125             freq = pfreq\n    126         else:\n--> 127             raise ValueError(\"You must specify a freq or x must be a \"\n    128                              \"pandas object with a timeseries index with \"\n    129                              \"a freq not set to None\")\n\nValueError: You must specify a freq or x must be a pandas object with a time-series index with a freq not set to None\n", "TypeError: seasonal_decompose() got an unexpected keyword argument 'period'\n"], [], [], [], [], ["import speedtest\nwifi = speedtest.Speedtest()\n", "from speedtest import Speedtest\n\ninternet = Speedtest()\n\ndownload_speed = internet.download()\nupload_speed = internet.upload() \n\nprint(\"Download \\t:\", download_speed)\nprint(\"Upload   \\t:\", upload_speed)\n"], [], [], ["{pip install --upgrade openpyxl}\n"], [], [], ["    pip uninstall opencv-python\n    pip install opencv-python==4.7.0.68\n"], ["pip install numpy --upgrade\n"], [], [], ["pyenv install 3.10.1\npyenv global 3.10.1\ncurl -sSL https://install.python-poetry.org | python3.10 -\n"], [], ["pip install pip --upgrade\npip install pyopenssl --upgrade\n"], [], [], ["curl -sSL https://install.python-poetry.org | python3 -\n", "export PATH=\"/home/tperes/.local/bin:$PATH\"\n"], ["async def _check_channel(self, message, pool):\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\n                \"SELECT ignore_channel_id FROM guild_channel_settings WHERE guild_id = %s\",\n                (message.author.guild.id,),\n            )\n            in_database = await cursor.fetchone()\n\n    if in_database and in_database[0] is not None:\n        channel_list = in_database[0].split(\" \")\n        for channelid in channel_list:\n\n            try:\n                channel_id_int = int(channelid)\n            except ValueError:\n                continue\n\n            if int(message.channel.id) == channel_id_int:\n                return False\n\n\nasync def _get_role_count(self, message, pool):\n    async with pool.acquire() as conn:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\n                \"SELECT ignore_role_id, bonus_role_id FROM guild_role_settings WHERE guild_id = %s\",\n                (message.author.guild.id,),\n            )\n            in_database = await cursor.fetchone()\n    if in_database:\n        first_item, second_item, *_ = in_database\n        if first_item is not None:\n            role_list = first_item.split(\" \")\n            for roleid in role_list:\n                try:\n                    roleid_int = int(roleid)\n                except ValueError:\n                    continue\n\n                role = message.author.guild.get_role(roleid_int)\n                if role is None:\n                    continue\n                if role in message.author.roles:\n                    return False\n\n        if second_item is not None:\n            role_list = second_item.split(\" \")\n            count = 0\n            for roleid in role_list:\n                try:\n                    roleid_int = int(roleid)\n                except ValueError:\n                    continue\n\n                role = message.author.guild.get_role(roleid_int)\n                if role is None:\n                    continue\n                if role in message.author.roles:\n                    count += 1\n            return count\n\n\n@commands.Cog.listener(\"on_message\")\nasync def on_message(self, message):\n    if message.author.bot:\n        return\n    if message.type != discord.MessageType.default:\n        return\n    if isinstance(message.channel, discord.channel.DMChannel):\n        return\n\n    # Cooldown\n\n    self.member_cooldown_list = [\n        i\n        for i in self.member_cooldown_list\n        if i[1] + self.cooldown_val > int(time.time())\n    ]\n    member_index = next(\n        (\n            i\n            for i, v in enumerate(self.member_cooldown_list)\n            if v[0] == message.author.id\n        ),\n        None,\n    )\n    if member_index is not None:\n        if self.member_cooldown_list[member_index][1] + self.cooldown_val > int(\n            time.time()\n        ):\n            return\n\n    self.member_cooldown_list.append((message.author.id, int(time.time())))\n\n    loop = asyncio.get_running_loop()\n    db_pool = await aiomysql.create_pool(\n        minsize=3,\n        host=\"<host>\",\n        port=3306,\n        user=\"<user>\",\n        password=\"<password>\",\n        db=\"<db_name>\",\n        autocommit=False,\n        loop=loop,\n    )\n    count = 1\n\n    check_channel_task = asyncio.create_task(\n        self._check_channel(self, message, db_pool)\n    )\n    role_count_task = asyncio.create_task(self._get_role_count(self, message, db_pool))\n\n    # write to database\n\n    mydb = await db_pool.acquire()\n    mycursor = await mydb.cursor()\n    await mycursor.execute(\n        \"SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s\",\n        (message.author.guild.id, message.author.id),\n    )\n    in_database = await mycursor.fetchone()\n\n    role_count = await role_count_task\n    check_channel = await check_channel_task\n    if False in (role_count, check_channel):\n        await mycursor.close()\n        db_pool.release(mydb)\n        db_pool.close()\n        await db_pool.wait_closed()\n        return\n    if role_count:\n        count += role_count\n    if in_database:\n        await mycursor.execute(\n            \"INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s) ON DUPLICATE KEY UPDATE message_count = message_count + 1\",\n            (message.author.id, count, message.author.guild.id),\n        )\n\n    await mydb.commit()\n    await mycursor.close()\n    db_pool.release(mydb)\n    db_pool.close()\n    await db_pool.wait_closed()\n"], [], [], [], ["       number_1= int(input(\"Enter first number\"))\n       number_2= int(input(\"Enter second number\"))\n       number_3= int(input(\"Enter third number\"))\n       if number_1 > number_2 and number_1 > number_3:\n         print(\"the largest number\", number_1)\n       elif number_2 > number_1 and number_2 > number_3:\n         print(\"The largest number is\", number_1)\n       elif number_3 > number_1 and number_3 > number_2:\n         print(\"The largest number is\",number_1)\n\n\n       Enter first number 70\n       Enter second number60\n       Enter third number50\n\n       the largest number 70\n"], ["conda install gdal\n"], [], [], [], [], ["conda install -n yourEnv yourPackage    \n"], ["pip install seaborn --user\n"], ["{\n    \"python.testing.pytestArgs\": [\n        \"tests\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true\n}\n", "from pathlib import Path\nimport os\nimport sys\n\nmain_folder = Path(__file__).parent.parent\nsys.path.insert(0, str(main_folder))\nsys.path.insert(0, str(main_folder / 'app'))\nsys.path.insert(0, str(main_folder / 'tests'))\nos.chdir(main_folder / 'app')\n\nimport app\n", "@report_execution_time\ndef main_test():\n\n    os.system('python -m pytest ..')\n\n\nif __name__ == \"__main__\":\n\n    main_test()\n"], ["wget https://files.pythonhosted.org/packages/00/3f/ea5cfb789dddb327e6d2cf9377c36d9d8607af85530af0e7001165587ae7/pyOpenSSL-22.1.0-py3-none-any.whl\n", "python3 -m easy_install pyOpenSSL-22.1.0-py3-none-any.whl\n"], [], [], [], ["from pydantic import BaseModel\nfrom typing import Optional\n\nclass Blog(BaseModel):\n    title: str\n    body: str\n\nclass UpdateBlog(BaseModel):\n    title: Optional[str]\n    body: Optional[str]\n", "# Update Blog\n@app.put('/blog/{id}', status_code=status.HTTP_202_ACCEPTED)\ndef update(id, request_body: UpdateBlog, db: Session = Depends(get_db)):\n    blog = db.query(models.Blog).filter(models.Blog.id == id).first()\n\n    if not blog:\n        content={'success': False, 'message': f\"Blog with id {id} don't exists\"}\n        return JSONResponse(status_code=status.HTTP_404_NOT_FOUND, content=content)\n    \n    # exclude_none=True will only update the field which you want to update\n    # If you don't want to update \"body\" then only pass the \"title\" and \"body\" field will stay as it is\n    db.query(models.Blog).filter(models.Blog.id == id).update(request_body.dict(exclude_none=True))\n    db.commit()\n\n    content={'success': True, 'message': f\"Blog with id {id} Updated\"}\n    return JSONResponse(status_code=status.HTTP_200_OK, content=content)\n"], [], [], [], [], [], ["conda env create -f=\"Python 310.yml\"\n"], [], ["for key in history.history:\n    print(key)\nloss\naccuracy\nauc_4\nprecision_4\nrecall_4\ntrue_positives_4\ntrue_negatives_4\nfalse_positives_4\nfalse_negatives_4\nval_loss\nval_accuracy\nval_auc_4\n", "for something in something_else:\n    tf.keras.backend.clear_session()  # resets the session\n    model = define_model(...)\n    history = train_model(...)\n"], [], [], [], [], ["CMD [\"gunicorn\",\"--workers\", \"3\", \"--timeout\", \"1000\", \"--bind\", \"0.0.0.0:8000\", \"wsgi:app\"]\n"], ["import pandas as pd\nurl=\"https://drive.google.com/file/d/1a7qwzU2mbaJPkFQZMJCkdE37Ne2DbgHA/view?usp=share_link\"\nreconstructed_url='https://drive.google.com/uc?id=' + url.split('/')[-2]\ndf = pd.read_csv(reconstructed_url)\ndf\n"], [], ["sudo apt remove python3-openssl\n"], ["from dash import Dash\n\nfrom layouts import my_layout\nfrom callbacks import my_callback, my_callback_inputs, my_callback_outputs\n\nif __name__ == \"__main__\":\n    app = Dash(__name__)\n\n    app.layout = my_layout\n    app.callback(my_callback_outputs, my_callback_inputs)(my_callback)\n\n    app.run_server()\n", "my_callback_inputs = []\nmy_callback_outputs = []\n\n\ndef my_callback():\n    return\n"], [], ["import yfinance as yf\n\ndf = yf.download(your_ticks_or_a_tick_list, start=start_date, end=end_date)\n"], ["from PIL import Image\nimport pillow_heif\n\n\npillow_heif.register_heif_opener()\n\nimg = Image.open('c:\\image.HEIC')\nimg.save('c:\\image_name.png', format('png'))\n"], ["sudo rm -rf /usr/lib/python3/dist-packages/OpenSSL\nsudo pip3 install pyopenssl\nsudo pip3 install pyopenssl --upgrade\n"], ["pip uninstall opencv-contrib-python\npip uninstall opencv-contrib-python-headless\n", "pip uninstall opencv-python\npip install opencv-python\n"], [], [], ["from sklearn.preprocessing import OneHotEncoder\noh= OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\none_hot_encoded=oh.fit_transform(df[[\"Profession\"]])\ndf = pd.concat([df,one_hot_encoded],axis=1).drop(columns=[\"Profession\"])\n"], ["source ~/.zshrc\n", "poetry --version\n"], [], [], [], [], ["X509_V_FLAG_CB_ISSUER_CHECK\n", "AttributeError: module 'lib' has no attribute 'OpenSSL_add_all_algorithms'\n", "sudo rm -rf /usr/lib/python3/dist-packages/OpenSSL\n", "sudo pip install pyopenssl\nRequirement already satisfied: pyopenssl in /usr/lib/python3/dist-packages (19.0.0)\n", "pip install pip --upgrade\nSuccessfully installed pip-22.3.1\n\npip install weasyprint\nSuccessfully installed Pyphen-0.13.2 ... weasyprint-57.2 zopfli-0.2.2\n"], [], ["import io\nfrom PIL import Image\nimport pillow_heif\nfrom werkzeug.datastructures import FileStorage\n\nclass Converter:\n\n    def convert_heic_to_jpeg(self, file):\n        # Check if file is a .heic or .heif file\n        if file.filename.endswith(('.heic', '.heif', '.HEIC', '.HEIF')):\n            # Open image using PIL\n            # image = Image.open(file)\n\n            heif_file = pillow_heif.read_heif(file)\n            image = Image.frombytes(\n                heif_file.mode,\n                heif_file.size,\n                heif_file.data,\n                \"raw\",\n            )\n\n            # Convert to JPEG\n            jpeg_image = image.convert('RGB')\n\n            # Save JPEG image to memory temp_img\n            temp_img = io.BytesIO()\n            jpeg_image.save(temp_img, format(\"jpeg\"))\n\n            # Reset file pointer to beginning of temp_img\n            temp_img.seek(0)\n\n            # Create a FileStorage object\n            file_storage = FileStorage(temp_img, filename=f\"{file.filename.split('.')[0]}.jpg\")\n\n            # Set the mimetype to \"image/jpeg\"\n            file_storage.headers['Content-Type'] = 'image/jpeg'\n\n            return file_storage\n        else:\n            raise ValueError(\"File must be of type .heic or .heif\")\n"], [], [], ["import datetime as dt\nimport yfinance as yf\n\ncompany = 'TATAELXSI.NS'\n\n# Define a start date and End Date\nstart = dt.datetime(2020,1,1)\nend =  dt.datetime(2022,1,1)\n\n# Read Stock Price Data \ndata = yf.download(company, start , end)\n\ndata.tail(10)\n"], ["from pathlib import Path\nimport pandas as pd\n\npaths = Path(\"/home/data\").glob(\"*.json\")\ndf = pd.DataFrame([pd.read_json(p, typ=\"series\") for p in paths])\n"], ["conda uninstall pywin32\npip uninstall pywin32\n", "conda uninstall pywin32\nor \npip uninstall pywin32\n"], ["=== LOAD (deserialize)\ndataclass-wizard:  1.742989194\npydantic:          5.31538175\n=== DUMP (serialize)\ndataclass-wizard:  2.300118940\npydantic:          5.582638598\n"], [], ["conda install pycryptodome pycryptodomex\nconda uninstall pandas-datareader\npip install git+https://github.com/raphi6/pandas-datareader.git@ea66d6b981554f9d0262038aef2106dda7138316\n", "! pip install pycryptodome pycryptodomex\n! pip uninstall --yes pandas-datareader\n! pip install git+https://github.com/raphi6/pandas-datareader.git@ea66d6b981554f9d0262038aef2106dda7138316\n", "git clone https://github.com/raphi6/pandas-datareader.git\ncd pandas-datareader\nconda uninstall pandas-datareader\nconda install pycryptodome pycryptodomex\ngit checkout 'Yahoo!_Issue#952'\npython setup.py install --record installed_files.txt\n"], ["pip install --upgrade pywin32==305\n"], [], [">>> import pandas_datareader as dtr\n>>> from datetime import datetime\n>>> initial_portfolio=['AAPL', 'MA', 'F', 'MSFT', '^GSPC']\n>>> startdate = datetime(2022,12,1)\n>>> enddate=datetime(2022,12,10)\n>>> stock_data=dtr.yahoo.daily.YahooDailyReader(initial_portfolio,start=startdate,end=enddate).read()\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"lib/python3.9/site-packages/pandas_datareader/base.py\", line 258, in read\n    df = self._dl_mult_symbols(self.symbols)\n  File \"lib/python3.9/site-packages/pandas_datareader/base.py\", line 268, in _dl_mult_symbols\n    stocks[sym] = self._read_one_data(self.url, self._get_params(sym))\n  File \"lib/python3.9/site-packages/pandas_datareader/yahoo/daily.py\", line 153, in _read_one_data\n    data = j[\"context\"][\"dispatcher\"][\"stores\"][\"HistoricalPriceStore\"]\nTypeError: string indices must be integers\n", "Python 3.9.1 (default, Dec 28 2020, 11:22:14)\n[Clang 11.0.0 (clang-1100.0.33.17)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from pandas_datareader import data as pdr\n>>> import yfinance as yf\n>>> yf.pdr_override()\n>>> y_symbols = ['SCHAND.NS', 'TATAPOWER.NS', 'ITC.NS']\n>>> from datetime import datetime\n>>> startdate = datetime(2022,12,1)\n>>> enddate = datetime(2022,12,15)\n>>> data = pdr.get_data_yahoo(y_symbols, start=startdate, end=enddate)\n[*********************100%***********************]  3 of 3 completed\n>>> data\n             Adj Close                                Close                           ...        Open                             Volume\n                ITC.NS   SCHAND.NS TATAPOWER.NS      ITC.NS   SCHAND.NS TATAPOWER.NS  ...      ITC.NS   SCHAND.NS TATAPOWER.NS    ITC.NS SCHAND.NS TATAPOWER.NS\nDate                                                                                  ...\n2022-12-01  339.549988  195.949997   224.850006  339.549988  195.949997   224.850006  ...  341.700012  191.600006   225.250000  16630417    544485      7833074\n2022-12-02  337.149994  196.600006   225.250000  337.149994  196.600006   225.250000  ...  339.350006  196.000000   225.449997   8388835    122126      7223274\n2022-12-05  336.750000  191.050003   224.199997  336.750000  191.050003   224.199997  ...  337.649994  200.850006   225.250000   9716390    107294     10750610\n2022-12-06  337.299988  196.399994   228.800003  337.299988  196.399994   228.800003  ...  334.100006  191.000000   224.199997   6327430    102911     20071039\n2022-12-07  340.100006  187.350006   225.850006  340.100006  187.350006   225.850006  ...  338.500000  198.000000   228.800003   9813208    122772      7548312\n2022-12-08  338.399994  181.850006   225.050003  338.399994  181.850006   225.050003  ...  340.200012  186.000000   226.000000   6200447    114147      7507975\n2022-12-09  341.399994  176.899994   219.399994  341.399994  176.899994   219.399994  ...  339.750000  183.899994   225.899994   8132228    179660     13087278\n2022-12-12  343.200012  177.350006   217.699997  343.200012  177.350006   217.699997  ...  341.000000  177.750000   219.750000  11214662    133507      8858525\n2022-12-13  345.600006  178.449997   218.850006  345.600006  178.449997   218.850006  ...  344.500000  179.350006   218.800003  10693426     74873      7265105\n2022-12-14  345.399994  179.149994   222.699997  345.399994  179.149994   222.699997  ...  346.000000  180.449997   219.800003   7379878     32085      9179593\n\n[10 rows x 18 columns]\n>>>\n"], ["pipenv update pyOpenSSL\n"], ["    \"settings\": {\n            \"python.defaultInterpreterPath\": \"C:/Users/yyguy/.plotting_test/Scripts/python.exe\",\n            \"code-runner.executorMap\": {\"python\": \"call C:/Users/yyguy/.plotting_test/Scripts/activate.bat && python -u\"}\n    }\n}\n"], [], [], ["pip uninstall spacy\n", "pip install -U pip setuptools wheel\npip install -U spacy\npython -m spacy download en_core_web_sm\n"], [], [], [], ["class MtnPayer(BaseModel):\n  partyIdType: str\n  partyId: str\n\nclass MtnPayment(BaseModel):\n  financialTransactionId: str\n  externalId: str\n  amount: str\n  currency: str\n  payer: MtnPayer\n  payerMessage: str\n  payeeNote: str\n  status: str\n  reason: str\n"], [], [], [], [], [], [], ["sudo apt remove python3-pip \nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\n", "pip install pyopenssl --upgrade\n"], [], [], ["  \"python.terminal.activateEnvironment\": true,\n\n  \"python.venvPath\": \"Add_Venv_DirectoryPath_here\",\n"], ["import pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\nclass CategoricalOneHot(BaseEstimator, TransformerMixin):\n    def __init__(self, list_key_words=None):\n        self.oh_dict = {}\n        self.list_key_words = list_key_words\n\n    def fit(self, X, y=None):\n        self.list_cat_col = []\n        for key_word in self.list_key_words:\n            self.list_cat_col += [col for col in X.columns if key_word in col]\n        for col in self.list_cat_col:\n            oh = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n            oh.fit(X[[col]])\n            names = oh.get_feature_names_out()\n            self.oh_dict[col] = (oh, names)\n        return self\n\n    def transform(self, X):\n        _X = X.copy()\n        for col in self.list_cat_col:\n            oh = self.oh_dict[col][0]\n            df_oh = pd.DataFrame(\n                data=oh.transform(_X[[col]]),\n                columns=self.oh_dict[col][1],\n                index=_X.index)\n            _X = pd.concat([_X, df_oh], axis=1)\n            _X.drop(col, axis=1, inplace=True)\n        return _X\n\nif __name__ == \"__main__\":\n    tex = pd.DataFrame({'city': ['a', 'a', 'e', 'b'], 'state': ['f', 'c', 'd', 'd']})\n    coh = CategoricalOneHot(list_key_words=['city', 'state'])\n    print(coh.fit_transform(tex))\n", "  city state\n0    a     f\n1    a     c\n2    e     d\n3    b     d\n", "   city_a  city_b  city_e  state_c  state_d  state_f\n0     1.0     0.0     0.0      0.0      0.0      1.0\n1     1.0     0.0     0.0      1.0      0.0      0.0\n2     0.0     0.0     1.0      0.0      1.0      0.0\n3     0.0     1.0     0.0      0.0      1.0      0.0\n"], [], [], [], [], [], ["$appsFld=\"$env:USERPROFILE\\AppData\\Local\\Microsoft\\WindowsApps\"; \n$pyPath=(Resolve-Path \"$env:USERPROFILE\\AppData\\Local\\Programs\\Python\\Python*\\\")\n$Env:Path = (($Env:Path.Split(';') | Where-Object { $_ -ne \"$appsFld\" }) -join ';'); \n$Env:Path = (($Env:Path.Split(';') | Where-Object { $_ -ne \"$pyPath\" }) -join ';'); \n$Env:Path += \";$pyPath\";\n$Env:Path +=\";$appsFld\";\n[Environment]::SetEnvironmentVariable(\"PATH\", \"$Env:Path\", \"Machine\")\n", "> python\nPython 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> quit\n\n"], ["from dataclasses import dataclass\nfrom functools import cached_property\n\n@dataclass(frozen=True)\nclass Register:\n    subsection: str\n    name: str\n    abbreviation: str\n    address: int\n    n_bits: int\n    _get_method: Callable[[int], int]\n    _set_method: Callable[[int, int], None]\n    _save_method: Callable[[int, int], None]\n\n    @cached_property\n    def bit_mask(self) -> int:\n        # The cache is used to avoid recalculating since this is a static value\n        # (hence max_size = 1)\n        return create_bitmask(\n            n_bits=self.n_bits,\n            start_bit=0,\n            size=self.n_bits,\n            set_val=True\n            )\n\n    def get(self) -> int:\n        raw_value = self._get_method(self.address)\n        return raw_value & self.bit_mask\n\n    def set(self, value: int) -> None:\n        self._set_method(\n            self.address,\n            value & self.bit_mask\n            )\n\n    def save(self, value: int) -> None:\n        self._save_method(\n            self.address,\n            value & self.bit_mask\n            )\n"], ["from typing import Any\n\nfrom pydantic import BaseModel, Field\nfrom pymapme.models.mapping import MappingModel\n\n\nclass Person(BaseModel):\n    name: str\n    surname: str\n\n\nclass Profile(BaseModel):\n    nickname: str\n    person: Person\n\n\nclass User(MappingModel):\n    nickname: str = Field(source='nickname')\n    first_name: str = Field(source='person.name')\n    surname: str = Field(source='person.surname')\n    full_name: str = Field(source_func='_get_full_name')\n\n    @staticmethod\n    def _get_full_name(model: Profile, default: Any):\n        return model.person.name + ' ' + model.person.surname\n\n\nprofile = Profile(nickname='baobab', person=Person(name='John', surname='Smith'))\nuser = User.build_from_model(profile)\nprint(user.dict())  # {'nickname': 'baobab', 'first_name': 'John', 'surname': 'Smith', 'full_name': 'John Smith'}\n", "d = {\n    \"p_id\": 1,\n    \"billing\": {\n        \"first_name\": \"test\"\n    }\n}\n\n\nclass Billing(BaseModel):\n    first_name: str\n\n\nclass Data(BaseModel):\n    p_id: int\n    billing: Billing\n\n\nclass Order(MappingModel):\n    p_id: int\n    pre_name: str = Field(source='billing.first_name')\n\n\norder = Order.build_from_model(Data(**d))\nprint(order.dict())\n"], [">>> TMDB_Category(strCategory=\"name\", strCategoryDescription=\"description\").json()\n'{\"name\": \"name\", \"description\": \"description\"}'\n", ">>> TMDB_Category(name=\"name\", description=\"description\")\nTMDB_Category(strCategory='name', strCategoryDescription='description')\n"], [], [], [], ["pip install Werkzeug~=2.0.0\n", "pip install jinja2~=3.0.3\n"], [], ["which poetry\n\n# $HOME/.local/bin/poetry  # if installed with Brew\n# maybe elsewhere: \"$HOME/.poetry/bin:$PATH\"\n", "export SHELL_RCFILE=\"~/.zshrc\"\necho \"export POETRY_PATH=$HOME/.local/bin/ && export PATH=\"$POETRY_PATH:$PATH\" >> $SHELL_RCFILE\n"], ["for layer in model.children():\n    if isinstance(layer, nn.Linear):\n        print(layer.state_dict())\n", "OrderedDict([\n('weight', tensor([[-0.0039, -0.0045...]])),\n('bias', tensor([[-0.0019, -0.0025...]]))\n])\n", "for layer in model.children():\n    if isinstance(layer, nn.Linear):\n        print('weight:', layer.weight\n        print('bias:', layer.bias\n"], [], ["pyenv global 3.x.x\n"], ["CB_ISSUER_CHECK = _lib.X509_V_FLAG_CB_ISSUER_CHECK\\r\n", "apt-get --reinstall install python-apt\napt-get --reinstall install apt-transport-https\napt-get install build-essential libssl-dev libffi-dev python-dev\n"], [], [], [], [], ["virtualenv --python=\"/YOUR PATH/python3.9\" \"name of your env\"\n"], [], ["from docx import Document\n\nfolder_data = 'C:\\\\Users\\\\...\\\\Data\\\\'\nfolder_output = 'C:\\\\Users\\\\...\\\\Output\\\\'\n\nclient_ = 'Client 1'; price_ = 99.99\n\ndocument_ = Document(f'{folder_data}invoiceTemplate.docx')\ndocument_.paragraphs[3].add_run(f'{price_} EUR')\n\n# ... more code ...\n\ndocument_.save(f'{folder_output}{client_} invoice.docx')\n"], [], [], [], ["pip3 uninstall urllib3\npip3 install urllib3\n"], ["{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${fileDirname}\",\n            \"env\": {\"PYTHONPATH\": \"${workspaceFolder}/mySubdir:${env:PYTHONPATH}\"},\n        }\n    ]\n}\n"], ["brew install poetry\n"], [], ["pip install --upgrade flask_login\n"], ["from keras.preprocessing.image import img_to_array\n", "from keras_preprocessing.image import img_to_array\n"], [], [], ["beyond top level package error\n", "ModuleNotFoundError: No module named 'convenience'\n", "ModuleNotFoundError: No module named 'convenience'\n", "\"terminal.integrated.env.windows\"\n", "\"terminal.integrated.env.linux\"\n"], ["# build a nested dict from list_example and build df\ndf = pd.DataFrame.from_dict({k: {'time': v} for d in list_example for k,v in d.items()}, orient='index')\nprint(df)\n                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], [" python -m pip install --upgrade pip setuptools virtualenv\n"], [], [], ["  __init__.py file was missing.\n"], ["# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnumeric_X_train = X_train.drop(low_cardinality_cols, axis=1)\nnumeric_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nnew_X_train = pd.concat([numeric_X_train, OH_cols_train], axis=1)\nnew_X_valid = pd.concat([numeric_X_valid, OH_cols_valid], axis=1)\nprint(new_X_train)\n"], ["import pandas as pd\nimport numpy as np\nfrom numba import jit\n\ndf = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], 'b': [1, 3, 5, 7, 9, 11]})\n\n@jit\ndef f(w):\n    # we have access to both columns of the dataframe here\n    return np.max(w), np.min(w)\n\ndf.rolling(3, method='table').apply(f, raw=True, engine='numba')\n"], ["conda env create -n spa --file .\\environment.yml\n", "conda env create -n spa --file environment.yml\n"], [], [], ["C:\\Users\\Default\\AppData\\Local\\Programs\\Python\\Python37\nC:\\Users\\Default\\AppData\\Local\\Programs\\Python\\Python37\\Scripts\n"], [], [">> code ~/.zshrc\n"], [], [], [], ["import yt_dlp # pip install yt_dlp\n\ndef hook(d):\n    if d['status'] == 'finished':\n        filename = d['filename']\n        print(filename) # Here you will see the PATH where was saved.\n\ndef client(video_url, download=False):\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n         return ydl.extract_info(video_url, download=download)\n\nydl_opts = { \n        'format': 'bestaudio/best',\n        'outtmpl': '%(title)s.%(ext)s', # You can change the PATH as you want\n        'download_archive': 'downloaded.txt',\n        'noplaylist': True,   \n        'quiet': True,\n        'no_warnings': True,\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'mp3',\n            'preferredquality': '192',\n        }],\n        'progress_hooks': [hook]\n}\n", "for song in lista:\n    song_details = client(f'ytsearch:{song}', download=False) # get the json details about the song  \n    download     = client(f'ytsearch:{song}', download=True)  # download the song\n\n    song_details['title']\n    song_details['format_note']\n    song_details['audio_ext']\n    song_details['filesize']\n"], [], [], [], [], [], [], [], ["python3 -m virtualenv my_env\n", "sudo apt install virtualenv\n", "virtualenv my_env\n"], [], [], ["import plotly.express as px\ndf = px.data.gapminder().query(\"country=='Canada'\")\nfig = px.line(df, x=\"year\", y=\"lifeExp\", title='Life expectancy in Canada',color_discrete_sequence=[\"#ff97ff\"])\nfig.show()\n"], [], [], ["ggg = pd.DataFrame({\"a\":[1,2,3,4,5,6,7], \"b\":[7,6,5,4,3,2,1]})\n\ndef my_rolling_apply2(df, fun, window):\n    prepend = [None] * (window - 1)\n    end = len(df) - window\n    mid = map(lambda start: fun(df[start:start + window]), np.arange(0,end))\n    last =  fun(df[end:])\n    return [*prepend, *mid, last]\n\nmy_rolling_apply2(ggg, lambda df: (df[\"a\"].max(), df[\"b\"].min()), 3)\n", "[None, None, (3, 5), (4, 4), (5, 3), (6, 2), (7, 1)]\n"], [], [], [], ["FROM apache/airflow:2.3.2\nCOPY requirements.txt /requirements.txt\nRUN pip install --user --upgrade pip\nRUN pip install --no-cache-dir --user -r /requirements.txt\n", "docker build . --tag pyrequire_airflow:2.3.2\n", "image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.2}\n", "image: ${AIRFLOW_IMAGE_NAME:-pyrequire_airflow:2.3.2}\n"], [], [], ["Remove-Item $env:USERPROFILE\\AppData\\Local\\Microsoft\\WindowsApps\\python*.exe\n"], [], [], [], ["del python.exe\ndel python3.exe\n"], [], [], [], ["def _defrost(cls):\n    cls.stash_setattr = cls.__setattr__\n    cls.stash_delattr = cls.__delattr__\n    cls.__setattr__ = object.__setattr__\n    cls.__delattr__ = object.__delattr__\n\ndef _refreeze(cls):\n    cls.__setattr__ = cls.stash_setattr\n    cls.__delattr__ = cls.stash_delattr\n    del cls.stash_setattr\n    del cls.stash_delattr\n\ndef temp_unfreeze_for_postinit(func):\n    assert func.__name__ == '__post_init__'\n    def wrapper(self, *args, **kwargs):\n        _defrost(self.__class__)\n        func(self, *args, **kwargs)\n        _refreeze(self.__class__)\n    return wrapper\n", "@dataclasses.dataclass(frozen=True)\nclass SimpleClass:\n    a: int\n\n    @temp_unfreeze_for_postinit\n    def __post_init__(self, adder):\n        self.b = self.a + adder\n"], [], ["# using sqlalchemy\n\n  from sqlalchemy import create_engine\n\n    def connect_to_db():\n        \"\"\"Create database connection.\"\"\"\n        url = f\"mysql+pymysql://{user}:{db_pwd}@{host}:{port}/{db_name}\"\n        try:\n            engine = create_engine(url)\n            return engine.connect()\n        except Exception as e:\n            raise e\n    \n    \n    def execute_query(query):\n        connection = connect_to_db()\n        with connection as con:\n            result = con.execute(query)\n            return result\n\n    results = execute_query(\"show tables;\")\n    print(\"count = \",results.rowcount)\n\n    output: count =  1298\n"], [], ["(k := next(iter(d)), d.pop(k))\n", "d.popitem()\n"], [], ["checkpoint = ModelCheckpoint(\"mnist-cnn-keras.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n", "checkpoint = ModelCheckpoint(\"./\", monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')\n"], [], ["sudo pip3 uninstall virtualenv\n\n"], [], [], [], ["    #Added code\n    \n        setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n        \n   \n#Initial code that failed with the error\n        \n        (train_examples, validation_examples), info = tfds.load(\n            'cats_vs_dogs',\n            split=['train[:80%]', 'train[80%:]'],\n            with_info=True,\n            as_supervised=True,\n        )\n\n#Complete code together\n\nsetattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n(train_examples, validation_examples), info = tfds.load(\n    'cats_vs_dogs',\n    split=['train[:80%]', 'train[80%:]'],\n    with_info=True,\n    as_supervised=True,\n)\n"], [], [], ["setattr(tfds.image_classification.cats_vs_dogs, '_URL',\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\")\n"], ["pip3 list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 pip3 install -U \n"], [], ["pd.DataFrame(list_example).stack().droplevel(0).to_frame('time')\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["result = pd.concat([ pd.Series(d.values(), index=d.keys())\n    for d in list_example ]).to_frame('time')\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["df = (pd.concat(map(pd.Series, list_example))\n        .to_frame('time')\n      )\n", "                                time\ncompanies_info_5000_5100  121.201472\ncompanies_info_5100_5200  116.492211\n"], ["from PIL import Image\n\nimport pillow_heif\n\nheif_file = pillow_heif.read(r\"E:\\image\\20210914_150826.heic\")\n\nimage = Image.frombytes(\n    heif_file.mode,\n    heif_file.size,\n    heif_file.data,\n    \"raw\",\n\n)\n\nimage.save(r\"E:\\image\\test.png\", format(\"png\"))\n"], ["pip install snowflake-connector-python\n"], ["import pandas as pd\nimport redis\nimport zlib\nimport pickle\n\ndf=pd.DataFrame({'A':[1,2,3]})\nr = redis.Redis(host='localhost', port=6379, db=0)\nr.set(\"key\", zlib.compress( pickle.dumps(df)))\ndf=pickle.loads(zlib.decompress(r.get(\"key\")))\n"], ["pip uninstall opencv-python\npip uninstall opencv-contrib-python\n\npip install opencv-contrib-python\npip install opencv-python\n"], [], [], ["pygame.mouse.set_cursor(pygame.cursors.Cursor(pygame.SYSTEM_CURSOR_HAND))\n"], ["ImportError: cannot import name 'safe_str_cmp' from 'werkzeug.security\n"], [], ["model_history = model.fit(x=aug.flow(X_train, y_train, batch_size=16), epochs=EPOCHS,validation_data=[X_val, y_val], callbacks=[callbacks_list])\n"], ["[tool.pytest.ini_options]\npythonpath = [\n  \".\"\n]\n"], [], [], [], ["pygame.mouse.set_cursor()\n\nPygame Cursor Constant           Description\n--------------------------------------------\npygame.SYSTEM_CURSOR_ARROW       arrow\npygame.SYSTEM_CURSOR_IBEAM       i-beam\npygame.SYSTEM_CURSOR_WAIT        wait\npygame.SYSTEM_CURSOR_CROSSHAIR   crosshair\npygame.SYSTEM_CURSOR_WAITARROW   small wait cursor\n                                 (or wait if not available)\npygame.SYSTEM_CURSOR_SIZENWSE    double arrow pointing\n                                 northwest and southeast\npygame.SYSTEM_CURSOR_SIZENESW    double arrow pointing\n                                 northeast and southwest\npygame.SYSTEM_CURSOR_SIZEWE      double arrow pointing\n                                 west and east\npygame.SYSTEM_CURSOR_SIZENS      double arrow pointing\n                                 north and south\npygame.SYSTEM_CURSOR_SIZEALL     four pointed arrow pointing\n                                 north, south, east, and west\npygame.SYSTEM_CURSOR_NO          slashed circle or crossbones\npygame.SYSTEM_CURSOR_HAND        hand\n\nfor example:\npygame.mouse.set_cursor(pygame.SYSTEM_CURSOR_HAND)\n"], ["conda env create -n myenv-dev --file my_env.yml\n"], [], ["from sklearn.utils.class_weight import compute_class_weight\nclass_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_labels), y= train_labels)\n"], [], [], [], ["hist = model.fit(...)\nfor key in hist.history:\nprint(key)\n"], [], ["sudo apt-get install libpq-dev #required for psycop2-binary installation\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n\nsudo apt-get -y install postgresql-13 #or other version number\n\nsudo apt install postgis postgresql-13-postgis-3\n\nsudo -i -u postgres\ncreateuser yourusername\ncreatedb postgis_db -O yourusername #create your db\npsql -d postgis_db\nCREATE EXTENSION postgis;\n\n#make sure these are all installed:\n\nsudo apt-get install binutils libproj-dev \nsudo apt-get install gdal-bin\nsudo apt-get install libgeos++\nsudo apt-get install proj-bin\n"], ["C:\\ProgramData\\Anaconda3>py Scripts\\pywin32_postinstall.py -install\n"], ["import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nloop = asyncio.get_event_loop()\nt1 = loop.create_task(delay(1))\nt2 = loop.create_task(delay(2))\nloop.run_until_complete(t1)\nloop.close()\n", "sleeping for 1 second(s)\nsleeping for 2 second(s)\ndone sleeping for 1 second(s)\nTask was destroyed but it is pending!\ntask: <Task pending name='Task-1' coro=<delay() running at aio.py:10> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7fa794c5b970>()]>\n", "import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nloop = asyncio.get_event_loop()\nt1 = loop.create_task(delay(1))\nt2 = loop.create_task(delay(2))\nloop.run_until_complete(t1)\n\npending = asyncio.all_tasks(loop=loop)\ngroup = asyncio.gather(*pending)\nloop.run_until_complete(group)\n\nloop.close()\n", "import asyncio\n\n\nasync def delay(n):\n    print(f\"sleeping for {n} second(s)\")\n    await asyncio.sleep(n)\n    print(f\"done sleeping for {n} second(s)\")\n\n\nasync def main():\n    t1 = asyncio.create_task(delay(1))\n    t2 = asyncio.create_task(delay(2))\n    await t2\n\n\nasyncio.run(main())\n", "import asyncio\n\nasyncio.run(client.start('token'))\n"], [], [], [], [], [], ["class_weights = compute_class_weight(\n                                        class_weight = \"balanced\",\n                                        classes = np.unique(train_classes),\n                                        y = train_classes                                                    \n                                    )\nclass_weights = dict(zip(np.unique(train_classes), class_weights))\nclass_weights\n"], ["await mycursor.execute(\"SELECT * FROM guild_message_count WHERE guild_id = %s AND user_id = %s\", (message.author.guild.id, message.author.id))\n        in_database2 = await mycursor.fetchone()\n        if in_database2:\n            await mycursor.execute(\"UPDATE guild_message_count SET user_id = %s, message_count = message_count + %s WHERE guild_id = %s AND user_id = %s\", (message.author.id, count, message.author.guild.id, message.author.id))\n        else:\n            await mycursor.execute(\"INSERT INTO guild_message_count (user_id, message_count, guild_id) VALUES (%s, %s, %s)\", (message.author.id, count, message.author.guild.id))\n", "INSERT INTO guild_message_count\n        (user_id, message_count, guild_id)\n        VALUES \n        (%s, %s, %s)\n    ON DUPLICATE KEY UPDATE\n        message_count = message_count + 1\n"], [], ["addopts= --cov <path> -ra\n", "\"python.testing.pytestArgs\": [\"--no-cov\"],\n", "    \"python.testing.pytestArgs\": [\n        \"--rootdir\",\"${workspaceFolder}/<path-to-directory-with-tests>\"\n    ],\n"], [], [], ["apt list --installed | grep virtualenvwrapper \napt list --installed | grep virtualenvwrapper \n", "pip install virtualenvwrapper virtualenvwrapper \n", "export WORKON_HOME=$HOME/.virtualenvs\nexport PROJECT_HOME=$HOME/amd\nexport VIRTUALENVWRAPPER_SCRIPT=/home/robot/.local/bin/virtualenvwrapper.sh\nexport VIRTUALENVWRAPPER_PYTHON=$(which python3)\nsource /home/robot/.local/bin/virtualenvwrapper.sh\n"], ["from PIL import Image\nimport pillow_heif\n\n    heif_file = pillow_heif.read_heif(\"HEIC_file.HEIC\")\n    image = Image.frombytes(\n        heif_file.mode,\n        heif_file.size,\n        heif_file.data,\n        \"raw\",\n    \n    )\n\n    image.save(\"./picture_name.png\", format(\"png\"))\n"], [], [], ["set PATH=c:\\...\\Lib\\site-packages\\pywin32_system32;%PATH%\n", "import os\nprint(os.environ[\"PATH\"])\n", "os.environ[\"PATH\"] = r\"c:\\...\\pywin32_system32;\" + os.environ[\"PATH\"]\n", "os.environ[\"PATH\"] = r\"/.../pywin32_system32:\" + os.environ[\"PATH\"]\n"], ["def fn_cat_onehot(df):\n\n    \"\"\"Generate onehoteencoded features for all categorical columns in df\"\"\"\n\n    printmd(f\"df shape: {df.shape}\")\n\n    # NaN handing\n    nan_count = df.isna().sum().sum()\n    if nan_count > 0:\n        printmd(f\"NaN = **{nan_count}** will be categorized under feature_nan columns\")\n\n    # generation\n    from sklearn.preprocessing import OneHotEncoder\n\n    model_oh = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    for c in df.select_dtypes(\"category\").columns:\n        printmd(f\"Encoding **{c}**\")  # which column\n        matrix = model_oh.fit_transform(\n            df[[c]]\n        )  # get a matrix of new features and values\n        names = model_oh.get_feature_names_out()  # get names for these features\n        df_oh = pd.DataFrame(\n            data=matrix, columns=names, index=df.index\n        )  # create df of these new features\n        display(df_oh.plot.hist())\n        df = pd.concat([df, df_oh], axis=1)  # concat with existing df\n        df.drop(\n            c, axis=1, inplace=True\n        )  # drop categorical column so that it is all numerical for modelling\n\n    printmd(f\"#### New df shape: **{df.shape}**\")\n    return df\n"], [], ["pip list | grep opencv\n", "pip install opencv-python==4.5.4.60\n"], ["python3 -m venv venv\n"], [], ["pip show pywin32\n", "pip install pywin32==300 --upgrade\n"], [], ["import speedtest\n\ntest = speedtest.Speedtest()\n\nprint(\"Loading server list...\")\ntest.get_servers()\nprint(\"Choosing best server...\")\nbest = test.get_best_server()\n\nprint(f\"Found: {best['host']} located in {best['country']}\")\n\nprint(\"Performing download test...\")\ndownload_result = test.download()\nprint(\"Performing upload test...\")\nupload_result = test.upload()\nping_result = test.results.ping\n\nprint(f\"Download speed: {download_result / 1024 / 1024:.2f}Mbit/s\")\nprint(f\"Upload speed: {upload_result / 1024 / 1024:.2f}Mbit/s\")\nprint(f\"Ping: {ping_result}ms\")\n"], [], ["import pytest\nfrom fastapi.testclient import TestClient\nfrom app.main import app\nimport multiprocessing\nfrom uvicorn import Config, Server\n\n\nclass UvicornServer(multiprocessing.Process):\n\n    def __init__(self, config: Config):\n        super().__init__()\n        self.server = Server(config=config)\n        self.config = config\n\n    def stop(self):\n        self.terminate()\n\n    def run(self, *args, **kwargs):\n        self.server.run()\n\n\n\n\n@pytest.fixture(scope=\"session\")\ndef server():\n    config = Config(\"app.main:app\", host=\"127.0.0.1\", port=5000, log_level=\"debug\")\n    instance = UvicornServer(config=config)\n    instance.start()\n    yield instance\n    instance.stop()\n\n@pytest.fixture(scope=\"module\")\ndef mock_app(server):\n    client = TestClient(app)\n    yield client\n", "def test_root(mock_app):\n    response = mock_app.get(\"\")\n    assert response.status_code == 200\n"], ["pip3 install fitz\npip3 install PyMuPDF==1.16.14\n"], [], [], [], ["import sys\nprint(sys.executable)\n", "/path/to/python/used/by/vs/code/python -m pip install pillow\n"], ["pip install Pillow\n"], ["class Person:\n    def __init__(self, name):\n        self.name = name\n    def greeting(self):\n        # Should return \"hi, my name is \" followed by the name of the Person.\n        return name\n\n# Create a new instance with a name of your choice\nsome_person =  Person(\"XYZ\")\n# Call the greeting method\nprint(f\"hi, my name is {some_person.name}\")\n"], [], ["workbook.Close(SaveChanges=True)\nxl.Quit()\n"], ["import fitz\n\nmy_pdf = r\"C:\\Users\\Test\\FileName.pdf\"\ndoc = fitz.open(my_pdf) \ndef pdftype(doc):\n    i=0\n    for page in doc:\n        if len(page.getText())>0: #for scanned page it will be 0\n            i+=1\n    if i>0:\n        print('full/partial text PDF file')\n    else:\n        print('only scanned images in PDF file')\npdftype(doc)\n"], [], ["line_color=\"#0000ff\"\n", " fig['data'][0]['line']['color']=\"#00ff00\"\n", " fig.data[0].line.color = \"#00ff00\"\n", "import plotly.graph_objects as go\nimport numpy as np\n\nt = np.linspace(0, 10, 100)\ny = np.cos(t)\ny2= np.sin(t)\nfig = go.Figure(data=go.Scatter(x=t, y=y,mode='lines+markers', line_color='#ffe476'))\nfig.add_trace(go.Scatter(x=t, y=y2,mode='lines+markers', line=dict(color=\"#0000ff\")))\nfig.show()\n", "fig['data'][0]['line']['color']=\"#00ff00\"\nfig.show()\n"], [], ["from django.test import TestCase\n\nclass views(TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        import django\n        django.setup()\n\n    def test_something(self,):\n        from user.model import something\n        ...\n"], [], [], ["import pandas as pd\nurl='https://drive.google.com/file/d/0B6GhBwm5vaB2ekdlZW5WZnppb28/view?usp=sharing'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\ndf = pd.read_csv(url)\n"], ["     workspace\n       .env\n       ./src\n            __init__.py\n            code1.py\n            code2.py\n       ./tests\n            __init__.py\n            test_code1.py\n            test_code2.py\n", "    {\n        \"workbench.colorTheme\": \"Visual Studio Dark\",\n        \"editor.fontFamily\": \" monospace, Menlo, Monaco, Courier New\",\n        \"python.testing.unittestEnabled\": false,\n        \"python.testing.cwd\": \".\",\n        \"terminal.integrated.inheritEnv\": true,\n        \"python.envFile\": \"${workspaceFolder}/.env\",\n        \"python.defaultInterpreterPath\": \n        \"~/anaconda3/envs/mycurrentenv/bin/python\",\n        \"pythonTestExplorer.testFramework\": \"pytest\",\n        \"python.testing.pytestEnabled\": true,\n        \"python.testing.pytestArgs\": [\n           \"tests\"\n        ],\n        \"python.terminal.activateEnvironment\": true,\n        \"python.terminal.activateEnvInCurrentTerminal\": true,\n        \"terminal.integrated.env.osx\": {\n            \"PYTHONPATH\": \"${workspaceFolder}/src:${env:PYTHONPATH}\"\n          }\n    }\n"], ["from pydantic import BaseModel, Field\n\n\nclass TMDB_Category(BaseModel):\n    name: str = Field(alias=\"strCategory\")\n    description: str = Field(alias=\"strCategoryDescription\")\n\n\ndata = {\n    \"strCategory\": \"Beef\",\n    \"strCategoryDescription\": \"Beef is ...\"\n}\n\n\nobj = TMDB_Category.parse_obj(data)\n\n# {'name': 'Beef', 'description': 'Beef is ...'}\nprint(obj.dict())\n"], ["count_query = query.with_only_columns(func.count(Foo.id))\ncount = session.execute(count_query).scalar_one()\n"], [], [], ["pip install black \"black[jupyter]\"\nblack {source_file_or_directory}\n"], [], ["from copy import deepcopy\n\n@dataclass(frozen=True)\nclass A:\n    a: str = ''\n    b: int = 0\n\n    def mutate(self, **options):\n        new_config = deepcopy(self.__dict__)\n        # some validation here\n        new_config.update(options)\n        return self.__class__(**new_config)\n", "from dataclasses import dataclass, InitVar\n\n\n@dataclass(frozen=True)\nclass A:\n    a: str = ''\n    b: int = 0\n    config: InitVar[dict] = None\n\n    def __post_init__(self, config: dict):\n        if config:\n            self.__init__(**config)\n", "A(config={'a':'a', 'b':1})\n", "A(a='a', b=1)\n", "A(config={'a':'a', 'b':1, 'config':{'a':'b'}})\n", "A(a='b', b=1)\n"], [], ["!python -m spacy download en\n", "spacy.load('en_core_web_sm')\n"], [], [], [], ["python -m spacy download en_core_web_lg\npython -m spacy download en_core_web_sm\n", "python -m spacy download en\n"], [], ["python -m pip install <module>\n", "python3 -m pip install <module>\n"], [], [], [], ["decompose_result = seasonal_decompose(df.Sales, model='multiplicative', period=1)\ndecompose_result.plot();\n", "**Signature:**\nseasonal_decompose(\n    x,\n    model='additive',\n    filt=None,\n    period=None,\n    two_sided=True,\n    extrapolate_trend=0,\n)\n"], [], [], ["brew install --cask miniforge\nconda init zsh\nconda activate\nconda install numpy scipy scikit-learn\n"], ["class Person:\ndef __init__(self, name):\n    self.name = name\ndef greeting(self):\n    # Should return \"hi, my name is \" followed by the name of the Person.\n    return name\n\n# Create a new instance with a name of your choice\nsome_person = Person(\"Bob\")\n# Call the greeting method\nprint(f\"hi, my name is {some_person.name}\")\n"], [], ["export PATH=\"$HOME/.poetry/bin:$PATH\"\n"], [], [], ["pip install cryptography==3.1.1\n", "python -m pip install --upgrade pip\n\nsudo pip install -U pip setuptools\n"], [], [], [], [], ["import os\n\nos.add_dll_directory(r\"C:\\Program Files\\GTK3-Runtime Win64\\bin\")\n\nfrom weasyprint import HTML\n\nHTML('https://weasyprint.org/').write_pdf('weasyprint-website.pdf')\n"], ["{\n    \"python.defaultInterpreterPath\": \"/path/to/your/venv/bin/python\",\n}\n"], [], ["DELETE FROM public.django_migrations\nWHERE public.django_migrations.app = 'target_app_name';\n"], ["import spacy.cli\nspacy.cli.download(\"en_core_web_lg\")\n"]]