[["from pandas import DataFrame as df\nimport numpy as np\nimport pandas as pd \n\nitems = [1,2,3,4,5]\nplace = [6,7,8,9,10]\nquality = [11,np.nan,12,13,np.nan]\n\n\ndf = pd.DataFrame({\"A\":items, \"B\":place, \"C\":quality})\nprint(df)\n\"\"\"\n  A   B     C\n0  1   6  11.0\n1  2   7   NaN\n2  3   8  12.0\n3  4   9  13.0\n4  5  10   NaN\n\n\"\"\"\n\naa = df.ffill(axis=1).astype(int)\nprint(aa)\n\"\"\"\n   A   B   C\n0  1   6  11\n1  2   7   7\n2  3   8  12\n3  4   9  13\n4  5  10  10\n\"\"\"\n"], [], ["sudo apt install python -y\n"], ["\ndef to_shape(x, target_shape):\n    padding_list = []\n    for x_dim, target_dim in zip(x.shape, target_shape):\n        pad_value = int(target_dim - x_dim)\n        pad_tuple = ((pad_value//2, pad_value//2 + pad_value%2))\n        padding_list.append(pad_tuple)\n    \n    return np.pad(x, tuple(padding_list), mode='constant')\n\n"], [], ["pip install ipykernel --upgrade\n"], ["Applied execute package: core_AllUsers, result: 0x80070643, restart: None\n"], ["def digitsoflife(number):\nsum_ = [i for i in str(number)]\nsum_ = sum(list(map(int, sum_)))\nsum_total = 0\nwhile len(str(sum_)) > 1:\n    for i in str(sum_):\n        sum_total += int(i)\n        sum_ = sum_total\n    print(sum_total)\n    break\nelse:\n    print(sum_)\n"], ["from tkinter import *\n\nleft_click = False\n\ndef left_click_start(event):\n    global left_click\n    left_click = True\n\ndef left_click_stop(event):\n    global left_click\n    left_click = False\n\nwin = Tk()\n\nwin.bind(\"<Button-1>\", click_start)\nwin.bind(\"<B1-ButtonRelease>\", click_stop)\n\nwhile True:\n    if left_click:\n        # put the code here\n    win.update()\n\n"], ["pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"], [], ["plt.scatter(x=X_features_main, y=y_label_main,color='black')  #The X-Features vs. The Real Label\nplt.plot(X_features_main, y_predicted_from_X_features_main,color='blue') #The X- Features vs. The predicted label\nplt.show()#To show your figures code here\n"], ["mlflow gc --backend-store-uri sqlite:////path/to/mlflow.db --experiment-ids 42\n"], ["# Download/run the legacy macOS installer (pick which one for your sys)\nhttps://www.python.org/downloads/release/python-2718/\n\n# Add pip for python2.7\ncurl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip2.py\npython2 get-pip2.py\n\n# Optionally check for pip updates (in case of post-eol patches)\npython2 -m pip install --upgrade pip\n\n# Optionally add the helpers like easy_install back onto your path\n# In your ~/.zprofile or whatever bash/shell profile equivalent\nPATH=\"/Library/Frameworks/Python.framework/Versions/2.7/bin:${PATH}\"\nexport PATH\n\n# Optionally add some helpers while editing shell profile\nalias pip2=\"python2 -m pip\"\nalias venv2=\"virtualenv -p python2\"\nalias venv3=\"virtualenv -p python3\"\n\n# Optionally some apple-specific std libraries are missing, search\n# and download them. Example: plistlib.py\ncurl https://raw.githubusercontent.com/python/cpython/2.7/Lib/plistlib.py -o /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plistlib.py\n\n# Lastly, there is no symlink /usr/bin/python anymore\n# /usr/bin is system protected so you can't add one either\n# \n# Change your programs to use /usr/local/bin/python\n# or google how to disable macOS SIP to make a symlink in /usr/bin\n", "brew update\nbrew install python3\n\n# Add pip for python 3 in case it is missing\ncurl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython3 get-pip.py\n\n# Check for pip updates\npython3 -m pip install --upgrade pip\n\n# Optionally add a helper in ~/.zprofile\nalias venv3=\"virtualenv -p python3\"\n", "~ % python --version\nPython 2.7.18\n\n~ % python2 --version\nPython 2.7.18\n\n~ % python3 --version\nPython 3.9.10\n\n# Running older python2\npython2 -m pip install...\npython2 ...\n\n# Testing the venv2 alias from above\nvenv2 foo\nsource foo/bin/activate\npip -V # pip 20... from... python2.7\npip install -y -r req.txt\npip uninstall -y -r req.txt\npip freeze\ndeactivate\n\n# Testing the venv3 alias from above\nvenv3 foo3\nsource foo3/bin/activate\npip -V # pip22... from ...python3.9\npip install -y -r req.txt\npip uninstall -y -r req.txt\npip freeze\ndeactivate\n", "# Credit to https://www.macupdate.com/app/mac/5880/python/uninstall  \n# for many of the tips in this section.\n\n# Sometimes there are problems related to accepting xcode \n# tool agreement. Open XCode to make sure it finished \n# installing its tool updates.\n\n# Remove old python Application installs\n# open the apps dir and delete Python 2, 3 via Finder\nopen /Applications\n\n# Remove old brew installs \nbrew list | grep python\nbrew uninstall python\nbrew uninstall python3\n\n# find/remove lingering unlinked kegs\nls /usr/local/Cellar/ | grep python \n\n# Cleanup binaries\nsudo rm -rf /Library/Frameworks/Pyth*\nrm /usr/local/bin/pip*\n\n# Cleanup symlinks\nwhich -a python # check results, and rm each one\nwhich -a python2 # check results, and rm each one\nwhich -a python3 # check results, and rm each one\n\nbrew cleanup # prunes symlinks\n"], [], [], ["#import numba\n\ndf['change'] = df['close'].diff()\ndf['gain'] = df.change.mask(df.change < 0, 0.0)\ndf['loss'] = -df.change.mask(df.change > 0, -0.0)\n\n#@numba.jit\ndef rma(x, n):\n    \"\"\"Running moving average\"\"\"\n    a = np.full_like(x, np.nan)\n    a[n] = x[1:n+1].mean()\n    for i in range(n+1, len(x)):\n        a[i] = (a[i-1] * (n - 1) + x[i]) / n\n    return a\n\ndf['avg_gain'] = rma(df.gain.to_numpy(), 14)\ndf['avg_loss'] = rma(df.loss.to_numpy(), 14)\n\ndf['rs'] = df.avg_gain / df.avg_loss\ndf['rsi'] = 100 - (100 / (1 + df.rs))\n", "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'close':[4724.89, 4378.51,6463.00,9838.96,13716.36,10285.10,\n                          10326.76,6923.91,9246.01,7485.01,6390.07,7730.93,\n                          7011.21,6626.57,6371.93,4041.32,3702.90,3434.10,\n                          3813.69,4103.95,5320.81,8555.00,10854.10]})\nn = 14\n\n\ndef rma(x, n, y0):\n    a = (n-1) / n\n    ak = a**np.arange(len(x)-1, -1, -1)\n    return np.r_[np.full(n, np.nan), y0, np.cumsum(ak * x) / ak / n + y0 * a**np.arange(1, len(x)+1)]\n\ndf['change'] = df['close'].diff()\ndf['gain'] = df.change.mask(df.change < 0, 0.0)\ndf['loss'] = -df.change.mask(df.change > 0, -0.0)\ndf['avg_gain'] = rma(df.gain[n+1:].to_numpy(), n, np.nansum(df.gain.to_numpy()[:n+1])/n)\ndf['avg_loss'] = rma(df.loss[n+1:].to_numpy(), n, np.nansum(df.loss.to_numpy()[:n+1])/n)\ndf['rs'] = df.avg_gain / df.avg_loss\ndf['rsi_14'] = 100 - (100 / (1 + df.rs))\n", "         close   change     gain     loss  avg_gain  avg_loss    rs    rsi  rsi_14\n0      4724.89      NaN      NaN      NaN       NaN       NaN   NaN    NaN     NaN\n1      4378.51  -346.38     0.00   346.38       NaN       NaN   NaN    NaN     NaN\n2      6463.00  2084.49  2084.49     0.00       NaN       NaN   NaN    NaN     NaN\n3      9838.96  3375.96  3375.96     0.00       NaN       NaN   NaN    NaN     NaN\n4     13716.36  3877.40  3877.40     0.00       NaN       NaN   NaN    NaN     NaN\n5     10285.10 -3431.26     0.00  3431.26       NaN       NaN   NaN    NaN     NaN\n6     10326.76    41.66    41.66     0.00       NaN       NaN   NaN    NaN     NaN\n7      6923.91 -3402.85     0.00  3402.85       NaN       NaN   NaN    NaN     NaN\n8      9246.01  2322.10  2322.10     0.00       NaN       NaN   NaN    NaN     NaN\n9      7485.01 -1761.00     0.00  1761.00       NaN       NaN   NaN    NaN     NaN\n10     6390.07 -1094.94     0.00  1094.94       NaN       NaN   NaN    NaN     NaN\n11     7730.93  1340.86  1340.86     0.00       NaN       NaN   NaN    NaN     NaN\n12     7011.21  -719.72     0.00   719.72       NaN       NaN   NaN    NaN     NaN\n13     6626.57  -384.64     0.00   384.64       NaN       NaN   NaN    NaN     NaN\n14     6371.93  -254.64     0.00   254.64    931.61    813.96  1.14  53.37   53.37\n15     4041.32 -2330.61     0.00  2330.61    865.06    922.29  0.94  48.40   48.40\n16     3702.90  -338.42     0.00   338.42    803.27    880.59  0.91  47.70   47.70\n17     3434.10  -268.80     0.00   268.80    745.90    836.89  0.89  47.13   47.13\n18     3813.69   379.59   379.59     0.00    719.73    777.11  0.93  48.08   48.08\n19     4103.95   290.26   290.26     0.00    689.05    721.60  0.95  48.85   48.85\n20     5320.81  1216.86  1216.86     0.00    726.75    670.06  1.08  52.03   52.03\n21     8555.00  3234.19  3234.19     0.00    905.86    622.20  1.46  59.28   59.28\n22    10854.10  2299.10  2299.10     0.00   1005.37    577.75  1.74  63.51   63.51\n"], [], [], [], ["pip uninstall nltk\npip install nltk\n"], [], ["import tensorflow as tf \n\nfrom tensorflow import keras\n\nfrom keras.api._v2 import keras as KerasAPI\n\n\nKerasAPI.applications.ResNet50() \n"], ["plt.figure()\nsurf = ssr2D.T.plot.surface(label=\"ssr\")\nsurf._facecolors2d  = surf._facecolor3d\nsurf._edgecolors2d  = surf._edgecolor3d\nplt.legend()\nplt.show()\n"], [], ["public static String numberStream(String s) {\n    char[] ch = s.toCharArray();\n    for (int i = 0; i < ch.length; i++) {\n        int cur_count = 1;\n        for (int j = i + 1; j < ch.length; j++) {\n            if (ch[i] != ch[j]) {\n                break;\n            }\n            cur_count += 1;\n            if (Integer.toString(cur_count).equals(ch[i] + \"\")) {\n                return \"true\";\n            }\n        }\n    }\n    return \"false\";\n}\n"], [], [], [], ["import humps\nhumps.camelize('jack_in_the_box')  # jackInTheBox\nhumps.decamelize('rubyTuesdays')  # ruby_tuesdays\nhumps.pascalize('red_robin')  # RedRobin\n\narray = [{\"attrOne\": \"foo\"}, {\"attrOne\": \"bar\"}]\nhumps.decamelize(array) # [{\"attr_one\": \"foo\"}, {\"attr_one\": \"bar\"}]\n"], [" files = request.files.getlist(\"file\")\n"], ["def get_closest_divisor(num, divisor):\n    for i in range(num):\n        if ( num % divisor > 0): \n            num = num + 1\n    return num\n", "get_closest_divisor(33756, 512)\n[Out]: 33792\n"], [], ["import requests\n\npackage = 'django'  # replace with the package you want to check\nresponse = requests.get(f'https://pypi.org/pypi/{package}/json')\nlatest_version = response.json()['info']['version']\n"], ["def get_nearest_neighbors(gdf1, gdf2, k_neighbors=2):\n'''\nFind k nearest neighbors for all source points from a set of candidate points\nmodified from: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html    \n\nParameters\n----------\ngdf1 : geopandas.DataFrame\n    Geometries to search from.\ngdf2 : geopandas.DataFrame\n    Geoemtries to be searched.\nk_neighbors : int, optional\n    Number of nearest neighbors. The default is 2.\n\nReturns\n-------\ngdf_final : geopandas.DataFrame\n    gdf1 with distance, index and all other columns from gdf2.\n'''\n\nsrc_points = [(x,y) for x,y in zip(gdf1.geometry.x , gdf1.geometry.y)]\ncandidates =  [(x,y) for x,y in zip(gdf2.geometry.x , gdf2.geometry.y)]\n\n# Create tree from the candidate points\ntree = BallTree(candidates, leaf_size=15, metric='euclidean')\n\n# Find closest points and distances\ndistances, indices = tree.query(src_points, k=k_neighbors)\n\n# Transpose to get distances and indices into arrays\ndistances = distances.transpose()\nindices = indices.transpose()\n\nclosest_gdfs = []\nfor k in np.arange(k_neighbors):\n    gdf_new = gdf2.iloc[indices[k]].reset_index()\n    gdf_new['distance'] =  distances[k]\n    gdf_new = gdf_new.add_suffix(f'_{k+1}')\n    closest_gdfs.append(gdf_new)\n    \nclosest_gdfs.insert(0,gdf1)    \ngdf_final = pd.concat(closest_gdfs,axis=1)\n\nreturn gdf_final\n"], [], [], [], ["date = '19991229'\nlist1=[]\nfor i in date:\n    list1.append(int(i)) \n\ndate2 = str(sum(list1))\nlist2 = []\n\nfor i in date2:\n    list2.append(int(i))\n    \ntotal = sum(list2)\nprint(total)\n"], ["task1 = PythonOperator(\n    task_id = 'task',\n    python_callable=my_callable_func, # True\n    # python_callable=\"my_callable_func\", # False\n    dag = dag\n)\n"], [], ["import queue\n\nq = queue.Queue() \n\nprint(list(q.queue))\nq.put(1)\nprint(list(q.queue))\n"], [], [" .\\python -m pip install osmnx-1.3.0-py3-none-any.whl\n", ".\\python -m pip install shapely --upgrade\n"], ["@permission_classes([IsAuthenticated])\n"], ["n = 14\ndf['rsi14'] = df['Close'].diff(1).mask(df['Close'].diff(1) < 0, 0).ewm(alpha=1/n, adjust=False).mean().div(df['Close'].diff(1).mask(df['Close'].diff(1) > 0, -0.0).abs().ewm(alpha=1/n, adjust=False).mean()).add(1).rdiv(100).rsub(100)\n"], ["def calc_pyramid_volume(base_length, base_width, pyramid_height):\n\narea = length * width\n\nvolume = area * height * 1/3\n\nreturn volume\n"], ["def is_prime(number:int):\n check = 0\n for i in range(2,number):\n    if number % i == 0:\n        check += 1\n if check == 0:\n    return True\n else:\n    return False\n\ndef next_prime(value):\n check = value + 1\n while is_prime(check) is False:\n    check += 1\n return check\n\nvalue = int(input(\"Insert the number: \"))\nprint(next_prime(value))\n"], ["pipenv --rm  && rm -rf Pipfile.lock && pipenv install --dev .\n"], ["pip install opencv-contrib-python\n", "\nimport cv2\n\n\ncap = cv2.VideoCapture(\"video_path\")\ntracker = cv2.TrackerCSRT_create()\nret, frame = cap.read()\nbbox = cv2.selectROI(frame, False)\ntracker.init(frame, bbox)\n\nwhile True:   \n    frame_id = 0 \n    ret, frame = cap.read() \n    success, bbox = tracker.update(frame)    \n    if success:\n        x, y, w, h = [int(i) for i in bbox]\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)    \n    cv2.imshow(\"Tracking\", frame)    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\ncap.release()\ncv2.destroyAllWindows()\n"], [], ["def build_mosaic(list_of_images, num_mosaic_rows, num_mosaic_cols):\n\n    list_of_mosaic_rows = []\n\n    for row_number in range(num_mosaic_rows):\n\n        list_of_mosaic_rows = list_of_images[row_number*num_mosaic_cols,(row_number+1)*num_mosaic_cols]\n\n    mosaic = np.vstack(list_of_mosaic_rows)\n\n    return mosaic\n"], [], ["python -> 3.6.2\n\npip install tensorflow==1.3.0\npip install keras==2.1.2\npip install 'h5py==2.10.0' --force-reinstall\n"], [], ["'''\n Adapted and extended from \n https://github.com/huggingface/transformers/issues/1950#issuecomment-558679189\n\n'''\nimport pandas as pd\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\n\ndef get_sentence_similarity(tokenizer,model,s1,s2):\n\n    s1 = tokenizer.encode(s1)  \n    s2 = tokenizer.encode(s2)\n\n    print(\"1 len(s1) s1\",len(s1),s1) # prints length of tokens - input_ids 8 [101, 7592...\n    print(\"1 len(s2) s2\",len(s2),s2)\n    s1 = torch.tensor(s1)\n    #print(\"2\",s1) # prints tensor([ 101, 7592, ...\n    s1 = s1.unsqueeze(0) # add an extra dimension, why ? the model needs to be fed in batches, we give a dummy batch 1\n    #print(\"3\",s1) # prints tensor([[ 101, 7592, \n    s2 = torch.tensor(s2).unsqueeze(0)\n\n    # Pass it to the model for inference\n    with torch.no_grad():\n        output_1 = model(s1)\n        output_2 = model(s2)\n\n    logits_s1 = output_1[0]  # The last hidden-state is the first element of the output tuple\n    logits_s2 = output_2[0].detach()\n    #print(\"logits_s1 before detach\",logits_s1) # prints  tensor([[[-0.1162,  0.2388, ...-0.2128]]], grad_fn=<NativeLayerNormBackward0>)\n    logits_s1 = logits_s1.detach() # to remove the last part we call detach\n\n    print(\"logits_s1.shape\",logits_s1.shape ) # prints ([1, <length of tokens>, 768]) - Each token is rep by a 768 row vector for the base Bert Model!\n    print(\"logits_s2.shape\",logits_s2.shape ) # 1 the dummy batch dimension we added to the model by un-squeeze\n    logits_s1 = torch.squeeze(logits_s1) #lets remove the batch dimension by squeeze\n    logits_s2 = torch.squeeze(logits_s2)\n    print(\"logits_s1.shape\",logits_s1.shape ) # prints ([<length of tokens>, 768]) say torch.Size([8, 768])\n    print(\"logits_s2.shape\",logits_s2.shape )\n    a = logits_s1.reshape(1,logits_s1.numel()) # we lay the vector flat make it 1, **768 via reshape; numel is number of elements\n    b = logits_s2.reshape(1,logits_s2.numel())\n    print(\"a.shape\",a.shape ) # torch.Size([1, 6144])\n    print(\"b.shape\",b.shape ) # the shape will be 1, 768* no of tokens in b sentence - need not be similar\n\n    # we can  mean over the rows to give it better similarity - but that is giving poor output\n    # a = sentence_vector_1.mean(axis=1) this is giving cosine similarity as 1\n    # b = sentence_vector_2.mean(axis=1)\n    #cos_sim = F.cosine_similarity(a.reshape(1,-1),b.reshape(1,-1), dim=1)\n\n    # so we pad the tensors to be same shape\n    if  a.shape[1] <  b.shape[1]:\n        pad_size = (0, b.shape[1] - a.shape[1]) \n        a = torch.nn.functional.pad(a, pad_size, mode='constant', value=0)\n    else:\n        pad_size = (0, a.shape[1] - b.shape[1]) \n        b = torch.nn.functional.pad(b, pad_size, mode='constant', value=0)\n\n    print(\"After padding\")\n    print(\"a.shape\",a.shape ) # 1,N\n    print(\"b.shape\",b.shape ) # 1, N\n\n\n    # Calculate the cosine similarity\n    cos_sim = cosine_similarity(a,b)\n    #print(\"got cosine similarity\",cos_sim) # output [[0.80432487]]\n    return cos_sim\n\n\n\nif __name__ == \"__main__\":\n\n\n    s1 = \"John loves dogs\" \n    s2 = \"dogs love John\"\n\n    # Tokenize the text using BERT tokenizer\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    model = BertModel.from_pretrained(\"bert-base-uncased\") #Not good for sentence similarity\n    model.eval()\n    \n    cos_sim = get_sentence_similarity(tokenizer,model,s1,s2)\n    print(\"got cosine similarity\",cos_sim) # output [[0.738616]]\n\n    # Let's try the same with a better model - say for sentence embedding\n    # From https://www.sbert.net/docs/pretrained_models.html\n    # They have been extensively evaluated for their quality to embedded sentences \n    # (Performance Sentence Embeddings) and to embedded search queries & paragraphs \n\n    # better to use AutoTokenizer for other models see https://github.com/huggingface/transformers/issues/5587\n    tokenizer = BertTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n    model = BertModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n    model.eval()\n    cos_sim = get_sentence_similarity(tokenizer,model,s1,s2)\n    print(\"got cosine similarity\",cos_sim) # output [[0.5646803]]\n"], [], ["# fix windows registry stuff\nimport mimetypes\nmimetypes.add_type('application/javascript', '.js')\nmimetypes.add_type('text/css', '.css')\n"], ["resource \"aws_glue_job\" \"my_job\" {\n  name              = \"my_job\"\n  role_arn          = aws_iam_role.glue.arn\n  worker_type       = \"Standard\"\n  number_of_workers = 2\n  glue_version      = \"4.0\"\n\n  command {\n    script_location = \"s3://my-bucket/my-script.py\"\n    python_version  = \"3\"\n  }\n\n  default_arguments = {\n    \"--enable-job-insights\" = \"true\",\n    \"--additional-python-modules\" : \"boto3==1.26.52,pandas==1.5.2,SQLAlchemy==1.4.46,requests==2.28.2\",\n  }\n}\n", "resource \"aws_glue_job\" \"my_job\" {\n  name         = \"my-job\"\n  role_arn     = aws_iam_role.glue.arn\n  glue_version = \"1.0\"\n  max_capacity = 1\n\n  connections = [\n    aws_glue_connection.redshift.name\n  ]\n\n  command {\n    name            = \"pythonshell\"\n    script_location = \"s3://my-bucket/my-script.py\"\n    python_version  = \"3.9\"\n  }\n\n  default_arguments = {\n    \"--enable-job-insights\" = \"true\",\n    \"--library-set\" : \"analytics\",\n  }\n}\n"], [], ["def permanently_delete_experiments_on_mlflow(list_of_experiments_id: list):\n    mlflow_client = MlflowClient(tracking_uri=YOUR_EC2_TRACKING_URI)\n    commands = []\n    for experiment_id in list_of_experiments_id:\n        print(f'deleting experiment {experiment_id}')\n        os.system(f\"aws s3 rm {YOUR_S3_ARTIFACTS_STORE} \"\n                  f\"--recursive --exclude '*' --include '{experiment_id}/*'\")\n        try:\n            mlflow_client.delete_experiment(experiment_id)\n        except Exception as e:\n            print_red(f'failed to execute mlflow_client.delete_experiment({experiment_id}) \\n {str(e)}')\n        commands.append(f\"YOUR_PATH_TO_DATABASE_ON_EC2{os.sep}database.db{os.sep}{experiment_id} \")\n        commands.append(f\"YOUR_PATH_TO_DATABASE_ON_EC2{os.sep}database.db{os.sep}.trash{os.sep}{experiment_id} \")\n    # format commands to send via ssh to EC2\n    commands = f\"ssh -i {YOUR_EC2_SSH_KEY_PATH} ubuntu@{YOUR_EC2_IP} rm -r \" \\\n               + ' '.join(commands)\n    print('executing on EC2 the following command: \\n   ', commands)\n    result = subprocess.Popen(commands, shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n    response, err = result.communicate()\n    print('response:', response)\n"], [], [], ["conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\npython3 -m pip install tensorflow\n"], [], ["def split_by_commas(s):\n    lst = list()\n    brackets = 0\n    word = \"\"\n    for c in s:\n        if c == \"[\":\n            brackets += 1\n        elif c == \"]\":\n            if brackets > 0:\n                brackets -= 1\n        elif c == \",\" and not brackets:\n            lst.append(word)\n            word = \"\"\n            continue\n        word += c\n    lst.append(word)\n    return lst\n"], ["import keyboard as k\n\nk.add_hotkey(\"alt+s\", lambda: k.write('Hello'))\nk.wait('ctrl+shift+1') #this is a combo that I know I don't use so the program keeps running\n"], ["def to_shape(a, shape):\n    z = np.zeros(shape)\n    z[:a.shape[0], :a.shape[1]] = a\n    return z\n\n"], ["dataSet.set_index('key').apply(lambda ss:np.array(ss.tolist()).flatten(),axis=1)\n", "#    key                                                                  combined\n# 0  1_1                    [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\n# 1  1_2           [7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12]\n# 2  1_3  [13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18]\n"], ["def dict_replace_value(d: dict, old: str, new: str) -> dict:\n    x = {}\n    for k, v in d.items():\n        if isinstance(v, dict):\n            v = dict_replace_value(v, old, new)\n        elif isinstance(v, list):\n            v = list_replace_value(v, old, new)\n        elif isinstance(v, str):\n            v = v.replace(old, new)\n        x[k] = v\n    return x\n\n\ndef list_replace_value(l: list, old: str, new: str) -> list:\n    x = []\n    for e in l:\n        if isinstance(e, list):\n            e = list_replace_value(e, old, new)\n        elif isinstance(e, dict):\n            e = dict_replace_value(e, old, new)\n        elif isinstance(e, str):\n            e = e.replace(old, new)\n        x.append(e)\n    return x\n\n# See input and output below\noutput = dict_replace_value(input, 'string', 'something')\n", "input = {\n    'key1': 'a string',\n    'key2': 'another string',\n    'key3': [\n        'a string',\n        'another string',\n        [1, 2, 3],\n        {\n            'key1': 'a string',\n            'key2': 'another string'\n        }\n    ],\n    'key4': {\n        'key1': 'a string',\n        'key2': 'another string',\n        'key3': [\n            'a string',\n            'another string',\n            500,\n            1000\n        ]\n    },\n    'key5': {\n        'key1': [\n            {\n                'key1': 'a string'\n            }\n        ]\n    }\n}\n", "print(output)\n\n{\n   \"key1\":\"a something\",\n   \"key2\":\"another something\",\n   \"key3\":[\n      \"a something\",\n      \"another something\",\n      [\n         1,\n         2,\n         3\n      ],\n      {\n         \"key1\":\"a something\",\n         \"key2\":\"another something\"\n      }\n   ],\n   \"key4\":{\n      \"key1\":\"a something\",\n      \"key2\":\"another something\",\n      \"key3\":[\n         \"a something\",\n         \"another something\",\n         500,\n         1000\n      ]\n   },\n   \"key5\":{\n      \"key1\":[\n         {\n            \"key1\":\"a something\"\n         }\n      ]\n   }\n}\n"], ["sudo apt-get -y install xorg xvfb gtk2-engines-pixbuf\nsudo apt-get -y install dbus-x11 xfonts-base xfonts-100dpi xfonts-75dpi xfonts-cyrillic xfonts-scalable\n", "Xvfb -ac :99 -screen 0 1280x1024x16 &\nexport DISPLAY=:99\n"], ["import pandas as pd\nimport numpy as np\n\ndef locate_in_df(df, value):\n    a = df.to_numpy()\n    row = np.where(a == value)[0][0]\n    col = np.where(a == value)[1][0]\n    return row, col\n"], ["> pip install jupyter\n... cut ...\nSuccessfully installed jupyter-1.0.0\n", "> jupyter-notebook\n[I 11:47:56.970 NotebookApp] Serving notebooks from local directory: C:\\Users\\USER\\Documents\\VS Code - learn\n[I 11:47:56.971 NotebookApp] Jupyter Notebook 6.5.2 is running at:\n[I 11:47:56.971 NotebookApp] http://localhost:8888/?token=3e14829cf931c6aa61474c740ddf09eb34bd457f3dba20b3\n[I 11:47:56.971 NotebookApp]  or http://127.0.0.1:8888/?token=3e14829cf931c6aa61474c740ddf09eb34bd457f3dba20b3\n[I 11:47:56.971 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 11:47:57.000 NotebookApp] \n\nTo access the notebook, open this file in a browser:\n    file:///C:/Users/USER/AppData/Roaming/jupyter/runtime/nbserver-13876-open.html\nOr copy and paste one of these URLs:\n    http://localhost:8888/?token=3e14829cf931c6aa61474c740ddf09eb34bd457f3dba20b3\n or http://127.0.0.1:8888/?token=3e14829cf931c6aa61474c740ddf09eb34bd457f3dba20b3\n[W 11:48:36.001 NotebookApp] Forbidden\n[W 11:48:36.001 NotebookApp] 403 GET /api/sessions?1673977715999 (127.0.0.1) 1.000000ms referer=None\n[W 11:48:37.425 NotebookApp] Forbidden\n[W 11:48:37.426 NotebookApp] 403 GET /api/kernels?1673977717423 (127.0.0.1) 1.000000ms referer=None\n... cut ...\n"], ["wsl --update \n"], [], [], ["import polars as pl\n\n#Create new column list(can be created dynamically as well)\n\nnew_cols=['new_col1','new_col2','new_col3',.....,new_coln]\n\n#Define expression\n\nexpr = [pl.col('col1').str.split('/').arr.get(i).alias(col)\n        for i,col in enumerate(new_cols)\n        ] \n        \n#Apply Expression\n\ndf.with_columns(expr)\n"], ["  dropdown_locator = page.locator(DROPDOWN_LOCATOR)\n  dropdown_locator = dropdown_locator.filter(has_text=\"Banana\")\n  options_txt = activity_locator.text_content()\n  options = options_txt.split('\\n')\n  print(\"Monkey searching banana in \" + str(options))\n  full_label = \"\"\n  for item in options:\n      if \"Banana\" in item:\n          full_label = item.strip()\n  print(full_label)\n\n  if len(full_label) != 0:\n      page.select_option(DROPDOWN_LOCATOR, label=full_label)\n"], [], ["replaced ._model.norm_layer.0.1.2: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.0.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.0.1.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.0.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.0.1.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.2.0.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.2.0.1.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.2.3.0.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.2.3.0.1.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\nreplaced ._model.norm_layer.backbone.encoder.blocks.0.1.2.3.4.0.conv1.bn1.relu: <class 'torch.nn.modules.activation.ReLU'>->Hardswish()\n"], [], ["pip install PyMuPDF\n", "pip uninstall fitz\n"], [], [], [], ["const { Builder } = require('selenium-webdriver')\nconst chrome = require('selenium-webdriver/chrome')\nconst options = new chrome.Options()\noptions.excludeSwitches(['enable-logging'])\nconst driver = new Builder()\n   .forBrowser('chrome')\n   .setChromeOptions(options)\n   .build()\n"], [], ["conda install -c anaconda openssl\n"], [], [], ["    n=14\ndf['rsi14'] = 100 - (100 / (1 + df['Close'].diff(1).mask(df['Close'].diff(1) < 0, 0).ewm(alpha=1/n, adjust=False).mean() / df['Close'].diff(1).mask(df['Close'].diff(1) > 0, -0.0).abs().ewm(alpha=1/n, adjust=False).mean()))\n", "rows    np      loop    native\n23      1.0     1.3     0.8\n230     1.1     1.4     0.9\n2300    1.1     1.3     0.9\n23000   3.4     1.8     1.2\n"], ["# serialize model to JSON\nmodel_json =  loaded_model2.to_json()\nwith open('/content/drive/MyDrive/dataset/extract/model_5.json', \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\n\nloaded_model2.save_weights('/content/drive/MyDrive/model_5.h5')\nprint(\"Saved model to disk\")\n", "model_new = tf.keras.Sequential()\nmodel_new.add(tf.keras.applications.VGG19(include_top=false, weights='imagenet',pooling='avg',input_shape=(220,220,3)))\nmodel_new.add(tf.keras.layers.Dense(2,activation=\"softmax\"))\nopt = tf.keras.optimizers.SGC(0,004)\nmodel_new.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n", "model_new.load_weights('/home/pi/projects/models/model_5.h5)\n"], [], ["echo \"alias python=/usr/bin/python3\" >> ~/.zshrc\n"], ["bq ls --transfer_config --transfer_location=US --format=prettyjson|jq -r \".[]|[.name,.displayName,.dataSourceId,.state,.userId]|@csv\"|tr -d \"\\\"\"\n"], ["def convert_case(str_camelcase):\n    # This function takes in a string in camelCase and converts it to snake_case\n    str_snake_case = \"\"\n    for ele in list(str_camelcase):\n        if ele.islower():\n            str_snake_case = str_snake_case + ele\n        else:\n            str_snake_case = str_snake_case + \"_\" + ele.lower()\n    return str_snake_case\n    \n\ndef convert_json(json_dict):\n    # This function takes in a dictionary and converts the keys of the dictionary to snake_case\n    temp = {}\n    for item in json_dict:\n        new_item = convert_case(item)\n        temp[new_item]=json_dict[item]\n        new_list = []\n        if type(json_dict[item]) is list:\n            for ele in json_dict[item]:\n                if type(ele) is dict:\n                    new_list.append(convert_json(ele))\n            if len(new_list)!=0:\n                temp[new_item] = new_list\n        if type(json_dict[item]) is dict:\n           # if the value is a dictionary, recursively convert it to snake_case\n           temp[new_item] = convert_json(json_dict[item])\n    return temp\n\njson = {\n   \"firstName\":\"abc\",\n   \"lastName\":\"xyz\",\n   \"favoriteMovies\":[\n      \"Star Wars\",\n      \"The lone ranger\"\n   ],\n   \"favoriteCountries\":[\n      {\n         \"country\":\"China\",\n         \"capitalCity\":\"Beiging\"\n      },\n      {\n         \"country\":\"India\",\n         \"capitalCity\":\"New Delhi\"\n      }\n   ]\n}\n\nprint(convert_json(json))\n", "{\n   \"first_name\":\"abc\",\n   \"last_name\":\"xyz\",\n   \"favorite_movies\":[\n      \"Star Wars\",\n      \"The lone ranger\"\n   ],\n   \"favorite_countries\":[\n      {\n         \"country\":\"China\",\n         \"capital_city\":\"Beiging\"\n      },\n      {\n         \"country\":\"India\",\n         \"capital_city\":\"New Delhi\"\n      }\n   ]\n}\n"], [], ["pip uninstall serial\n", "pip uninstall pyserial\n", "pip install pySerial\n"], [], [], ["brew install python\n", "brew link python3\n"], ["import pydicom\nprint(pydicom.__version__) # output is a string: 'x.y.z'\n", "import pydicom\nprint(pydicom.__version_info__) # output is a tuple: ('x', 'y', 'z')\n"], [], [], [], ["df.columns[df.isin(['Yes']).any()]\n"], ["handle.selectOption({\"label\": \"Banana\"})\n"], [], [], [], ["def calc_base_area(base_length, base_width):\n   return base_length * base_width\ndef calc_pyramid_volume(base_length,base_width,pyramid_height):\n    return (length*width) * height * 1/3\nlength = float(input())\nwidth = float(input())\nheight = float(input())\nprint('Volume for', length, width, height, \"is:\", calc_pyramid_volume(length, width, height))\n"], ["import asyncio\nimport uvicorn\nfrom fastapi import FastAPI\napp = FastAPI()\n\n@app.get('/')\nasync def root():\n    print('Sleeping for 10')\n    await asyncio.sleep(10)\n    print('Awake')\n    return {'message': 'hello'}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"], [], ["def NumberStream(string):\n\n    j=0\n\n    for i in string:\n\n        j=i*int(i)\n\n        if j in string:\n\n            return True\n\n            break\n\n    else:\n        return False\n\n\nprint(NumberStream(\"653999923335\"))\n"], [], ["# Those are the imports, that actualy load the correct code\nimport tensorflow.keras as tfk\nimport tensorflow.keras.layers as layers\n\n# This is for typehinting and intllisense\nimport tensorflow.python.keras as _tfk\nimport tensorflow.python.keras.layers as _layers\n\n# This gets highlighted as error by my linter, but it runs\ntfk: _tfk\nlayers: _layers\n\n# from now on, the intellisense and docstrings work\n# ...\n"], [], ["python -m streamlit run <filename.py>\n"], ["docker-compose build \ndocker-compose up -d\n"], ["streamlit run hello.py\n"], [], [], ["RUN apt update --fix-missing && \\\napt install python3.8 -y && \\\nupdate-alternatives --install /usr/bin/python python /usr/bin/python3.8 10\n", "RUN apt install python3-pip -y && \\\npython -m pip install --upgrade pip\n"], ["def romanToDecimal(self, S): \n    d = {\n        'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000\n    }\n    res = 0\n    old = 'I'\n    \n    for i in S[::-1]:\n        if d[i] >= d[old]:\n            res = res + d[i]\n        else:\n            res = res - d[i]\n        old = i\n        \n    return res\n"], [], ["!pip install importlib-metadata==4.13.0\n"], ["pip install pywin32==300\n"], ["import tensorflow as tf\nimport tensorflow.keras as keras\n", "import tensorflow as tf\nkeras = tf.keras\n"], [], [], [], [], [], ["buildings.insert(0, 'Number', range(0,len(buildings)))\n\nbuildings.set_index('Number' , inplace = True)\n"], [], ["conda install --channel conda-forge geopandas\n"], [], ["conda update conda\nconda update anaconda-navigator\nconda update navigator-updater\n"], [], ["[network]\ngenerateResolvConf = false\n"], [], ["% which python\n/Users/jl/.pyenv/shims/python\n\n% python --version\nPython 3.10.6\n"], [], [], ["(df\n.with_column(pl.col(\"col1\").str.split(\"/\"))\n.with_columns(\n[pl.col(\"col1\").arr.get(i).alias(str(i)) for i in range(len(df[0,\"col1\"].split('/')))\n]\n)\n)\n"], ["import polars as pl\nfrom polars import col\n\ndf = pl.DataFrame({\n    'col1': [\"a/b/c/d\", \"e/f/j/k\"]\n})\n\nprint(df)\n", "df = df.with_columns([\n    col('col1'),\n    *[col('col1').apply(lambda s, i=i: s.split('/')[i]).alias(col_name)\n      for i, col_name in enumerate(['a', 'b', 'c', 'd'])]\n\n    # or without 'for'\n    # col('col1').apply(lambda s: s.split('/')[0]).alias('a'),\n    # col('col1').apply(lambda s: s.split('/')[1]).alias('b'),\n    # col('col1').apply(lambda s: s.split('/')[2]).alias('c'),\n    # col('col1').apply(lambda s: s.split('/')[3]).alias('d')\n])\n\nprint(df)\n"], [], ["base_area = base_length * base_width\n\nvolume = (base_area * pyramid_height) / 3\n\nreturn (base_length * base_width) * pyramid_height / 3\n"], [], ["chromeOptions.add_argument(\"--window-size=1920,1080\")\n"], [], [], [], ["import dask.dataframe as dd\ndf = dd.read_csv('path_to_large_file.csv')\n...\n"], [], [], [" pip install virtualenv\n", "source /python -m virtualenv .\n", " source /venv/Scripts/activate \n", " source /venv/bin/activate \n", "(venv) source  /pip list\n", "(venv) source  /pip install ursina\n"], [], [], [], [], [], ["sudo ln -s /usr/bin/python3 /usr/local/bin/python\n"], ["docker ps\n", "docker container rm {airflow-webserver}\n"], [], [], ["Traceback (most recent call last):\n  File \"D:\\Random Work\\youtube scraping\\yt_vc.py\", line 5, in <module>\n    result = pafy.new(url)\n  File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\pafy.py\", line 124, in new\n    return Pafy(url, basic, gdata, size, callback, ydl_opts=ydl_opts)\n  File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\backend_youtube_dl.py\", line 31, in __init__\n    super(YtdlPafy, self).__init__(*args, **kwargs)\n  File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\backend_shared.py\", line 97, in __init__\n    self._fetch_basic()\n  File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\backend_youtube_dl.py\", line 54, in _fetch_basic\n    self._dislikes = self._ydl_info['dislike_count']\nKeyError: 'dislike_count'\n", "File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\backend_youtube_dl.py\", line 54, in _fetch_basic\n    self._dislikes = self._ydl_info['dislike_count']\n", "File \"C:\\Users\\username\\Anaconda3\\envs\\wsWork\\lib\\site-packages\\pafy\\backend_youtube_dl.py\n"], [], [], ["from fastapi import FastAPI, BackgroundTasks\nimport time\napp = FastAPI()\n\ndef sleep(msg):\n    time.sleep(10)\n    print(msg)\n\n@app.get('/')\nasync def root(background_tasks: BackgroundTasks):\n    msg= 'Sleeping for 10'\n    background_tasks.add_task(sleep, msg)\n    print('Awake')\n    return {'message': 'hello'}\n\n"], [], ["class myModel(tf.keras.Model):\n\n  def __init__(self):\n\n    self.conv1 = Conv2D(32)\n    self.conv2 = Conv2D(32)\n    self.conv3 = Conv2D(16)\n\n  def call(self, inputs):\n\n    net1 = self.conv1(inputs)\n    net2 = self.conv2(inputs)\n    net = tf.concat([net1, net2], axis=2)\n    net = self.conv3(net)\n    return end_points = tf.nn.softmax(net)  # Change this line\n"], ["curl -sS https://bootstrap.pypa.io/get-pip.py | python3.8\n"], ["from sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n\n# Two lists of sentences\nsentences1 = ['The cat sits outside',\n             'A man is playing guitar',\n             'The new movie is awesome']\n\nsentences2 = ['The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\n#Compute embedding for both lists\nembeddings1 = model.encode(sentences1, convert_to_tensor=True)\nembeddings2 = model.encode(sentences2, convert_to_tensor=True)\n\n#Compute cosine-similarits\ncosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n\n#Output the pairs with their score\nfor i in range(len(sentences1)):\n    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n"], ["curl https://bootstrap.pypa.io/pip/3.5/get-pip.py -o get-pip.py\npython3 get-pip.py\n"], [], [], [], [], [], [], [], ["$ streamlit hello\n", "$ streamlit run script.py\n"], [], ["import win32api\n\nwhile True:\n    if win32api.GetKeyState(0x01)<0: #if mouse left button is pressed\n       print(\"Pressed\")\n    else: #if mouse left button is not pressed\n       print(\"Released\")\n"], ["def base_area(base_length, base_width):\n    return base_length * base_width\n\ndef pyramid_volume(base_length, base_width, pyramid_height):\n    return (base_area(base_length, base_width) * pyramid_height)/3\n\nlength = float(input())\nwidth = float(input())\nheight = float(input())\nprint('Volume for 4.5, 2.1, 3.0 is:', pyramid_volume(length, width, height))\n"], ["def pyramid_volume(length, width, height):\n        return (length * width) * height/3\n        \n    length = float(input())\n    width = float(input())\n    height = float(input())\n    print('Volume for', length, width, height, \"is:\", pyramid_volume(length, width, height)\n"], [], [], ["conda install -c conda-forge librosa\n"], [], [], ["   A      B\n0  a  a x d\n2  c  q m c\n"], ["pip install git+https://github.com/Cupcakus/pafy\n"], [], ["      A = np.random.random([n,1])\n      return A\n"], ["surf._facecolors2d = surf._facecolor3d\nsurf._edgecolors2d = surf._edgecolor3d \n"], [], [], ["- curl -O https://bootstrap.pypa.io/pip/2.7/get-pip.py\n- python get-pip.py\n- python -m pip install --upgrade \"pip < 21.0\"\n", "python -m pip install --upgrade \"pip < 19.2\"\n"], ["dataSet['combined'] = [[e for l in x for e in l]\n                       for _,x in dataSet.filter(like='value').iterrows()]\n", "   key                    valueA                    valueB                    valueN                                                                  combined\n0  1_1        [1, 2, 3, 4, 5, 6]        [1, 2, 3, 4, 5, 6]        [1, 2, 3, 4, 5, 6]                    [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\n1  1_2     [7, 8, 9, 10, 11, 12]     [7, 8, 9, 10, 11, 12]     [7, 8, 9, 10, 11, 12]           [7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12]\n2  1_3  [13, 14, 15, 16, 17, 18]  [13, 14, 15, 16, 17, 18]  [13, 14, 15, 16, 17, 18]  [13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18]\n"], ["dataSet['Lists'] = dataSet['valueA'] + dataSet['valueB'] + dataSet['valueN']\ndataSet.drop(columns=['valueA','valueB',\"valueN\"],inplace=True)\nprint(dataSet)\n"], ["import pandas as pd\n\ndata = {\n    'key': ['1_1', '1_2', '1_3'],\n    'valueA': [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]],\n    'valueB': [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]],\n    'valueN': [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18]],\n}\ndataSet = pd.DataFrame(data)\n\ncolumns_to_combine = ['valueA', 'valueB', 'valueN']\n\ndataSet['combined'] = dataSet[columns_to_combine].sum(axis=1)\ndataSet.drop(columns=columns_to_combine, inplace=True) # remove the old columns\n\nprint(dataSet)\n\n# output:\n#    key                                                                  combined\n# 0  1_1                    [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\n# 1  1_2           [7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12, 7, 8, 9, 10, 11, 12]\n# 2  1_3  [13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18, 13, 14, 15, 16, 17, 18]\n"], [], ["merge_columns = list(dataSet.columns)\nmerge_columns.remove(\"key\")\ndataSet[\"combined\"] = dataSet[merge_columns].values.tolist()\n", "dataSet[\"combined\"] = dataSet[\"combined\"].apply(lambda x: [item for sublist in x for item in sublist])\n"], ["sudo apt autoremove python3 -y\n"], [], [], ["def bd_digit(string):\n    total = 0\n    for digit in string:\n        total+=int(digit)\n    \n    if total >=10:\n        return bd_digit(str(total))\n    else:\n        return total   \nprint(bd_digit('19991229'))\n"], [], ["a = set(range(1, 6))\nb = set(range(3, 9))\nc = a & b\nprint(tuple(c))\n"], [], ["brew install pyenv\npyenv install 2.7.18\n", "pyenv global 2.7.18\n", "PATH=$(pyenv root)/shims:$PATH\n"], ["number = int(input())\n \nleft = 1\nright = number\n \nwhile left < right:\n    print(left)\n    print(right)\n    left += 1\n    right -= 1\n# In case of odd numbers\nif left == right:\n    print(left)`\n"], ["docker buildx build --platform=linux/amd64 -t rahul86s/rsharmp12_model:latest .\n"], [], ["import random\n\nlist_1 = [random.randint(1, 10_000) for i in range(100_000)]\nlist_2 = [random.randint(1, 10_000) for i in range(100_000)]\n", "list_1 = [random.randint(1, 10_000) for i in range(100_000)]\nlist_2 = [random.randint(10_001, 20_000) for i in range(100_000)]\n", "import random \n\nlist_1 = [random.randint(1, n) for i in range(n)]\nlist_2 = [random.randint(1, n) for i in range(n)]\n", "import random \n\nlist_1 = [random.randint(1, n ** 2) for i in range(n)]\nlist_2 = [random.randint(1, n ** 2) for i in range(n)]\n", "list_1 = list(range(n))\nlist_2 = list(range(n, 2 * n))\n", "import random \n\nlist_1 = [random.randint(1, n) for i in range(10 * n)]\nlist_2 = [random.randint(1, n) for i in range(10 * n)]\n"], ["np.bincount(list_1).astype(bool)[list_2]\n"], ["list_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\ns = set(list_2)\nbooleans = []\nfor i in list_1:\n   booleans.append(i in s)\nprint(booleans)\n", "s = set(list_2)\nbooleans = [i in s for i in list_1]\n", "list_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\nprint(set(list_1).intersection(set(list_2)))\n", "{1, 2}\n", "print(list(set(list_1).intersection(set(list_2))))\n"], [], [], ["list_1 = [0,0,1,2,0,0]\nlist_2 = [1,2,3,4,5,6]\n\nbooleans = []\n\nset_1 = set(list_1)\nset_2 = set(list_2)\n\nif(set_1 & set_2):\n  print(set_1 & set_2)\nelse:\n  print(\"No common elements\")\n", "{1, 2}\n"], [], [], ["#!/usr/bin/env python\n\nimport sys\nimport time\nimport math\n\ndef next_prime(number):\n    if number < 0:\n        raise ValueError('Negative numbers can not be primes')\n    # Base case\n    if number <= 1:\n        return 2\n\n    # if even go back 1\n    if number % 2 == 0:\n        number -= 1\n    while True:\n        # only odds\n        number += 2\n        #only need to check up to and including the sqrt\n        max_check = int(math.sqrt(number))+2\n        # don't need to check even numbers\n        for divider in range(3, max_check, 2):\n            # if 'divider' divides 'number', then 'number' is not prime\n            if number % divider == 0:\n                break\n        # if the for loop didn't break, then 'number' is prime\n        else:\n            return number\n\nif __name__ == '__main__':\n    number = int(sys.argv[1].strip())\n    t0 = time.time()\n    print('{0:d} is the next prime from {1:d}'.format(next_prime(number), number))\n    run_time = time.time() - t0\n    print('run_time = {0:.8f}'.format(run_time))\n"], ["# Explicitly import lazy-loaded modules to support autocompletion.\n# pylint: disable=g-import-not-at-top\nif _typing.TYPE_CHECKING:\n  from tensorflow_estimator.python.estimator.api._v2 import estimator as estimator\n  from keras.api._v2 import keras\n  from keras.api._v2.keras import losses\n  from keras.api._v2.keras import metrics\n  from keras.api._v2.keras import optimizers\n  from keras.api._v2.keras import initializers\n# pylint: enable=g-import-not-at-top\n"], ["lib_path=\"c:\\\\users\\\\user\\\\python_39\\\\lib\\\\site-packages\\\\\"\nMODULE_NAME = \"module_to_import\"\nMODULE_PATH = lib_path+MODULE_NAME+\"\\\\__init__.py\"\nimport importlib\nimport sys\nspec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)\nmodule = importlib.util.module_from_spec(spec)\nsys.modules[spec.name] = module \nspec.loader.exec_module(module)\nimport module_to_import\n"], [], ["from ursina import *\n\napp= Ursina()\n(code)\napp.run()\n"], [], ["mask = (df['column2'].isna()) | (df['column2']==0)\ndf.loc[mask, \"column2\"] = df.loc[mask, \"column1\"]\n"], ["#Creates boolean array conditionCheck, checking conditions for each row in df\n#Where() will only update when conditionCheck == False, so inverted boolean values using \"~\"\n\nconditionCheck = ~((df['column2'].isna()) | (df['column2']==0))\ndf[\"column2\"].where(conditionCheck,df[\"column1\"],inplace=True)\n\nprint(df)\n", "import numpy as np\nimport pandas as pd\n\ndata = [\n        [5263,5400,5400]\n        ,[4354,6567,None]\n        ,[5656,5456,0]  \n        ,[5565,6768,3489]\n        ,[4500,3490,None]\n        ]\ndf = pd.DataFrame(data,columns=[\"id\",\"column1\",\"column2\"],dtype=pd.Int64Dtype())\n"], ["\n#we start by creating an empty list \ncolumn2 = []\n\n#for each row in the dataframe \nfor i in df.index:\n    # if the value col2 is null or 0, then it takes the value of col1\n    if df.loc[i, 'column2'] in ['null', 0]:\n        column2.append(df.loc[i, 'column1'])\n    #else it takes the value of column 2\n    else: \n        column2.append(df.loc[i, 'column2'])\n\n#we replace the current column 2 by the new one !\ndf['column2'] = column2```\n"], ["out = df_1.loc[df_1.apply(lambda x: x['A'] in x['B'], axis=1)]\nprint(out)\n\n# Output\n   A      B\n0  a  a x d\n2  c  q m c\n", "df_1.loc[np.apply_along_axis(lambda x: x[0] in x[1], axis=1, arr=df_1)]\n"], ["from operator import contains\n\ndf_1.loc[map(contains, *map(df_1.get, ['B', 'A']))]\n\n   A      B\n0  a  a x d\n2  c  q m c\n", "df_1.loc[map(str.__contains__, *map(df_1.get, ['B', 'A']))]\n"], ["df_1[df_1.apply(lambda x:x['A'] in x['B'],axis=1)]\n"], ["df.apply(lambda row: row['A'] in row['B'], axis = 1)\n", "df.loc[df.apply(lambda row: row['A'] in row['B'], axis = 1)]\n"], [], [], ["import functools\n\ndef your_func(value1):\n    return \"\"\n\ntrigger_report = PythonOperator(\n    task_id=\"aaaa\",\n    python_callable=functools.partial(your_func, value1=1),\n    provide_context=True,\n    dag=dag\n)\n"], [], ["self._dislikes = 0 # self._ydl_info['dislike_count']\n"], [], [], ["from google.cloud import bigquery_datatransfer\n\nclient = bigquery_datatransfer.DataTransferServiceClient()\nparent = client.common_project_path(\"<PROJECT-ID>\")\nresp = client.list_transfer_configs(parent=parent)\nprint(resp)\n"], ["import pandas as pd\nimport json\nfrom subprocess import PIPE, run, call\n\nresponse = run('bq ls --transfer_config --transfer_location=US --format=prettyjson', \n               stdout=PIPE, \n               stderr=PIPE, \n               universal_newlines=True, \n               shell=True)\n\nresponse\n", "CompletedProcess(args='bq ls --transfer_config --transfer_location=US --format=prettyjson', returncode=0, stdout='[\\n  {\\n    \"dataSourceId\": \"scheduled_query\",\\...\n", "data = json.loads(response.stdout)\ndf = pd.json_normalize(data)\n", "dataSourceId\ndatasetRegion\ndestinationDatasetId\ndisabled\ndisplayName\nname\nschedule\nstate\nupdateTime\nuserId\nemailPreferences.enableFailureEmail\nparams.destination_table_name_template\n\n### sql located in this one\nparams.query\n\nparams.write_disposition\nscheduleOptions.startTime\nparams.overwrite_destination_table\nparams.source_dataset_id\nparams.source_project_id\nscheduleOptions.endTime\nnextRunTime\n"], ["wget https://bootstrap.pypa.io/pip/2.7/get-pip.py\npython get-pip.py\npip install virtualenv # This will not work, use below\n~/Library/Python/2.7/bin/pip  install virtualenv\n~/Library/Python/2.7/bin/virtualenv --python=/usr/bin/python venv_twisted\nsource venv_twisted/bin/activate\n"], ["pip install pipwin,\n"], [], ["msg = QMessageBox()\nmsg.setIcon(QMessageBox.Icon.Information)\n\nmsg.setText('Teste')\nmsg.setInformativeText(\"This is additional information\")\nmsg.setWindowTitle(\"MessageBox demo\")\n#msg.setDetailedText(\"The details are as follows:\")\nmsg.setStandardButtons(QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.Cancel)\nmsg.buttonClicked.connect(self.msgbtn)\nmsg.exec()\n\ndef msgbtn(self, i):\n    print( \"Button pressed is:\",i.text() )\n"], ["import pandas as pd\nrsi_period = 14\ndf = pd.Series(coinaalist)\nchg = df.diff(1)\ngain = chg.mask(chg<0,0)\nloss = chg.mask(chg>0,0)\navg_gain = gain.ewm(com = rsi_period-1,min_periods=rsi_period).mean()\navg_loss = loss.ewm(com = rsi_period-1,min_periods=rsi_period).mean()\nrs = abs(avg_gain / avg_loss)\ncrplaa = 100 - (100/(1+rs))\ncoinaarsi = crplaa.iloc[-1]\n"], [], [], ["cv2.TrackerMOSSE_create()\n", "cv2.legacy.TrackerMOSSE_create()\n"], [], [], ["from collections import defaultdict\nimport operator\ndef NumberStream(string):\n    count={}\n    for i in range(len(string)-1):\n        if string[i]==string[i+1]:\n            count[str(i)]=string[i]\n            count[str(i+1)]=string[i+1]\n    print(count)   \n    res=defaultdict(int)\n    for key, item in count.items():\n        res[item]+=1\n    print(res)\n    stream=0\n    for key, item in res.items():\n        if int(key)== item:\n            stream=key\n    \n    if int(stream)> 0:\n        return \"true\"\n    else: return \"false\"\n\n\n    \n"], [], ["$ pip install ipykernel --upgrade\n", "$ pip install traitlets --upgrade\n$ pip install notebook --upgrade\n"], [], ["def valid(num, count=0):\n    num, r = divmod(num, 10) # extract the last digit (r)\n    if num == 0:             # we exhausted the number\n        return count%2==0    # is there an even number of zeros?\n    else:\n        return valid(num, count=count+int(r==0))\n", "def valid(num, count=None):\n    if count is None:\n        count = int(num==0)\n    num, r = divmod(num, 10)\n    if num == 0:\n        return count%2==0\n    else:\n        return valid(num, count=count+int(r==0))\n", ">>> valid(12340006)\nFalse\n\n>>> valid(10203)\nTrue\n\n>>> valid(0)\nFalse\n"], ["def valid(n):\n    \n    zeros = str(n).count(\"0\")\n    \n    if zeros == 0:\n        return False\n    else:\n        return zeros % 2 == 0\n"], ["def valid(n):\n    number = str(n)\n    position = number.find(\"0\")\n    if \"0\" not in number:\n        return 0\n    return 1 + valid(number[(position+1):])\nprint(\"True\" if valid(12340006)%2 ==0 else \"False\")\n"], ["python -m pip install 'traitlets==4.3.3' --force-reinstall\n"], [], ["sudo apt install mupdf\nsudo apt install libmupdf-dev\npip3 install PyMuPDF==1.16\n"], [], ["def calc_base_area(base_length, base_width):\n    return base_length * base_width\n    \ndef calc_pyramid_volume(base_area, pyramid_heigth):\n    return calc_base_area * pyramid_heigth \n    \nlength = float(input())\nwidth = float(input())\nheight = float(input())\nbase = calc_base_area(length, width)\nprint('Volume for', length, width, height, \"is:\", calc_pyramid_volume(base, height))\n"], [], ["    new_dinner = ['ali','zeshan','raza']\n    print ('this is old friend', str(new_dinner))\n\n    #Try turning the list into a strang only\n"], ["@permission_classes((AllowAny,))\n"], [], ["result = re.split(r\",(?!(?:[^,\\[\\]]+,)*[^,\\[\\]]+])\", subject, 0)\n", "year:2020,concepts:[ab553,cd779],publisher:elsevier,year:2020,concepts:[ab553,cd779,xx345],publisher:elsevier\n"], ["(?:[^,]*\\[[^][]*])+[^,]*|[^,]+\n", "s = \"year:2020,concepts:[ab553,cd779],publisher:elsevier\"\nparams = re.findall(r\"(?:[^,]*\\[[^][]*])+[^,]*|[^,]+\", s)\nprint(params)\n", "['year:2020', 'concepts:[ab553,cd779]', 'publisher:elsevier']\n"], [",(?=[^,]+?:)\n"], ["s = \"year:2020,concepts:[ab553,cd779],publisher:elsevier\"\n\n\ndef split_by_commas(s):\n    lst = list()\n    last_bracket = ''\n    word = \"\"\n    for c in s:\n        if c == '[' or c == ']':\n            last_bracket = c\n        if c == ',' and last_bracket == ']':\n            lst.append(word)\n            word = \"\"\n            continue\n        elif c == ',' and last_bracket == '[':\n            word += c\n            continue\n        elif c == ',':\n            lst.append(word)\n            word = \"\"\n            continue\n        word += c\n    lst.append(word)\n    return lst\nmain_lst = split_by_commas(s)\n\nprint(main_lst)\n", "['year:2020', 'concepts:[ab553,cd779]', 'publisher:elsevier']\n"], ["options = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome(options=options)\n"], ["df_labevents_temp['age']  = ((df['date_of_admission'].values  - df['DOB'].values).astype(np.int)/8.64e13//365).astype(np.int)\n"], ["conda deactivate\n", "conda activate sample\n"], [], [], [], [], [], ["surf = ax.plot_surface(X, Y, Z, label='h=0')\nsurf._facecolors2d=surf._facecolors3d\nsurf._edgecolors2d=surf._edgecolors3d\n", "surf._facecolors2d = surf._facecolor3d\nsurf._edgecolors2d = surf._edgecolor3d\n"], [], [], [], [], ["options = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\nbrowser = webdriver.Chrome(options=options)\n"], [], [], ["Read more about pip instalation steps [here][3]\n", "python get-pip.py\n"], ["l = list(range(1,6))\ndef index_generator():\n    while True:\n        yield 0\n        yield -1\n\nindex = index_generator()\nresult = []\nwhile l:\n    result.append(l.pop(next(index)))\n"], ["# install py3.8 and dependencies for the pip3 bootstrap script\nadd-apt-repository -y ppa:deadsnakes/ppa && \\\n    apt install -y python3.8 python3.8-distutils\n\n# download and run the pip3 bootstrap script\ncd /tmp && wget https://bootstrap.pypa.io/get-pip.py && \\\n    python3.8 /tmp/get-pip.py\n\n# use pip py3.8 module to install python packages\npython3.8 -m pip install numpy pandas\n"], ["pip install h5py==2.10.0 --force-reinstall\n"], [], ["x = flag = 1\nfor i in range(n-1, -1, -1):\n    print(x)\n    flag, x = -flag, x+flag*i\n"], [], [], ["nums = list(range(1, int(input(\"Please type in a number:\"))+1))\nwhile nums:\n    print(nums.pop(0))\n    if nums:\n        print(nums.pop())\n"], ["brew install --cask anaconda\n", "conda create --prefix=/MY_FOLDER/NAME_OF_ENVIRONMENT python=2.7.18\n", "conda env list\n", "conda activate NAME_OF_ENVIRONMENT\n"], ["def get_permissions(self):\n    return [permission() for permission in self.permission_classes]\n", "return [permission() for permission in self.permission_classes] if isinstance(self.permission_classes], Iterable) else [self.permission_classes]\n", "try:\n    from collections.abc import Iterable  # for Python >= 3.6\nexcept ImportError:\n    from collections import Iterable\n"], ["output = tf.keras.layers.Conv2D(5, (1, 1), activation = \"softmax\")(c9)\n"], [" def some_func(x):\n       x1 = x * some variables\n       x2 = x1 + some variables #x2 discontinued after here\n       x3 = x1 / some variables\n       return x3\n"], [], ["CONFIG_FILE=inventory/mycluster/hosts.yaml python3.6 contrib/inventory_builder/inventory.py ${IPS[@]}\n"], ["OPENBLAS_CORETYPE=ARMV8 python\n", "nano ~/.bashrc\n", "export OPENBLAS_CORETYPE=ARMV8\n"], [], ["This issue is adressed only in this version of Django\n"], [], [], [], [], ["pip install django --upgrade\n", "./manage.py makemigration\n./manage.py migrate\n./manage.py runserver\n\n"], ["python manage.py shell\nfrom django.contrib.auth.models import User\nUser.objects.filter(is_superuser=True)   \n"], [], ["pip install --upgrade django==2.1.5\n", "python manage.py makemigrations\npython manage.py migrate\npython manage.py createsuperuser\n"], ["Find the largest element.  Move it to the location A[0].\nFor i from 1 upto n:\n  For j from 0 upto i:\n    swap A[i] and A[j] if A[j] is greater than A[i]\n    Invariant: A[0...j] is sorted\n    Invariant: A[j+1...i-1] is sorted\n    Invariant: A[i] is larger than anything in A[0...j]\n\n  Invariant: A[0...i] contains the same elements sorted, but now sorted\n", "                      i\n 10  20  30  40  99  25\n", "[10] 20  30  40  99  25\n 10 [20] 30  40  99  25\n 10  20 [30] 40  99  25\n", " 10  20  25 [40] 99  30\n 10  20  25  30 [99] 40\n 10  20  25  30  40 [99]\n", "1 2 3 4 5 6 7\n Y  | | | | |\n|  Y  | | | |\n| |  Y  | | |\n| | |  Y  | |\n| | | |  Y  |\n| | | | |  Y\n| | | | | | *\n", "1 2 3 * 5 6 7 8\n", "5 1 2 3 * 6 7 8\n X  | |\n|  X  |\n| |  X\n", "1 2 3 4 5 6 7 8\n", "5 1 2 3 4 6 7 8\n X  | | |\n|  X  | |\n| |  X  |\n| | |  X\n", "6 5 1 2 3 4 6 7 8\n X  | | | |\n|  X  | | |\n| |  X  | |\n| | |  X  |\n| | | |  X\n", "1 2 3 4 5 6 7\n X  | | | | |\n|  X  | | | | \n X   X  | | |\n|  X   X  | | \n X   X   X  |\n|  X   X   X\n X   X   X  |\n|  X   X  | |\n X   X  | | |\n|  X  | | | |\n X  | | | | |\n"], ["before: [1, 12, 13, 8, 15, 18, 19, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [19, 1, 12, 8, 13, 15, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 19, 12, 8, 13, 15, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 12, 19, 8, 13, 15, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 8, 12, 19, 13, 15, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 8, 12, 13, 19, 15, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 8, 12, 13, 15, 19, 18, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 8, 12, 13, 15, 18, 19, 16, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 8, 12, 13, 15, 16, 18, 19, 7, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 7, 8, 12, 13, 15, 16, 18, 19, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 7, 8, 11, 12, 13, 15, 16, 18, 19, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 6, 7, 8, 11, 12, 13, 15, 16, 18, 19, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 6, 7, 8, 11, 12, 13, 14, 15, 16, 18, 19, 3, 2, 9, 5, 4, 0, 10, 17]\n        [1, 3, 6, 7, 8, 11, 12, 13, 14, 15, 16, 18, 19, 2, 9, 5, 4, 0, 10, 17]\n        [1, 2, 3, 6, 7, 8, 11, 12, 13, 14, 15, 16, 18, 19, 9, 5, 4, 0, 10, 17]\n        [1, 2, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 5, 4, 0, 10, 17]\n        [1, 2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 4, 0, 10, 17]\n        [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 0, 10, 17]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 18, 19, 10, 17]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 17]\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nafter:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n", "[1, 7, 8, 12, 13, 15, 16, 18, 19, 11, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n", "[1, 7, 8, 11, 13, 15, 16, 18, 19, 12, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n", "[1, 7, 8, 11, 12, 15, 16, 18, 19, 13, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n", "[1, 7, 8, 11, 12, 13, 16, 18, 19, 15, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n[1, 7, 8, 11, 12, 13, 15, 18, 19, 16, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n[1, 7, 8, 11, 12, 13, 15, 16, 19, 18, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n[1, 7, 8, 11, 12, 13, 15, 16, 18, 19, 6, 14, 3, 2, 9, 5, 4, 0, 10, 17]\n"], ["options = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome(options=options)\ndriver.get('https://something.com/login')\ndriver.maximize_window()\n"], ["options = webdriver.ChromeOptions()\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome(options=options)\n"], ["for i from 0 to n-1:\n    for j from 0 to n-1:\n        if A[j] > A[i]:\n            swap A[i] and A[j]\n    print(A) <- add here\n", "A = [5, 5, 0, 9, 2]\n0.  [9, 5, 0, 5, 2]\n1.  [5, 9, 0, 5, 2]\n2.  [0, 5, 9, 5, 2]\n3.  [0, 5, 5, 9, 2]\n4.  [0, 2, 5, 5, 9]\n", "for i from 0 to n-1:\n    for j from 0 to i: <- claim this line can be changed\n        if A[j] > A[i]:\n            swap A[i] and A[j]\n"], [], ["def nextprime(n):\n    if n < 0:\n      raise ValueError\n  \n    for i in range(n + 1, n +200):\n        if i > 1:\n            pr = True\n            for j in range(2, i):\n                if (i % j) == 0:\n                    pr = False\n                    break\n            if pr:\n                return i\n    return 'not found'\n", "def is_prime(x):\n    return all(x % i for i in range(2, x))\n\ndef next_prime(x):\n    return min([a for a in range(x+1, 2*x) if is_prime(a)])\n\nprint(next_prime(32))\n", "from sympy import *\nnextprime(32) \n"], [], ["def next_prime(n):\n    while True:\n        n=n+1\n        for i in range (2,int(n/2)):\n            if n%i==0:\n                break\n        else:\n            return n\n\n\nprint(next_prime(67)) \n"], ["import cv2\n\nimage = cv2.imread('1.jpg')\n\nalpha = 1.95 # Contrast control (1.0-3.0)\nbeta = 0 # Brightness control (0-100)\n\nmanual_result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n\ncv2.imshow('original', image)\ncv2.imshow('manual_result', manual_result)\ncv2.waitKey()\n", "import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# Automatic brightness and contrast optimization with optional histogram clipping\ndef automatic_brightness_and_contrast(image, clip_hist_percent=1):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # Calculate grayscale histogram\n    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n    hist_size = len(hist)\n    \n    # Calculate cumulative distribution from the histogram\n    accumulator = []\n    accumulator.append(float(hist[0]))\n    for index in range(1, hist_size):\n        accumulator.append(accumulator[index -1] + float(hist[index]))\n    \n    # Locate points to clip\n    maximum = accumulator[-1]\n    clip_hist_percent *= (maximum/100.0)\n    clip_hist_percent /= 2.0\n    \n    # Locate left cut\n    minimum_gray = 0\n    while accumulator[minimum_gray] < clip_hist_percent:\n        minimum_gray += 1\n    \n    # Locate right cut\n    maximum_gray = hist_size -1\n    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n        maximum_gray -= 1\n    \n    # Calculate alpha and beta values\n    alpha = 255 / (maximum_gray - minimum_gray)\n    beta = -minimum_gray * alpha\n    \n    '''\n    # Calculate new histogram with desired range and show histogram \n    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n    plt.plot(hist)\n    plt.plot(new_hist)\n    plt.xlim([0,256])\n    plt.show()\n    '''\n\n    auto_result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n    return (auto_result, alpha, beta)\n\nimage = cv2.imread('1.jpg')\nauto_result, alpha, beta = automatic_brightness_and_contrast(image)\nprint('alpha', alpha)\nprint('beta', beta)\ncv2.imshow('auto_result', auto_result)\ncv2.waitKey()\n", "import cv2\nimport numpy as np\n# from matplotlib import pyplot as plt\n\ndef convertScale(img, alpha, beta):\n    \"\"\"Add bias and gain to an image with saturation arithmetics. Unlike\n    cv2.convertScaleAbs, it does not take an absolute value, which would lead to\n    nonsensical results (e.g., a pixel at 44 with alpha = 3 and beta = -210\n    becomes 78 with OpenCV, when in fact it should become 0).\n    \"\"\"\n\n    new_img = img * alpha + beta\n    new_img[new_img < 0] = 0\n    new_img[new_img > 255] = 255\n    return new_img.astype(np.uint8)\n\n# Automatic brightness and contrast optimization with optional histogram clipping\ndef automatic_brightness_and_contrast(image, clip_hist_percent=25):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Calculate grayscale histogram\n    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n    hist_size = len(hist)\n\n    # Calculate cumulative distribution from the histogram\n    accumulator = []\n    accumulator.append(float(hist[0]))\n    for index in range(1, hist_size):\n        accumulator.append(accumulator[index -1] + float(hist[index]))\n\n    # Locate points to clip\n    maximum = accumulator[-1]\n    clip_hist_percent *= (maximum/100.0)\n    clip_hist_percent /= 2.0\n\n    # Locate left cut\n    minimum_gray = 0\n    while accumulator[minimum_gray] < clip_hist_percent:\n        minimum_gray += 1\n\n    # Locate right cut\n    maximum_gray = hist_size -1\n    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n        maximum_gray -= 1\n\n    # Calculate alpha and beta values\n    alpha = 255 / (maximum_gray - minimum_gray)\n    beta = -minimum_gray * alpha\n\n    '''\n    # Calculate new histogram with desired range and show histogram \n    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n    plt.plot(hist)\n    plt.plot(new_hist)\n    plt.xlim([0,256])\n    plt.show()\n    '''\n\n    auto_result = convertScale(image, alpha=alpha, beta=beta)\n    return (auto_result, alpha, beta)\n\nimage = cv2.imread('1.jpg')\nauto_result, alpha, beta = automatic_brightness_and_contrast(image)\nprint('alpha', alpha)\nprint('beta', beta)\ncv2.imshow('auto_result', auto_result)\ncv2.imwrite('auto_result.png', auto_result)\ncv2.imshow('image', image)\ncv2.waitKey()\n"], ["gradients = tape.gradient(loss, model.trainable_variables)\n", "gradients = tape.gradient(loss, model.trainable_variables, \n                unconnected_gradients=tf.UnconnectedGradients.ZERO)\n"], [], ["def digitoflife(date):\n    if len(date) > 8:\n        print(\"date inputed is long\")\n    else:\n        num = num2 = 0\n        for digit in date:\n            num += int(digit)\n        for i in str(num):\n                num2 += int(i)\n        return num2\ndate = str(input('Enter the date: '))\nprint(\"The digit of life is: \",digitoflife(date))\n"], [], [], [], ["conda config --prepend channels conda-forge\nconda create -n ox --strict-channel-priority osmnx\n"], ["wget https://bootstrap.pypa.io/get-pip.py\npython3.8 get-pip.py\n"], [], [], [], [], [], [], ["function NstreamsOfNumberN (str) {\n    \n    for (let i = 0; i < str.length; i++) {\n        \n        let numBeingConsidered = Number(str[i]);\n                \n        let numOfComparisonsToBeDone = numBeingConsidered - 1;\n        \n        for (let j = i; j < numOfComparisonsToBeDone + i; j++) {\n                \n            if (str[j] != str[j+1]) {break}//compare neigbourin nums\n\n            else if ((j - i + 1) === numOfComparisonsToBeDone)  \n            { let theNwithNstreams = numBeingConsidered\n              return [str, (theNwithNstreams), true]} \n\n        //(j - i + 1) equals num of comparisons that has been done.\n        }\n    }\n    return false \n}\n\nNstreamsOfNumberN('334775555583444582')\n\n9 streams of the number 9 \n8 streams of the number 8 \n7 streams of the number 7 ...\n3 streams of the number 3\n2 streams of the number 2.\n"], [], [], ["third_parties = ('futures-', 'six-', 'geomet-')\n", "third_parties = ('futures-', 'geomet-')\n"], ["$ which python\n", "$ /usr/local/bin/python /usr/bin/cqlsh\n"], ["import numpy as np\n\ndef get_whole_ceil(n,near):\n    nn = np.divide(n,np.linspace(1,np.ceil(n/near),int(np.ceil(n/near))))\n    return(nn[nn%1==0][-1])\n\ndef get_whole_floor(n,near):\n    nn = np.divide(n,np.linspace(np.floor(n/near),n,int(n-np.floor(n/near)+1)))\n    return(nn[nn%1==0][0])\n\nget_whole_ceil(2040906,1440)\n\nOut[1]: 48593.0\n\nget_whole_floor(2040906,1440)\n\nOut[1]: 42.0\n"], ["third_parties = ('futures-', 'six-', 'geomet-')\n\nfor lib in third_parties:\n    lib_zip = find_zip(lib)\n    if lib_zip:\n        sys.path.insert(0, lib_zip)\n", "vim /usr/bin/cqlsh\n", "yum install six\n"], [], ["from PIL import Image\n\n\nimport os\n\npath_to_file ='tiff-files'\n\n\nimages = []\n\n\n\nfor i in os.listdir(path_to_file):\n    with Image.open(path_to_file+'/'+i) as im:\n        images.append(im.copy())\n\n    \nnew_image = Image.new(images[0].mode, (images[0].size[0]*3,images[0].size[1]*5))\n\n\n\nnew_image.paste(images[0])\nnew_image.paste(images[1],(images[0].size[0]*1,0))\nnew_image.paste(images[2],(images[0].size[0]*2,0))\nnew_image.paste(images[3],(0,images[0].size[1]*1))\nnew_image.paste(images[4],(images[0].size[0]*1,images[0].size[1]*1))\nnew_image.paste(images[5],(images[0].size[0]*2,images[0].size[1]*1))\nnew_image.paste(images[6],(0,images[0].size[1]*2))\nnew_image.paste(images[7],(images[0].size[0]*1,images[0].size[1]*2))\nnew_image.paste(images[8],(images[0].size[0]*2,images[0].size[1]*2))\nnew_image.paste(images[9],(0,images[0].size[1]*3))\nnew_image.paste(images[10],(images[0].size[0]*1,images[0].size[1]*3))\nnew_image.paste(images[11],(images[0].size[0]*2,images[0].size[1]*3))\nnew_image.paste(images[12],(0,images[0].size[1]*4))\nnew_image.paste(images[13],(images[0].size[0]*1,images[0].size[1]*4))\nnew_image.paste(images[14],(images[0].size[0]*2,images[0].size[1]*4))\n\nnew_image.show()\n", "from PIL import Image\nimport os\n\npath_to_file ='tiff-files'\n\n\n\ndef stich_tile(path_to_file, xx , yy):\n    images = []\n    for i in os.listdir(path_to_file):\n            images.append(i)\n\n    \n    if len(images) >= xx*yy:\n        pass\n    \n    else:\n        raise ValueError('not enough images in path_to_file !!!!!!!!!!!')\n        \n    \n    sq_x = xx\n    sq_y = yy\n    img_x = (Image.open(path_to_file+'/'+images[0]).size[0])\n    img_y = (Image.open(path_to_file+'/'+images[0]).size[1])\n    img_mode = (Image.open(path_to_file+'/'+images[0]).mode)\n    \n    new_image = Image.new(img_mode, (img_x*sq_x, img_y*sq_y))\n    \n    x = 0\n    y = 0\n    cnt = 0\n    for i in images:\n        with Image.open(path_to_file+'/'+i) as img:\n            new_image.paste(img, (x,y))\n            cnt += 1\n            x += img_x \n            if cnt == sq_x:\n                x = 0\n                y += img_y\n                cnt = 0\n            else:\n                pass\n                \n  \n    return new_image\n \n\nstich_tile(path_to_file, 3, 5).show()\n", "import numpy as np\nfrom PIL import Image\nimport os\n\n# path_to_file ='tiff-files'\n\npath_to_file ='tiff-files2'\n\n# path_to_file ='tiff-files3'\n\n\n\n    \n\nimage = []\nfor i in os.listdir(path_to_file):\n    with Image.open(path_to_file+'/'+i) as im:\n        image.append(im.copy()) \n        \n     \n\n\nw, h = image[0].size\n\n\n\nnew_image = np.zeros((4 * h, 3 * w)).astype('uint8')\n\n\ncol = 0\nrow = -1\nfor i, img in enumerate(image):\n    if not i % 3 :\n        row += 1\n        col = 0\n    img = np.array(img)\n    new_image[row * h: (row + 1) * h, col * w: (col + 1) * w] = img\n    col += 1\n\n\n\n\nimage_pillow = Image.fromarray(new_image, mode = 'L')\n\nimage_pillow.save('prova.tif', mode = 'L')\n\n\nimage_pillow.show()\n", "import numpy as np\nfrom PIL import Image\nimport os\n\npath_to_file ='tiff-files'\n\n# path_to_file ='tiff-files2'\n\n# path_to_file ='tiff-files3'\n\n# path_to_file ='tiff-files5'\n\n    \ndef stich_img(path_to_file, x , y):\n\n    image = []\n    for i in os.listdir(path_to_file):\n            image.append(path_to_file+'/'+i)\n    \n    print(image)\n         \n    if len(image) >= x*y:\n        pass\n    \n    else:\n        # raise ValueError('not enough images in path_to_file !!!!!!!!!!!')\n        raise ValueError('EXCEPTION not enough images in path_to_file !!!!!!!!!!!', x*y ,'images  needed : ', len(image),'images present !!!')\n    \n    \n    image = image[:x*y] #-----> riduce lista immagini al numero richiesto\n    \n    \n    with Image.open(image[0]) as img0:\n        w, h = img0.size\n   \n    \n    \n    \n    # new_image = np.zeros((4 * h, 3 * w)).astype('uint8')\n    new_image = np.zeros((y * h, x * w)).astype('uint8')\n    \n    \n     \n    col = 0\n    row = -1\n    for i, imgs in enumerate(image):\n        with Image.open(imgs) as img:\n            if not i % x :\n                row += 1\n                col = 0\n            img = np.array(img)\n            new_image[row * h: (row + 1) * h, col * w: (col + 1) * w] = img\n            col += 1\n            \n    \n\n    \n    image_pillow = Image.fromarray(new_image, mode = 'L')\n    \n    return image_pillow\n\nimg_stiched = stich_img(path_to_file, 3,5)   \n\n# img_stiched.save('prova.tif', mode = 'L')\n\n\nimg_stiched.show()\n"], ["    #!/usr/bin/env python3\nimport numpy as np\nfrom imageio import imread, imwrite\nfrom pathlib import Path\n\n\ndef tile_images(images, cols):\n    \"\"\"Tile images of same size to grid with given number of columns.\n    \n    Args:\n        images (collection of ndarrays)\n        cols (int): number of colums \n    \n    Returns:\n        ndarray: stitched image\n    \"\"\"\n    images = iter(images)\n    first = True\n    rows = []\n    i = 0\n    while True:\n        \n        try:\n            im = next(images)\n            print(f\"add image, shape: {im.shape}, type: {im.dtype}\")\n        except StopIteration:\n            if first:\n                break\n            else:\n                im = np.zeros_like(im)  # black background\n                \n        if first:\n            row = im  # start next row\n            first = False  \n        else:    \n            row = np.concatenate((row, im), axis=1)  # append to row\n            \n        i += 1\n        if not i % cols:\n            print(f\"row done, shape: {row.shape}\")\n            rows.append(row) # finished row\n            first = True\n            \n    tiled = np.concatenate(rows)   # stitch rows    \n    return tiled        \n\ndef main():\n    images = (imread(f) for f in Path().glob(\"*.*\") if f.suffix in (\".jpg\", \".png\") if f.name != \"new.png\") \n    new = tile_images(images, cols=3)\n    imwrite(\"new.png\", new)\n\n\ndef test():\n    im1 = np.arange(65536).reshape(256,256)\n    im2 = np.arange(65536/2).reshape(128,256)\n    \n    images = [im1,im1,im1,im2,im2,im2]\n    \n    # works\n    new = tile_images(images, 3)\n    imwrite(\"new.png\", new)\n    \n    # failes\n    new = tile_images(images, 2)\n    imwrite(\"new2.png\", new)\n    \n    \nif __name__ == \"__main__\":\n    main()\n    # test()\n"], ["from sklearn.neighbors import BallTree\nimport numpy as np\n\ndef get_nearest(src_points, candidates, k_neighbors=2):\n    \"\"\"\n    Find nearest neighbors for all source points from a set of candidate points\n    modified from: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html\n    \"\"\"\n    \n\n    # Create tree from the candidate points\n    tree = BallTree(candidates, leaf_size=15, metric='euclidean')\n\n    # Find closest points and distances\n    distances, indices = tree.query(src_points, k=k_neighbors)\n\n    # Transpose to get distances and indices into arrays\n    distances = distances.transpose()\n    indices = indices.transpose()\n\n    # Get closest indices and distances (i.e. array at index 0)\n    # note: for the second closest points, you would take index 1, etc.\n    closest = indices[0]\n    closest_dist = distances[0]\n    closest_second = indices[1] # *manually add per comment above*\n    closest_second_dist = distances[1] # *manually add per comment above*\n\n    # Return indices and distances\n    return (closest, closest_dist, closest_sec, closest_sec_dist)\n", "# easier to read\nin_pts = [(row.geometry.x, row.geometry.y) for idx, row in gdf1.iterrows()]\nqry_pts = [(row.geometry.x, row.geometry.y) for idx, row in gdf2.iterrows()]\n\n# faster (by about 7X)\nin_pts = [(x,y) for x,y in zip(gdf1.geometry.x , gdf1.geometry.y)]\nqry_pts =  [(x,y) for x,y in zip(gdf2.geometry.x , gdf2.geometry.y)]\n", "idx_nearest, _, idx_2ndnearest, _ = get_nearest(in_pts, qry_pts)\n"], ["magick montage -tile 3x -geometry +0+0 09*tif result.tif\n", "magick montage -background magenta -tile 5x -geometry +5+15 09*tif result.tif\n", "for x in {a..o} ; do magick xc: +noise random -scale 80x50\\! 09$x.tif ; done \n"], [], [], ["DELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM params WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs where experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n));\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n);\nDELETE FROM experiments where lifecycle_stage='deleted';\n"], ["import pymysql\n\ndef perm_delete_exp():\n    connection = pymysql.connect(\n        host='localhost',\n        user='user',\n        password='password',\n        db='mlflow',\n        cursorclass=pymysql.cursors.DictCursor)\n    with connection.cursor() as cursor:\n        queries = \"\"\"\n            USE mlflow;\n            DELETE FROM experiment_tags WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\");\n            DELETE FROM latest_metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"));\n            DELETE FROM metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"));\n            DELETE FROM tags WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"));\n            DELETE FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\");\n            DELETE FROM experiments where lifecycle_stage=\"deleted\";\n        \"\"\"\n        for query in queries.splitlines()[1:-1]:\n            cursor.execute(query.strip())\n    connection.commit()\n    connection.close()\n"], ["DELETE FROM experiment_tags WHERE experiment_id in (\n    SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    );\nDELETE FROM latest_metrics WHERE run_uuid in (\n    SELECT run_uuid FROM runs WHERE experiment_id in (\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM metrics WHERE run_uuid in (\n    SELECT run_uuid FROM runs WHERE experiment_id in (\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM tags WHERE run_uuid in (\n    SELECT run_uuid FROM runs WHERE experiment_id in (\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n    )\n);\nDELETE FROM params WHERE run_uuid in (\n    SELECT run_uuid FROM runs where experiment_id in (\n        SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n));\nDELETE FROM runs WHERE experiment_id in (\n    SELECT experiment_id FROM experiments where lifecycle_stage='deleted'\n);\nDELETE FROM experiments where lifecycle_stage='deleted';\n"], ["pip uninstall nltk==3.2.5\n", "pip install nltk==3.6.2\n", "import nltk\nprint('The nltk version is {}.'.format(nltk.__version__))\n"], ["pip install nltk==3.4\n"], [], [], ["py -m streamlit run hello.py\n"], ["pip install wget\n"], ["page.select_option('select#colors', label='Banana')\n", "await page.selectOption('select#colors', { label: 'Banana' });\n"], ["class Queue:\n\n    def __init__(self):\n     self.items = []\n\n \n    def push(self, e):\n      self.items.append(e)\n \n    def pop(self):\n      head = self.items[0]\n      self.items = self.item[1:]\n      return head\n\n    def print(self):\n      for e in self.items:\n          print(e)\nq = Queue()\nq.push(1)\nq.push(23)\nq.print()\n\n", "1\n23\n"], ["pip3 uninstall keras\npip3 uninstall tensorflow\npip3 install --upgrade pip3\npip3 install tensorflow\npip3 install keras\n"], [], [], ["from PyQt6.QtWidgets import QMessageBox\n\nreply = QMessageBox()\nreply.setText(\"Some random text.\")\nreply.setStandardButtons(QMessageBox.StandardButton.Yes | \n                     QMessageBox.StandardButton.No)\n\nx = reply.exec()\n\nif x == QMessageBox.StandardButton.Yes:\n    print(\"Hello!\")\n"], [], [], [], ["/usr/local/bin/brew tap-new ${USER}/homebrew-python2\n\n/usr/local/bin/brew extract python@2 ${USER}/homebrew-python2\n\n/usr/local/bin/brew install /usr/local/Homebrew/Library/Taps/${USER}/homebrew-python2/Formula/python@2.7.17.rb\n\n# https://github.com/Homebrew/brew/issues/5734#issuecomment-464705002\n/usr/local/bin/brew untap ${USER}/python2\n"], [], [], [], ["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\nNum GPUs Available:  1\n"], ["unescape = getattr(html, 'unescape', html_parser.HTMLParser().unescape)\nAttributeError: 'HTMLParser' object has no attribute 'unescape'\n", "pip3 install --upgrade setuptools\n", "pip3 install --upgrade pip\npip3 install --upgrade distlib\n", "sudo apt-get install python3-dev\n"], [], ["def to(self, **kwargs):\n    module = super(VariationalGenerator, self).to(**kwargs)\n    module._train_noise = self._train_noise.to(**kwargs)\n    module._eval_noise = self._eval_noise.to(**kwargs)\n\n    return module\n"], [], ["from django.urls import re_path\n\nfrom . import consumers\n\nwebsocket_urlpatterns = [\n    re_path(r'ws/chat/(?P<room_name>\\w+)/$', consumers.ChatConsumer.as_asgi()),\n]\n"], [], [], ["$ cat /proc/sys/vm/overcommit_memory\n0\n", ">>> 156816 * 36 * 53806 / 1024.0**3\n282.8939827680588\n", "$ echo 1 > /proc/sys/vm/overcommit_memory\n", ">>> import numpy as np\n>>> a = np.zeros((156816, 36, 53806), dtype='uint8')\n>>> a.nbytes\n303755101056\n"], [], ["cars = (['rav4'], ['td5'], ['yaris'], ['land rover tdi']) \n\nprint(\"I like the \"+cars[0][0]+\" ...\")\n"], [], [], [], ["t = \"5723399999999\"\ndef function(j):\n    k = len(j)\n    count = 1\n    a = []\n    for i in j:\n        i = int(i)\n        a.append(i)\n    for index,i in enumerate(a): \n        try:\n            if a[index]== a[index+1]:\n                count = count+1\n                if count == a[index+1]:\n                    count = 1\n                    return True\n            if a[index]!=a[index+1]:\n                count = 1\n        except IndexError:\n            pass\n    return False\n\n  function(t)\n"], ["pip install --upgrade tensorflow\npip install --upgrade keras\n"], ["TestArr = [None, None, None, None]\nArrTest = [x.testFunc() for x in TestArr if x != None]\nprint(ArrTest)\n"], ["emails = [get_user(uuid).email for uuid in user_uuids if get_user(uuid)]\n"], [], ["pip install --upgrade pip\npip install -U PyMuPDF\n"], ["html = driver.page_source\nfile = open(\"foo.html\",\"w\")\nfile.write(html)\nfile.close()\n", "from fake_useragent import UserAgent\nuser_agent = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.2 (KHTML, like Gecko) Chrome/22.0.1216.0 Safari/537.2'\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument(f'user-agent={user_agent}')\ndriver = webdriver.Chrome(executable_path = f\"your_path\",chrome_options=chrome_options)\n"], [], [], ["pip list --outdated | grep name_of_package\n"], ["$ tar -xzf ta-lib-0.4.0-src.tar.gz\n$ cd ta-lib/\n$ ./configure --prefix=/usr\n$ make\n$ sudo make install\n", "$ brew install ta-lib\n", "export TA_INCLUDE_PATH=\"$(brew --prefix ta-lib)/include\"\nexport TA_LIBRARY_PATH=\"$(brew --prefix ta-lib)/lib\"\n", "$ export TA_LIBRARY_PATH=$PREFIX/lib\n$ export TA_INCLUDE_PATH=$PREFIX/include\n$ python setup.py install # or pip install ta-lib\n"], [], ["sudo apt install python3.9\n"], [], ["1. Delete db.sqlite3\n2. python manage.py makemigrations\n3. python manage.py migrate\n4. python manage.py createsuperuser \n"], ["python3 -m pip install -U numpy --no-cache-dir --no-binary numpy\n", "sudo apt install gcc-8 g++-8\nsudo rm /usr/bin/gcc\nsudo ln -s /usr/bin/gcc-8 /usr/bin/gcc\n\nsudo rm /usr/bin/g++\nsudo ln -s /usr/bin/g++-8 /usr/bin/g++\n", "gcc --version\ng++ --version\n"], ["def operations(h,w):\n    \"\"\"\n    Takes two inputs, h and w, and makes two Numpy arrays A and B of size\n    h x w, and returns A, B, and s, the sum of A and B.\n    Arg:\n      h - an integer describing the height of A and B\n      w - an integer describing the width of A and B\n    Returns (in this order):\n      A - a randomly-generated h x w Numpy array.\n      B - a randomly-generated h x w Numpy array.\n      s - the sum of A and B.\n    \"\"\"\n    A = np.random.random([h,w])\n    B = np.random.random([h,w])\n    s = A + B\n    return A,B,s\nA,B,s = operations(3,4)\nassert(A.shape == B.shape == s.shape)*\n\n"], [], [], ["ax._facecolors2d = ax._facecolor\n"], ["pip install tensorflow-gpu==2.4.1\npip install Keras==2.4.3\n"], ["if QtWidgets.QMessageBox.critical(self,\"Foo\",\"PROTECTION NOT FOUND - Exit\",QtWidgets.QMessageBox.StandardButtons.Yes):\n                print(\"Exit\")\n"], [], ["if QtWidgets.QMessageBox.StandardButtons(x) == QtWidgets.QMessageBox.StandardButtons.Yes:\n            print(\"Hello!\")\n"], [], ["pip install 'h5py==2.10.0' --force-reinstall\n"], ["def split(word):\n    return [char for char in word]\n\ndate = input(\"Enter your date of birth in YYYYMMDD format: > \")\nsum_list = []\n\ndef digitOfLife(date):\n    sum = 0\n    if(len(date) > 8):\n        print(\"Input data too long\")\n        return\n    else:\n        date_list = []\n        for char in date:\n            date_list.append(int(char))\n            \n        for num in date_list:\n            sum+=num\n            if sum > 9:\n                sum = sum%10 +sum//10\n    return sum\n    \nprint(digitOfLife(date))\n", "Enter your date of birth in YYYYMMDD format: > 19991229\n6\n"], ["def digitOfLife(date):\n    sum_ = 0\n    if (len(date) > 8):\n        print(\"Input data too long\")\n        return\n    else:\n        date_list = []\n        for char in date:\n            date_list.append(int(char))\n\n        sum_ = sum(date_list) # calculate the sum of list of digits\n        while len(str(sum_)) > 1: # repeat while the sum has more than 1 digit\n            sum_ = sum(date_list)\n            date_list = [int(x) for x in str(sum_)]\n\n    return sum_\n"], ["def dol(N): return N if N < 10 else dol(N//10+N%10)\n\ndate = \"19991229\"\ndol(int(date)) # 6\n"], ["    while len(date_list) >= 1:\n        print(len(date_list))\n        # check if it's over 9\n        if sum > 9:\n            sum =0\n        num = date_list.pop()\n        sum += num\n", "L = [1, 2, 3]\nx = L.pop()\nprint(x) # this will return 3\n"], ["import subprocess\nimport sys\ndef check(name):\n    latest_version = str(subprocess.run([sys.executable, '-m', 'pip', 'install', '{}==random'.format(name)], capture_output=True, text=True))\n    latest_version = latest_version[latest_version.find('(from versions:')+15:]\n    latest_version = latest_version[:latest_version.find(')')]\n    latest_version = latest_version.replace(' ','').split(',')[-1]\n\n    current_version = str(subprocess.run([sys.executable, '-m', 'pip', 'show', '{}'.format(name)], capture_output=True, text=True))\n    current_version = current_version[current_version.find('Version:')+8:]\n    current_version = current_version[:current_version.find('\\\\n')].replace(' ','') \n\n    if latest_version == current_version:\n        return True\n    else:\n        return False\n"], [">>> import gekko\n>>> gekko.__version__\n'0.2.0'\n", ">>> import importlib.metadata\n>>> importlib.metadata.version(\"gekko\")\n'0.2.0'\n", ">>> import pkg_resources\n>>> pkg_resources.get_distribution(\"gekko\").version\n'0.2.0'\n", ">>> import luddite\n>>> luddite.get_version_pypi(\"gekko\")\n'0.2.3'\n"], [], ["ln -s /usr/include/locale.h /usr/include/xlocale.h\n"], [], ["with pdfplumber.open(file_name) as pdf:\n    page = pdf.pages[0]\n    text = page.extract_text()\n    print(text)\n"], [], ["df[df.eq(var1).any(1)]\n"], ["surf._facecolors2d = surf._facecolors3d\nsurf._edgecolors2d = surf._edgecolors3d\n", "surf._facecolors2d = surf._facecolor3d\nsurf._edgecolors2d = surf._edgecolor3d\n"], [], ["python -m pip --version\n", "sudo apt remove python-pip\n", "pip --version\n", "sudo apt install python3-pip\n", "pip3 --version\n", "python -m pip install pip\n", "nano ~/.bashrc\n", "# set PATH so it includes user's private bin if it exists\nif [ -d \"$HOME/.local/bin\" ] ; then\n    PATH=\"$HOME/.local/bin:$PATH\"\nfi\n", "source ~/.bashrc\n", "pip --version\n"], ["handle.selectOption([{'label': 'Banana'}])  # selects 'Banana'\nhandle.selectOption([{'index': 3}])         # selects 'Carrot'\nhandle.selectOption([{'value': ''}])        # selects the empty option (works even though it is disabled)\n"], ["plt.figure(figsize=(10,10))\nplt.scatter(true_value, predicted_value, c='crimson')\nplt.yscale('log')\nplt.xscale('log')\n\np1 = max(max(predicted_value), max(true_value))\np2 = min(min(predicted_value), min(true_value))\nplt.plot([p1, p2], [p1, p2], 'b-')\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.axis('equal')\nplt.show()\n"], ["CHANNEL_LAYERS = {\n    'default': {\n        'BACKEND': '',\n        'CONFIG': {\n           \"hosts\":[127.0.0.1],\n        },\n    },\n}\n"], ["model.save_weights(\"NMT_model_weight.tf\",save_format='tf')\n"], ["app.py\nroutes/\n  |__helloworld.py\n  |_*.py\n"], [], [], ["model_ = load_model('path to your model.h5',custom_objects={'Scale': Scale()}, compile=False)\nsgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\nmodel_.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n"], ["permission_classes = (\n    permissions.IsAuthenticated\n)\n", "permission_classes = (\n    permissions.IsAuthenticated,\n#                              ^\n#                         a comma here\n)\n"], ["conda deactivate\n", "conda update anaconda-navigator\n"], [], [], ["cd ~\nwget https://raw.githubusercontent.com/Homebrew/homebrew-core/86a44a0a552c673a05f11018459c9f5faae3becc/Formula/python@2.rb\nbrew install python@2.rb\nrm python@2.rb\n"], ["pip install -U Werkzeug==0.16.0\n"], [], [], ["new_dinner = ['ali','zeshan','raza']\nprint ('this is old friend', *new_dinner)\n"], ["python script.py\n", "ModuleNotFoundError: No module named 'fitz'\n"], [], [], ["def replace_bn(module, name):\n    '''\n    Recursively put desired batch norm in nn.module module.\n\n    set module = net to start code.\n    '''\n    # go through all attributes of module nn.module (e.g. network or layer) and put batch norms if present\n    for attr_str in dir(module):\n        target_attr = getattr(m, attr_str)\n        if type(target_attr) == torch.nn.BatchNorm2d:\n            print('replaced: ', name, attr_str)\n            new_bn = torch.nn.BatchNorm2d(target_attr.num_features, target_attr.eps, target_attr.momentum, target_attr.affine,\n                                          track_running_stats=False)\n            setattr(module, attr_str, new_bn)\n\n    # iterate through immediate child modules. Note, the recursion is done by our code no need to use named_modules()\n    for name, immediate_child_module in module.named_children():\n        replace_bn(immediate_child_module, name)\n\nreplace_bn(model, 'model')\n"], ["@app.get(\"/my-app/{rest_of_path:path}\")\nasync def serve_my_app(request: Request, rest_of_path: str):\n    print(\"rest_of_path: \"+rest_of_path)\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n", "@app.route(\"/{full_path:path}\")\nasync def catch_all(request: Request, full_path: str):\n    print(\"full_path: \"+full_path)\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n"], ["pip install Django --upgrade\n", "python manage.py migrate\n\npython manage.py makemigrations app\n\npython manage.py migrate\n"], [], [], ["conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1 -c pytorch\n"], [], ["import multiprocessing as mp\nfrom multiprocessing import queues\n\n\nclass IterQueue(queues.Queue):\n\n    def __init__(self, *args, **kwargs):\n        ctx = mp.get_context()\n        kwargs['ctx'] = ctx\n        super().__init__(*args, **kwargs)\n\n    # <----  Iter Protocol  ------>\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            if not self.empty():\n                return self.get()  # block=True | default\n            else:\n                raise StopIteration\n        except ValueError:  # the Queue is closed\n            raise StopIteration\n\n", "def sample_func(queue_ref):\n    for i in range(10):\n        queue_ref.put(i)\n\n\nIQ = IterQueue()\n\np = mp.Process(target=sample_func, args=(IQ,))\np.start()\np.join()\n\nprint(list(IQ))  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n"], ["C:\\ThanosDodd\\Python3.6\\pythonw.exe C:\\\\Python\\Scripts\\moveDLs.py\n", "import sched\nimport time\n\nevent_schedule = sched.scheduler(time.time, time.sleep)\n\ndef do_something():\n    print(\"Hello, World!\")\n    event_schedule.enter(30, 1, do_something, (sc,))\n\nevent_schedule.enter(30, 1, do_something, (s,))\nevent_schedule.run()\n", "taskkill /pid processId /f\n"], ["from nltk import bigrams\ndocToken= [['the', 'wildlings', 'are', 'dead'], [], ['do', 'the', 'dead', 'frighten', 'you', 'ser', 'waymar']]\nfor i in range(3):\n    print (i)\n    print (list(nltk.bigrams(docToken[i])))\n", "0\n[('the', 'wildlings'), ('wildlings', 'are'), ('are', 'dead')]\n1\n---------------------------------------------------------------------------\nStopIteration                             Traceback (most recent call last)\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\util.py in ngrams(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\n    467     while n > 1:\n--> 468         history.append(next(sequence))\n    469         n -= 1\n\nStopIteration: \n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-58-91f35cae32ed> in <module>\n      2 for i in range(3):\n      3     print (i)\n----> 4     list(nltk.bigrams(docToken[i]))\n\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\util.py in bigrams(sequence, **kwargs)\n    489     \"\"\"\n    490 \n--> 491     for item in ngrams(sequence, 2, **kwargs):\n    492         yield item\n    493 \n\nRuntimeError: generator raised StopIteration\n", "docToken= [['the', 'wildlings', 'are', 'dead'], [], ['do', 'the', 'dead', 'frighten', 'you', 'ser', 'waymar']]\ndocToken = [x for x in docToken if x]\nbigram = []\nfor i in range(len(docToken)):\n    bigram.append([\"_\".join(w) for w in  bigrams(docToken[i])])\nbigram\n", "[['the_wildlings', 'wildlings_are', 'are_dead'],\n ['do_the',\n  'the_dead',\n  'dead_frighten',\n  'frighten_you',\n  'you_ser',\n  'ser_waymar']]\n"], [], ["import sys\n!{sys.executable} -m pip install --user numpy\n"], ["pip install PyMuPDF==1.16.14\n"], ["from flask import Flask\nfrom flask_restful import Resource, Api\n\n\napp = Flask(__name__)\napi = Api(app)\n\n\nclass Root(Resource):\n    def get(self):\n        return {\"message\": \"hello\"}\n\n\napi.add_resource(Root, \"/\")\n", "from fastapi import FastAPI\n\n\napp = FastAPI(debug=False)\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"hello\"}\n", "Concurrency Level:      500\nTime taken for tests:   0.577 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      720000 bytes\nHTML transferred:       95000 bytes\nRequests per second:    8665.48 [#/sec] (mean)\nTime per request:       57.700 [ms] (mean)\nTime per request:       0.115 [ms] (mean, across all concurrent requests)\nTransfer rate:          1218.58 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    6   4.5      6      30\nProcessing:     6   49  21.7     45     126\nWaiting:        1   42  19.0     39     124\nTotal:         12   56  21.8     53     127\n\nPercentage of the requests served within a certain time (ms)\n  50%     53\n  66%     64\n  75%     69\n  80%     73\n  90%     81\n  95%     98\n  98%    112\n  99%    116\n 100%    127 (longest request)\n", "Concurrency Level:      500\nTime taken for tests:   1.562 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      720000 bytes\nHTML transferred:       95000 bytes\nRequests per second:    3200.62 [#/sec] (mean)\nTime per request:       156.220 [ms] (mean)\nTime per request:       0.312 [ms] (mean, across all concurrent requests)\nTransfer rate:          450.09 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    8   4.8      7      24\nProcessing:    26  144  13.1    143     195\nWaiting:        2  132  13.1    130     181\nTotal:         26  152  12.6    150     203\n\nPercentage of the requests served within a certain time (ms)\n  50%    150\n  66%    155\n  75%    158\n  80%    160\n  90%    166\n  95%    171\n  98%    195\n  99%    199\n 100%    203 (longest request)\n", "Concurrency Level:      500\nTime taken for tests:   27.827 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      830000 bytes\nHTML transferred:       105000 bytes\nRequests per second:    179.68 [#/sec] (mean)\nTime per request:       2782.653 [ms] (mean)\nTime per request:       5.565 [ms] (mean, across all concurrent requests)\nTransfer rate:          29.13 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   87 293.2      0    3047\nProcessing:    14 1140 4131.5    136   26794\nWaiting:        1 1140 4131.5    135   26794\nTotal:         14 1227 4359.9    136   27819\n\nPercentage of the requests served within a certain time (ms)\n  50%    136\n  66%    148\n  75%    179\n  80%    198\n  90%    295\n  95%   7839\n  98%  14518\n  99%  27765\n 100%  27819 (longest request)\n", "Server Software:        waitress\nServer Hostname:        127.0.0.1\nServer Port:            8000\n\nDocument Path:          /\nDocument Length:        21 bytes\n\nConcurrency Level:      1000\nTime taken for tests:   3.403 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      830000 bytes\nHTML transferred:       105000 bytes\nRequests per second:    1469.47 [#/sec] (mean)\nTime per request:       680.516 [ms] (mean)\nTime per request:       0.681 [ms] (mean, across all concurrent requests)\nTransfer rate:          238.22 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0    4   8.6      0      30\nProcessing:    31  607 156.3    659     754\nWaiting:        1  607 156.3    658     753\nTotal:         31  611 148.4    660     754\n\nPercentage of the requests served within a certain time (ms)\n  50%    660\n  66%    678\n  75%    685\n  80%    691\n  90%    702\n  95%    728\n  98%    743\n  99%    750\n 100%    754 (longest request)\n", "Server Software:        uvicorn\nServer Hostname:        127.0.0.1\nServer Port:            8000\n\nDocument Path:          /\nDocument Length:        19 bytes\n\nConcurrency Level:      1000\nTime taken for tests:   0.634 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      720000 bytes\nHTML transferred:       95000 bytes\nRequests per second:    7891.28 [#/sec] (mean)\nTime per request:       126.722 [ms] (mean)\nTime per request:       0.127 [ms] (mean, across all concurrent requests)\nTransfer rate:          1109.71 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   28  13.8     30      62\nProcessing:    18   89  35.6     86     203\nWaiting:        1   75  33.3     70     171\nTotal:         20  118  34.4    116     243\n\nPercentage of the requests served within a certain time (ms)\n  50%    116\n  66%    126\n  75%    133\n  80%    137\n  90%    161\n  95%    189\n  98%    217\n  99%    230\n 100%    243 (longest request)\n", "Server Software:        uvicorn\nServer Hostname:        127.0.0.1\nServer Port:            8000\n\nDocument Path:          /\nDocument Length:        19 bytes\n\nConcurrency Level:      1000\nTime taken for tests:   1.147 seconds\nComplete requests:      5000\nFailed requests:        0\nTotal transferred:      720000 bytes\nHTML transferred:       95000 bytes\nRequests per second:    4359.68 [#/sec] (mean)\nTime per request:       229.375 [ms] (mean)\nTime per request:       0.229 [ms] (mean, across all concurrent requests)\nTransfer rate:          613.08 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   20  16.3     17      70\nProcessing:    17  190  96.8    171     501\nWaiting:        3  173  93.0    151     448\nTotal:         51  210  96.4    184     533\n\nPercentage of the requests served within a certain time (ms)\n  50%    184\n  66%    209\n  75%    241\n  80%    260\n  90%    324\n  95%    476\n  98%    504\n  99%    514\n 100%    533 (longest request)\n"], ["$ brew tap-new <user>/homebrew-python2\n$ brew extract python@2 <user>/homebrew-python2\n$ brew install /usr/local/Homebrew/Library/Taps/<user>/homebrew-python2/Formula/python@2.7.17.rb\n", "$ brew install https://raw.githubusercontent.com/Homebrew/homebrew-core/86a44a0a552c673a05f11018459c9f5faae3becc/Formula/python@2.rb\nUpdating Homebrew...\n==> Auto-updated Homebrew!\nUpdated Homebrew from 88f17b8b6 to c9b8a3ef6.\n...\nError: Calling Installation of python@2 from a GitHub commit URL is disabled! Use 'brew extract python@2' to stable tap on GitHub instead.\n"], [], [], [], [], [], [], ["# Filepaths\nstops = gpd.read_file('data/pt_stops_helsinki.gpkg')\nbuildings = read_gdf_from_zip('data/building_points_helsinki.zip')\n", "from sklearn.neighbors import BallTree\nimport numpy as np\n\ndef get_nearest(src_points, candidates, k_neighbors=1):\n    \"\"\"Find nearest neighbors for all source points from a set of candidate points\"\"\"\n\n    # Create tree from the candidate points\n    tree = BallTree(candidates, leaf_size=15, metric='haversine')\n\n    # Find closest points and distances\n    distances, indices = tree.query(src_points, k=k_neighbors)\n\n    # Transpose to get distances and indices into arrays\n    distances = distances.transpose()\n    indices = indices.transpose()\n\n    # Get closest indices and distances (i.e. array at index 0)\n    # note: for the second closest points, you would take index 1, etc.\n    closest = indices[0]\n    closest_dist = distances[0]\n\n    # Return indices and distances\n    return (closest, closest_dist)\n\n\ndef nearest_neighbor(left_gdf, right_gdf, return_dist=False):\n    \"\"\"\n    For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n\n    NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n    \"\"\"\n\n    left_geom_col = left_gdf.geometry.name\n    right_geom_col = right_gdf.geometry.name\n\n    # Ensure that index in right gdf is formed of sequential numbers\n    right = right_gdf.copy().reset_index(drop=True)\n\n    # Parse coordinates from points and insert them into a numpy array as RADIANS\n    left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n    right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.x * np.pi / 180, geom.y * np.pi / 180)).to_list())\n\n    # Find the nearest points\n    # -----------------------\n    # closest ==> index in right_gdf that corresponds to the closest point\n    # dist ==> distance between the nearest neighbors (in meters)\n\n    closest, dist = get_nearest(src_points=left_radians, candidates=right_radians)\n\n    # Return points from right GeoDataFrame that are closest to points in left GeoDataFrame\n    closest_points = right.loc[closest]\n\n    # Ensure that the index corresponds the one in left_gdf\n    closest_points = closest_points.reset_index(drop=True)\n\n    # Add distance if requested\n    if return_dist:\n        # Convert to meters from radians\n        earth_radius = 6371000  # meters\n        closest_points['distance'] = dist * earth_radius\n\n    return closest_points\n", "# Find closest public transport stop for each building and get also the distance based on haversine distance\n# Note: haversine distance which is implemented here is a bit slower than using e.g. 'euclidean' metric\n# but useful as we get the distance between points in meters\nclosest_stops = nearest_neighbor(buildings, stops, return_dist=True)\n", "# Rename the geometry of closest stops gdf so that we can easily identify it\nclosest_stops = closest_stops.rename(columns={'geometry': 'closest_stop_geom'})\n\n# Merge the datasets by index (for this, it is good to use '.join()' -function)\nbuildings = buildings.join(closest_stops)\n"], [], [], [], ["data['label'] = data['label'].astype(np.uint8)\n"], ["from fastapi import APIRouter\n\nrouter = APIRouter()\n\n@router.get(\"/some\")\nasync def some_path():\n    pass\n\n@router.get(\"/path\")\nasync def some_other_path():\n    pass\n\n@router.post(\"/some_post_path\")\nasync def some_post_path():\n    pass\n", "from routers import my_router\n", "from fastapi import FastAPI\nfrom routers import my_router\n\napp = FastAPI()\n", "from fastapi import FastAPI\nfrom routers import my_router\n\napp = FastAPI()\n\napp.include_router(my_router.router)\n", "from fastapi import FastAPI\nfrom routers import my_router\n\napp = FastAPI()\n\n\napp.include_router(\n    my_router.router,\n    prefix=\"/custom_path\",\n    tags=[\"We are from router!\"],\n)\n"], [], ["<script defer src=\"flong.js\"></script>\n"], [], ["print(str(request.files))\n", "file = request.files['file']\n", "file = request.files['']\n"], [], ["optimizer.apply_gradients(\n    (grad, var) \n    for (grad, var) in zip(gradients, model.trainable_variables) \n    if grad is not None\n)\n"], ["@tf.function\ndef train_step(x_batch):\n    with tf.GradientTape() as tape:\n        loss = self.encoder_model.loss.compute_loss(x_batch)\n    gradients = tape.gradient(loss, self.encoder_model.get_trainable_variables())\n    self.optimizer.apply_gradients(zip(gradients, self.encoder_model.get_trainable_variables()))\n"], [], [], ["$ pip3 install pytest\nCollecting pytest\n  Cache entry deserialization failed, entry ignored\n  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b23344eb8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pytest/\n  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b23a7e748>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pytest/\n  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b23a7e940>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pytest/\n  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b23a7e390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pytest/\n  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f3b23a7eb00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pytest/\n  Could not find a version that satisfies the requirement pytest (from versions: )\nNo matching distribution found for pytest\n", "# This file was automatically generated by WSL. To stop automatic generation of this file, remove this line.\n8.8.8.8\n4.4.4.4\n"], ["def romanToInt(s):\n    mapping  = {\n        'I': 1,\n        'V': 5,\n        'X': 10,\n        'L': 50,\n        'C': 100,\n        'D': 500,\n        'M': 1000,\n        }\n        \n    min_ = None\n    total = 0\n    for c in s:\n        val = mapping[c]\n        print(val)\n        if min_ and val > min_:\n            total -= min_*2\n        else:\n            min_ = val\n        total += val\n                \n    return total\n"], ["brew install pr0d1r2/python2/python@2.7.17 --build-from-source\n"], ["[i for l in map(lambda x: (x, -x), range(6, 10)) for i in l]\n", "[6, -6, 7, -7, 8, -8, 9, -9]\n"], [], [], ["import pygame\n\n(width, height) = (400, 250)\nscreen = pygame.display.set_mode((width, height))\npygame.display.set_caption('Title')\nscreen.fill((255,255,255))\npygame.display.flip()    \n\nrunning = True\nwhile running:\n    for event in pygame.event.get():\n        if event.type == pygame.MOUSEBUTTONDOWN:\n            # your code here\n        if event.type == pygame.QUIT:\n            running = False\n", "import pygame\n\n(width, height) = (400, 250)\nscreen = pygame.display.set_mode((width, height))\npygame.display.set_caption('Title')\nscreen.fill((255,255,255))\npygame.display.flip()    \n\nrunning = True\nclock = pygame.time.Clock()\nwhile running:\n    for event in pygame.event.get():\n        if event.type == pygame.MOUSEBUTTONDOWN:\n            # your code here\n        if event.type == pygame.QUIT:\n            running = False\n    clock.tick(30) # capped at 30 fps\n"], ["from pynput.mouse import Listener\n\ndef on_move(x, y):\n    pass\n\ndef on_click(x, y, button, pressed):\n    if pressed:\n        # Your code here\n\ndef on_scroll(x, y, dx, dy):\n    pass\n\nwith Listener(on_move=on_move, on_click=on_click, on_scroll=on_scroll) as listener:\n    listener.join()\n"], [], ["def romanToInt(self, s: str) -> int:\n\n roman = {'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000 }    \n num = 0\n\n for i in range(len(s)):\n\n    if i!= len(s)-1 and roman[s[i]] < roman[s[i+1]]:\n         num += roman[s[i]]*-1\n    else:\n         num += roman[s[i]]\n\n  return num\n", "def romanToInt(self, s: str) -> int:\n\n roman = {'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000 }    \n num = 0\n\n s = s.replace(\"IV\", \"IIII\").replace(\"IX\", \"VIIII\")\n s = s.replace(\"XL\", \"XXXX\").replace(\"XC\", \"LXXXX\")\n s = s.replace(\"CD\", \"CCCC\").replace(\"CM\", \"DCCCC\")\n\n for x in s:\n    num += roman[x]\n\n return num\n", "def romanToInt(self, s: str) -> int:\n\n roman = {'I':1, 'V':5, 'X':10, 'L':50, 'C':100, 'D':500, 'M':1000 }    \n num = 0\n\n for i in range(len(s)-1):\n    if roman[s[i]] < roman[s[i+1]]:\n        num += roman[s[i]]*-1\n        continue\n\n     num += roman[s[i]]\n\n  num +=roman[s[-1]]\n\n  return num\n"], ["dlib==19.19.0\nimutils==0.5.3\nnumpy==1.18.4\nopencv-contrib-python==4.2.0.34\nscipy==1.4.1\n"], ["from queue import Queue\nq = Queue.Queue()\nq.put(1)\nq.put(2)\nq.put(3)\nprint q.queue\n", "for q_item in q.queue:\n    print q_item\n"], [], [], ["x = 5       # get the cached `5` object and bind it to x\ny = x       # take whatever object is bound to x and also bind it to y\nx += 1      # take whatever object is bound to x, call its `__iadd__` \n            # method and bind the return value to x. `int.__iadd__`\n            # adds the integers and since it is small, returns the\n            # cached `6` object.\n"], [], [], [], [], ["from werkzeug import secure_filename,FileStorage\n", "from werkzeug.utils import secure_filename\nfrom werkzeug.datastructures import  FileStorage\n"], ["d = {'M':1000, 'D':500, 'C':100, 'L':50, 'X':10, 'V':5, 'I':1}\n\ndef romanToInt(self, s):\n    res, p = 0, 'I'\n    for c in s[::-1]:\n        res, p = res - d[c] if d[c] < d[p] else res + d[c], c\n    return res\n", "def romanToInt(s: str) -> int:\n        # Define integer value to each roman \n        rom_val = {'I': 1, 'V': 5, 'X': 10, 'L': 50,\n                  'C': 100, 'D': 500, 'M': 1000}\n        # A list of integer values\n        value = list(map(rom_val.get, s))\n\n        # List to keep the checked roman\n        checked = []\n        for i, j in enumerate(value):\n            if i == len(value) - 1:\n                checked.append(j)\n            elif j >= value[i+1]:\n                checked.append(j)\n            elif j < value[i+1]:\n                checked.append(value[i+1] - j)\n\n        print(checked)\n        return sum(checked)\n\nprint(romanToInt(\"LVIII\"))\n"], [], [], [">>> list(itertools.chain(range(6, 10), range(-9, -5)))\n[6, 7, 8, 9, -9, -8, -7, -6]\n"], [], ["new_dinner = ['ali','zeshan','raza']\nprint ('this is old friend', new_dinner)\n"], [], ["def mirror_numbers(start,stop):\n  if start<stop:\n    val=range(start,stop)\n    return [j if i < len(val) else -j for i,j in enumerate([x for x in val]*2) ]\n\nmirror_numbers(6,10)\n"], ["pip install django==2.2\n", "python manage.py makemigrations,\npython manage.py migrate \npython manage.py creatingsuperuser. \n"], ["if int(minor_ver) < 3:\n        tracker = cv2.cv2.Tracker_create(tracker_type)\n", "        if k == 27:\n            cv2.destroyAllWindows()\n            break\n"], [], ["def interleaved_negatives(it):\n    for i in it:\n        yield i\n        yield -i\n", "list(interleaved_negatives(range(6, 10)))\n"], [">>> import itertools\n>>> list(itertools.chain.from_iterable((x, -x) for x in range(6, 10)))\n[6, -6, 7, -7, 8, -8, 9, -9]\n", ">>> list(itertools.chain(*((x, -x) for x in range(6, 10))))\n[6, -6, 7, -7, 8, -8, 9, -9]\n"], ["[x for x in range(6, 10)] + [-x for x in range(6, 10)]\n"], ["nums = [y for x in range(6,10) for y in (x,-x)]\nprint(nums)\n[6, -6, 7, -7, 8, -8, 9, -9]\n"], ["def range_with_negs(start, stop):\n    for value in range(-(stop-1), stop):      \n        if (value <= -start) or (value >= start):\n            yield value\n", "In [1]: list(range_with_negs(6, 10))\nOut[1]: [-9, -8, -7, -6, 6, 7, 8, 9]\n"], [">>> list(range(6, 10)) + list(range(-9, -5))\n[6, 7, 8, 9, -9, -8, -7, -6]\n", ">>> list(itertools.chain(range(6, 10), range(-9, -5)))\n[6, 7, 8, 9, -9, -8, -7, -6]\n"], ["def range_with_negatives(start, end):\n    for x in range(start, end):\n        yield x\n        yield -x\n", "list(range_with_negatives(6, 10))\n"], [], ["import itertools\nlist(itertools.chain(range(6, 10), range(-9, -5)))\n"], [], ["pip install django==2.1.7\n", "python3 manage.py makemigrations\n\npython3 manage.py migrate\n"], [], ["import talib   \nclose = df['close']\nrsi = talib.RSI(close, timeperiod=14)\n", "upperBB, middleBB, lowerBB = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n", "upperBBrsi, MiddleBBrsi, lowerBBrsi = talib.BBANDS(rsi, timeperiod=50, nbdevup=2, nbdevdn=2, matype=0)\n", "normrsi = (rsi - lowerBBrsi) / (upperBBrsi - lowerBBrsi)\n"], [], ["{\n  \"python.linting.enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"python.pythonPath\": \"C:\\\\Users\\\\ben\\\\Anaconda3\\\\python.exe\"\n}\n"], [], ["conda create -n my_env geopandas ipykernel\n", "conda install -n base nb_conda_kernels\n"], [], [], ["    def __enter__(self):\n    # Some SQLite schema alterations need foreign key constraints to be\n     # disabled. Enforce it here for the duration of the schema edition.\n     if not self.connection.disable_constraint_checking():\n         raise NotSupportedError(\n             'SQLite schema editor cannot be used while foreign key '\n             'constraint checks are enabled. Make sure to disable them '\n             'before entering a transaction.atomic() context because '\n             'SQLite3 does not support disabling them in the middle of '\n             'a multi-statement transaction.'\n         )\n     self.connection.cursor().execute('PRAGMA legacy_alter_table = ON')\n     return super().__enter__()\n", "     def __enter__(self):\n    # Some SQLite schema alterations need foreign key constraints to be\n    # disabled. Enforce it here for the duration of the transaction.\n    self.connection.disable_constraint_checking()\n    self.connection.cursor().execute('PRAGMA legacy_alter_table = ON')\n    return super().__enter__()\n"], ["conda config --show\n", "conda config --remove-key proxy_servers. \n", "conda clean --source-cache\n", "set http_proxy=http://username: password@domain.com:{port}\nset https_proxy=https://username: password@domain.com:{port}\n"], ["conda install -c menpo wget\n"], [], [], [], [], ["df['age'] = df.apply(lambda e: (e['date_of_admission'] - e['DOB']).days/365, axis=1)\n"], [], [], [], [], [], ["def repfree(str):\nl=list(str)\nfor i in range(len(l)):\n    check=str.count(l[i])\n    if check>1:\n        flag=1\n    else:\n        flag=0\nif(flag==1):\n    return False\nelse:\n    return True\n"], [], ["chars = 'abcdefghijklmnopqrstuvwxyz'\ndef repfree(s):\n    for char in chars:\n        count = s.count(char)\n        if count > 1:\n            return False\n    return True\n", "import collections\ndef repfree(s):\n    results = collections.Counter(s)\n    for i in results:\n        if results[i] > 1:\n            return False\n    return True\n"], ["def repfree(s):\n  if len(set(s)) == len(s):\n    return True\n  return False\n\n", "def repfree(s):\n\n  return len(set(s)) == len(s)\n\n", "set('shubham')\n", "{'a', 'b', 'h', 'm', 's', 'u'}\n"], ["len(s) == len(set(s))\n", "any(s.count(c) > 1 for c in s)\n"], ["def repfree(s):\n    se = set(string)\n    print(len(se) == len(s))\n"], ["def repfree(s):\n    if re.search(r'^.*(.).*\\1.*$', s):\n        print(\"True\")\n    else:\n        print(\"False\")\n"], ["import re\ndef to_snake(s):\n  return re.sub('([A-Z]\\w+$)', '_\\\\1', s).lower()\n\ndef t_dict(d):\n   if isinstance(d, list):\n      return [t_dict(i) if isinstance(i, (dict, list)) else i for i in d]\n   return {to_snake(a):t_dict(b) if isinstance(b, (dict, list)) else b for a, b in d.items()}\n", "data = {'firstName': 'abc', 'lastName': 'xyz', 'favoriteMovies': ['Star Wars', 'The lone ranger'], 'favoriteCountries': [{'country': 'China', 'capitalCity': 'Beiging'}, {'country': 'India', 'capitalCity': 'New Delhi'}]} \nprint(t_dict(data))\n", "{'first_name': 'abc', 'last_name': 'xyz', \n 'favorite_movies': ['Star Wars', 'The lone ranger'], \n 'favorite_countries': [\n    {'country': 'China', 'capital_city': 'Beiging'}, \n    {'country': 'India', 'capital_city': 'New Delhi'}\n  ]\n}\n"], ["data = {\n     'firstName': 'abc',\n     'lastName': 'xyz',\n     'favoriteMovies': ['Star Wars', 'The lone ranger'],\n     'favoriteCountries': [\n          {'country': 'China', 'capitalCity': 'Beiging'},\n          {'country': 'India', 'capitalCity': 'New Delhi'}\n     ]\n}\n\nnew_data = {}\nfor key, value in data.items():\n    new_key_list = ['_' + x.lower() if x.isupper() else x for x in key]\n    new_key = ''.join(new_key_list)\n    if isinstance(value[0],dict):\n        new_value = []\n        for item in value:\n            temp_dict = {}\n            for key2, value2 in item.items():\n                new_key_list = ['_' + x.lower() if x.isupper() else x for x in key2]\n                new_key = ''.join(new_key_list)\n                temp_dict[new_key] = value2\n            new_value.append(temp_dict)\n        new_data[new_key] = new_value\n    else:\n        new_data[new_key] = value\n", "{'first_name': 'abc', 'last_name': 'xyz', 'favorite_movies': ['Star Wars', 'The lone ranger'], 'capital_city': [{'country': 'China', 'capital_city': 'Beiging'}, {'country': 'India', 'capital_city': 'New Delhi'}]}\n"], ["def camel_to_snake(str):\n   return re.sub(r'(?<!^)(?=[A-Z])', '_', str).lower()\n"], [], [], [" __init__.py \n"], ["  def _compute_gradients(tensor, var_list):\n      grads = tf.gradients(tensor, var_list)\n  return [grad if grad is not None else tf.zeros_like(var)\n      for var, grad in zip(var_list, grads)]\n"], [], ["mask = np.zeros(edges.shape)\n", "mask = np.zeros(edges.shape,dtype='uint8')\n"], [], [], [], ["import shutil, os, time\n\nwhile True:\n    for filename in os.listdir('folderToMoveFrom'):\n        if filename.endswith((desired file extensions)):\n            shutil.move( (folderToMoveFrom + filename), folderToMoveTo)\n    time.sleep(6)\n"], ["import time\n\nwhile True:\n   your_script_here\n   time.sleep(300)\n"], [], ["def NumberStream(string):\n    global j\n    l = len(string)\n    count = 0\n    res = string[0]\n    print(\"Length of the string is \", l)\n    for i in range(l):\n        count += 1\n        cur_count = 1\n        print(\"Looping\")\n        for j in range(i+1, l):\n            if string[i] != string[j]:\n                break\n            cur_count += 1\n            print(string[i], \" and \", string[j], \" with cur_count = \", cur_count, \" and count = \", count)\n        if string[i] == string[j-1] and string[j-1] == str(cur_count):\n            return \"True\"\n    return \"False\"\n\n\nprint(NumberStream(\"6539923335\"))\n", "test_str = \"6539923335\"\n\ndef NumberStream(input_str):\n    for i in range(10):\n        consec_num = str(i)*i\n        if consec_num in input_str:\n            print(\"Found\", consec_num)\n            return \"True\"\n    return \"False\"\n\nprint(NumberStream(test_str))\n", "test_str = \"6539923335\"\n\ndef NumberStream(input_str):\n    consec_count = 1\n    for j, number in enumerate(input_str):\n        if j == 0:\n            continue\n        if number == input_str[j-1]:\n            consec_count += 1\n        else:\n            consec_count = 1\n        if str(consec_count) == number:\n            print (\"Found consecutive numbers:\", number)\n            return \"True\"\n    return\n\nNumberStream(test_str)\n"], [], [], ["# check_version.py\n\nimport json\nimport urllib.request\nimport sys\n\ntry:\n    from importlib.metadata import version\nexcept ImportError:\n    from importlib_metadata import version\n\nfrom distutils.version import LooseVersion\n\n\nif __name__ == '__main__':\n    name = sys.argv[1]\n    installed_version = LooseVersion(version(name))\n\n    # fetch package metadata from PyPI\n    pypi_url = f'https://pypi.org/pypi/{name}/json'\n    response = urllib.request.urlopen(pypi_url).read().decode()\n    latest_version = max(LooseVersion(s) for s in json.loads(response)['releases'].keys())\n\n    print('package:', name, 'installed:', installed_version, 'latest:', latest_version)\n", "$ python check_version.py setuptools\npackage: setuptools installed: 41.2.0 latest: 41.6.0\n", "from distutils.version import LooseVersion\n\n...\n\nLooseVersion(s)\n", "from packaging.version import parse\n\n...\n\nparse(s)\n"], [], ["pip install --upgrade pip johnnydep\npip install gekko==0.2.0\n", ">>> from johnnydep.lib import JohnnyDist\n>>> dist = JohnnyDist(\"gekko\")\n>>> dist.version_installed  \n'0.2.0'\n>>> dist.version_latest \n'0.2.3'\n"], [], ["from django.contrib.auth.models import User\nfrom rest_framework.generics import UpdateAPIView\nfrom .serializers import UserSerializer\n\nclass UpdateView(UpdateAPIView):\n    queryset = User.objects.all()\n    serializer_class = UserSerializer\n    permission_classes = IsAuthenticated\n", "    from django.contrib.auth.models import User\n    from rest_framework.generics import UpdateAPIView\n    from .serializers import UserSerializer\n\n    class UpdateView(UpdateAPIView):\n        queryset = User.objects.all()\n        serializer_class = UserSerializer\n        permission_classes = [IsAuthenticated]\n"], [], ["import pandas as pd\nx = pd.read_csv(r\"filePath\")\nx.columns = x.columns.str.lower().str.replace(' ', '_')\ny = x.columns.values\nz = y.tolist()\nprint(\"Note: It take Case Sensitive Values.\")\nkeyWord = input(\"Type a Keyword to Search: \")\ntry:\n    for k in range(len(z)-1):\n        l = x[x[z[k]].str.match(keyWord)]\n        print(l.head(10))\n        k = k+1\nexcept:\n    print(\"\")\n"], [], ["g=plt.scatter(y_test1, y_pred_test_Forestreg)\ng.axes.set_yscale('log')\ng.axes.set_xscale('log')\ng.axes.set_xlabel('True Values ')\ng.axes.set_ylabel('Predictions ')\ng.axes.axis('equal')\ng.axes.axis('square')\n\n", "g=plt.plot(y_test1 - y_pred_test_Forestreg,marker='o',linestyle='')\n"], [], ["file = request.files['file']\n"], [], ["for name,child in net.named_children():\n    if isinstance(child,nn.ReLU) or isinstance(child,nn.SELU):\n        net._modules['relu'] = nn.SELU()\n"], [], ["some_list = list(range(x, y))\n", "from itertools import islice\nvalues = islice(df.iloc, 1, 5)  # returns an iterator\nvalues = list(values)  # extracts the values from the iterator into a list\n"], ["df.iloc[1:4]\n", "def.iloc[slice(1,4)]\n", "df.iloc.__getitem__(slice(1,4))\n"], ["mylist=list(range(1,5))\nprint(mylist)\n"], ["range(1, 5)\n", ">>> list(range(1, 5)\n[1, 2, 3, 4]\n"], [], [], [], [], [], [], [], ["~/myproject$ python -c 'from datalake import utils'\n"], ["import sys\nsys.path.append(\"your/own/modules/folder\")  # like sys.path.append(\"../tests\")\n"], [], ["def _apply(self, fn):\n    super(VariationalGenerator, self)._apply(fn)\n    self._train_noise = fn(self._train_noise)\n    self._eval_noise = fn(self._eval_noise)\n    return self\n"], [], [], [], [], ["image = cv2.imread('mY7ep.jpg')\n\n# make mask and inverted mask for colored areas\nb,g,r = cv2.split(cv2.blur(image,(5,5)))\nnp.seterr(divide='ignore', invalid='ignore') # 0/0 --> 0\nm = (np.fmin(np.fmin(b, g), r) / np.fmax(np.fmax(b, g), r)) * 255\n_,mask_inv = cv2.threshold(np.uint8(m), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nmask = cv2.bitwise_not(mask_inv)\n\n# local thresholding of grayscale image\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ntext = cv2.ximgproc.niBlackThreshold(gray, 255, cv2.THRESH_BINARY, 41, -0.1, binarizationMethod=cv2.ximgproc.BINARIZATION_NICK)\n\n# create background (text) and foreground (color markings)\nbg = cv2.bitwise_and(text, text, mask = mask_inv)\nfg = cv2.bitwise_and(image, image, mask = mask)\n\nout = cv2.add(cv2.cvtColor(bg, cv2.COLOR_GRAY2BGR), fg) \n", "image = cv2.imread('mY7ep.jpg')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ntext = cv2.ximgproc.niBlackThreshold(gray, 255, cv2.THRESH_BINARY, at_bs, -0.3, binarizationMethod=cv2.ximgproc.BINARIZATION_NICK)\n"], ["magick image.jpg -colorspace HCL -channel 1 -separate +channel tmp1.png\n", "magick tmp1.png -auto-threshold otsu tmp2.png\n", "magick image.jpg -colorspace gray -negate -lat 20x20+10% -negate tmp3.png\n", "magick tmp3.png \\( image.jpg tmp2.png -alpha off -compose copy_opacity -composite \\) -compose over -composite result.png\n", "#!/bin/python3.7\n\nfrom wand.image import Image\nfrom wand.display import display\nfrom wand.version import QUANTUM_RANGE\n\nwith Image(filename='text.jpg') as img:\n    with img.clone() as copied:\n        with img.clone() as hcl:\n            hcl.transform_colorspace('hcl')\n            with hcl.channel_images['green'] as mask:\n                mask.auto_threshold(method='otsu')\n                copied.composite(mask, left=0, top=0, operator='copy_alpha')\n                img.transform_colorspace('gray')\n                img.negate()\n                img.adaptive_threshold(width=20, height=20, offset=0.1*QUANTUM_RANGE)\n                img.negate()\n                img.composite(copied, left=0, top=0, operator='over')\n                img.save(filename='text_process.jpg')\n"], ["conda update conda\n", "conda search \"^python$\"\n", "python                     3.4.0                         0  defaults\npython                     3.4.1                         0  defaults\npython                     3.4.1                         1  defaults\npython                     3.4.1                         2  defaults\npython                     3.4.1                         3  defaults\npython                     3.4.1                         4  defaults\npython                     3.4.2                         0  defaults\npython                     3.4.3                         0  defaults\npython                     3.4.3                         2  defaults\npython                     3.4.4                         0  defaults\npython                     3.4.4                         5  defaults\npython                     3.4.5                         0  defaults\n"], ["from skimage.filters import threshold_yen\nfrom skimage.exposure import rescale_intensity\nfrom skimage.io import imread, imsave\n\nimg = imread('mY7ep.jpg')\n\nyen_threshold = threshold_yen(img)\nbright = rescale_intensity(img, (0, yen_threshold), (0, 255))\n\nimsave('out.jpg', bright)\n"], ["# libraries required\nimport pandas as pd\nimport numpy as np\n\n# create dataframe\ndf = pd.DataFrame({'close':[4724.89, 4378.51,6463.00,9838.96,13716.36,10285.10,\n                          10326.76,6923.91,9246.01,7485.01,6390.07,7730.93,\n                          7011.21,6626.57,6371.93,4041.32,3702.90,3434.10,\n                          3813.69,4103.95,5320.81,8555.00,10854.10]})\n\ndf['change'] = df['close'].diff(1) # Calculate change\n\n# calculate gain / loss from every change\ndf['gain'] = np.select([df['change']>0, df['change'].isna()], \n                       [df['change'], np.nan], \n                       default=0) \ndf['loss'] = np.select([df['change']<0, df['change'].isna()], \n                       [-df['change'], np.nan], \n                       default=0)\n\n# create avg_gain /  avg_loss columns with all nan\ndf['avg_gain'] = np.nan \ndf['avg_loss'] = np.nan\n\nn = 14 # what is the window\n\n# keep first occurrence of rolling mean\ndf['avg_gain'][n] = df['gain'].rolling(window=n).mean().dropna().iloc[0] \ndf['avg_loss'][n] = df['loss'].rolling(window=n).mean().dropna().iloc[0]\n# Alternatively\ndf['avg_gain'][n] = df.loc[:n, 'gain'].mean()\ndf['avg_loss'][n] = df.loc[:n, 'loss'].mean()\n\n# This is not a pandas way, looping through the pandas series, but it does what you need\nfor i in range(n+1, df.shape[0]):\n    df['avg_gain'].iloc[i] = (df['avg_gain'].iloc[i-1] * (n - 1) + df['gain'].iloc[i]) / n\n    df['avg_loss'].iloc[i] = (df['avg_loss'].iloc[i-1] * (n - 1) + df['loss'].iloc[i]) / n\n\n# calculate rs and rsi\ndf['rs'] = df['avg_gain'] / df['avg_loss']\ndf['rsi'] = 100 - (100 / (1 + df['rs'] ))\n"], [], [], ["import numpy as np\nimport pandas as pd\nfrom datetime import dt\n\ndef change(x):\n    return x.date()\n\ndf['date_of_admission'] = df['date_of_admission'].apply(change)\n\ndf['age'] = df['date_of_admission'].subtract(df['DOB']).dt.days // 365\n"], ["import pandas as pd\n\n\ndf['date_of_admission'] = pd.to_datetime(df['date_of_admission']).dt.date\n\ndf['DOB'] = pd.to_datetime(df['DOB']).dt.date\n\ndf['age'] = ((df['date_of_admission']-df['DOB']).dt.days) //365\n\n", "#Now I have use DOB AND date_of_admission data from the question and it is working fine\n\ndf = pd.DataFrame(data={\"DOB\":['2000-05-07','1965-01-30','NaT'],\n                   \"date_of_admission\":[\"2019-01-19 12:26:00\",\"2019-03-21 02:23:12\", \"2018-11-02 18:30:10\"]})\n\ndf['DOB'] = pd.to_datetime(df['DOB']).dt.date\ndf['date_of_admission'] = pd.to_datetime(df['date_of_admission']).dt.date\ndf['age'] = ((df['date_of_admission']-df['DOB']).dt.days) //365\n", "DOB       date_of_admission   age\n2000-05-07  2019-01-19       18.0\n1965-01-30  2019-03-21       54.0\nNaT         2018-11-02       NaN\n"], [], [], [], ["from selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as ec\nfrom selenium.webdriver.common.by import By\n\nwait = WebDriverWait(self.driver, timeout)\ndata = wait.until(ec.visibility_of_element_located((By.CSS_SELECTOR, '[id*=\"t_trigger_TSLA\"]')))\n"], [], [], [], [], [], ["def to_shape(a, shape):\n    y_, x_ = shape\n    y, x = a.shape\n    y_pad = (y_-y)\n    x_pad = (x_-x)\n    return np.pad(a,((y_pad//2, y_pad//2 + y_pad%2), \n                     (x_pad//2, x_pad//2 + x_pad%2)),\n                  mode = 'constant')\n", "a = np.ones((41,13))\nshape = [93, 13]\nto_shape(a, shape).shape\n# (93, 13)\n", "shape = [100, 121]\nto_shape(a, shape).shape\n# (100, 121)\n"], ["import numpy as np\narray  = [[None] * 10]*10\n#print(array)\ntestarray = np.zeros((93,13))\nfor index,row in enumerate(array):\n    for i in range(0,len(row)-1):\n        testarray[index][i]= row[i]\n"], ["import numpy as np\na = np.ones((41,11))\n\ndesired_rows = 91\ndesired_cols = 13\nb = np.pad(a, ((0, desired_rows-a.shape[0]), (0, desired_cols-a.shape[1])), 'constant', constant_values=0)\nprint(b)\n\"\"\"\nprints\n[[1. 1. 1. ... 1. 0. 0.]\n [1. 1. 1. ... 1. 0. 0.]\n [1. 1. 1. ... 1. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\"\"\"\n"], ["bigram={}\nfor i in range(size):\n    ls=[]\n    for j in range(len(docToken[i])-1):\n        for k in range(j,len(docToken[i])-1):\n            ls.append([docToken[i][j],docToken[i][k+1]])\n\n    bigram[i]=ls\n"], ["from pprint import pprint\ncars = ['rav4'], ['td5'], ['yaris'], ['land rover tdi']\nWord = str(cars[0])\npprint(\"I like the {0} ...\".format(Word))\n"], ["cars = ['rav4', 'td5', 'yaris', 'land rover tdi']\nprint(\"I like the \"+cars[0]+\" ...\")\n"], ["(['rav4'], ['td5'], ['yaris'], ['land rover tdi'])\n"], [], [], [], ["d = {'id': '10', 'datastreams': [{'current_value': '5'}, {'current_value': '4'}]}\n\nfor elem in d['datastreams']:      # for each elem in the list datastreams\n    for k,v in elem.items():       # for key,val in the elem of the list \n        if 'current_value' in k:   # if current_value is in the key\n            elem[k] = int(v)       # Cast it to int\nprint(d)\n", "{'id': '10', 'datastreams': [{'current_value': 5}, {'current_value': 4}]}\n"], ["def f(d):\n    for k,v in d.items():\n        if k == 'current_value':\n            d[k] = int(v)\n        elif type(v) is list:\n            for item in v:\n                if type(item) is dict:\n                    f(item)\n\n>>> d = {'id': '10', 'datastreams': [{'current_value': '5'}, {'current_value': '4'}]}\n>>> f(d)\n>>> d\n{'id': '10', 'datastreams': [{'current_value': 5}, {'current_value': 4}]}  \n"], ["for value in d.values():\n    for element in value:\n        if 'current_value' in element:\n            element['current_value'] = int(element['current_value'])\n"], ["import ast\nd = {'id': '10', 'datastreams': [{'current_value': '5'}, {'current_value': '4'}]}\nfor i in d['datastreams']:\n    for k,v in i.items():\n        if 'current_value' in k and isinstance(ast.literal_eval(v),int):\n            i[k] = int(v)\n#Output:\nprint(d)\n{'id': '10', 'datastreams': [{'current_value': 5}, {'current_value': 4}]}\n"], ["d['datastreams'] = [{'current_value': int(ds['current_value'])} if ('current_value' in ds) else ds for ds in d['datastreams']]\n"], [], [], [], ["jupyter notebook list\n"], ["def to(device):\n    new_self = super(VariationalGenerator, self).to(device)\n    new_self._train_noise = new_self._train_noise.to(device)\n    new_self._eval_noise = new_self._eval_noise.to(device)\n\n    return new_self\n"], [], ["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "net.to(device)\ninput = input.to(device)\n"], [">>> print(list(q.queue))\n"], [], ["d1 = {'a': [2, 4, 5, 6, 8, 10], 'b': [1, 2, 5, 6, 9, 12], 'c': [0, 4, 5, 8, 10, 21], 'e':[0,0,0]}\nd2 = {'a': [12, 15], 'b': [14, 16], 'c': [23, 35], 'd': [13, 3]}\n\nd2_keys_not_in_d1 = d2.keys() - d1.keys()\nd1_keys_not_in_d2 = d1.keys() - d2.keys()\ncommon_keys = d2.keys() & d1.keys()\n\nfor i in common_keys:\n    d[i]=d1[i]+d2[i]\nfor i in d1_keys_not_in_d2:\n    d[i]=d1[i]\nfor i in d2_keys_not_in_d1:\n    d[i]=d2[i]\nd\n{'a': [2, 4, 5, 6, 8, 10, 12, 15],\n 'b': [1, 2, 5, 6, 9, 12, 14, 16],\n 'c': [0, 4, 5, 8, 10, 21, 23, 35],\n 'd': [13, 3],\n 'e': [0, 0, 0]}\n"], ["combined_keys = d1.keys() | d2.keys()\nd_comb = {key: d1.get(key, []) + d2.get(key, []) for key in combined_keys}\n"], ["d1 = {\n 'a': [2,4,5,6,8,10],\n 'b': [1,2,5,6,9,12],\n 'c': [0,4,5,8,10,21]\n }\nd2 = {\n 'a': [12,15],\n 'b': [14,16],\n 'c': [23,35]\n  }\n\nd_comb = {key:[*d1[key], *d2[key]] for key in d1}\n\nprint(d_comb)\n", "{'c': [0, 4, 5, 8, 10, 21, 23, 35], 'b': [1, 2, 5, 6, 9, 12, 14, 16], 'a': [2, 4, 5, 6, 8, 10, 12, 15]}\n"], ["from itertools import chain\nd_comb = {key: list(chain(d1[key], d2[key])) for key in d1}\n"], ["{key: d1[key] + d2[key] for key in d1}\n\n{'a': [2, 4, 5, 6, 8, 10, 12, 15],\n 'b': [1, 2, 5, 6, 9, 12, 14, 16],\n 'c': [0, 4, 5, 8, 10, 21, 23, 35]}\n"], [], ["df.isin(['bal1']).any()\nA        False\nB         True\nC        False\nCLASS    False\ndtype: bool\n", "df[df.isin(['bal1'])].stack() # level 0 index is row index , level 1 index is columns which contain that value \n0  B    bal1\n1  B    bal1\ndtype: object\n"], [], [], ["REST_FRAMEWORK = {\n   ...\n   'DEFAULT_PERMISSION_CLASSES': ( 'rest_framework.permissions.IsAdminUser', ),\n   ...\n}\n"], ["[3DA0:2968][2018-12-05T20:46:18]e000: Error 0x80070643: Failed to install MSI package.\n[3DA0:2968][2018-12-05T20:46:18]e000: Error 0x80070643: Failed to execute MSI package.\n[2610:03A4][2018-12-05T20:46:18]e000: Error 0x80070643: Failed to configure per-machine MSI package.\n[2610:03A4][2018-12-05T20:46:18]i319: Applied execute package: core_AllUsers, result: 0x80070643, restart: None\n[2610:03A4][2018-12-05T20:46:18]e000: Error 0x80070643: Failed to execute MSI package\n"], []]