[["import os\nos.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = r'path/to/qt/plugins/platforms'\n", "import os\nimport PyQt5\nprint(os.path.dirname(PyQt5.__file__))\n"], ["def count_substring(string, sub_string):\nlis = list(string)\nM = list(sub_string)\nd = 0\nw = 0\nfor i in range(len(lis)-(len(M)-1)):\n    if lis[i] == M[0]:\n        d = 0\n        for k in range(len(M)):\n            if M[k] == lis[k+i]:\n                d+=1\n            else:\n                break\n        if d == len(M):\n            w+=1\n                \nreturn w\n"], [], ["'import os\ncwd = os.getcwd()\nprint(cwd)'\n", "Error:-\n\nFile \"C:\\Python311\\Lib\\site-packages\\PIL\\Image.py\", line 3218, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '1.jpg\n", "import os\nfrom tkinter import *\nfrom PIL import Image,ImageTk\nroot = Tk()\ncwd=os.getcwd()\nprint (cwd)\nroot.geometry(\"500x500\")\nroot.minsize(100,100)\nroot.maxsize(1200,700)\ni1 = Image.open(\"1.jpg\")\nphoto = ImageTk.PhotoImage(i1)\nl1 = Label(image=photo)\nl1.pack()\nroot.mainloop()\n", "PS C:\\Users\\91840\\Documents\\Code Practice> \n", "C:\\Users\\91840\\Documents\\Code Practice>Python Practice\n", "PS C:\\Users\\91840\\Documents\\Code Practice> cd Python Practice \n", "Note:-\nThis is my answer on Stack overflow and \n I will continue to contribute in solving problems of others.\n"], [], [], ["[build-system]\nrequires = [\"setuptools\", \"setuptools-scm\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"foo\"\nauthors = [\n    {name = \"...\", email = \"...\"},\n]\ndescription = \"...\"\nreadme = \"README.md\"\nrequires-python = \">=3.9\"\ndependencies = [\n    \"pydantic\",\n    \"jinja2\"\n]\nversion = \"1.0.1\"\n\n[project.scripts]\nfoo = \"foo:main\"\n"], [], ["df1.fillna(df1.merge(df2, how=\"left\", on=\"a\", suffixes=[\"_old\", \"\"]))\n", "    a   b   e\n0   1   0.0 a\n1   2   1.0 1\n2   3   0.0 2\n3   4   1.0 b\n"], ["cd ~ && mkdir python-lib && cd python-lib\necho 'psycopg2-binary' > requirements.txt\n", "docker run -v \"$PWD\":/var/task \"mlupin/docker-lambda:python3.11-build\" /bin/sh -c \"pip install -r requirements.txt -t python/lib/python3.11/site-packages/; exit\"\nzip -r psycopg2.zip python\n"], ["class human:\n  def __init__(self, choice = False, **kwargs):\n    self.details = [kwargs if choice is False else self._filterr(kwargs)][0]\n\n  def _filterr(self, param):\n    filtered = {k:v for k,v in param.items() if v is not None}\n    return filtered\n\njason = human(choice = True ,name = \"jason\", age = None, height = None, gender = None, programmer = True)\n\nprint(jason.details)\n", "{'name': 'jason', 'programmer': True}\n\n[Program finished]\n", "from dataclasses import dataclass, asdict\n\n@dataclass\nclass Human:\n    name: str\n    age: int = None\n    height: float = None\n    gender: str = None\n    programmer: bool = False\n\njason = Human(name=\"jason\", programmer=True)\n\njason_details = asdict(jason)\nprint(jason_details)\n"], [], ["DATABASES = {\n   'default': {\n       'CONN_HEALTH_CHECKS': True,\n       'CONN_MAX_AGE': 60,  # <= Mysql `interactive_timeout`/`wait_timeout`  \n   }\n}\n"], ["from enum import Enum\nfrom typing import Type, TypeVar, Union\n\nclass ProtocolType(Enum):\n    HTTP = \"http\"\n    HTTPS = \"https\"\n\nT = TypeVar(\"T\")\n\ndef validate_enum(value: T, enum_type: Type[Enum]) -> T:\n    if not any(value == item.value for item in enum_type):\n        raise ValueError(f\"Invalid value: {value}\")\n    return value\n\ndef my_request(protocol_type: Union[ProtocolType, str], url: str):\n    protocol_type = validate_enum(protocol_type, ProtocolType)\n    \n    print(f\"Making a {protocol_type} request to {url}\")\n\n# valid calls\nmy_request(\"http\", \"example.com\")\nmy_request(\"https\", \"example.com\")\n\n# invalid call\nmy_request(\"ftp\", \"example.com\")\n"], [], ["conda upgrade numpy\n"], [], [], ["$ pip install -U \\\n    -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-[YOUR_VERSION_of_ubuntu] \\\n    wxPython\n", "$ pip install -U \\\n    -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04 \\\n    wxPython\n"], ["test_case=int(input())\nwhile test_case!=0:\n     n=int(input())\n     a=list(map(int,input().split()))\n     test_case-=1\n"], [], [], [], [], [], ["wget https://archive.mariadb.org//connector-c-3.3.1/mariadb-connector-c-3.3.1-src.zip\nunzip mariadb-connector-c-3.3.1-src.zip\ncd mariadb-connector-c-3.3.1-src/\nmkdir build\ncd build/\ncmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local\nmake\nsudo make install\n", "sudo pip3 uninstall mariadb\nsudo pip3 install mariadb\n"], ["(virtual-env) shayon@shayon-X556UQK:~/Documents/python-fast-api$ ls\nmain.py  __pycache__  README.md  requirements.txt  virtual-env\n", "(virtual-env) shayon@shayon-X556UQK:~/Documents/python-fast-api$ pwd\n/home/shayon/Documents/python-fast-api\n", "(virtual-env) shayon@shayon-X556UQK:~/Documents/python-fast-api$ cat main.py\n", "from fastapi import FastAPI\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef index():\n    return {\"Hello\": \"World\"}\n"], ["nested_dict = {'A': {'key_B': 'value_B'},\n           'B': {'key_C': 'value_C'},\n           'C': {'key_C': {'key_D':'value_D'}}\n           }\n\n\ndef loop_nested(dictionary: dict):\n  for key in dictionary:\n  value = dictionary[key]\n  print(key)\n  if isinstance(value, dict):\n    loop_nested(value)\n  else:\n   print(value)\n\n\nloop_nested(nested_dict)\n"], [], [], ["def args_to_dict(**kwargs):\n    global dict_params\n    dict_params = locals()[\"kwargs\"]\n\ndef str_to_dict_args(s):\n    code = f\"args_to_dict({s})\" \n    exec(code)\n    return dict_params\n\ntest_args = \"a=5, c='10', d=20\"\nstr_to_dict_args(test_args)\n>>> {'a': 5, 'c': '10', 'd': 20}\n"], [], ["$ conda rename -p path/to/env-123 env-123\n"], [], [], [], ["if \"Unnamed: 0\" in data:\n        data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n", "data.rename(columns={\"indexname1\": \"raw_index\"}, inplace=True)\ndata.rename(columns={\"indexname2\": \"raw_index\"}, inplace=True)\n....\n", "data.reset_index(inplace=True)\n", "data.to_csv(filepath, index=True)\n", "data = pd.read_csv(index_col=0)\n"], [], [], [], [], [], ["from b import foo_b\n\ndef foo_a():\n    print(\"Foo A\")\n    foo_b()\n", "from a import foo_a\n\ndef foo_b():\n    print(\"Foo B\")\n    foo_a()\n"], ["prototxtPath = r\"face_detector/deploy.prototxt\"\nweightsPath = r\"face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n"], [], [], [], [], ["def multiencoder_factory(*encoders):\n    class MultipleJsonEncoders(json.JSONEncoder):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.encoders = [encoder(*args, **kwargs) for encoder in encoders]\n            \n        def default(self, o):\n            for encoder in self.encoders:\n                try:\n                    return encoder.default(o)\n                except TypeError:\n                    pass\n            return super().default(o)\n\n    return MultipleJsonEncoders\n", "import json\nimport enum\nimport datetime\n\nclass JsonDateEncoder(json.JSONEncoder):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n    def default(self, o):\n        if isinstance(o, (datetime.datetime, datetime.date)):\n            return o.isoformat()\n        return super().default(o)\n\nclass JsonEnumEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, enum.Enum):\n            return o.name\n        return super().default(o)\n\nclass Enumm(enum.Enum):\n    X = enum.auto()\n\nobj = {'time': datetime.datetime.now(), 'enum': Enumm.X}\nencoder = multiencoder_factory(JsonDateEncoder, JsonEnumEncoder)\n\njson.dumps(obj, cls=encoder)\n", "In [2]: isinstance(encoder(), json.JSONEncoder)\nOut[2]: True\n"], ["Public crumb As String\nPublic cookie As String\n\nPublic Sub YahooGetCrumb()\n    Dim http As MSXML2.XMLHTTP60\n    Dim strHeader As String\n    Dim strFields() As String\n    \n    Set http = New MSXML2.XMLHTTP60\n    \n    http.Open \"GET\", \"https://fc.yahoo.com\"\n    http.setRequestHeader \"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n    http.send\n    \n    strHeader = http.getAllResponseHeaders\n    strFields = Split(strHeader, vbCrLf)\n    cookie = Trim(Split(Split(strFields(5), \";\")(0), \":\")(1)) & \"; \" & Trim(Split(Split(strFields(6), \";\")(0), \":\")(1))\n\n    http.Open \"GET\", \"https://query2.finance.yahoo.com/v1/test/getcrumb\"\n    http.setRequestHeader \"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n    http.setRequestHeader \"Cookie\", cookie\n    http.send\n    \n    crumb = http.responseText\n    \nEnd Sub\n\nPublic Function GetYahooData(sSymbol As String) As String\n    Dim http As MSXML2.XMLHTTP60\n    \n    If crumb = \"\" Then\n        YahooGetCrumb\n    End If\n    \n    Set http = New MSXML2.XMLHTTP60\n    \n    http.Open \"GET\", \"https://query2.finance.yahoo.com/v7/finance/quote?symbols=\" & sSymbol & \"&crumb=\" & crumb, False\n    http.setRequestHeader \"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n    http.send\n    \n    GetYahooData = http.responseText\n    \nEnd Function\n\nPublic Sub test()\n    Dim JSON As Object\n\n    responseText = GetYahooData(\"AAPL\")\n    \n    Set JSON = ParseJson(responseText)\nend Sub    \n"], ["# run as\n# pytest test_plotting.py\n\nfrom matplotlib import pyplot as plt\n\n\ndef plot_fn():\n    plt.plot([1,2,3])\n    plt.show()\n    assert False # to check that the code gets here\n\ndef test_plot_fn():\n    with plt.ion():\n        plot_fn()\n"], ["from moduleone import ModuleOne\n", "from myproject.moduleone import ModuleOne\n"], [], [], ["pip uninstall selenium\npip install selenium\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\n\nservice = Service(ChromeDriverManager().install())\ndriver = webdriver.Chrome(service=service)\n"], [], [], [], ["sudo apt install llvm-{version}\n", "LLVM_CONFIG={path to llvm-config file} pip install {package}\n", "LLVM_CONFIG=/usr/bin/llvm-config-8 pip install numba==0.48\n"], [], ["[\n    {'health': 'good', 'status': 'up', 'date': '2022_03_10', 'device_id': 'device01'},\n    {'health': 'poor', 'status': 'down', 'date': '2022_03_10', 'device_id': 'device02'}\n]\n"], ["from flask import Flask, Response\nfrom textwrap import dedent\n\nclass LiveReload:\n    def __init__(self, app: Flask = None):\n        if app is not None:\n            self.init_app(app)\n\n    def init_app(self, app: Flask):\n        app.after_request(self.after_request)\n\n    def after_request(self, response: Response):\n        if response.status_code != 200:\n            return response\n\n        mimetype = response.mimetype or \"\"\n        if not mimetype.startswith(\"text/html\"):\n            return response\n\n        if not isinstance(response.response, list):\n            return response\n\n        body = b\"\".join(response.response).decode()\n        tag = self.make_tag()\n        body = body.replace(\"</head>\", f\"{tag}\\n</head>\")\n        response.response = [body.encode(\"utf8\")]\n        response.content_length = len(response.response[0])\n        return response\n\n    def make_tag(self):\n        return dedent(\"\"\"\n            <script>\n              document.write('<script src=\"http://' + (location.host || 'localhost').split(':')[0] +\n              ':35729/livereload.js?snipver=1\"></' + 'script>')\n            </script>\n        \"\"\").strip()\n", "from flask import Flask\nfrom livereload import LiveReload\n\napp = Flask(__name__)\nLiveReload(app)\n"], ["\"python.terminal.activateEnvInCurrentTerminal\": true\n", "\"python.defaultInterpreterPath\": \"/path/to/bin/python\"\n"], [], [], [], [], [], ["pip install discord.py>=2.3.2\n"], ["class RandomChoice(RandomTransforms):\n    def __call__(self, img):\n        t = random.choice(self.transforms)\n        return t(img)\n", "import random\nimport torchvision.transforms as T\n\nclass RandomChoice(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.t = random.choice(self.transforms)\n\n    def __call__(self, img):\n        return self.t(img)\n", "transform = RandomChoice([\n     T.RandomHorizontalFlip(), \n     T.RandomVerticalFlip()\n])\ndisplay(transform(img_a)) # both img_a and img_b will\ndisplay(transform(img_b)) # have the same transform\n\ntransform = RandomChoice([\n    T.RandomHorizontalFlip(), \n    T.RandomVerticalFlip()\n])\ndisplay(transform(img_c)) # both img_c and img_d will\ndisplay(transform(img_d)) # have the same transform\n", "import random\nimport torchvision.transforms as T\n\nclass RandomChoice(torch.nn.Module):\n    def __init__(self, transforms):\n       super().__init__()\n       self.transforms = transforms\n\n    def __call__(self, imgs):\n        t = random.choice(self.transforms)\n        return [t(img) for img in imgs]\n", "transform = RandomChoice([\n     T.RandomHorizontalFlip(), \n     T.RandomVerticalFlip()\n])\n\nimg_at, img_bt = transform([img_a, img_b])\ndisplay(img_at) # both img_a and img_b will\ndisplay(img_bt) # have the same transform\n\nimg_ct, img_dt = transform([img_c, img_d])\ndisplay(img_ct) # both img_c and img_d will\ndisplay(img_dt) # have the same transform\n"], ["df[[bool(x) for x in df.col]]\n", "df[[*map(bool, df.col)]]\n"], ["import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncompanies = pd.read_csv(r'E:\\SimpleLearn ML\\1000_Companies.csv')\nX = companies.iloc[:, :-1].values\ny = companies.iloc[:, 4].values\ncompanies.head()\n\nlabelencoder = LabelEncoder()\nX[:, 3] = labelencoder.fit_transform(X[:,3])\n\nonehotencoder = ColumnTransformer([(\"State\", OneHotEncoder(), [3])], remainder = \"passthrough\")\nX = onehotencoder.fit_transform(X)\n\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\n"], [], ["def to_camel_case(text):              \n    s = text.replace('_',' ').replace('-',' ').title().replace(' ','');\n    return text[:1]+s[1:]\n"], [], [], ["brew install gcc@11 \n\nLDFLAGS=\"-L/usr/local/opt/zlib/lib\" CC=$(which gcc-11) pyenv install 3.8.3 --patch < <(curl -sSL https://github.com/python/cpython/commit/8ea6353.patch)\n"], ["from fastapi import FastAPI, Request, HTTPException, Depends\nimport time\n\n# Initialize FastAPI app\napp = FastAPI()\n\n# In-memory storage for request counters\nrequest_counters = {}\n\n# Custom RateLimiter class with dynamic rate limiting values per route\nclass RateLimiter:\n    def __init__(self, requests_limit: int, time_window: int):\n        self.requests_limit = requests_limit\n        self.time_window = time_window\n\n    async def __call__(self, request: Request):\n        client_ip = request.client.host\n        route_path = request.url.path\n\n        # Get the current timestamp\n        current_time = int(time.time())\n\n        # Create a unique key based on client IP and route path\n        key = f\"{client_ip}:{route_path}\"\n\n        # Check if client's request counter exists\n        if key not in request_counters:\n            request_counters[key] = {\"timestamp\": current_time, \"count\": 1}\n        else:\n            # Check if the time window has elapsed, reset the counter if needed\n            if current_time - request_counters[key][\"timestamp\"] > self.time_window:\n                # Reset the counter and update the timestamp\n                request_counters[key][\"timestamp\"] = current_time\n                request_counters[key][\"count\"] = 1\n            else:\n                # Check if the client has exceeded the request limit\n                if request_counters[key][\"count\"] >= self.requests_limit:\n                    raise HTTPException(status_code=429, detail=\"Too Many Requests\")\n                else:\n                    request_counters[key][\"count\"] += 1\n\n        # Clean up expired client data (optional)\n        for k in list(request_counters.keys()):\n            if current_time - request_counters[k][\"timestamp\"] > self.time_window:\n                request_counters.pop(k)\n\n        return True\n\n# Include the custom RateLimiter dependency on specific routes\n@app.get(\"/limited\", dependencies=[Depends(RateLimiter(requests_limit=10, time_window=60))])\nasync def limited_endpoint():\n    return {\"message\": \"This endpoint has rate limiting (10 requests per 60 seconds).\"}\n\n@app.get(\"/limited/other\", dependencies=[Depends(RateLimiter(requests_limit=5, time_window=60))])\nasync def limited_other_endpoint():\n    return {\"message\": \"This endpoint has rate limiting (5 requests per 60 seconds).\"}\n\n@app.get(\"/unlimited\")\nasync def unlimited_endpoint():\n    return {\"message\": \"This endpoint has no rate limiting.\"}\n"], ["sudo apt install ./libffi6_3.2.1-8_amd64.deb\n", "apt install libffi-dev\n"], ["> chat_response.choices[0][\"message\"].to_dict()\n\n{'role': 'assistant', 'content': None, 'function_call': <OpenAIObject at 0x7...docs\\\"}\"\n}}\n\n> chat_response.choices[0][\"message\"].to_dict_recursive()\n\n{'role': 'assistant', 'content': None, 'function_call': {'name': 'ls', 'arguments': '{\"path\": \"/docs\"}'}}\n"], [], [], [], [], [], [], ["pip3 install --upgrade openai\n", "sudo sh -c 'echo python3 \"$(python3 -m site --user-site)\"/openai/_openai_scripts.py \\$\\@ > /usr/local/bin/openai'\n", "sudo chmod +x /usr/local/bin/openai\n", "openai --help\n"], ["import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf = pd.read_csv('data.csv', headers=None)\n\n#  Creating blank lists for XY data\nx = []\ny = []\n\n# Creating list with first entry being the desired starting point\nl=[[df.iloc[0][0],df.iloc[1][0]]]\n\n# Creating dataframe that does not contain the starting point\ndf2=df.iloc[1:]\n\n# Iterating through each data point\nfor i in l:\n    # Once the list reaches the same length as the original dataframe the\n    # process has examined all data points and breaks\n    if len(l) == len(df):\n        break\n\n    else:\n        # Calculating the distance to each point\n        d = np.sqrt((df2[0]-i[0])**2+(df2[1]-i[1])**2)\n    \n        # Removing any duplicates to the current point\n        dd = d[d!=0]\n    \n        # Setting a minimum distance threshold for the points,\n        # helps to deal with noisy data and should be adjusted to your data\n        if d.min()<5:        \n        \n            # Adding the sorted X & Y data to lists for easy plotting\n            x.append(df2.loc[dd.idxmin()][0])\n            y.append(df2.loc[dd.idxmin()][1])\n       \n            # Adding the next data point for analysis to the list\n            l.append([df2.loc[dd.idxmin()][0],df2.loc[dd.idxmin()][1]])\n        \n            # Removing the current datapoint from the dataframe to prevent\n            # multiple analysis\n            df2=df2.drop(index=dd.idxmin())\n\n# plotting the sorted data\nplt.plot(x,y)\nplt.scatter(x,y,c='r')\n"], ["firefox -v\n", "sudo snap install firefox\n"], ["workbook = xlsxwriter.Workbook(path, {\"nan_inf_to_errors\": True})\n", "def replace_nan(worksheet, row, col, value, format=None):\n    if math.isnan(value):\n        return worksheet.write_blank(row, col, None, format)\n    else:\n        return None  # let xlsxwriter do its thing\n\nworksheet.add_write_handler(float, replace_nan)\n"], ["pip install Flask==2.0.1\n", "pip install Flask==2.1.0\n", "pip install Flask==1.1.4\npip install markupsafe==2.0.1\n"], ["class CustomUserAdmin(UserAdmin):\n# Customize the display of fields for the user model\nlist_display = ('username', 'email', 'is_staff', 'is_active')\n", "admin.site.register(MyUser, CustomUserAdmin)\n"], [], ["\"pylint.args\": [\n    \"--disable=missing-module-docstring\",\n    \"--disable=missing-class-docstring\",\n    \"--disable=missing-function-docstring\"\n],\n"], ["[tool.pytest.ini_options]\naddopts = \"--import-mode=importlib\"\n"], ["> rm -f /Users/boss/opt/anaconda3/lib/python3.9/site-packages/googleapis_common_protos-1.57.0-py3.9-nspkg.pth\n"], ["{ // To collapse active cell input\n    \"key\": \"ctrl+[\",\n    \"command\": \"notebook.cell.collapseCellInput\",\n    \"when\": \"notebookCellListFocused && !inputFocus && !notebookCellInputIsCollapsed\"\n},\n{ // To expand active cell input\n    \"key\": \"ctrl+[\",\n    \"command\": \"notebook.cell.expandCellInput\",\n    \"when\": \"notebookCellInputIsCollapsed && notebookCellListFocused\"\n},\n{ //To collapse active cell output\n    \"key\": \"ctrl+]\",\n    \"command\": \"notebook.cell.collapseCellOutput\",\n    \"when\": \"notebookCellHasOutputs && notebookCellListFocused && !inputFocus && !notebookCellOutputIsCollapsed\"\n},\n{ // To expand active cell output\n    \"key\": \"ctrl+]\",\n    \"command\": \"notebook.cell.expandCellOutput\",\n    \"when\": \"notebookCellListFocused && notebookCellOutputIsCollapsed\"\n},\n{ // To collapse all cell inputs\n    \"key\": \"ctrl+shift+[\",\n    \"command\": \"notebook.cell.collapseAllCellInputs\"\n},\n{ // To expand all cell inputs\n    \"key\": \"ctrl+shift+alt+[\",\n    \"command\": \"notebook.cell.expandAllCellInputs\"\n},\n{ // To collapse all cell outputs\n    \"key\": \"ctrl+shift+]\",\n    \"command\": \"notebook.cell.collapseAllCellOutputs\"\n},\n{ // To expand all cell outputs\n    \"key\": \"ctrl+shift+alt+]\",\n    \"command\": \"notebook.cell.expandAllCellOutputs\"\n}\n"], [], ["model.save(MODEL_PATH, save_format='tf')\n"], [], ["conda activate\nconda create --name py38 python=3.8  # or just conda install python==3.8\n"], [], ["WARNING: The script openai.exe is installed in 'C:\\Users\\...\\...\\local-packages\\Python39\\Scripts' which is not on PATH.\n", "export PATH=\"~\\...\\...\\local-packages\\Python39\\Scripts:$PATH\"\n", "export PATH=\"/path/to/scripts:$PATH\" \n\n", "setx /m PATH \"%PATH%;C:\\path\\to\\scripts\" \n"], [], [], [], [], ["dataiter = iter(trainloader)\nimages, labels = next(dataiter)\n", "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"], [], ["// ackermann.cpp\n#include <iostream>\n#include <vector>\n#include <chrono>\n#include <string>\n#include \"num.hpp\"\nusing namespace std;\n\nclass MyTimer {\n    std::chrono::time_point<std::chrono::system_clock> start;\n\npublic:\n    void startCounter() {\n        start = std::chrono::system_clock::now();\n    }\n\n    int64_t getCounterNs() {\n        return std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::system_clock::now() - start).count();\n    }\n\n    int64_t getCounterMs() {\n        return std::chrono::duration_cast<std::chrono::milliseconds>(std::chrono::system_clock::now() - start).count();\n    }\n\n    double getCounterMsPrecise() {\n        return std::chrono::duration_cast<std::chrono::nanoseconds>(std::chrono::system_clock::now() - start).count()\n                / 1000000.0;\n    }\n};\n\nextern \"C\" {\n  int64_t ackermann(int64_t m, int64_t n) {\n    static std::vector<int64_t> cache[4];\n    // special signal to clear cache\n    if (m < 0 && n < 0) {      \n      for (int i = 0; i < 4; i++) {\n        cache[i].resize(0);\n        cache[i].shrink_to_fit();      \n      }\n      return -1;\n    }\n    \n    if (n >= cache[m].size()) {\n      int cur = cache[m].size();\n      cache[m].resize(n + 1);\n      for (int i = cur; i < n; i++) cache[m][i] = ackermann(m, i);\n    }\n\n    if (cache[m][n]) return cache[m][n];\n    if (m == 0) return cache[m][n] = n + 1;\n    \n    // These 3 lines are kinda cheating, since it uses math formula for special case\n    // So I commented them out because the question is about optimizing recursion.\n    // if (m == 1) return cache[m][n] = n + 2;\n    // if (m == 2) return cache[m][n] = 2 * n + 3;\n    // if (m == 3) return cache[m][n] = (1LL << (n + 3)) - 3;\n\n    if (n == 0) return cache[m][n] = ackermann(m - 1, 1);\n\n    return cache[m][n] = ackermann(m - 1, ackermann(m, n - 1));        \n  }\n\n  Num ackermann_bignum_smallres(int64_t m, int64_t n) {\n    static std::vector<Num> cache[4];\n    // special signal to clear cache\n    if (m < 0 && n < 0) {      \n      for (int i = 0; i < 4; i++) {\n        cache[i].resize(0);\n        cache[i].shrink_to_fit();      \n      }\n      return -1;\n    }\n    \n    if (n >= cache[m].size()) {\n      int cur = cache[m].size();\n      cache[m].resize(n + 1);\n      for (int i = cur; i < n; i++) cache[m][i] = ackermann(m, i);\n    }\n\n    if (cache[m][n] > 0) return cache[m][n];\n    if (m == 0) return cache[m][n] = n + 1;\n\n    if (n == 0) return cache[m][n] = ackermann(m - 1, 1);\n\n    return cache[m][n] = ackermann(m - 1, ackermann(m, n - 1));\n  }\n\n  //-----\n  Num bignum_pow(const Num& x, const Num& n) {\n    if (n == 0) return 1;\n    Num mid = bignum_pow(x, n / 2);\n    if (n % 2 == 0) return mid * mid;\n    else return mid * mid * x;\n  }\n\n  Num ackermann_bignum(Num m, Num n) {\n    if (m <= 1) return n + (m + 1);\n    else if (m == 2) return Num(2) * n + 3;\n    else if (m == 3) return bignum_pow(2, n + 3) - 3;\n    else {\n      cout << \"Don't put m >= 4\\n\";\n      exit(1);\n    } \n  }\n}\n\nNum dummy = 0;\nint main(int argc, char* argv[])\n{\n  int test_type = 0;\n  if (argc > 1) {\n    try {\n      test_type = std::stoi(string(argv[1]));\n    } catch (...) {\n      test_type = 0;\n    }\n  }\n  int lim = (test_type == 0) ? 63 : 17;\n\n  MyTimer timer;\n  timer.startCounter();\n    \n  for (int m = 0; m <= 3; m++)\n  for (int n = 0; n <= lim; n++) {\n    if (test_type == 0) {\n      dummy = ackermann_bignum(m, n);      \n    } else if (test_type == 1) {\n      dummy = ackermann_bignum_smallres(m, n);\n    } else {\n      dummy = ackermann(m, n);      \n    }\n    cout << \"ackermann(\" << m << \", \" << n << \") = \" << dummy << \"\\n\";    \n  }\n\n  cout << \"ackermann cost = \" << timer.getCounterMsPrecise() << \"\\n\";\n}\n\n", "g++ -o main_cpp ackermann.cpp -O3 -std=c++17\n./main_cpp\n", "def A_Stefan_row_class(m, n):\n    class A0:\n        def __getitem__(self, n):\n            return n + 1\n    class A:\n        def __init__(self, a):\n            self.a = a\n            self.n = 0\n            self.value = a[1]\n        def __getitem__(self, n):\n            while self.n < n:\n                self.value = self.a[self.value]\n                self.n += 1\n            return self.value\n    a = A0()\n    for _ in range(m):\n        a = A(a)\n    return a[n]\n\n\nfrom collections import defaultdict\n\ndef A_Stefan_row_lists(m, n):\n    memo = defaultdict(list)\n    def a(m, n):\n        if not m:\n            return n + 1\n        if m not in memo:\n            memo[m] = [a(m-1, 1)]\n        Am = memo[m]\n        while len(Am) <= n:\n            Am.append(a(m-1, Am[-1]))\n        return Am[n]\n    return a(m, n)\n\n\nfrom itertools import count\n\ndef A_Stefan_generators(m, n):\n    a = count(1)\n    def up(a, x=1):\n        for i, ai in enumerate(a):\n            if i == x:\n                x = ai\n                yield x\n    for _ in range(m):\n        a = up(a)\n    return next(up(a, n))\n\n\ndef A_Stefan_paper(m, n):\n    next = [0] * (m + 1)\n    goal = [1] * m + [-1]\n    while True:\n        value = next[0] + 1\n        transferring = True\n        i = 0\n        while transferring:\n            if next[i] == goal[i]:\n                goal[i] = value\n            else:\n                transferring = False\n            next[i] += 1\n            i += 1\n        if next[m] == n + 1:\n            return value\n\n\ndef A_Stefan_generators_2(m, n):\n    def a0():\n        n = yield\n        while True:\n            n = yield n + 1\n    def up(a):\n        next(a)\n        a = a.send\n        i, x = -1, 1\n        n = yield\n        while True:\n            while i < n:\n                x = a(x)\n                i += 1\n            n = yield x\n    a = a0()\n    for _ in range(m):\n        a = up(a)\n    next(a)\n    return a.send(n)\n\n\ndef A_Stefan_m_recursion(m, n):\n    ix = [None] + [(-1, 1)] * m\n    def a(m, n):\n        if not m:\n            return n + 1\n        i, x = ix[m]\n        while i < n:\n            x = a(m-1, x)\n            i += 1\n        ix[m] = i, x\n        return x\n    return a(m, n)\n\n\ndef A_Stefan_function_stack(m, n):\n    def a(n):\n        return n + 1\n    for _ in range(m):\n        def a(n, a=a, ix=[-1, 1]):\n            i, x = ix\n            while i < n:\n                x = a(x)\n                i += 1\n            ix[:] = i, x\n            return x\n    return a(n)\n\n\nfrom itertools import count, islice\n\ndef A_Stefan_generator_stack(m, n):\n    a = count(1)\n    for _ in range(m):\n        a = (\n            x\n            for a, x in [(a, 1)]\n            for i, ai in enumerate(a)\n            if i == x\n            for x in [ai]\n        )\n    return next(islice(a, n, None))\n\n\nfrom itertools import count, islice\n\ndef A_Stefan_generator_stack2(m, n):\n    a = count(1)\n    def up(a):\n        i, x = 0, 1\n        while True:\n            i, x = x+1, next(islice(a, x-i, None))\n            yield x\n    for _ in range(m):\n        a = up(a)\n    return next(islice(a, n, None))\n\n\ndef A_Stefan_generator_stack3(m, n):\n    def a(m):\n        if not m:\n            yield from count(1)\n        x = 1\n        for i, ai in enumerate(a(m-1)):\n            if i == x:\n                x = ai\n                yield x\n    return next(islice(a(m), n, None))\n\n\ndef A_Stefan_generator_stack4(m, n):\n    def a(m):\n        if not m:\n            return count(1)\n        return (\n            x\n            for x in [1]\n            for i, ai in enumerate(a(m-1))\n            if i == x\n            for x in [ai]\n        )\n    return next(islice(a(m), n, None))\n\n\ndef A_templatetypedef(i, n):\n    positions = [-1] * (i + 1)\n    values = [0] + [1] * i\n    \n    while positions[i] != n:       \n        values[0]    += 1\n        positions[0] += 1\n            \n        j = 1\n        while j <= i and positions[j - 1] == values[j]:\n            values[j] = values[j - 1]\n            positions[j] += 1\n            j += 1\n\n    return values[i]\n\nimport ctypes\nmylib = ctypes.CDLL('./ackermann.so')\nmylib.ackermann.argtypes = [ctypes.c_int64, ctypes.c_int64]\nmylib.ackermann.restype = ctypes.c_int64\n\ndef c_ackermann(m, n):\n    return mylib.ackermann(m,n)\n\nfuncs = [\n    c_ackermann,\n    A_Stefan_row_class,\n    A_Stefan_row_lists,\n    A_Stefan_generators,\n    A_Stefan_paper,\n    A_Stefan_generators_2,\n    A_Stefan_m_recursion,\n    A_Stefan_function_stack,\n    A_Stefan_generator_stack,\n    A_Stefan_generator_stack2,\n    A_Stefan_generator_stack3,\n    A_Stefan_generator_stack4,\n    A_templatetypedef\n]\n\nN = 18\nargs = (\n    [(0, n) for n in range(N)] +\n    [(1, n) for n in range(N)] +\n    [(2, n) for n in range(N)] +\n    [(3, n) for n in range(N)]\n)\n\nfrom time import time\n\ndef print(*args, print=print, file=open('out.txt', 'w')):\n    print(*args)\n    print(*args, file=file, flush=True)\n    \nexpect = none = object()\nfor _ in range(3):\n  for f in funcs:\n    t = time()\n    result = [f(m, n) for m, n in args]\n    # print(f'{(time()-t) * 1e3 :5.1f} ms ', f.__name__)\n    print(f'{(time()-t) * 1e3 :5.0f} ms ', f.__name__)\n    if expect is none:\n        expect = result\n    elif result != expect:\n        raise Exception(f'{f.__name__} failed')\n    del result\n  print()\n\n  c_ackermann(-1, -1)\n\n", "   32 ms  c_ackermann\n 1897 ms  A_Stefan_row_class\n 1427 ms  A_Stefan_row_lists\n  437 ms  A_Stefan_generators\n 1366 ms  A_Stefan_paper\n  479 ms  A_Stefan_generators_2\n  801 ms  A_Stefan_m_recursion\n  725 ms  A_Stefan_function_stack\n  716 ms  A_Stefan_generator_stack\n 1113 ms  A_Stefan_generator_stack2\n  551 ms  A_Stefan_generator_stack3\n  682 ms  A_Stefan_generator_stack4\n 1622 ms  A_templatetypedef\n"], ["def A_Stefan_generator_stack3(m, n):\n    def a(m):\n        if not m:\n            yield from count(1)\n        x = 1\n        for i, ai in enumerate(a(m-1)):\n            if i == x:\n                x = ai\n                yield x\n    return next(islice(a(m), n, None))\n", " 1325 ms  A_Stefan_row_class\n 1228 ms  A_Stefan_row_lists\n  544 ms  A_Stefan_generators\n 1363 ms  A_Stefan_paper\n  459 ms  A_Stefan_generators_2\n  866 ms  A_Stefan_m_recursion\n  704 ms  A_Stefan_function_stack\n  468 ms  A_Stefan_generator_stack\n  945 ms  A_Stefan_generator_stack2\n  582 ms  A_Stefan_generator_stack3\n  467 ms  A_Stefan_generator_stack4\n 1652 ms  A_templatetypedef\n", "def A_Stefan_row_class(m, n):\n    class A0:\n        def __getitem__(self, n):\n            return n + 1\n    class A:\n        def __init__(self, a):\n            self.a = a\n            self.n = 0\n            self.value = a[1]\n        def __getitem__(self, n):\n            while self.n < n:\n                self.value = self.a[self.value]\n                self.n += 1\n            return self.value\n    a = A0()\n    for _ in range(m):\n        a = A(a)\n    return a[n]\n\n\nfrom collections import defaultdict\n\ndef A_Stefan_row_lists(m, n):\n    memo = defaultdict(list)\n    def a(m, n):\n        if not m:\n            return n + 1\n        if m not in memo:\n            memo[m] = [a(m-1, 1)]\n        Am = memo[m]\n        while len(Am) <= n:\n            Am.append(a(m-1, Am[-1]))\n        return Am[n]\n    return a(m, n)\n\n\nfrom itertools import count\n\ndef A_Stefan_generators(m, n):\n    a = count(1)\n    def up(a, x=1):\n        for i, ai in enumerate(a):\n            if i == x:\n                x = ai\n                yield x\n    for _ in range(m):\n        a = up(a)\n    return next(up(a, n))\n\n\ndef A_Stefan_paper(m, n):\n    next = [0] * (m + 1)\n    goal = [1] * m + [-1]\n    while True:\n        value = next[0] + 1\n        transferring = True\n        i = 0\n        while transferring:\n            if next[i] == goal[i]:\n                goal[i] = value\n            else:\n                transferring = False\n            next[i] += 1\n            i += 1\n        if next[m] == n + 1:\n            return value\n\n\ndef A_Stefan_generators_2(m, n):\n    def a0():\n        n = yield\n        while True:\n            n = yield n + 1\n    def up(a):\n        next(a)\n        a = a.send\n        i, x = -1, 1\n        n = yield\n        while True:\n            while i < n:\n                x = a(x)\n                i += 1\n            n = yield x\n    a = a0()\n    for _ in range(m):\n        a = up(a)\n    next(a)\n    return a.send(n)\n\n\ndef A_Stefan_m_recursion(m, n):\n    ix = [None] + [(-1, 1)] * m\n    def a(m, n):\n        if not m:\n            return n + 1\n        i, x = ix[m]\n        while i < n:\n            x = a(m-1, x)\n            i += 1\n        ix[m] = i, x\n        return x\n    return a(m, n)\n\n\ndef A_Stefan_function_stack(m, n):\n    def a(n):\n        return n + 1\n    for _ in range(m):\n        def a(n, a=a, ix=[-1, 1]):\n            i, x = ix\n            while i < n:\n                x = a(x)\n                i += 1\n            ix[:] = i, x\n            return x\n    return a(n)\n\n\nfrom itertools import count, islice\n\ndef A_Stefan_generator_stack(m, n):\n    a = count(1)\n    for _ in range(m):\n        a = (\n            x\n            for a, x in [(a, 1)]\n            for i, ai in enumerate(a)\n            if i == x\n            for x in [ai]\n        )\n    return next(islice(a, n, None))\n\n\nfrom itertools import count, islice\n\ndef A_Stefan_generator_stack2(m, n):\n    a = count(1)\n    def up(a):\n        i, x = 0, 1\n        while True:\n            i, x = x+1, next(islice(a, x-i, None))\n            yield x\n    for _ in range(m):\n        a = up(a)\n    return next(islice(a, n, None))\n\n\ndef A_Stefan_generator_stack3(m, n):\n    def a(m):\n        if not m:\n            yield from count(1)\n        x = 1\n        for i, ai in enumerate(a(m-1)):\n            if i == x:\n                x = ai\n                yield x\n    return next(islice(a(m), n, None))\n\n\ndef A_Stefan_generator_stack4(m, n):\n    def a(m):\n        if not m:\n            return count(1)\n        return (\n            x\n            for x in [1]\n            for i, ai in enumerate(a(m-1))\n            if i == x\n            for x in [ai]\n        )\n    return next(islice(a(m), n, None))\n\n\ndef A_templatetypedef(i, n):\n    positions = [-1] * (i + 1)\n    values = [0] + [1] * i\n    \n    while positions[i] != n:       \n        values[0]    += 1\n        positions[0] += 1\n            \n        j = 1\n        while j <= i and positions[j - 1] == values[j]:\n            values[j] = values[j - 1]\n            positions[j] += 1\n            j += 1\n\n    return values[i]\n\n\nfuncs = [\n    A_Stefan_row_class,\n    A_Stefan_row_lists,\n    A_Stefan_generators,\n    A_Stefan_paper,\n    A_Stefan_generators_2,\n    A_Stefan_m_recursion,\n    A_Stefan_function_stack,\n    A_Stefan_generator_stack,\n    A_Stefan_generator_stack2,\n    A_Stefan_generator_stack3,\n    A_Stefan_generator_stack4,\n    A_templatetypedef,\n]\n\nN = 18\nargs = (\n    [(0, n) for n in range(N)] +\n    [(1, n) for n in range(N)] +\n    [(2, n) for n in range(N)] +\n    [(3, n) for n in range(N)]\n)\n\nfrom time import time\n\ndef print(*args, print=print, file=open('out.txt', 'w')):\n    print(*args)\n    print(*args, file=file, flush=True)\n    \nexpect = none = object()\nfor _ in range(3):\n  for f in funcs:\n    t = time()\n    result = [f(m, n) for m, n in args]\n    # print(f'{(time()-t) * 1e3 :5.1f} ms ', f.__name__)\n    print(f'{(time()-t) * 1e3 :5.0f} ms ', f.__name__)\n    if expect is none:\n        expect = result\n    elif result != expect:\n        raise Exception(f'{f.__name__} failed')\n    del result\n  print()\n"], ["docker login -u AWS -p $(aws ecr get-login-password) https://$(aws sts get-caller-identity --query 'Account' --output text).dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\n", "ARG NODE_IMAGE=node:16.17.1-alpine\nARG NODE_IMAGE=public.ecr.aws/docker/library/node:16.17.1-alpine\n"], ["def up_arrow(a, b):\n  if b <= 2:\n    if b < 0:\n      raise ValueError\n    return (1, 2, 4)[b]\n  elif a == 1:\n    if b >> 363:\n      raise ValueError\n    return 1 << b  # This may run out of memory for large b.\n  elif a == 2:\n    if b > 5:\n      raise ValueError\n    if b == 5:\n      return 1 << 65536\n    return (16, 65536)[b - 3]\n  elif a == 3:\n    if b > 3:\n      raise ValueError\n    return 65536\n  else:\n    raise ValueError\n", "def ack(m, n):\n  if n < 0:\n    raise ValueError\n  if m in (0, 1):\n    return n + (m + 1)  # This may run out of memory for large n.\n  elif m == 2:\n    return (n << 1) + 3  # This may run out of memory for large n.\n  elif m == 3:\n    if n >> 360:\n      raise ValueError\n    return (1 << (n + 3)) - 3  # This may run out of memory for large n.\n  elif m == 4:\n    if n > 2:\n      raise ValueError\n    if n == 2:\n      return (1 << 65536) - 3\n    return (13, 65533)[n]\n  elif m == 5:\n    if n > 0:\n      raise ValueError\n    return 65533\n  else:\n    raise ValueError\n\nprint([ack(m, 0) for m in range(6)])\nprint([ack(m, 1) for m in range(5)])\nprint([ack(m, 2) for m in range(5)])\nprint([ack(m, 3) for m in range(4)])\nprint([ack(m, 4) for m in range(4)])\n"], ["440.30 ms  A(m, n)\n431.11 ms  Ackermann = cache(A); Ackermann(m, n)\n  1.74 ms  B.cache_clear(); B(m, n)\n", "from timeit import repeat\nimport sys\n\nsys.setrecursionlimit(999999)\n\nsetup = '''\nfrom functools import cache\n\ndef A(m, n):\n    if not m:\n        return n + 1\n    return A(m - 1, A(m, n - 1)) if n else A(m - 1, 1)\n\n@cache\ndef B(m, n):\n    if not m:\n        return n + 1\n    return B(m - 1, B(m, n - 1)) if n else B(m - 1, 1)\n\nm, n = 3, 8\n'''\n\ncodes = [\n    'A(m, n)',\n    'Ackermann = cache(A); Ackermann(m, n)',\n    'B.cache_clear(); B(m, n)',\n]\n\nfor code in codes:\n    t = min(repeat(code, setup, number=1))\n    print(f'{t*1e3:6.2f} ms ', code)\n"], [], ["def ack(m, n):\n  if m == 0: return n + 1\n  return ack(m - 1, 1) if n == 0 else ack(m - 1, ack(m, n - 1))\n", "c = {}\n\ndef ack(m, n):\n  global c\n\n  if \"{}-{}\".format(m, n) in c: return c[\"{}-{}\".format(m, n)]\n  else:\n    if m == 0: ret = n + 1\n    else: ret = ack(m - 1, 1) if n == 0 else ack(m - 1, ack(m, n - 1))\n\n    c[\"{}-{}\".format(m, n)] = ret\n    return ret\n", "c = {}\n\ndef ack(m, n):\n  global c\n\n  if m == 0: return n + 1\n  else:\n    if \"{}-{}\".format(m, n) in c: return c[\"{}-{}\".format(m, n)]\n    else: ret = ack(m - 1, 1) if n == 0 else ack(m - 1, ack(m, n - 1))\n\n    c[\"{}-{}\".format(m, n)] = ret\n    return ret\n", "c = {}\n\ndef ack(m, n):\n  global c\n\n  if m == 0: return n + 1\n  else:\n    key = \"{}-{}\".format(m, n)\n    if key in c: return c[key]\n    else: ret = ack(m - 1, 1) if n == 0 else ack(m - 1, ack(m, n - 1))\n\n    c[key] = ret\n    return ret\n"], [], ["from selenium import webdriver\ndriver = webdriver.Chrome()\n#.......\ndriver.quit()\n", "from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nservice = Service(executable_path=\"PATH_TO_DRIVER\")\noptions = webdriver.ChromeOptions()\ndriver = webdriver.Chrome(service=service, options=options)\n#......\n[![enter image description here][1]][1]driver.quit()\n"], [], ["s = 'my str'\nprint(type(s))\n# output: str\n\ns = 'my str',\nprint(type(s))\n# output: tuple\n", "class Transaction(models.Model):\n    trasaction_status = models.CharField(max_length=255, choices=TransactionStatus.choices())\n    transaction_type = models.CharField(max_length=255, choices=TransactionType.choices())\n", "class TransactionType(Enum):\n\n    IN = \"IN\"\n    OUT = \"OUT\"\n\n    @classmethod\n    def choices(cls):\n        print(tuple((i.name, i.value) for i in cls))\n        return tuple((i.name, i.value) for i in cls)\n\nclass TransactionStatus(Enum):\n\n    INITIATED = \"INITIATED\"\n    PENDING = \"PENDING\"\n    COMPLETED = \"COMPLETED\"\n    FAILED = \"FAILED\"\n    ERROR = \"ERROR\"\n\n    @classmethod\n    def choices(cls):\n        print(tuple((i.value, i.name) for i in cls))\n        return tuple((i.value, i.name) for i in cls)\n"], [], [" export QT_QPA_PLATFORM=offscreen \n"], [], ["[tool.setuptools.packages.find]\nwhere = [\"src\"]\n\n[tool.setuptools.package-data]\nmypkg = [\"*.txt\", \"*.rst\"]\n"], ["import requests\n\n\ndef get_yahoo_cookie():\n    cookie = None\n\n    user_agent_key = \"User-Agent\"\n    user_agent_value = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n\n    headers = {user_agent_key: user_agent_value}\n    response = requests.get(\n        \"https://fc.yahoo.com\", headers=headers, allow_redirects=True\n    )\n\n    if not response.cookies:\n        raise Exception(\"Failed to obtain Yahoo auth cookie.\")\n\n    cookie = list(response.cookies)[0]\n\n    return cookie\n\n\ndef get_yahoo_crumb(cookie):\n    crumb = None\n\n    user_agent_key = \"User-Agent\"\n    user_agent_value = \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n\n    headers = {user_agent_key: user_agent_value}\n\n    crumb_response = requests.get(\n        \"https://query1.finance.yahoo.com/v1/test/getcrumb\",\n        headers=headers,\n        cookies={cookie.name: cookie.value},\n        allow_redirects=True,\n    )\n    crumb = crumb_response.text\n\n    if crumb is None:\n        raise Exception(\"Failed to retrieve Yahoo crumb.\")\n\n    return crumb\n\n\n# Usage\ncookie = get_yahoo_cookie()\ncrumb = get_yahoo_crumb(cookie)\n"], ["response.choices[0].message.content\n", "messages = [\n     {\n         \"role\": \"system\",\n         \"content\": \"You are an expert at blah blah\"),\n     },\n]\nresponse = openai.ChatCompletion.create(\n      messages=messages + [{\"role\": \"user\", \"content\": prompt}],\n      model=\"gpt-4\",\n      temperature=0.7,\n)\n"], ["\n<?php\n\n/* 1 - Get cookie */\n//https://stackoverflow.com/questions/76065035/yahoo-finance-v7-api-now-requiring-cookies-python\n$url_yahoo = \"https://fc.yahoo.com\";\n$yahoo_headers = get_headers($url_yahoo, true);\n//print_r($yahoo_headers);\n$cookie_name = 'Set-Cookie';\n\n/* 2 - Get crumb , setting cookie */\n$url_yahoo2 = \"https://query2.finance.yahoo.com/v1/test/getcrumb\";\n$c = curl_init($url_yahoo2);\ncurl_setopt($c, CURLOPT_VERBOSE, 1);\ncurl_setopt($c, CURLOPT_COOKIE, $yahoo_headers[$cookie_name]);\ncurl_setopt($c, CURLOPT_RETURNTRANSFER, 1);\n$crumb = curl_exec($c);\ncurl_close($c);\n//echo \"<BR>Crumb:\" . $crumb;\n\n/* 3 - Get quotes with crumb, setting cookie. Using sample tickets*/\n$tickets_list = \"AAPL,TSLA\";\n$url_cotacao = \"https://query2.finance.yahoo.com/v7/finance/quote?symbols=\" . $tickets_list . \"&crumb=\" . $crumb;\n$c = curl_init($url_cotacao);\ncurl_setopt($c, CURLOPT_VERBOSE, 1);\ncurl_setopt($c, CURLOPT_COOKIE, $yahoo_headers[$cookie_name]);\ncurl_setopt($c, CURLOPT_RETURNTRANSFER, 1);\n$data_quote = curl_exec($c);\ncurl_close($c);\n\n\n/* 4 - Get data from yahoo */\n$resJson_decode = json_decode($data_quote, false);\nif (!$resJson_decode->quoteResponse->result) {\n    $resultado = \"Ticket dont exists in yahoo!\";\n} else {\n    foreach ($resJson_decode->quoteResponse->result as $ticket_result){\n        echo \"<BR>Ticket:\" . $ticket_result->symbol;\n        echo \"<BR>Price:\" . $ticket_result->regularMarketPrice;\n    }\n    \n}\n\n"], ["pip install undetected-chromedriver\npip install selenium\n", "import undetected_chromedriver as uc\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nclass Main:\n  def __init_(self) -> None:\n    self.url    = 'https://accounts.google.com/ServiceLogin'\n    self.driver = driver = uc.Chrome(use_subprocess=True)\n    self.time   = 10\n    \n  def login(self, email, password):\n    # edit: fixed missing end-quotes on below lines\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, 'identifier'))).send_keys(f'{email}\\n')\n    WebDriverWait(self.driver, 20).until(EC.visibility_of_element_located((By.NAME, 'password'))).send_keys(f'{password}\\n')\n                                                                                \n    self.code()\n                                                                                  \n  def code(self):\n    # [ ---------- paste your code here ---------- ]\n    time.sleep(self.time)                                                                                  \n                                                                                  \nif __name__ == \"__main__\":\n  #  ---------- EDIT ----------\n  email = 'email' # replace email\n  password = 'password' # replace password\n  #  ---------- EDIT ----------                                                                                                                                                         \n \n  driver = Main()\n  driver.login(email, password) \n"], ["source openaiwork/bin/activate\n\npip install openai\n", "pip install pandas\n"], [], ["python -m pip install --upgrade pytube\n", "from pytube import Playlist\nimport os\nimport re\n\ndef download(playlist):\n    progress = None\n    PATH = f'./{title}'\n    with open(PATH+'/dependList.txt','r') as f:\n        progress = int(f.read())\n\n    videos = playlist.videos\n    for i,video in enumerate(videos):\n        if i >= progress:\n            v = video.streams.filter(progressive=True,file_extension='mp4').order_by('resolution').desc().first()\n            v.download(PATH+'/')\n            with open(PATH+'/dependList.txt','w') as f:\n                f.write(f'{i+1}')\n        print(f'Downloaded {i+1} video')\n\nlink = input('Enter link:')\npl = Playlist(link)\ntotal_videos = len(pl.videos)\nprint(f'Total Videos:{total_videos}')\ntitle = ' '.join(re.findall(\"[a-zA-Z]+\", pl.title))\nprint(title)\n\nif os.path.exists(title):\n    length_file = len(os.listdir(title)) - 1\n    if length_file!=total_videos:\n        download(pl)\n        print(\"DONE\")\n    else:\n        print('Already Exist')\nelse:\n    os.makedirs(f'./{title}')\n    with open(f'./{title}/dependList.txt','w') as f:\n        f.write('0')\n    download(pl)\n    print('DONE')\n"], ["sudo apt install python3-*pandas*\n"], [], [], [" sudo sh -c 'echo python3 /home/<MY_USERNAME>/.local/lib/python3.10/site-packages/openai/_openai_scripts.py \\$\\@ > /usr/local/bin/openai'\n"], [], [], ["AttributeError: 'EntryPoints' object has no attribute 'get'\n", " for ep in importlib_metadata.entry_points().get(namespace, [])\n", "for ep in importlib_metadata.entry_points(group ='namespace') \n"], ["pip install -r requirements.txt --break-system-packages\n"], ["import pandas as pd\n\npd.read_csv(uri)\n"], ["Option Explicit\nSub sbGoogleLogin()\n    Dim driver As ChromeDriver\n    Set driver = New ChromeDriver\n    Dim sURL As String\n    sURL = \"https://mail.google.com/mail/u/0/?pli=1#inbox\"\n    driver.Start (\"edge\")\n    driver.get (sURL)\n    sbDelay (100000)\n    driver.FindElementByXPath(\"//input[@type='email']\").SendKeys (\"current, valid gmail address required here\")\n    sbDelay (100000)\n    driver.FindElementByXPath(\"/html/body/div/div/div[2]/div/c-wiz/div/div[2]/div/div[2]/div/div/div/div/button\").Click\n    MsgBox \"wait\"\n    driver.Quit\nEnd Sub\nSub sbDelay(delay As Long): Dim i As Long: For i = 1 To delay:  DoEvents: Next i: End Sub\n"], [], [], [], [], ["import openai\n\nresponse = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n    ]\n)\n", "my_openai_obj = list(response.choices)[0]\nmy_openai_obj.to_dict()['message']['content']\n"], ["> pip3 install aws-psycopg2 -t .\n\nCollecting aws-psycopg2\n  Using cached aws_psycopg2-1.3.8-py3-none-any.whl (73.8 MB)\nInstalling collected packages: aws-psycopg2\nSuccessfully installed aws-psycopg2-1.3.8\n"], ["upload_parser = api.parser()\nupload_parser.add_argument('file', location='files',\n                           type=FileStorage, required=True)\n\nhostauth_create_fields = api.model(\n    'HostAuthCreate', {\n        'name': fields.String(description=\"The name of the instance\", required=True),\n        'username': fields.String(description=\"Username to connect\", required=True),\n        'password': fields.String(description=\"Password to connect\", required=False)\n    }\n)\n\n@api.route('/api/hostauth')\nclass HostAuthView(Resource):\n    @api.expect(upload_parser, hostauth_create_fields)\n    def post(self):\n        args = upload_parser.parse_args()\n        args.get('file')\n        api.payload.get('name') # This line will cause a error\n        return {'name': args.get('name')}, 201\n", "upload_parser = api.parser()\nupload_parser.add_argument('file', location='files',\n                           type=FileStorage, required=True)\nupload_parser.add_argument('name', type=str, required=True, location=\"form\")\nupload_parser.add_argument('username', type=str, required=True, location=\"form\")\nupload_parser.add_argument('password', type=str, location=\"form\")\n", "args.get('name')\n"], [], [" ![Tux, the Linux mascot](./assets/images/tux.png)\n", " # ![title](./assets/images/tux.png)\n"], ["# use livereload==2.5.1 only\nfrom flask import Flask, render_template\nfrom livereload import Server\n\napp = Flask(__name__)\napp.debug = True\n\n@app.get(\"/\")\ndef index():\n    return render_template(\"index.html\")\n\n# don't use flask run, use python3 app.py\nserver = Server(app.wsgi_app)\nserver.watch(\"templates/*.*\")  # or what have you\nserver.serve(port=5000) # if you want the standard Flask port\n"], [], [], [], ["df1 = pd.DataFrame({'a': [1,2,3,4], 'b': [0,1,np.nan, 1], 'e': ['a', 1, 2,'b']})\ndf2 = pd.DataFrame({'a': [1,2,3,4], 'b': [np.nan, 1, 0, 1]})\n", "df1.b.fillna(df1.a.map(df2.set_index('a').b),inplace=True)\ndf1\nOut[173]: \n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  3  0.0  2\n3  4  1.0  b\n", "df = df1.combine_first(df1[['a']].merge(df2, on='a', how='left'))\ndf\nOut[184]: \n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  3  0.0  2\n3  4  1.0  b\n", "df1.fillna(df1[['a']].merge(df2, on='a', how='left'))\nOut[185]: \n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  3  0.0  2\n3  4  1.0  b\n"], [], ["parser = reqparse.RequestParser()\nparser.add_argument(\"mac\", type=str)\nargs = parser.parse_args()\n", "parser = reqparse.RequestParser()\nparser.add_argument(\"mac\", type=str ,location='args')\nargs = parser.parse_args()\n"], ["$ pip install -U -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04 wxPython\n$ sudo apt-get install libsdl2-mixer-2.0-0 libsdl2-image-2.0-0 libsdl2-2.0-0\n", "python3\nPython 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import wx\n>>> \n", "$ sudo apt-get install dpkg-dev build-essential python3-dev freeglut3-dev libgl1-mesa-dev libglu1-mesa-dev libgstreamer-plugins-base1.0-dev libgtk-3-dev libjpeg-dev libnotify-dev libpng-dev libsdl2-dev libsm-dev libtiff-dev libwebkit2gtk-4.0-dev libxtst-dev\n$ pip install attrdict3\n$ pip install wxpython\n"], ["C:\\python\\Scripts\\pip3.exe install numpy\nC:\\python\\Scripts\\pip3.exe install pandas\nC:\\python\\Scripts\\pip3.exe install mlflow\n"], ["bob = [\"a\", \"b\", \"b\"]\nbob = set(bob)\nbob = list(bob)\n"], ["from dataclasses import dataclass, field\n\n@dataclass\nclass SomeClass:\n    \"\"\"\n    \"\"\"\n\n    some_list: list = field(default_factory=lambda: [\"your_values\"])\n", "from dataclasses import dataclass, field\n\nSHARED_LIST = [\"your_values\"]\n    \n@dataclass\nclass SomeClass:\n    \"\"\"\n    \"\"\"\n    \n    some_list: list = field(default_factory=lambda: SHARED_LIST)\n"], [], [], ["import pandas as pd\nfrom io import StringIO\nfrom azure.identity import InteractiveBrowserCredential\nfrom azure.storage.blob import BlobServiceClient, ContainerClient\n\n# name of the file \nfile_name = 'sample_file.csv' \n# Note:- include folders if you have a folder structure in the blob \n# container ex: -> main/child/sample.csv\n\n# storage account URL\nSTORAGE_ACCOUNT_URL = 'https://sampleblob.blob.core.windows.net'\n# name of the container that holds your CSV file\nBLOB_STORAGE_CONTAINER_NAME = \"sample-storage-container\"\n# Here I am using the interactive credential, you may use any other credential\nCREDENTIAL = InteractiveBrowserCredential()\n\n# Create the BlobServiceClient object\nblob_service_client = BlobServiceClient(STORAGE_ACCOUNT_URL, credential=CREDENTIAL)\ncontainer_client = blob_service_client.get_container_client(container=BLOB_STORAGE_CONTAINER_NAME)\nblob_client = container_client.get_blob_client(file_name)\nif blob_client.exists() # check if blob exists\n    download_stream = blob_client.download_blob() # read file\n    df = pd.read_csv(StringIO(download_stream.content_as_text())) # use text as input to pandas\n    print(f\"Shape of File {file_name} is {df.shape}\")\n"], [], ["from pytube import Playlist\nfrom pytube import YouTube\n\nfrom pytube import Playlist\nplaylist = Playlist('https://www.youtube.com/watch?v=UPFKAG9rYOE&list=PLknwEmKsW8OtK_n48UOuYGxJPbSFrICxm')\nprint('Number of videos in playlist: %s' % len(playlist.video_urls))\nfor video_url in playlist.video_urls:\n    print(video_url)\n    video=YouTube(video_url)\n    try:\n       #video.streams.first().download()\n       video.streams.filter(res=\"720p\").first().download()\n    except:\n         continue\n"], ["[tool.pytest.ini_options]\npythonpath = \"src\"\naddopts = [\n    \"--import-mode=importlib\",\n]\n"], ["brew install python\n", "pip3 install openai\n", "pip install openai\n", "ENJOY a cup of coffee ;)\n"], [], ["from skimage import img_as_ubyte\nfrom PIL import Image\n\nnew_image = Image.fromarray(img_as_ubyte(image))\n"], [], [], ["import requests\n\nresponse = requests.post('http://127.0.0.1:5000/video/1', {'likes': 10})\n\nresponse_data_forced_json = response.get_json(force=True)\n"], [], ["fieldsets = (\n    (None, {\"fields\": (\"username\")}),\n    (_(\"Personal info\"), {\"fields\": (\"first_name\", \"last_name\", \"email\")}),\n    (\n        _(\"Permissions\"),\n        {\n            \"fields\": (\n                \"is_active\",\n                \"is_staff\",\n                \"is_superuser\",\n                \"groups\",\n                \"user_permissions\",\n            ),\n        },\n    ),\n    (_(\"Important dates\"), {\"fields\": (\"last_login\", \"date_joined\")}),\n)\n"], [], ["HKEY_LOCAL_MACHINE\\SOFTWARE WOW6432Node\\Mozilla\\Mozilla Firefox\\[VERSION]\\Main\\PathToExe\n\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Mozilla\\Mozilla Firefox\\[VERSION]\\Main\\PathToExe\n", "HKEY_LOCAL_MACHINE\\SOFTWARE\\Mozilla\\Mozilla Firefox 109.0\\bin\n", "HKEY_LOCAL_MACHINE\\SOFTWARE\\mozilla.org\\Mozilla\n", "var options = new FirefoxOptions();\n...\noptions.BrowserExecutableLocation = @\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\";\nDriver = new FirefoxDriver(options);\n"], [], [], ["from selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.binary_location = r'C:\\Program Files\\Mozilla Firefox\\firefox.exe'\ndriver = webdriver.Firefox(executable_path=r'C:\\WebDrivers\\geckodriver.exe', options=options)\ndriver.get('http://google.com/')\n"], [], ["Version: 1.74.3 (user setup)\nCommit: 97dec172d3256f8ca4bfb2143f3f76b503ca0534\nDate: 2023-01-09T16:59:02.252Z\nElectron: 19.1.8\nChromium: 102.0.5005.167\nNode.js: 16.14.2\nV8: 10.2.154.15-electron.0\nOS: Windows_NT x64 10.0.19044\nSandboxed: No\nJupyter extension: v2022.11.1003412109\n"], ["from fastapi import APIRouter, File, status, Depends, HTTPException,  UploadFile\n\nimport shutil\nfrom pathlib import Path\n\nfrom database.user_functions import *\nfrom database.auth_functions import *\nfrom database.form_functions import *\n\nfrom model import *\nfrom model_form import *\n\nfile_routes = APIRouter()\n\n\n# @file_routes.post(\"/files/\")\n# async def create_file(file: bytes = File()):\n#     return {\"file_size\": len(file)}\n\n\n# @file_routes.post(\"/uploadfile/\")\n# async def create_upload_file(file: UploadFile):\n#     return {\"filename\": file.filename}\n\n\n@file_routes.post(\"/upload-file/\")\nasync def create_upload_file(uploaded_file: UploadFile = File(...)):    \n\n    file_location = f\"./{uploaded_file.filename}\"\n    with open(file_location, \"wb+\") as file_object:\n        shutil.copyfileobj(uploaded_file.file, file_object)    \n    return {\"info\": f\"file '{uploaded_file.filename}' saved at '{file_location}'\"}\n"], [], [" class Rescaling(tf.keras.layers.Layer):\n\"\"\"Multiply inputs by `scale` and adds `offset`.\nFor instance:\n1. To rescale an input in the `[0, 255]` range\nto be in the `[0, 1]` range, you would pass `scale=1./255`.\n2. To rescale an input in the `[0, 255]` range to be in the `[-1, 1]` \nrange,\nyou would pass `scale=1./127.5, offset=-1`.\nThe rescaling is applied both during training and inference.\nInput shape:\nArbitrary.\nOutput shape:\nSame as input.\nArguments:\nscale: Float, the scale to apply to the inputs.\noffset: Float, the offset to apply to the inputs.\nname: A string, the name of the layer.\n\"\"\"\n\ndef __init__(self, scale, offset=0., name=None, **kwargs):\n  self.scale = scale\n  self.offset = offset\n  super(Rescaling, self).__init__(name=name, **kwargs)\n\ndef call(self, inputs):\n  dtype = self._compute_dtype\n  scale = tf.cast(self.scale, dtype)\n  offset = tf.cast(self.offset, dtype)\n  return tf.cast(inputs, dtype) * scale + offset\n\ndef compute_output_shape(self, input_shape):\n  return input_shape\n\ndef get_config(self):\n  config = {\n      'scale': self.scale,\n      'offset': self.offset,\n  }\n  base_config = super(Rescaling, self).get_config()\n  return dict(list(base_config.items()) + list(config.items()))\n"], ["df1.merge(df2, ...).merge(df3, ...)\n", "# Setup.\nnp.random.seed(0)\nA = pd.DataFrame({'key': ['A', 'B', 'C', 'D'], 'valueA': np.random.randn(4)})    \nB = pd.DataFrame({'key': ['B', 'D', 'E', 'F'], 'valueB': np.random.randn(4)})\nC = pd.DataFrame({'key': ['D', 'E', 'J', 'C'], 'valueC': np.ones(4)})\ndfs = [A, B, C] \n\n# Note: the \"key\" column values are unique, so the index is unique.\nA2 = A.set_index('key')\nB2 = B.set_index('key')\nC2 = C.set_index('key')\n\ndfs2 = [A2, B2, C2]\n", "# Merge on `key` column. You'll need to set the index before concatenating\npd.concat(\n    [df.set_index('key') for df in dfs], axis=1, join='inner'\n).reset_index()\n\n  key    valueA    valueB  valueC\n0   D  2.240893 -0.977278     1.0\n\n# Merge on `key` index.\npd.concat(dfs2, axis=1, sort=False, join='inner')\n\n       valueA    valueB  valueC\nkey                            \nD    2.240893 -0.977278     1.0\n", "A3 = pd.DataFrame({'key': ['A', 'B', 'C', 'D', 'D'], 'valueA': np.random.randn(5)})\npd.concat([df.set_index('key') for df in [A3, B, C]], axis=1, join='inner')\n", "# Join on `key` column. Set as the index first.\n# For inner join. For left join, omit the \"how\" argument.\nA.set_index('key').join([B2, C2], how='inner').reset_index()\n\n  key    valueA    valueB  valueC\n0   D  2.240893 -0.977278     1.0\n\n# Join on `key` index.\nA3.set_index('key').join([B2, C2], how='inner')\n\n       valueA    valueB  valueC\nkey                            \nD    1.454274 -0.977278     1.0\nD    0.761038 -0.977278     1.0\n"], [], ["df = df.where(df.notnull(), None)\n"], [], ["# from this :\nwith open('Data.pkl', 'rb') as f:\n     t = pickle.load(f)\n\n# to this:\nimport pandas as pd \nwith open('Data.pkl', 'rb') as f:\n      t = pd.read_pickle(f)\n"], [], ["import tkinter as tk\nfrom PIL import Image, ImageTk\n\nimg = tk.PhotoImage(file=\"assets/logo.png\")\nlogo_widget = tk.Label(frame1, image=img, bg=bg_color)\nlogo_widget.pack()\n", "img = ImageTk.PhotoImage(file=\"assets/logo.png\")\nlogo_widget = tk.Label(frame1, image=img, bg=bg_color)\nlogo_widget.pack()\n"], ["[tool.hatch.build]\ninclude = [\n  \"myModule/**/*.csv\",\n  \"**/*.py\",  \n]\nexclude = [\n  \"tests/**\",\n]\n"], [], ["Could not load dynamic library 'libnvinfer.so.7'\nlibnvinfer_plugin.so.7: cannot open shared object file\n"], ["#foo() is a function that exists already...\nstring = 'bar = 1'\nif string == 'bar = 1':\n    foo(bar=1)\n"], ["df.replace(np.nan, None)\n"], [], [], [], [], [], ["sass.compiler = require('node-sass');\n\nto\n\nsass.compiler = require('sass');\n", "const sass = require('gulp-sass')\n\nto\n\nconst sass = require('gulp-sass')(require('sass'))\n"], ["# pip install camelCasing\nfrom camelCasing import camelCasing as cc\n\nfor s in ['the_stealth_warrior', 'The-Stealth-Warrior', 'A-B-C']:\n    print(cc.toCamelCase(s=s, user_acronyms=None))\n\ntheStealthWarrior\ntheStealthWarrior\nABC\n"], [], ["from dataclasses import dataclass, field\n", "@dataclass\nclass Foo:\n    bar: list = field(default_factory=list)\n"], ["x = {&quot;choices&quot;: [{&quot;finish_reason&quot;: &quot;length&quot;,\n                  &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;}], }\n\ntext = x['choices'][0]['text']\nprint(text)  # , everyone, and welcome to the first installment of the new opening\n"], ["FROM ubuntu:18.04\n", "FROM public.ecr.aws/lts/ubuntu:latest\n"], ["%env OPENAI_API_KEY=sk-Kz8Weh1234ddgYBmsdfinsdf7ndsfg55532432\n", "!pip install -Uq openai\n", "import openai\n", "!openai tools fine_tunes.prepare_data -f \"/content/sample_data/promptdata.csv\"\n", "!openai tools fine_tunes.prepare_data -f \"/content/sample_data/promptdata_prepared.jsonl\"\n", "!openai api fine_tunes.create -t \"/content/sample_data/promptdata_prepared.jsonl\"\n"], [], ["openai --help\n", "/Users/<USER>/DIR_TO_PYTHON/site-packages/openai/_openai_scripts.py\n", "/Users/<USER>/DIR_TO_PYTHON/site-packages/\n", "sudo vim /bin/openai\n", "python3 /Users/<USER>/DIR_TO_PYTHON/site-packages/openai/_openai_scripts.py $@\n", "chmod +x /bin/openai\n"], [], ["conda install pytorch torchvision torchaudio cpuonly -c pytorch\n", "import sys\nprint(sys.path)\n", "{\n \"argv\": [\n  \"<ANACONDA_INSTALL_DIR>\\\\envs\\\\<KERNAL_NAME>\\\\python.exe\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n    \"env\": {\n    \"PYTHONPATH\": \"..\\\\..\\\\..\\\\Users\\\\<USER_NAME>\\\\<ANACONDA_INSTALL_DIR_NAME>\\\\envs\\\\<KERNAL_NAME>\\\\Lib\\\\site-packages\"\n},\n \"display_name\": \"<KERNAL_NAME>\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n }\n}\n"], ["    @dataclass\n    class D:\n        x: list = field(default_factory=list) \n", "   @dataclass\n   class D:\n       x: list = field(default_factory=list)\n\n   assert D().x is not D().x\n"], ["   Year  Month  Day  Hour  Price\n0  2018      1    1     1   6.74\n1  2018      1    1     2   4.74\n2  2018      1    1     3   3.66\n3  2018      1    1     4   2.30\n4  2018      1    1     5   2.30\n5  2018      1    1     6   2.06\n6  2018      1    1     7   2.06\n7  2018      1    1     8   2.06\n8  2018      1    1     9   2.30\n9  2018      1    1    10   2.30\n", "   Year  Month  Day  Hour  Price\n0  2019      1    1     1  66.88\n1  2019      1    1     2  66.88\n2  2019      1    1     3  66.00\n3  2019      1    1     4  63.64\n4  2019      1    1     5  58.85\n5  2019      1    1     6  55.47\n6  2019      1    1     7  56.00\n7  2019      1    1     8  61.09\n8  2019      1    1     9  61.01\n9  2019      1    1    10  61.00\n", "import pandas as pd\n\nframes = [Price2018, Price2019]\n\ndf_merged = pd.concat(frames)\n", "                 Date         1         2  ...        51        52        53\n0 2010-01-01 00:00:00  0.565919  0.892376  ...  0.593049  0.775082  0.680621\n1 2010-01-01 01:00:00  0.358960  0.531418  ...  0.734619  0.480450  0.926735\n2 2010-01-01 02:00:00  0.531870  0.221768  ...  0.902369  0.027840  0.398864\n3 2010-01-01 03:00:00  0.475463  0.245810  ...  0.306405  0.645762  0.541882\n4 2010-01-01 04:00:00  0.954546  0.867960  ...  0.912257  0.039772  0.627696\n", "                     Price\nDate                      \n2010-01-01 00:00:00  29.10\n2010-01-01 01:00:00   9.57\n2010-01-01 02:00:00   0.00\n2010-01-01 03:00:00   0.00\n2010-01-01 04:00:00   0.00\n", "df_merged = pd.merge(Price, Geo, left_index=True, right_on='Date')\n", "   Price                Date         1  ...        51        52        53\n0  29.10 2010-01-01 00:00:00  0.565919  ...  0.593049  0.775082  0.680621\n1   9.57 2010-01-01 01:00:00  0.358960  ...  0.734619  0.480450  0.926735\n2   0.00 2010-01-01 02:00:00  0.531870  ...  0.902369  0.027840  0.398864\n3   0.00 2010-01-01 03:00:00  0.475463  ...  0.306405  0.645762  0.541882\n4   0.00 2010-01-01 04:00:00  0.954546  ...  0.912257  0.039772  0.627696\n"], [], ["  parser.add_argument(\"email\", type=str, required=True)\n+ parser.add_argument(\"email\", type=str, required=True, location='form')\n"], [], [], ["pip install wtforms[email]\n"], [], [], [], ["from django.db import models\nfrom django_enum import EnumField\n\nclass MyModel(models.Model):\n\n    class TextEnum(models.TextChoices):\n\n        VALUE0 = 'V0', 'Value 0'\n        VALUE1 = 'V1', 'Value 1'\n        VALUE2 = 'V2', 'Value 2'\n\n    class IntEnum(models.IntegerChoices):\n\n        ONE   = 1, 'One'\n        TWO   = 2, 'Two',\n        THREE = 3, 'Three'\n\n    # this is equivalent to:\n    #  CharField(max_length=2, choices=TextEnum.choices, null=True, blank=True)\n    txt_enum = EnumField(TextEnum, null=True, blank=True)\n\n    # this is equivalent to\n    #  PositiveSmallIntegerField(choices=IntEnum.choices)\n    int_enum = EnumField(IntEnum)\n", "instance = MyModel.objects.create(\n    txt_enum=MyModel.TextEnum.VALUE1,\n    int_enum=3  # by-value assignment also works\n)\n\nassert instance.txt_enum == MyModel.TextEnum('V1')\nassert instance.txt_enum.label == 'Value 1'\n\nassert instance.int_enum == MyModel.IntEnum['THREE']\nassert instance.int_enum.value == 3\n", "from enum_properties import s\nfrom django_enum import TextChoices  # use instead of Django's TextChoices\nfrom django.db import models\n\nclass TextChoicesExample(models.Model):\n\n    class Color(TextChoices, s('rgb'), s('hex', case_fold=True)):\n\n        # name   value   label       rgb       hex\n        RED     = 'R',   'Red',   (1, 0, 0), 'ff0000'\n        GREEN   = 'G',   'Green', (0, 1, 0), '00ff00'\n        BLUE    = 'B',   'Blue',  (0, 0, 1), '0000ff'\n\n        # any named s() values in the Enum's inheritance become properties on\n        # each value, and the enumeration value may be instantiated from the\n        # property's value\n\n    color = EnumField(Color)\n\ninstance = TextChoicesExample.objects.create(\n    color=TextChoicesExample.Color('FF0000')\n)\nassert instance.color == TextChoicesExample.Color('Red')\nassert instance.color == TextChoicesExample.Color('R')\nassert instance.color == TextChoicesExample.Color((1, 0, 0))\n\n# direct comparison to any symmetric value also works\nassert instance.color == 'Red'\nassert instance.color == 'R'\nassert instance.color == (1, 0, 0)\n\n# save by any symmetric value\ninstance.color = 'FF0000'\n\n# access any enum property right from the model field\nassert instance.color.hex == 'ff0000'\n\n# this also works!\nassert instance.color == 'ff0000'\n\n# and so does this!\nassert instance.color == 'FF0000'\n\ninstance.save()\n\n# filtering works by any symmetric value or enum type instance\nassert TextChoicesExample.objects.filter(\n    color=TextChoicesExample.Color.RED\n).first() == instance\n\nassert TextChoicesExample.objects.filter(color=(1, 0, 0)).first() == instance\n\nassert TextChoicesExample.objects.filter(color='FF0000').first() == instance\n"], ["from django import forms\nfrom django.contrib import admin\nfrom django.contrib.auth.models import Group\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.forms import ReadOnlyPasswordHashField\nfrom django.core.exceptions import ValidationError\n\nfrom customauth.models import MyUser\n\n\nclass UserCreationForm(forms.ModelForm):\n    \"\"\"A form for creating new users. Includes all the required\n    fields, plus a repeated password.\"\"\"\n    password1 = forms.CharField(label='Password', widget=forms.PasswordInput)\n    password2 = forms.CharField(label='Password confirmation', widget=forms.PasswordInput)\n\n    class Meta:\n        model = MyUser\n        fields = ('email', 'date_of_birth')\n\n    def clean_password2(self):\n        # Check that the two password entries match\n        password1 = self.cleaned_data.get(\"password1\")\n        password2 = self.cleaned_data.get(\"password2\")\n        if password1 and password2 and password1 != password2:\n            raise ValidationError(\"Passwords don't match\")\n        return password2\n\n    def save(self, commit=True):\n        # Save the provided password in hashed format\n        user = super().save(commit=False)\n        user.set_password(self.cleaned_data[\"password1\"])\n        if commit:\n            user.save()\n        return user\n\n\nclass UserChangeForm(forms.ModelForm):\n    \"\"\"A form for updating users. Includes all the fields on\n    the user, but replaces the password field with admin's\n    disabled password hash display field.\n    \"\"\"\n    password = ReadOnlyPasswordHashField()\n\n    class Meta:\n        model = MyUser\n        fields = ('email', 'password', 'date_of_birth', 'is_active', 'is_admin')\n\n\nclass UserAdmin(BaseUserAdmin):\n    # The forms to add and change user instances\n    form = UserChangeForm\n    add_form = UserCreationForm\n\n    # The fields to be used in displaying the User model.\n    # These override the definitions on the base UserAdmin\n    # that reference specific fields on auth.User.\n    list_display = ('email', 'date_of_birth', 'is_admin')\n    list_filter = ('is_admin',)\n    fieldsets = (\n        (None, {'fields': ('email', 'password')}),\n        ('Personal info', {'fields': ('date_of_birth',)}),\n        ('Permissions', {'fields': ('is_admin',)}),\n    )\n    # add_fieldsets is not a standard ModelAdmin attribute. UserAdmin\n    # overrides get_fieldsets to use this attribute when creating a user.\n    add_fieldsets = (\n        (None, {\n            'classes': ('wide',),\n            'fields': ('email', 'date_of_birth', 'password1', 'password2'),\n        }),\n    )\n    search_fields = ('email',)\n    ordering = ('email',)\n    filter_horizontal = ()\n\n\n# Now register the new UserAdmin...\nadmin.site.register(MyUser, UserAdmin)\n"], [], ["from django.contrib.auth.admin import UserAdmin as DefaultUserAdmin\n\nclass UserAdmin(DefaultUserAdmin):\n"], ["import uvicorn\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef index():\n    return {\"index\": \"root\"}\n\nif __name__ == '__main__':\n\n    uvicorn.run(f\"{Path(__file__).stem}:app\", host=\"127.0.0.1\", port=8888, reload=True)\n"], ["height = 0\nused_blocks = 0\n\nwhile blocks > 0:\n    used_blocks += 1\n    if blocks >= used_blocks:\n        blocks -=  used_blocks\n    else:\n        break\n    height += 1\n"], ["@router.post(path=\"/test\", tags=['File Upload'])\ndef color_classification_predict(uploadFile: UploadFile):\n    try:\n        if uploadFile.filename:\n            # saved_dir- directory path where we'll save the uploaded file \n            test_filename = os.path.join(saved_dir, uploadFile.filename)\n            with open(test_filename, \"wb+\") as file_object:\n                shutil.copyfileobj(uploadFile.file, file_object)\n    except Exception as e:\n        raise e\n    print('[INFO] Uploaded file saved.')\n"], ["{\n    \"python.defaultInterpreterPath\": \"C:\\\\Users\\\\Talha\\\\.virtualenvs\\\\django-okd21pq9\\\\Scripts\\\\python.exe\"\n}\n"], ["def my_request(protocol_type: Literal[\"http\",\"https\"], url: str):\n"], ["alphabet = 26\nletters_in_word = int(input(\"Enter the number of letters should be in the \npassword : \"))\nno_of_possibilities = alphabet**letters_in_word\nprint(\"No.of possible passwords formed with 6 letters are : \n\",no_of_possibilities)\n"], ["response = requests.post(BASE + 'video/1', json={\"likes\": 10})\n"], ["Could not load dynamic library 'libnvinfer.so.7'\n"], ["cd directory of code\nuvicorn fast-api:main --reload \n"], ["pip3 install itsdangerous==2.0.1\n"], ["def count_substring(string, sub_string):\na=len(sub_string) # for finding the length of substring\nb=[] # initializing a list\nc=0 # initializing a counter variable\nfor i in range(len(string)):\n    if a+i<=len(string):\n        b.append(string[i:a+i])\nfor i in b:\n    if i==sub_string:\n        c=c+1\nreturn c\n\nif __name__ == '__main__':\n   string = input().strip()\n   sub_string = input().strip()\n   count = count_substring(string, sub_string)\n   print(count)\n"], ["project\n  my_module\n    package.py\n  tests\n    __init__.py\n    my_tests.py\n"], [">>> import json\n>>> def encode_complex(obj):\n...     if isinstance(obj, complex):\n...         return [obj.real, obj.imag]\n...     raise TypeError(f'Object of type {obj.__class__.__name__} '\n...                     f'is not JSON serializable')\n...\n>>> json.dumps(2 + 1j, default=encode_complex)\n'[2.0, 1.0]'\n>>> json.JSONEncoder(default=encode_complex).encode(2 + 1j)\n'[2.0, 1.0]'\n>>> ''.join(json.JSONEncoder(default=encode_complex).iterencode(2 + 1j))\n'[2.0, 1.0]'\n\n", "def combine_encoders(*encs):\n    def combined(obj):\n        for enc in encs:\n            try:\n                return enc(obj)\n            except TypeError:\n                pass\n        raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n\n    return combined\n\ncombined_encoders = combine_encoders(encode_complex, encode_path)\nencoded = json.dumps(data, default=combined_encoders)\n"], ["!pip install azure-storage-blob\nfrom azure.storage.blob import BlobServiceClient\nimport pandas as pd\n\nSTORAGEACCOUNTURL= <storage_account_url>\nSTORAGEACCOUNTKEY= <storage_account_key>\nLOCALFILENAME= <local_file_name>\nCONTAINERNAME= <container_name>\nBLOBNAME= <blob_name>\n\n#download from blob\nblob_service_client_instance=BlobServiceClient(account_url=STORAGEACCOUNTURL, credential=STORAGEACCOUNTKEY)\nblob_client_instance = blob_service_client_instance.get_blob_client(CONTAINERNAME, BLOBNAME, snapshot=None)\nwith open(LOCALFILENAME, \"wb\") as my_blob:\n    blob_data = blob_client_instance.download_blob()\n    blob_data.readinto(my_blob)\n\n#import blob to dataframe\ndf = pd.read_csv(LOCALFILENAME)\n"], ["import pandas as pd\nfrom io import BytesIO\nfrom azure.storage.blob import BlobServiceClient\n\nCONNECTION_STRING= <connection_string>\nCONTAINERNAME= <container_name>\nBLOBNAME= <blob_name>\n\nblob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\ncontainer_client = blob_service_client.get_container_client(CONTAINERNAME)\nblob_client = container_client.get_blob_client(BLOBNAME)\n\nwith BytesIO() as input_blob:\n    blob_client.download_blob().download_to_stream(input_blob)\n    input_blob.seek(0)\n    df = pd.read_csv(input_blob)\n"], [], ["FROM public.ecr.aws/lts/ubuntu:latest\n", "FROM python:3.7\n", "ARG REPO=655606377847.dkr.ecr.us-west-2.amazonaws.com\n\nFROM ${REPO}/python:3.7\n"], [], ["from fastapi import FastAPI, File, UploadFile\n\napp = FastAPI()\n\n\n@app.post(\"/upload-file/\")\nasync def create_upload_file(uploaded_file: UploadFile = File(...)):\n    <b>file_location = f\"files/{uploaded_file.filename}\"\n    with open(file_location, \"wb+\") as file_object:\n        file_object.write(uploaded_file.file.read())</b>\n    return {\"info\": f\"file '{uploaded_file.filename}' saved at '{file_location}'\"}", "<b>import shutil</b>\nfrom fastapi import FastAPI, File, UploadFile\n\napp = FastAPI()\n\n\n@app.post(\"/upload-file/\")\nasync def create_upload_file(uploaded_file: UploadFile = File(...)):    \nfile_location = f\"files/{uploaded_file.filename}\"\n    with open(file_location, \"wb+\") as file_object:\n        <b>shutil.copyfileobj(uploaded_file.file, file_object)</b>    \nreturn {\"info\": f\"file '{uploaded_file.filename}' saved at '{file_location}'\"}"], [], ["pip install protobuf==3.20.*\n"], [], ["@app.post(\"/\")\nasync def post_endpoint(in_file: UploadFile=File(...)):\n    # ...\n    async with aiofiles.open(out_file_path, 'wb') as out_file:\n        content = await in_file.read()  # async read\n        await out_file.write(content)  # async write\n\n    return {\"Result\": \"OK\"}\n", "@app.post(\"/\")\nasync def post_endpoint(in_file: UploadFile=File(...)):\n    # ...\n    async with aiofiles.open(out_file_path, 'wb') as out_file:\n        while content := await in_file.read(1024):  # async read chunk\n            await out_file.write(content)  # async write chunk\n\n    return {\"Result\": \"OK\"}\n", "import shutil\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom typing import Callable\n\nfrom fastapi import UploadFile\n\n\ndef save_upload_file(upload_file: UploadFile, destination: Path) -> None:\n    try:\n        with destination.open(\"wb\") as buffer:\n            shutil.copyfileobj(upload_file.file, buffer)\n    finally:\n        upload_file.file.close()\n\n\ndef save_upload_file_tmp(upload_file: UploadFile) -> Path:\n    try:\n        suffix = Path(upload_file.filename).suffix\n        with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n            shutil.copyfileobj(upload_file.file, tmp)\n            tmp_path = Path(tmp.name)\n    finally:\n        upload_file.file.close()\n    return tmp_path\n\n\ndef handle_upload_file(\n    upload_file: UploadFile, handler: Callable[[Path], None]\n) -> None:\n    tmp_path = save_upload_file_tmp(upload_file)\n    try:\n        handler(tmp_path)  # Do something with the saved temp file\n    finally:\n        tmp_path.unlink()  # Delete the temp file\n"], ["blocks = int(input(\"Enter number of blocks: \"))\nprint(f'You can build a pyramid {int(0.5 * ((8 * blocks + 1)**0.5 - 1))} blocks high')\n"], ["blocks=int(input(\"Enter the number of blocks:\"))\nb=blocks\nn=1\nwhile n*(n+1)//2 <=b :\n    h=n\n    n=n+1\nprint(\"The height of the pyramid is\", h)\n"], ["names1 = [{'A':'Jack', 'B':'Jill'}]\n\nnames2 = [{'C':'Tommy', 'D':'Tammy'}]\n\ndf1=pd.DataFrame(names1)\ndf2=pd.DataFrame(names2)\ndf_merged= pd.merge(df1.assign(X=1), df2.assign(X=1), on='X').drop('X', 1)\n", "      A     B      C      D\n0  Jack  Jill  Tommy  Tammy\n"], ["from fastapi import UploadFile\n\nimport shutil\nfrom pathlib import Path\n\ndef save_upload_file(upload_file: UploadFile, destination: Path) -> str:\n    try:\n        with destination.open(\"wb\") as buffer:\n            shutil.copyfileobj(upload_file.file, buffer)\n            file_name = buffer.name\n            print(type(file_name))\n    finally:\n        upload_file.file.close()\n    return file_name\n", "def unique_id():\n    return str(uuid.uuid4())\n\ndef delete_file(filename):\n    os.remove(filename)\n", "@router.post(\"/use_upload_file\", response_model=dict)\nasync def use_uploaded_file(\n    file_one: UploadFile = File(),\n    file_two: UploadFile = File()\n    ):\n\n\n    file_one_path = save_upload_file(audio_one, Path(f\"{unique_id()}\"))\n    file_two_path = save_upload_file(audio_two, Path(f\"{unique_id()}\"))\n\n    result = YourFunctionThatUsestheSaveFile(audio_one_path, audio_two_path)\n\n    delete_file(audio_one_path)\n    delete_file(audio_two_path)\n\n    return result\n"], [], [], ["Name: openai\nVersion: 0.22.0\nSummary: Python client library for the OpenAI API\nHome-page: https://github.com/openai/openai-python\nAuthor: OpenAI\nAuthor-email: support@openai.com\nLicense: \nLocation: /Users/<USER>/DIR/TO/SOME/PYTHON/site-packages\nRequires: numpy, openpyxl, pandas, pandas-stubs, requests, tqdm\nRequired-by:\n"], [], ["import xlwings as xw\ndef df_to_excel_util(excel,sheet_to_dataFrame_map):\n\n    with xw.App(visible=False) as app:\n        wb = app.books.open(excel)            \n        current_sheets = [sheet.name for sheet in wb.sheets]\n        \n        for sheet_name in sheet_to_dataFrame_map.keys():\n            if sheet_name in  current_sheets:\n                wb.sheets[sheet_name].delete()\n            \n            new_sheet = wb.sheets.add(after=wb.sheets.count)\n            new_sheet.range('A1').value = sheet_to_dataFrame_map.get(sheet_name)\n            new_sheet.name = sheet_name\n        wb.save()\n"], [">>> student.year_in_school\n'FR'\n"], ["from pytube import YouTube\nfrom pytube import Playlist\n\n\nSAVE_PATH = \"E:/YouTube\" #to_do\n\n\n#link of the video to be downloaded\nlinks= \"https://youtube.com/playlist?list=PLblh5JKOoLUL3IJ4- yor0HzkqDQ3JmJkc\"\n\nplaylist = Playlist(links)\n\nPlayListLinks = playlist.video_urls\nN = len(PlayListLinks)\n#print('Number of videos in playlist: %s' % len(PlayListLinks))\n\nprint(f\"This link found to be a Playlist Link with number of videos equal to {N} \")\nprint(f\"\\n Lets Download all {N} videos\")\n\nfor i,link in enumerate(PlayListLinks):\n\n    yt = YouTube(link)\n    d_video = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n    d_video.download(SAVE_PATH)\n    print(i+1, ' Video is Downloaded.')\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))\n\nheight = 0\n\nlast_row_num = 0\n\nLRN = last_row_num\n\nwhile blocks > LRN:\n    LRN = LRN + 1\n    height = height + 1\n    blocks = blocks - LRN\n    if blocks <= LRN:\n        break\n\nprint(\"The height of the pyramid:\", height)\n"], [], [], ["[tool.setuptools.package-data]\nmyModule = [\"*.csv\"]\n"], ["alpate = 26\n\nletters = int(input(\"your number of letters = \"))\n\npossible_passwords = alpate**letters\n\nNo_passwords = ('possible passwords that can be formed with ' + str(letters) + 'is ' + str(possible_passwords))\n\nprint(No_passwords)\n"], [], ["def load_chrome_driver(headless):\n    chrome_loc = \"/home/ubuntu/Downloads/chromium-browser/\"\n    chrome_path = chrome_loc + \"chrome\"\n    chromedriver_path = chrome_loc + \"chromedriver\"\n    user_data_dir = \"/home/ubuntu/.config/chromium/custom_user\"\n    options = webdriver.ChromeOptions()\n    if headless:\n        options.add_argument('headless')\n    options.add_argument('--profile-directory=Default')\n    options.add_argument(\"--start-maximized\")\n    options.add_argument('--disable-gpu')\n    options.add_argument('--no-sandbox')\n    options.add_argument(\"--disable-dev-shm-usage\")\n    options.add_argument('--log-level=3')\n    options.binary_location = chrome_path\n    options.user_data_dir = user_data_dir\n    driver = ucdriver.Chrome(\n        executable_path=chromedriver_path, options=options)\n    driver.set_window_size(1920, 1080)\n    driver.set_window_position(0, 0)\n    return driver\n"], [], [], [], [], [], ["import requests, json\n\nBASE = \"http://127.0.0.1:5000/\"\n# Set request's header.\nheaders = {\"Content-Type\": \"application/json; charset=utf-8\"}\n# Set data.\ndata = {\"likes\": 10}\n# \nresponse = requests.post(BASE + 'video/1', headers=headers, json=data)\n\nprint(\"Status Code: \", response.status_code)\nprint(\"JSON Response: \", response.json())\n"], [], ["/*Given n objects, with each object has width wi. We need to arrange them in a pyramidal way such that : \n    Total width of ith is less than (i + 1)th.\n    Total number of objects in the ith is less than (i + 1)th.*/\npublic class Max_Height_Pyramid \n{\n    static int pyramidHeight(int n,int count)\n    {\n        while(n>=count)\n        {\n            n=n-count;\n            count++;\n            pyramidHeight(n,count);\n        }\n        return count-1;\n    }\n    public static void main(String args[])\n    {\n        int[] boxes= {10,20,30,50,60,70};\n        int n=boxes.length;\n        int count=1;\n        int result=pyramidHeight(n,count);\n        System.out.println(result);\n    }\n\n}\n\n"], [], [], [], ["dict = {'Name':['Martha', 'Tim', 'Rob', 'Georgia'],\n        'Maths':[87, 91, 97, 95],\n        'Science':[83, 99, 84, 76]\n       }\ndf = pd.DataFrame(dict)\n\noutput['OP.pool_results'] = df.values.to_list()\n", "Traceback (most recent call last):\n\n  File \"C:\\Users\\hshrima\\AppData\\Local\\Temp/ipykernel_24028/1619690827.py\", line 1, in <module>\n    df.values[0].to_list()\n\nAttributeError: 'numpy.ndarray' object has no attribute 'to_list'\n", "df.values.tolist()\n[['Martha', 87, 83], ['Tim', 91, 99], ['Rob', 97, 84], ['Georgia', 95, 76]]\n"], ["np.random.seed(0)\nleft = pd.DataFrame({'key': ['A', 'B', 'C', 'D'], 'value': np.random.randn(4)})\nright = pd.DataFrame({'key': ['B', 'D', 'E', 'F'], 'value': np.random.randn(4)})\n\nleft\n\n  key     value\n0   A  1.764052\n1   B  0.400157\n2   C  0.978738\n3   D  2.240893\n\nright\n\n  key     value\n0   B  1.867558\n1   D -0.977278\n2   E  0.950088\n3   F -0.151357\n", "left.merge(right, on='key')\n# Or, if you want to be explicit\n# left.merge(right, on='key', how='inner')\n\n  key   value_x   value_y\n0   B  0.400157  1.867558\n1   D  2.240893 -0.977278\n", "left.merge(right, on='key', how='left')\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n1   B  0.400157  1.867558\n2   C  0.978738       NaN\n3   D  2.240893 -0.977278\n", "left.merge(right, on='key', how='right')\n\n  key   value_x   value_y\n0   B  0.400157  1.867558\n1   D  2.240893 -0.977278\n2   E       NaN  0.950088\n3   F       NaN -0.151357\n", "left.merge(right, on='key', how='outer')\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n1   B  0.400157  1.867558\n2   C  0.978738       NaN\n3   D  2.240893 -0.977278\n4   E       NaN  0.950088\n5   F       NaN -0.151357\n", "(left.merge(right, on='key', how='left', indicator=True)\n     .query('_merge == \"left_only\"')\n     .drop('_merge', 1))\n\n  key   value_x  value_y\n0   A  1.764052      NaN\n2   C  0.978738      NaN\n", "left.merge(right, on='key', how='left', <b>indicator=True</b>)\n\n  key   value_x   value_y     _merge\n0   A  1.764052       NaN  left_only\n1   B  0.400157  1.867558       both\n2   C  0.978738       NaN  left_only\n3   D  2.240893 -0.977278       both", "(left.merge(right, on='key', how='right', <b>indicator=True</b>)\n     .query('_merge == \"right_only\"')\n     .drop('_merge', 1))\n\n  key  value_x   value_y\n2   E      NaN  0.950088\n3   F      NaN -0.151357", "(left.merge(right, on='key', how='outer', indicator=True)\n     .query('_merge != \"both\"')\n     .drop('_merge', 1))\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n2   C  0.978738       NaN\n4   E       NaN  0.950088\n5   F       NaN -0.151357\n", "left2 = left.rename({'key':'keyLeft'}, axis=1)\nright2 = right.rename({'key':'keyRight'}, axis=1)\n\nleft2\n\n  keyLeft     value\n0       A  1.764052\n1       B  0.400157\n2       C  0.978738\n3       D  2.240893\n\nright2\n\n  keyRight     value\n0        B  1.867558\n1        D -0.977278\n2        E  0.950088\n3        F -0.151357\n", "left2.merge(right2, left_on='keyLeft', right_on='keyRight', how='inner')\n\n  keyLeft   value_x keyRight   value_y\n0       B  0.400157        B  1.867558\n1       D  2.240893        D -0.977278\n", "left3 = left2.set_index('keyLeft')\nleft3.merge(right2, left_index=True, right_on='keyRight')\n\n    value_x keyRight   value_y\n0  0.400157        B  1.867558\n1  2.240893        D -0.977278\n", "right3 = right.assign(newcol=np.arange(len(right)))\nright3\n  key     value  newcol\n0   B  1.867558       0\n1   D -0.977278       1\n2   E  0.950088       2\n3   F -0.151357       3\n", "left.merge(right3[['key', 'newcol']], on='key')\n\n  key     value  newcol\n0   B  0.400157       0\n1   D  2.240893       1\n", "# left['newcol'] = left['key'].map(right3.set_index('key')['newcol']))\nleft.assign(newcol=left['key'].map(right3.set_index('key')['newcol']))\n\n  key     value  newcol\n0   A  1.764052     NaN\n1   B  0.400157     0.0\n2   C  0.978738     NaN\n3   D  2.240893     1.0\n", "left.merge(right3[['key', 'newcol']], on='key', how='left')\n\n  key     value  newcol\n0   A  1.764052     NaN\n1   B  0.400157     0.0\n2   C  0.978738     NaN\n3   D  2.240893     1.0\n", "left.merge(right, on=['key1', 'key2'] ...)\n", "left.merge(right, left_on=['lkey1', 'lkey2'], right_on=['rkey1', 'rkey2'])\n"], ["height = 0\nwidth = 1\nnext_width = 1\n        \nwhile blocks >= next_width:\n    blocks - width\n    height += 1\n    blocks -= next_width\n    next_width += 1\n           \nprint(\"The height of the pyramid:\", height)\n"], [], ["height = 1\ntotalBlocks = 0\n\nblocks = int(input(\"Enter the number of bricks:\"))\n\nwhile totalBlocks + height <= blocks:\n    totalBlocks += height\n    height += 1\n\nprint(\"The height of the pyramid is:\", height - 1)\n"], ["gcc: error trying to exec 'cc1plus': execvp: No such file or directory\nerror: command 'gcc' failed with exit status 1\n", "src/pyodbc.h:56:10: fatal error: sql.h: No such file or directory\n#include <sql.h>\n", "FROM python:3.8.3-slim \n\nRUN apt-get update \\\n&& apt-get -y install g++ libpq-dev gcc unixodbc unixodbc-dev\n"], ["conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n"], ["import os \nos.path.relpath(\"<path to the directory>/res10_300x300_ssd_iter_140000.caffemodel\")\n"], [], ["pil_img = Image.fromarray(np_img, 'RGB')\n"], [], ["uvicorn src/main:app --reload    \n", "uvicorn src.main:app --reload \n", "app = FastAPI()\n", "(venv) <username>@<pcname>:~/PycharmProjects/project_folder$ uvicorn src.main:app --reload\n"], ["aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n", "docker pull amazoncorretto:11-alpine\n", "docker images\n", "REPOSITORY                                                         TAG             IMAGE ID       CREATED         SIZE\namazoncorretto                                                     11-alpine       e9ae3c220b23   7 weeks ago     325MB\n", "docker tag e9ae3c220b23 public.ecr.aws/registry_alias/my-web-app\ndocker push public.ecr.aws/registry_alias/my-web-app\n"], ["from dataclasses import dataclass, field\nfrom typing import Optional, Union\nfrom dataclasses_json import LetterCase, dataclass_json, config\nimport pprint\n\n\ndef ExcludeIfNone(value):\n    \"\"\"Do not include field for None values\"\"\"\n    return value is None\n\n@dataclass_json(letter_case=LetterCase.CAMEL)\n@dataclass\nclass Op:\n    required: str\n    optional: Optional[int] = field(metadata=config(exclude=ExcludeIfNone), default=None)\n\n\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(clearsale.Op(required=\"test\").to_dict())\npp.pprint(clearsale.Op(required=\"test\", optional=10).to_dict())\n\nOutput:\n{'required': 'test'}\n{'optional': 10, 'required': 'test'}\n"], ["import pandas as pd\nimport numpy as np\n\nnp.random.seed([3, 14])\nleft = pd.DataFrame(data={'value': np.random.randn(4)}, \n                    index=['A', 'B', 'C', 'D'])    \nright = pd.DataFrame(data={'value': np.random.randn(4)},  \n                     index=['B', 'D', 'E', 'F'])\nleft.index.name = right.index.name = 'idxkey'\n\nleft\n           value\nidxkey          \nA      -0.602923\nB      -0.402655\nC       0.302329\nD      -0.524349\n\nright\n \n           value\nidxkey          \nB       0.543843\nD       0.013135\nE      -0.326498\nF       1.385076\n", "left.merge(right, left_index=True, right_index=True)\n\n         value_x   value_y\nidxkey                    \nB      -0.402655  0.543843\nD      -0.524349  0.013135\n", " left.join(right, how='inner', lsuffix='_x', rsuffix='_y')\n\n          value_x   value_y\n idxkey                    \n B      -0.402655  0.543843\n D      -0.524349  0.013135\n", " left.join(right)\n ValueError: columns overlap but no suffix specified: Index(['value'], dtype='object')\n", " left.rename(columns={'value':'leftvalue'}).join(right, how='inner')\n\n         leftvalue     value\n idxkey                     \n B       -0.402655  0.543843\n D       -0.524349  0.013135\n", " pd.concat([left, right], axis=1, sort=False, join='inner')\n\n            value     value\n idxkey                    \n B      -0.402655  0.543843\n D      -0.524349  0.013135\n", "right2 = right.reset_index().rename({'idxkey' : 'colkey'}, axis=1)\nright2\n \n  colkey     value\n0      B  0.543843\n1      D  0.013135\n2      E -0.326498\n3      F  1.385076\n\nleft.merge(right2, left_index=True, right_on='colkey')\n\n    value_x colkey   value_y\n0 -0.402655      B  0.543843\n1 -0.524349      D  0.013135\n", "left.merge(right, on='idxkey')\n\n         value_x   value_y\nidxkey                    \nB      -0.402655  0.543843\nD      -0.524349  0.013135\n", "left.merge(right2, left_on='idxkey', right_on='colkey')\n\n    value_x colkey   value_y\n0 -0.402655      B  0.543843\n1 -0.524349      D  0.013135\n"], [], [], ["import re\nfrom pytube import Playlist\n\nDOWNLOAD_DIR = input (\"Download dir\")\n\nplaylist = input (\"Link:\")\n\n\nplaylist._video_regex = re.compile(r\"\\\"url\\\":\\\"(/watch\\?v=[\\w-]*)\")\n\nprint(len(playlist.video_urls))\n\nfor url in playlist.video_urls:\n    print(url)\n\nfor video in playlist.videos:\n    audioStream = video.streams.get_highest_resolution()\n    audioStream.download(output_path=DOWNLOAD_DIR)\n"], [], ["pip uninstall python3-protobuf\npip uninstall protobuf\n", "pip install protobuf\n", "pip freeze\n"], [], ["![picTitle](attachment:ch01_05.jpg)\n", " ![picTitle](ch01_05.jpg)\n"], [], ["prototxtPath = \"face_detector/deploy.prototxt\"\nweightsPath = \"face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\nfaceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n\nmaskNet = load_model(\"mask_detector.model\")\n\nprint(\"[INFO] starting video stream...\")\nvs = VideoStream(src=0).start()\n"], [], ["pip install pytube\n", "D:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pytube\\\n", "function_patterns = [\n        r'a\\.[a-zA-Z]\\s*&&\\s*\\([a-z]\\s*=\\s*a\\.get\\(\"n\"\\)\\)\\s*&&\\s*.*\\|\\|\\s*(.*)\\(',\n        r'\\([a-z]\\s*=\\s*([a-zA-Z0-9$]{3})(\\[\\d+\\])?\\([a-z]\\)',\n    ]\n", "from pytube import Playlist\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLwdnzlV3ogoXUifhvYB65lLJCZ74o_fAk')\n\nplaylist._video_regex = re.compile(r\"\\\"url\\\":\\\"(/watch\\?v=[\\w-]*)\")\n\nprint(len(playlist.video_urls))\n\nfor url in playlist.video_urls:\n    print(url)\n\nfor video in playlist.videos:\n    video.streams.get_highest_resolution().download()\n"], ["#enums.py\nclass TransactionType(Enum):\n    IN = \"IN\"\n    OUT = \"OUT\"\n\n    @classmethod\n    def choices(cls):\n        return [(i, i.value) for i in cls]\n"], ["import pandas as pd\npd.__version__\n", "%pip install pandas==1.4.1\n"], [], [], [], [], ["n = int(input(\"Enter the number of blocks: \"))\nheight=0\ni=0\nwhile n>i:\n      i+=1\n      n=n+1\n      height+=1 \nprint(\"The height of the pyramid:\", height)\n\nthank you\n"], [], ["mydata = pd.read_csv(\n        f\"abfs://{blob_path}\",\n        storage_options={\n            \"connection_string\": os.environ[\"STORAGE_CONNECTION\"]\n    })\n"], [], ["  watch(%r{.+\\.py})\n  watch(%r{webface/.+\\.(css|js|html(\\.j2)?)})\n"], ["pip install aws-psycopg2\n"], [], [], ["from fastapi import FastAPI\nfrom slowapi.errors import RateLimitExceeded\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\n\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n@app.get(\"/home\")\n@limiter.limit(\"5/minute\")\nasync def homepage(request: Request):\n    return PlainTextResponse(\"test\")\n\n@app.get(\"/mars\")\n@limiter.limit(\"5/minute\")\nasync def homepage(request: Request, response: Response):\n    return {\"key\": \"value\"}\n"], ["# Did not work\n- same_name_project/\n    - same_name_project/\n    - tests/\n\n# Worked\n- different_named_project/\n    - a_unique_directory/\n    - tests/\n"], ["pip install celery==5.2.3\n"], ["celery==5.2.3\n", "pip install celery==5.2.3\n"], [], [], ["import enum\n\nfrom django.contrib.postgres.fields import ArrayField\nfrom django.db import models\nfrom django.utils.translation import gettext_lazy as _\n\n\nclass NotificationTemplate(models.Model):\nclass Meta:\n    verbose_name = _('notification template')\n    verbose_name_plural = _('notification templates')\n\n@enum.unique\nclass Name(str, enum.Enum):\n    ONBOARDING = 'onboarding'\n    TG_ERROR = 'tg_error'\n    FB_ERROR = 'fb_error'\n\n    @classmethod\n    def choices(cls):\n        return [(item.value, item.name) for item in cls]\n\n@enum.unique\nclass Type(int, enum.Enum):\n    PUSH = 1\n    EMAIL = 2\n    TELEGRAM = 3\n    VK = 4\n    OTHER = 5\n\n    @classmethod\n    def choices(cls):\n        return [(item.value, item.name) for item in cls]\n\nname = models.CharField(_('notification name'), max_length=64, unique=True, choices=Name.choices(), default=Name.ONBOARDING)\ntemplate_type = ArrayField(models.PositiveSmallIntegerField(_('type'), choices=Type.choices()))\nmax_count = models.PositiveSmallIntegerField(default=1)\n\ndef __str__(self):\n    return self.Name(self.name).name\n"], [], ["X_train = X_train.astype(np.float32)\n"], [], ["def count_substring(string, sub_string):\n  total = 0\n  for i in range(len(string)):\n      if string[i:].startswith(sub_string):\n          total += 1\n  return total\n"], ["def count_substring(string, sub_string):\n    n = len(sub_string)\n    ans = 0\n    for i in range(0,len(string)):\n        if(i+n > len(string)):#out of range problem\n            break\n        ans+=string.count(sub_string,i,i+n)\n    return ans\n"], [], [], [], ["# first solution\nnew_list_of_dictionaries = []\nfor dictionary in orig:\n    new_dictionary = {}\n    for k, v in dictionary.items():\n        new_dictionary[k.replace(\".\", \"_\")] = v.replace(\".\", \"_\")\n    new_list_of_dictionaries.append(new_dictionary)\norig = new_list_of_dictionaries\n# second_solution\nimport json\norig = json.loads(json.dumps(orig).replace(\".\", \"_\"))\n"], ["orig= [{\"health\": \"good\", \"status\": \"up\", \"date\":\"2022.03.10\",\"device.id\":\"device01\"}, {\"health\": \"poor\", \"status\": \"down\", \"date\":\"2022.03.10\",\"device.id\":\"device02\"}]\n\nresult = []\nfor inner_dict in orig:\n    new_inner = {}\n    for k, v in inner_dict.items():\n        new_inner[k.replace('.', '_')] = v.replace('.', '_')\n    result.append(new_inner)\n\nprint(result)\n", "[{'health': 'good', 'status': 'up', 'date': '2022_03_10', 'device_id': 'device01'}, {'health': 'poor', 'status': 'down', 'date': '2022_03_10', 'device_id': 'device02'}]\n"], ["def convert(list_dict, old_text, new_text):\n    def replace_dict(old_dict, old_text, new_text):\n        return {key.replace(old_text, new_text) : val.replace(old_text, new_text) for key, val in old_dict.items()}\n        \n    for i in range(len(list_dict)):\n        list_dict[i] = replace_dict(list_dict[i], old_text, new_text)\n\norig= [{\"health\": \"good\", \"status\": \"up\", \"date\":\"2022.03.10\",\"device.id\":\"device01\"}, {\"health\": \"poor\", \"status\": \"down\", \"date\":\"2022.03.10\",\"device.id\":\"device02\"}]\n\nconvert(orig, '.', '-')\nprint(orig)\n"], [], ["pip install Flask=2.0.3\n"], ["import pytube\nfrom pytube import Playlist\nplaylist = Playlist('plylist link')\nnum = 0\nfor v in playlist.videos:\n    print(v.watch_url)\n    one = pytube.YouTube(v.watch_url)\n    one_v = one.streams.get_highest_resolution()\n    name = f\"{0}\" + one_v.default_filename\n    one_v.download()\n    num = num + 1\n"], [], [], ["test/test2$ pylint test2.py  is wrong.\n\ntest$ pylint test2.py is correct.\n"], ["{\n    // Use IntelliSense to learn about possible attributes.\n    // Hover to view descriptions of existing attributes.\n    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: FastAPI\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"uvicorn\",\n            \"args\": [\n                \"main:app\"\n            ],\n            \"jinja\": true\n        }\n    ]\n}\n", "from fastapi import FastAPI\napp = FastAPI(\n    title=\"test\",\n    description=\"test\",\n    version=\"0.0.1\",\n)\nif __name__ == \"__main__\":\nimport uvicorn\n\nuvicorn.run(\n    \"main:app\",\n    host=\"0.0.0.0\",\n    reload=True,\n    port=3001,\n)\n"], [], [], [], [], [], ["myproject/\n  helpers/\n    moduleone.py\n    moduletwo.py\n  tests/\n    myproject_test.py\n  conftest.py\n", "pytest_plugins = ['helpers']\n"], ["flaskapp\n- src\n  -- app.py\n  -- utils\n  -- ...\n- tests\ndocs\nvenv\n", "import src.utils.calculator\n", "import sys, os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))\n"], [], ["from html2image import Html2Image\nhti = Html2Image()\nwith open('./test.html') as f:\n    hti.screenshot(f.read(), save_as='out.png')\n"], ["T = int(input())  #Enter the No. of Testcases\ninput_list = [[j for j in input().split(' ')] for i in range(T)]\n"], [], ["import pytest\nimport unittest\ntry:\n    # python 3.4+ should use builtin unittest.mock not mock package\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\n\n\n@patch(\"methylcheck.qc_plot.plt.show\")\ndef test_plot_fn(mock_this):\n   plot_fn()\n   plt.close('all')\n"], ["python_files = test_*.py\n"], [], [], ["set http_proxy=http://your_corp_username:password@<your_corp_proxy_host>:<port>\nset https_proxy=https://your_corp_username:password@<your_corp_proxy_host>:<port>\n", "pip --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org install <some_package>\n", "pip --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org install <some_package>  --user\n"], [], [], ["HTTPS_PROXY=\"http://username:password@proxy.example.com:8080\"\nhttps_proxy=\"http://username:password@proxy.example.com:8080\"\n", "HTTPS_PROXY=\"https://username:password@proxy.example.com:8080\"\nhttps_proxy=\"https://username:password@proxy.example.com:8080\"\n"], [], [], [], [], [], ["from json import JSONEncoder\nfrom typing import Any, Type\n\nclass CombinedEncoder():\n    \"\"\"\n    Combine multiple JSON encoders\n    https://stackoverflow.com/questions/65338261/combine-multiple-json-encoders\n    \"\"\"\n    def __new__(cls, *encoders: Type[JSONEncoder]):\n        def default(o: Any, *args: bool, **kwargs: bool) -> str | Any:\n            for encoder in encoders:\n                try:\n                    return encoder(*args, **kwargs).default(o)\n                except TypeError:\n                    pass\n            raise TypeError(\n                f'Object of type {o.__class__.__name__} is not JSON serializable')\n\n        encoder = type(\n            \"CombinedEncoder\",\n            (JSONEncoder,),\n            {\n                \"default\": default,\n                \"__doc__\": \"Combines JSONEncoders\"\n            }\n        )\n        return encoder\n"], ["  height +=1\n  blocks = blocks - height\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))\nlayer_block = 0\ni = 1\nheight = 0\nwhile True:\n    layer_block = layer_block + i\n    i = i + 1\n    if layer_block > blocks:\n        break\n    height = height + 1\nprint(\"The height of the pyramid:\", height)\n"], ["echo $PATH\nC:\\Users\\blah\\Documents\\blah\\Stock-down\\Dev\\this_api\\venv/Scripts:/c/Users/blah/bin:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/mingw64/bin:/usr/bin:/c/Users/blah/bin:/c/Program Files/Go/bin:/c/Python39/Scripts: \n", "which python\nC:\\Users\\blah\\Documents\\blah\\Stock-down\\Dev\\this_api\\venv/Scripts/C/Users/blah/Documents/blah/Stock-down/Dev/this_api/venv/Scripts/python.exe\n", "#unset irrelevant variables\ndeactivate nondestructive\n\n#VIRTUAL_ENV=\"C:\\Users\\blah\\Documents\\blah\\Stock-down\\Dev\\this_api\\venv\"\nVIRTUAL_ENV=\"/c/Users/blah/Documents/blah/Stock-down/Dev/st_api/venv\"\nexport VIRTUAL_ENV\n\n_OLD_VIRTUAL_PATH=\"$PATH\"\nPATH=\"$VIRTUAL_ENV/Scripts:$PATH\"\nexport PATH\n", "echo $PATH\n/c/Users/blah/Documents/blah/Stock-down/Dev/st_api/venv/Scripts:/c/Users/blah/bin:/mingw64/bin:/usr/local/bin:/usr/bin:/bin:/mingw64/bin:/usr/bin:/c/Users/blah/bin:/c/Program Files/Go/bin:/c/Python39/Scripts: \n"], ["conda install -c pytorch pytorch\n"], [], ["app.add_middleware(\n    RateLimitMiddleware,\n    authenticate=AUTH_FUNCTION,\n    backend=RedisBackend(),\n    config={\n        r\"^/user\": [Rule(second=5, block_time=60)],\n    },\n)\n"], ["driver = webdriver.Chrome(executable_path='C:\\Program Files\\Chrome Driver\\chromedriver.exe')\n", "service = Service(executable_path='C:\\Program Files\\Chrome Driver\\chromedriver.exe')\ndriver = webdriver.Chrome(service=service)\n", "service = Service('C:\\Program Files\\Chrome Driver\\chromedriver.exe')\n"], ["from walrus import Database, RateLimitException\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nimport uvicorn\n\ndb = Database()\nrate = db.rate_limit('xxx', limit=5, per=60)  # in 60s just can only click 5 times\n\napp = FastAPI()\n\n\n@app.exception_handler(RateLimitException)\ndef parse_rate_litmit_exception(request: Request, exc: RateLimitException):\n    msg = {'success': False, 'msg': f'please have a tea for sleep, your ip is: {request.client.host}.'}\n    return JSONResponse(status_code=429, content=msg)\n\n\n@app.get('/')\ndef index():\n    return {'success': True}\n\n\n@app.get('/important_api')\n@rate.rate_limited(lambda request: request.client.host)\ndef query_important_data(request: Request):\n    data = 'important data'\n    return {'success': True, 'data': data}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"code1228:app\", debug=True, reload=True)\n\n"], [], [" fastapi import (\n    FastAPI\n    UploadFile,\n    File,\n    status\n)\nfrom fastapi.responses import JSONResponse\n\nimport aiofiles\napp = FastAPI( debug = True ) \n\n@app.post(\"/upload_file/\", response_description=\"\", response_model = \"\")\nasync def result(file:UploadFile = File(...)):\n     try:\n        async with aiofiles.open(file.filename, 'wb') as out_file:\n            content = await file.read()  # async read\n            await out_file.write(content)  # async write\n\n    except Exception as e:\n        return JSONResponse(\n            status_code = status.HTTP_400_BAD_REQUEST,\n            content = { 'message' : str(e) }\n            )\n    else:\n        return JSONResponse(\n            status_code = status.HTTP_200_OK,\n            content = {\"result\":'success'}\n            )\n", " fastapi import (\n    FastAPI\n    UploadFile,\n    File,\n    status\n)\nfrom fastapi.responses import JSONResponse\n\nimport aiofiles\napp = FastAPI( debug = True ) \n@router.post(\"/upload_multiple_file/\", response_description=\"\", response_model = \"\")\n\nasync def result(files:List[UploadFile] = File(...), secret_key: str = Depends(secretkey_middleware)):\n    try:\n        \n        for file in files:\n\n            async with aiofiles.open(eventid+file.filename, 'wb') as out_file:\n                content = await file.read() \n                await out_file.write(content) \n                \n\n\n        pass\n    except Exception as e:\n      \n        return JSONResponse(\n            status_code = status.HTTP_400_BAD_REQUEST,\n            content = { 'message' : str(e) }\n            )\n    else:\n        return JSONResponse(\n            status_code = status.HTTP_200_OK,\n            content = {\"result\":'result'}\n            )\n\n"], [], ["model = tf.keras.Model(...)\nmodel.save_weights(\"some_path\")\n...\nmodel.load_weights(\"some_path\")\n"], [], ["import undetected_chromedriver as uc\nfrom selenium import webdriver\n\noptions = uc.ChromeOptions()\noptions.add_argument(\"--ignore-certificate-error\")\noptions.add_argument(\"--ignore-ssl-errors\")\n# e.g. Chrome path in Mac =/Users/x/Library/xx/Chrome/Default/\noptions.add_argument( \"--user-data-dir=<Your chrome profile>\")\ndriver = uc.Chrome(options=options)\nurl='https://accounts.google.com/servicelogin'\ndriver.get(url)\n"], [], [">>> a = 'apples and avocados and avocados and apples'\n>>> b = a.replace('apples', '#IamYourFather#').replace('avocados', 'apples').replace('#IamYourFather#', 'avocados')\n>>> print(b)\navocados and apples and apples and avocados\n"], [], ["$ export npm_config_python=/path/to/python\n", "$ npm config list\n...\n\n; environment configs\npython = \"/path/to/python\"\n", "python = \"/path/to/python\"\n", "npm config --global set python /path/to/python\n"], ["export PATH=/usr/bin/python:$PATH\n", "export PYTHON=/usr/bin/python\n"], ["def swap_words_regex1(s, x, y):\n    return re.sub(re.escape(x) + '|' + re.escape(y),\n                  lambda m: (x if m[0] == y else y),\n                  s)\n\ndef swap_words_regex2(s, x, y):\n    return re.sub(f'({re.escape(x)})|{re.escape(y)}',\n                  lambda m: x if m[1] is None else y,\n                  s)\n\ndef swap_words_replaces(s, x, y):\n    return s.replace(x, chr(0)).replace(y, x).replace(chr(0), y)\n", " 3.7 ms  1966 kB  swap_words_split\n10.7 ms  2121 kB  swap_words_regex1\n17.8 ms  2121 kB  swap_words_regex2\n 1.3 ms   890 kB  swap_words_replaces\n", "from timeit import repeat\nimport re\nimport tracemalloc as tm\n\ndef swap_words_split(s, x, y):\n    return y.join(part.replace(y, x) for part in s.split(x))\n\ndef swap_words_regex1(s, x, y):\n    return re.sub(re.escape(x) + '|' + re.escape(y),\n                  lambda m: (x if m[0] == y else y),\n                  s)\n\ndef swap_words_regex2(s, x, y):\n    return re.sub(f'({re.escape(x)})|{re.escape(y)}',\n                  lambda m: x if m[1] is None else y,\n                  s)\n\ndef swap_words_replaces(s, x, y):\n    return s.replace(x, chr(0)).replace(y, x).replace(chr(0), y)\n\nfuncs = swap_words_split, swap_words_regex1, swap_words_regex2, swap_words_replaces\n\nargs = 'apples and avocados and bananas and oranges and ' * 10000, 'apples', 'avocados'\n\nfor _ in range(3):\n    for func in funcs:\n        t = min(repeat(lambda: func(*args), number=1))\n        tm.start()\n        func(*args)\n        memory = tm.get_traced_memory()[1]\n        tm.stop()\n        print(f'{t * 1e3:4.1f} ms  {memory // 1000:4} kB  {func.__name__}')\n    print()\n"], ["def swap_words(s, x, y):\n    return y.join(part.replace(y, x) for part in s.split(x))\n", ">>> swap_words('apples and avocados and avocados and apples', 'apples', 'avocados')\n'avocados and apples and apples and avocados'\n>>>\n"], ["def swapwords(mystr, firstword, secondword):\n    splitstr = mystr.split(\" \")\n\n    for i in range(len(splitstr)):\n        if splitstr[i] == firstword:\n            splitstr[i] = secondword\n            i+=1\n        if splitstr[i] == secondword:\n            splitstr[i] = firstword\n            i+=1\n\n    newstr = \" \".join(splitstr)\n\n   return newstr\n"], [], ["# initiate the driver with undetetcted_chromedriver\nimport undetected_chromedriver.v2 as uc\ndriver = uc.Chrome()\n\n# operate the driver as you would with selenium\ndriver.get('https://my-url.com') \n\n# Example use of selenium imports to be used with the driver\nfrom selenium.common.exceptions import NoSuchElementException\nfrom selenium.webdriver.common.by import By\n\ntry:\n    driver.find_element(By.XPATH, '//*[@id=\"my-id\"]').click()\nexcept NoSuchElementException:\n    print(\"No Such Element Exception\")\n"], ["import os\n\nprototxtPath = os.path.join(os.getcwd(), 'face_detector', 'deploy.prototxt')\nweightsPath = os.path.join(os.getcwd(), 'face_detector', 'res10_300x300_ssd_iter_140000.caffemodel')\n\nfaceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n\nprint(\"[INFO] starting video stream...\")\nvs = VideoStream(src=0).start()\n"], [], ["from selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options\n\noptions = Options()\noptions.binary_location = r'C:\\Program Files\\Mozilla Firefox\\firefox.exe'\ndriver = webdriver.Firefox(options=options)\ndriver.get('http://google.com/')\n"], ["%matplotlib inline\nfrom IPython.display import Image\nImage('image_path')\n"], ["from sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport torch\n\na = np.array([0, 1,2])\nb = [[0, 1,2], [4, 5,6], [7,8,9]]\n\nbb = np.zeros((3,3))\nfor i in range(0, len(b)):\n    bb[i,:] = np.array(b[i])\n\n\na = torch.from_numpy(a)\nb = torch.from_numpy(bb)\n\na= a.float()\nb = b.float()\n\ncosine_scores = util.pytorch_cos_sim(b, a)\nprint(cosine_scores)\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))\nheight = 0\nwhile blocks > 0:             # as long as there are blocks\n    if blocks - 1 >= height:  # remaining blocks vs number needed to fill the height level\n        height += 1           # height is same as number of blocks in \"inverted level\"\n    blocks = blocks - height  # remain after level completion\nprint(\"The height of the pyramid:\", height)\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))\n\nheight = 0\nblocks_out = 1\n\nwhile height < blocks:    \n    blocks = blocks - blocks_out # blocks diminish, beginning with the first top block\n    height += 1                  # put first block, first level done\n    blocks_out += 1              # adittional block to each level that comes after\n\n\nprint(\"The height of the pyramid:\", height)\n"], [], ["df.to_excel(writer, sheet_name='Sheet1', header=None, index=False,\n         startcol=1, startrow=2)\n"], ["import xlwings as xw\nimport pandas as pd\n\n#create DF\ndf = pd.DataFrame([[7,2,3],[1,2,3]], columns=list('ABC'))\n\n#load workbook\napp = xw.App(visible=False)\nwb = xw.Book('doc.xlsx')  \nws = wb.sheets['Sheet1']\n\n#Update workbook at specified range\nws.range('A2').options(index=False).value = df\n\n#Close workbook\nwb.save()\nwb.close()\napp.quit()\n"], [], [], [], [], ["pytest_plugins = [\n    \"utilities.db_postgresql_inmemory\",\n]\n", "ImportError: Error importing plugin \"utilities.db_postgresql_inmemory\": No module named 'utilities'\n", "[me@linux ~/code/my_app]touch tests/utilities/__init__.py\n[me@linux ~/code/my_app]touch ./test_blank.py\n", "[me@linux ~/code/my_app]rm tests/utilities/__init__.py tests/__init__.py\n"], ["from selenium import webdriver\nfrom webdriver_auto_update import check_driver\n\n\n# Pass in the folder used for storing/downloading chromedriver\ncheck_driver('folder/path/of/your/chromedriver')\n\ndriver = webdriver.Chrome()\ndriver.get(\"http://www.python.org\")\n"], ["from re import sub\n\ndef to_camelcase(s):\n  s = sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\").replace(\"*\",\"\")\n  return ''.join([s[0].lower(), s[1:]])\n\nprint(to_camelcase('some_string_with_underscore'))\nprint(to_camelcase('Some string with Spaces'))\nprint(to_camelcase('some-string-with-dashes'))\nprint(to_camelcase('some string-with dashes_underscores and spaces'))\nprint(to_camelcase('some*string*with*asterisks'))\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))\nheight = 0\nlevels = []\n\nwhile blocks > 0:\n    height += 1\n    blocks -= height\n    levels.append(height)\n    if blocks <= levels[-1]:\n         break\n\nprint(\"The height of the pyramid:\", height)\nprint(levels)\nprint(blocks)\n"], [], ["pip install pmdarima\n"], ["from playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch()\n    page = browser.new_page()\n    page.goto('http://whatsmyuseragent.org/')\n    ua = page.query_selector(\".user-agent\");\n    print(ua.inner_html())\n    browser.close()\n", "print(ua.inner_text())\n"], [], ["deb https://mirrors.kernel.org/ubuntu bionic main\n", "sudo apt update\n", "sudo apt install libffi6\n", "$ sudo apt list | grep libffi[67]/\n\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\nlibffi6/bionic,now 3.2.1-8 amd64 [installed]\nlibffi7/focal,now 3.3-4 amd64 [installed,automatic]\n"], [" a) One shot expand/collapse all cells(code/output) in '.ipynb'(notebook)\n b) Single cell code/output expand/collapse in '.ipynb'(notebook)\n c) code folding in `.py` file with `#%%` (hierarchy style)\n", "        Expands/collapses all cells in one shot\n", "     Top menu File->Preferences->Keyboard Shortcuts\n     In search(\"Type to search in keybindings\"...top side) \n     type \"Notebook Expand\" or \"Notebook Collapse\", check out:\n\n             Notebook: Expand/Collapse All Cells, \n             Notebook: Expand/Collapse Cell Input and \n             Notebook: Expand/Collapse Cell Output\n\n\n     Its possible to expand/collapse single/all cells with these shortcuts keys.\n", "    1.  double click gutter area (immediate space on left side of code cell) to Expand/Collapse cell (code/output)\n\n    2.  right click on gutter area to get Expand/Collapse cell (code/output)\n"], [], ["def check_db(context):\n    # Do the code for running \"SELECT 1\" in the DB\n    return\n\nupdater.job_queue.run_repeating(check_db, interval=21600, first=21600)\n", "from django.db import connection\n\nconnection.close_if_unusable_or_obsolete()\n"], [], [], [], [": - use ED line editor command\n0,$ - run on all lines\ns - regular expression substitution\n/ - delimiter start search\n; - replace semicolon with\n/ - delimiter end search start replace\n;\\r - replace with semicolon followed by carriage return\n/ - delimiter end replace\ng - global - do more than once per line.\n"], ["from django.db import close_old_connections\n\ntry:\n    #do your long running operation here\nexcept django.db.utils.OperationalError:\n    close_old_connections()\n    #do your long running operation here\n", "from channels.db import database_sync_to_async\n\nasync def connect(self):\n    self.username = await database_sync_to_async(self.get_name)()\n\ndef get_name(self):\n    return User.objects.all()[0].name\n", "@database_sync_to_async\ndef get_name(self):\n    return User.objects.all()[0].name\n"], ["[mysqld]\n...\nmax_allowed_packet=128M\ninnodb_log_file_size = 128M # Fix kopano-server: SQL [00000088] info: MySQL server has gone away. Reconnecting, see https://jira.kopano.io/browse/KC-1053\n", "[mysqld]\n...\nwait_timeout = 288000 # Increase timeout to 80h before Mysql server will also go away\n"], [], ["blocks = int(input(\"Enter number of blocks: \"))\nheight = 0\nby_row = 0\ntotal = 0\nfor i in range(blocks):\n    if blocks <= total:\n        break\n    height += 1\n    by_row += 1\n    total += by_row\n\nprint(f\"The height of the pyramid:{height}\")\n"], [], [], ["SET SESSION wait_timeout = ...\n"], ["\"settings\": {\n    \"python.terminal.activateEnvInCurrentTerminal\": true,\n    \"python.defaultInterpreterPath\": \"~/venv/bin/python\"\n}\n"], ["from pytube import Playlist\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw')\nprint('Number of videos in playlist: %s' % len(playlist.video_urls))\n\n# Loop through all videos in the playlist and download them\nfor video in playlist.videos:\n    try:\n        print(video.streams.filter(file_extension='mp4'))\n        stream = video.streams.get_by_itag(137) # 137 = 1080P30\n        stream.download()\n    except AttributeError:\n        stream = video.streams.get_by_itag(22) # 22, 136 = 720P30; if 22 still don't work, try 136\n        stream.download()\n    except:\n        print(\"Something went wrong.\")\n"], [], [], [], [], ["$ sudo docker login --username=yourUsername\nPassword:\nWARNING: login credentials saved in C:\\Users\\sven\\.docker\\config.json\nLogin Succeeded\n"], [], ["uvicorn src.main:app --reload\n"], [], ["import pandas as pd\ndata = pd.read_csv('blob_sas_url')\n"], ["d={'a':{1:1,2:2},\"b\":0,'c':\"{}\"}\nprint(d)\ns=str(d)\n\ndictionary_stack,dictionary_depth=0,0\ndef push():\n    global dictionary_depth\n    global dictionary_stack\n    dictionary_stack+=1\n    dictionary_depth=max(dictionary_depth,dictionary_stack)\n\ndef pop():\n    global dictionary_stack\n    dictionary_stack-=1\n\nstring_safety=False\nfor c in s:\n    if c ==\"'\":\n        string_safety=not(string_safety)\n    \n    if not(string_safety) and c =='{':\n        push()\n    \n    if not(string_safety) and c =='}':\n        pop()\n\n\nprint(dictionary_depth)\n"], [".PHONY: test\ntest:\n    PYTHONPATH=. pytest\n"], [], ["def flatten(d, base=()):\n    for k, v in d.items():\n        if isinstance(v, dict):\n            yield from flatten(v, base + (k,))\n        else:\n            yield base + (k, v)\n", "d_level1 = {\"a\": 1, \"b\": 2, \"c\": 3}\nd_level2 = {\"group_1\": {\"a\": 1}, \"group_2\": {\"b\": 2, \"c\": 3}}\nd_level3 = {\"collection_1\": d_level2}\n\nfor items in flatten(d_level3):\n    print(items)\nprint('------------------------------')\nfor items in flatten(d_level3, depth=0):\n    print(items)\nprint('------------------------------')\nfor items in flatten(d_level3, depth=1):\n    print(items)\nprint('------------------------------')\nfor items in flatten(d_level3, depth=2):\n    print(items)\n", "('collection_1', 'group_1', 'a', 1)\n('collection_1', 'group_2', 'b', 2)\n('collection_1', 'group_2', 'c', 3)\n------------------------------\n('collection_1', {'group_1': {'a': 1}, 'group_2': {'b': 2, 'c': 3}})\n------------------------------\n('collection_1', 'group_1', {'a': 1})\n('collection_1', 'group_2', {'b': 2, 'c': 3})\n------------------------------\n('collection_1', 'group_1', 'a', 1)\n('collection_1', 'group_2', 'b', 2)\n('collection_1', 'group_2', 'c', 3)\n", "def flatten(d, base=(), depth=None):\n    for k, v in d.items():\n        if not isinstance(v, dict):\n            yield base + (k, v)\n        else:\n            if depth is None:\n                yield from flatten(v, base + (k,))\n            else:\n                if depth == 0:\n                    yield base + (k, v)\n                else:\n                    yield from flatten(v, base + (k,), depth - 1)\n"], ["d_level1 = {\"a\":1,\"b\":2,\"c\":3}\nd_level2 = {\"group_1\":{\"a\":1}, \"group_2\":{\"b\":2,\"c\":3}}\nd_level3 = {\"collection_1\":d_level2}\n\ndef flatten(d_in, base=()):\n    for k in d_in:\n        if type(d_in[k]) == dict:\n            flatten(d_in[k], base+(k,))\n        else:\n            print(base + (k, d_in[k]))\n\nflatten(d_level1)\n# ('a', 1)\n# ('b', 2)\n# ('c', 3)\n\nflatten(d_level2)\n#('group_1', 'a', 1)\n#('group_2', 'b', 2)\n#('group_2', 'c', 3)\n\nflatten(d_level3)\n# ('collection_1', 'group_1', 'a', 1)\n# ('collection_1', 'group_2', 'b', 2)\n# ('collection_1', 'group_2', 'c', 3)\n"], ["test for {'a': 1, 'b': 2, 'c': 3}\n['a', 1]\n['b', 2]\n['c', 3]\ntest for {'group_1': {'a': 1}, 'group_2': {'b': 2, 'c': 3}}\n['group_2', 'b', 2]\n['group_2', 'c', 3]\n['group_1', 'a', 1]\ntest for {'collection_1': {'group_1': {'a': 1}, 'group_2': {'b': 2, 'c': 3}}}\n['collection_1', 'group_2', 'b', 2]\n['collection_1', 'group_2', 'c', 3]\n['collection_1', 'group_1', 'a', 1]\ntest for {'a': 1, 'b': 2, 'c': 3, 'group_1': {'a': 1}, 'group_2': {'b': 2, 'c': 3}, 'collection_1': {'group_1': {'a': 1}, 'group_2': {'b': 2, 'c': 3}}}\n['a', 1]\n['b', 2]\n['c', 3]\n['collection_1', 'group_2', 'b', 2]\n['collection_1', 'group_2', 'c', 3]\n['collection_1', 'group_1', 'a', 1]\n['group_2', 'b', 2]\n['group_2', 'c', 3]\n['group_1', 'a', 1]\n"], ["df['target'] = df['target'].astype(np.float32)\n"], [], ["conda create --clone path/to/the/nameless_env -n named_env\n", "conda config --append envs_dirs /path/to/the/parent_dir\n"], [], ["%%writefile setup.sh\n\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n", "!sh setup.sh\n", "%%writefile setup.sh\n\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir ./\n"], ["class YearInSchool(models.TextChoices):\n    FRESHMAN = 'FR', _('Freshman')\n    SOPHOMORE = 'SO', _('Sophomore')\n    JUNIOR = 'JR', _('Junior')\n    SENIOR = 'SR', _('Senior')\n    GRADUATE = 'GR', _('Graduate')\n\nclass Student(models.Model):\n   year_in_school = models.CharField(\n        max_length=2,\n        choices=YearInSchool.choices,\n        default=YearInSchool.FRESHMAN,\n    )\n", "class Student(models.Model):\n\n    class YearInSchool(models.TextChoices):\n        FRESHMAN = 'FR', _('Freshman')\n        SOPHOMORE = 'SO', _('Sophomore')\n        JUNIOR = 'JR', _('Junior')\n        SENIOR = 'SR', _('Senior')\n        GRADUATE = 'GR', _('Graduate')\n\n    year_in_school = models.CharField(\n        max_length=2,\n        choices=YearInSchool.choices,\n        default=YearInSchool.FRESHMAN,\n    )\n"], [], ["$ curl -LO http://archive.ubuntu.com/ubuntu/pool/main/libf/libffi/libffi6_3.2.1-8_amd64.deb\n\n$ sudo dpkg -i libffi6_3.2.1-8_amd64.deb\n"], [], [], ["python -m venv env\n", ".\\env\\Scripts\\Activate\n"], ["import sys\nsys.path.append('.')\n"], [], ["conda env list\n", "conda activate <Folder>\n"], ["blocks = int(input(\"Enter the number of blocks: \"))\ny = 0\nx = 0\nfor x in range(0 , 99999999999999):\n     if  y == blocks:\n        height = x\n        break\n     elif y > blocks:\n        height = x - 1\n        break\n     else:\n        y += x  + 1\n        height = x - 1\nprint(\"The height of the pyramid:\", height)\n"], ["from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\ntransformer = ColumnTransformer([('one_hot_encoder', OneHotEncoder(), [0])],remainder='passthrough')\nx = py.array(transformer.fit_transform(x), dtype=py.float)\n\n\nonehotencoder = oneHotEncoder(categorical_features=[0]) \n"], ["protoc -I=./ --python_out=./ my_module.proto \n", "import my_module_pb2\n"], ["mlflow.log_metric(\"metric name\", [metric value])\nmlflow.pytorch.log_model(model, \"model\")\nmlflow.log_artifacts(output_dir)\n", "cd ...\\your-project\n", "...\\your-project> conda activate [myenv]\n", "(myenv) ...\\your-project> mlflow ui\n"], ["class YearInSchool(models.TextChoices):\n        FRESHMAN = 'FR', _('Freshman')\n        SOPHOMORE = 'SO', _('Sophomore')\n        JUNIOR = 'JR', _('Junior')\n        SENIOR = 'SR', _('Senior')\n        GRADUATE = 'GR', _('Graduate')\n\n    year_in_school = models.CharField(\n        max_length=2,\n        choices=YearInSchool.choices,\n        default=YearInSchool.FRESHMAN,\n    )\n", "class Suit(models.IntegerChoices):\n        DIAMOND = 1\n        SPADE = 2\n        HEART = 3\n        CLUB = 4\n\n    suit = models.IntegerField(choices=Suit.choices)\n"], ["pd.read_csv('file.csv', header=None)\n"], ["import os\n", "BASE_DIR = Path(__file__).resolve().parent.parent\n", "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n", "DATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.sqlite3',\n    'NAME': BASE_DIR / 'db.sqlite3',\n} }\n", "DATABASES = {\n'default': {\n    'ENGINE': 'django.db.backends.sqlite3',\n    'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n}}\n", "def index(request):\nreturn render(request,\"newapp/index.html\")\n"], [], [], ["TEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        ...\n    },\n]\n", "TEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [BASE_DIR / 'templates'], # Add templates directory to the list\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n"], ["RuntimeError: [enforce fail at inline_container.cc:145] . PytorchStreamReader failed reading zip archive: failed finding central directory`.\n"], ["wd.get(\"https://accounts.google.com/signin/v2/identifier?hl=en&passive=true&continue=https%3A%2F%2Fwww.google.com%2F%3Fgws_rd%3Dssl&ec=GAZAmgQ&flowName=GlifWebSignIn&flowEntry=ServiceLogin\");\n    Thread.sleep(2000);\n    wd.findElement(By.name(\"identifier\")).sendKeys(\"Email\"+Keys.ENTER);\n    Thread.sleep(3000);\n    wd.findElement(By.name(\"password\")).sendKeys(\"Password\"+Keys.ENTER);\n    Thread.sleep(5000);\n", "String framename=wd.findElement(By.tagName(\"iframe\")).getAttribute(\"name\");\n            wd.switchTo().frame(framename);\n    wd.findElement(By.xpath(\"//span[@id='recaptcha-anchor']\")).click();\n"], ["conda install -c conda-forge streamlit\n", "conda install -c conda-forge streamlit\n"], ["   from selenium import webdriver\n   from selenium_stealth import stealth\n\n   options = webdriver.ChromeOptions()\n   options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n   options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n   options.add_experimental_option('useAutomationExtension', False)\n   options.add_argument('--disable-blink-features=AutomationControlled')\n   driver = webdriver.Chrome(options=options)\n   stealth(driver,\n        languages=[\"en-US\", \"en\"],\n        vendor=\"Google Inc.\",\n        platform=\"Win32\",\n        webgl_vendor=\"Intel Inc.\",\n        renderer=\"Intel Iris OpenGL Engine\",\n        fix_hairline=True,\n        )\n   driver.get(\"https://www.google.com\")\n"], ["$ conda config --show | grep changeps1\n", "$ conda config --set changeps1 True\n"], [], ["FROM python:3.8.7-slim-buster\n", "psycopg2-binary~= <<version_number>>\n"], ["FROM python:3.8.3-slim #Image python:3.9.5-slim also works # Image python:3.9.5-slim-buster also works\n\nRUN apt-get update \\\n    && apt-get -y install libpq-dev gcc \\\n    && pip install psycopg2\n    \n"], ["pip install chromedriver-autoinstaller\n", "import chromedriver_autoinstaller\n", "from selenium import webdriver\nimport chromedriver_autoinstaller\n\n\nchromedriver_autoinstaller.install()  # Check if the current version of chromedriver exists\n                                      # and if it doesn't exist, download it automatically,\n                                      # then add chromedriver to path\n\ndriver = webdriver.Chrome()\ndriver.get(\"http://www.python.org\")\nassert \"Python\" in driver.title\n"], ["from selenium import webdriver\nimport zipfile\nimport requests\n\ntry:\n    version = requests.get('https://chromedriver.storage.googleapis.com/LATEST_RELEASE').text\n    url = 'https://chromedriver.storage.googleapis.com/{0}/{1}'.format(version, 'chromedriver_win32.zip')\n    r = requests.get(url, allow_redirects=True)\n    open('chromedriver.zip', 'wb').write(r.content)\n    with zipfile.ZipFile(\"chromedriver.zip\", \"r\") as zip_ref:\n        zip_ref.extractall()\nexcept:\n    pass\n"], [], ["FROM ubuntu:20.04\nRUN apt-get update && apt-get -y install libpq-dev gcc && pip install psycopg2\nCOPY requirements.txt /cs_account/\nRUN pip3 install -r requirements.txt\n", "psycopg2-binary~=2.8.6\n"], [], [], ["import re\nfrom pytube import Playlist\nplaylist = Playlist('https://www.youtube.com/playlist?list=Pd5k1hvD2apA0DwI3XMiSDqp')   \nDOWNLOAD_DIR = 'D:\\Video'\nplaylist._video_regex = re.compile(r\"\\\"url\\\":\\\"(/watch\\?v=[\\w-]*)\")    \nprint(len(playlist.video_urls))    \nfor url in playlist.video_urls:\n    print(url)    \nfor video in playlist.videos:\n    print('downloading : {} with url : {}'.format(video.title, video.watch_url))\n    video.streams.\\\n        filter(type='video', progressive=True, file_extension='mp4').\\\n        order_by('resolution').\\\n        desc().\\\n        first().\\\n        download(DOWNLOAD_DIR)\n"], [], ["image_file = Image.open(image_file)\nimage_file = image_file.convert('RGB')\n"], ["$ conda update -n base -c defaults conda\n"], ["blocks = int (input(\"Number of blocks: \"))\nheight = 0\ntotal = 0\n\n#loop until there are no more blocks    \nwhile blocks:\n   #this is the base condition which tells us when to exit the loop\n   if blocks <= total:\n      break\n   else:\n      height += 1\n      total += height\nprint(\"Height is: \", height)\n"], [], ["def foo():\n    from x import y\n"], [], ["playlist = Playlist('https://www.youtube.com/watch?v=VZclsCzhzt4&list=PLk-w4cD8sJ6N6ffzp5A4PQaD76RvdpHLP')\n\nfor video in playlist.videos:\n    print('downloading : {} with url : {}'.format(video.title, video.watch_url))\n    video.streams.\\\n        filter(type='video', progressive=True, file_extension='mp4').\\\n        order_by('resolution').\\\n        desc().\\\n        first().\\\n        download(cur_dir)\n"], [], [], ["from enum import Enum\n\nclass BaseEnum(Enum):\n    def __new__(cls, *args):\n        obj = object.__new__(cls)\n        obj._value_ = args[0]\n        obj.display_name = args[1]\n        return obj\n\n    @classmethod\n    def model_choices(cls):\n        return [(cls.__members__[member].value, cls.__members__[member].display_name)\n            for member in cls.__members__.keys()]\n", ">>> class TransactionType(BaseEnum):\n...     IN = ('in', 'In')\n...     OUT = ('out', 'Out')\n...\n>>> TransactionType.IN.value\n'in'\n>>> TransactionType.IN.display_name\n'In'\n>>> TransactionType.model_choices()\n[('in', 'In'), ('out', 'Out')]\n"], [], [], ["import pprint\nimport sys\npprint.pprint(sys.path)\n"], ["set FLASK_ENV=development\n"], ["row = 0\nfor col, data in enumerate(X):\n    try:\n        worksheet.write_column(row, col, data)\n    except:\n        pass\n"], [], [], [], [], ["for test_images, test_labels in test_loader:  \n    sample_image = test_images[0]    # Reshape them according to your needs.\n    sample_label = test_labels[0]\n", " mnist_test = datasets.MNIST('../MNIST/', train=False, transform=transform)\n", " for image, label in mnist_test:\n      # do something with image and other attributes\n", " inputs, classes = next(iter(dataloader))   \n"], ["testset = ImageFolderWithPaths(root=\"path/to/your/Image_Data/Test/\", transform=transform)\nsubset_indices = [0] # select your indices here as a list\nsubset = torch.utils.data.Subset(testset, subset_indices)\ntestloader_subset = torch.utils.data.DataLoader(subset, batch_size=1, num_workers=0, shuffle=False)\n"], [], ["blocks = int(input(\"Enter the number of blocks: \"))                      \n                                                                         \nheight = 0                                                                                                                              \nwhile height < blocks:                                                   \n    height += 1                                                          \n    blocks -= height\n                                                   \nprint(height)\n"], [], [], [], ["import tkinter as tk \n\n\nHEIGHT =  700 \nWidth = 800\n\n\nroot = tk.Tk()\n\ncanvas = tk.Canvas(root, height = HEIGHT , width = Width)\ncanvas.pack()\n\n\n\nbackground_image=tk.PhotoImage(file = \"sample.png\")\nbackground_label = tk.Label(root, image=background_image)\nbackground_label.place(x=0, y=0, relwidth=1, relheight=1)\n\nroot.mainloop()\n"], ["$ script.py\n", "ModuleNotFoundError: No module named 'torch'\n", "$ python script.py\n"], [], ["#@title **SSH**\n\n! pip install colab_ssh --upgrade &> /dev/null\n\nNgrok = True  \n\ntry:\n    if username:\n        pass\n    elif password:\n        pass\nexcept NameError:\n    !echo \"root:root\"   | chpasswd\n    username='root'\n    password='root'\n\n\n\n\n#@markdown [OAuth](https://dashboard.ngrok.com/auth)\nngrokToken = \"\" #@param {type:'string'}\n\n\ndef runNGROK():\n    from colab_ssh import launch_ssh\n    from IPython.display import clear_output\n    launch_ssh(ngrokToken, password)\n    clear_output()\n\n    print(\"ssh\", username, end='@')\n    ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n            \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'][6:].replace(':', ' -p '))\"\n\n\n\n\n\n\n\n\n\n\nif Ngrok:\n    if ngrokToken == \"\":\n        print(\"No ngrokToken Found, Please enter it\")\n    else:\n        runNGROK()\n", "ssh root@2.tcp.ngrok.io -p 13225\n", "print(\"No user found using username and password as 'root'\")\n    !echo \"root:t\"   | chpasswd\n    username='root'\n    password='root'\n\n"], [], [], [], ["    options.addArguments(\"--no-sandbox\");\n            options.addArguments(\"--disable-dev-shm-usage\");\n            options.addArguments(\"--disable-blink-features\");\n            options.setExperimentalOption(\"excludeSwitches\", Arrays.asList(\"enable-automation\"));\n            options.addArguments(\"--disable-blink-features=AutomationControlled\");\n            options.addArguments(\"--disable-infobars\");\n\n        options.addArguments(\"--remote-debugging-port=9222\");\n\noptions.setCapability(CapabilityType.UNEXPECTED_ALERT_BEHAVIOUR, UnexpectedAlertBehaviour.IGNORE);\n\ndriver.executeScript(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\");\n"], [], [], [], [], ["db = SQLAlchemy(app)\nmigrate = Migrate(app, db)\nfrom models import User\n"], [], [], ["import random\nimport torchvision.transforms.functional as TF\n\nif random.random() > 0.5:\n    image = TF.vflip(image)\n    mask  = TF.vflip(mask)\n"], ["# Data Preprocessing Template\n\n# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Importing the dataset\ndataset = pd.read_csv('Data.csv')\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,3].values\n\n# Splitting the dataset into the Training set and Test set\n#from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(X[:,1:3])\nX[:,1:3] = imputer.transform(X[:,1:3])\n\n#encoding Categorical Data\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nlabelencoder_X = LabelEncoder()\nX[:,0] = labelencoder_X.fit_transform(X[:,0])\nonehotencoder = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = \"passthrough\")\nX = onehotencoder.fit_transform(X)\n\n\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\n"], [], ["class C:\n    x = []\n    def add(self, element):\n        self.x.append(element)\n\no1 = C()\no2 = C()\no1.add(1)\no2.add(2)\nassert o1.x == [1, 2]\nassert o1.x is o2.x\n", "@dataclass\nclass D:\n    x: List = []\n    def add(self, element):\n        self.x += element\n", "class D:\n    x = []\n    def __init__(self, x=x):\n        self.x = x\n    def add(self, element):\n        self.x += element\n"], ["import os\nimport logging\n\nfrom fastapi import FastAPI, BackgroundTasks, File, UploadFile\n\nlog = logging.getLogger(__name__)\n\napp = FastAPI()\n\nDESTINATION = \"/\"\nCHUNK_SIZE = 2 ** 20  # 1MB\n\n\nasync def chunked_copy(src, dst):\n    await src.seek(0)\n    with open(dst, \"wb\") as buffer:\n        while True:\n            contents = await src.read(CHUNK_SIZE)\n            if not contents:\n                log.info(f\"Src completely consumed\\n\")\n                break\n            log.info(f\"Consumed {len(contents)} bytes from Src file\\n\")\n            buffer.write(contents)\n\n\n@app.post(\"/uploadfile/\")\nasync def create_upload_file(file: UploadFile = File(...)):\n    fullpath = os.path.join(DESTINATION, file.filename)\n    await chunked_copy(file, fullpath)\n    return {\"File saved to disk at\": fullpath}\n"], [], ["import asyncio\nfrom pyppeteer import launch\n\nasync def main():\n    browser = await launch()\n    page = await browser.newPage()\n    await page.goto('http://example.com')\n    await page.screenshot({'path': 'example.png', 'fullPage': 'true'})\n    await browser.close()\n\nasyncio.get_event_loop().run_until_complete(main())\n"], [], [], [], [], [], ["brew install python@3.8\n"], [], ["sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt install python3.9\n", "sudo apt install python3-pip\n", "sudo apt-key adv --recv-keys --keyserver keyserver.ubuntu.com 0xcbcb082a1bb943db\ncurl -LsS https://downloads.mariadb.com/MariaDB/mariadb_repo_setup | sudo bash\n", "sudo apt-get install -y apt-transport-https\n", "sudo apt-get update\nsudo apt-get upgrade\nsudo apt-get dist-upgrade\n", "sudo apt-get install libmariadb3\nsudo apt-get install libmariadb-dev\n", "sudo pip3 install mariadb\n"], [], ["import aioredis\nimport uvicorn\nfrom fastapi import Depends, FastAPI\n\nfrom fastapi_limiter import FastAPILimiter\nfrom fastapi_limiter.depends import RateLimiter\n\napp = FastAPI()\n\n\n@app.on_event(\"startup\")\nasync def startup():\n    redis = await aioredis.create_redis_pool(\"redis://localhost\")\n    FastAPILimiter.init(redis)\n\n\n@app.get(\"/\", dependencies=[Depends(RateLimiter(times=2, seconds=5))])\nasync def index():\n    return {\"msg\": \"Hello World\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", debug=True, reload=True)\n"], [], [], ["sudo apt purge libmariadb3 \nsudo apt purge libmariadb-dev\n", "sudo mv -f bin/mariadb_config /usr/bin/\nsudo mv -f include/mariadb    /usr/local/include/\nsudo mv -f lib/mariadb        /usr/local/lib/\n", "export LD_LIBRARY_PATH=/usr/local/lib/mariadb/\n"], ["import random\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\nimg_a = Image.open(\"sample_ajpg\") # note that two images have the same size\nimg_b = Image.open(\"sample_b.png\")\nimg_c, img_d = Image.open(\"sample_c.jpg\"), Image.open(\"sample_d.png\")\n\nif random.random() > 0.5:\n        image_a_flipped = transforms.functional_pil.vflip(img_a)\n        image_b_flipped = transforms.functional_pil.vflip(img_b)\nelse:\n    image_a_flipped = transforms.functional_pil.hflip(img_a)\n    image_b_flipped = transforms.functional_pil.hflip(img_b)\n\nif random.random() > 0.5:\n        image_c_flipped = transforms.functional_pil.vflip(img_c)\n        image_d_flipped = transforms.functional_pil.vflip(img_d)\nelse:\n    image_c_flipped = transforms.functional_pil.hflip(img_c)\n    image_d_flipped = transforms.functional_pil.hflip(img_d)\n    \ndisplay(image_a_flipped)\ndisplay(image_b_flipped)\n\ndisplay(image_c_flipped)\ndisplay(image_d_flipped)\n"], [], ["class MultipleJsonEncoders():\n    \"\"\"\n    Combine multiple JSON encoders\n    \"\"\"\n    def __init__(self, *encoders):\n        self.encoders = encoders\n        self.args = ()\n        self.kwargs = {}\n\n    def default(self, obj):\n        for encoder in self.encoders:\n            try:\n                return encoder(*self.args, **self.kwargs).default(obj)\n            except TypeError:\n                pass\n        raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n\n    def __call__(self, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n        enc = json.JSONEncoder(*args, **kwargs)\n        enc.default = self.default\n        return enc\n\n", "import json\nimport enum\nimport datetime\n\nclass JsonDateEncoder(json.JSONEncoder):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n    def default(self, o):\n        if isinstance(o, (datetime.datetime, datetime.date)):\n            return o.isoformat()\n        return super().default(o)\n\n\nclass JsonEnumEncoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, Enum):\n            return o.name\n        return super().default(o)\n\nclass Enumm(enum.Enum):\n    X = enum.auto()\n\nobj = {'time': datetime.datetime.now(), 'enum': Enumm.X}\nencoder = MultipleJsonEncoders(JsonDateEncoder, JsonEnumEncoder)\n\nIn [502]: json.dumps(obj, cls=encoder)\nOut[502]: '{\"time\": \"2020-12-23T08:51:43.646022\", \"enum\": \"X\"}'\n\n"], ["conda install -c saravji pmdarima \n", "pip install pmdarima\n"], [], ["from django.db.models.enums import TextChoices\n\nclass AutoEnumChoices(TextChoices):\n    def _generate_next_value_(name, start, count, last_values):  # @NoSelf\n        return name.lower()\n    \n    @property\n    def choices(cls):  # @NoSelf\n        empty = [(None, cls.__empty__)] if hasattr(cls, '__empty__') else []\n        return empty + [(member.value, member.label) for member in cls]\n", "class TransferBasicStatus(AutoEnumChoices):\n    NONE = auto()\n    WAITING = auto()\n    PENDING = auto()\n    PROGRESS = auto()\n    SUCCESS = auto()\n    DECLINED = auto()\n    ENDED =  'ended', _('Ended - The transfer has ended with mixed states')\n"], [], ["from selenium import webdriver\n\nbrowser = webdriver.Firefox('./geckodriver')\n\nbrowser.get('https://www.python.org/')\n"], [], [], [], ["X[np.isnan(X)] = 0.;\n"], [], ["pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n"], [], [], ["FROM python:alpine\nADD requirements.txt /\nRUN apk update --no-cache \\\n&& apk add build-base postgresql-dev libpq --no-cache --virtual .build-deps \\\n&& pip install --no-cache-dir --upgrade pip \\\n&& pip install --no-cache-dir -r /requirements.txt \\\n&& apk del .build-deps\nRUN apk add postgresql-libs libpq --no-cache\n", "django\ndjangorestframework\npsycopg2-binary\n"], ["$ uvicorn main:app --reload\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [40645] using statreload\nERROR:    Error loading ASGI app. Could not import module \"main\".\n", "uvicorn src.main:app --reload\n", "cd src \n", "$ uvicorn main:app --reload\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [40726] using statreload\nINFO:     Started server process [40728]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n"], [], ["$ cd path_to_python3.7_folder\n", "    $ ./configure --enable-optimizations\n    $ make\n    $ sudo make install\n", "    $ pip3.7 install jupyterlab\n    $ pip3.7 install notebook\n"], [], ["import file mismatch:\nimported module 'test_01_login_chrome.py' has this __file__ attribute:\n  /Users/avicii/PycharmProjects/Omni_Sanity/TestCase/Firefox_TestCases/test_01_login_chrome.py\nwhich is not the same as the test file we want to collect:\n  /Users/avicii/PycharmProjects/Omni_Sanity/TestCase/01_Chrome_TestCases/test_01_login_chrome.py\nHINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules\n"], [], ["for _ in range(int(input())):\n    input()    # gets length of list, but doesn't store it\n    input_list = list(map(int, input().split()))\n    print(input_list)\n"], ["kases = int(input())\nfor kase in range(kases):\n    N = int(input())\n    result = 1\n    for i in range(1, N + 1):\n        result = result * i\n    print (result)\n"], ["T = int(input())\n\ntestCases = []\nfor i in range(T):\n    l = int(input())\n    testCase = [int(x) for x in  input().split(\" \")]\n    testCases.append(testCase)\n\nprint(testCases)\n"], ["# ex path: /Users/username/opt/miniconda3/envs/`\nconda config --append envs_dirs <path to env folder here>\n", "conda activate <name of the env>\n"], ["blocks = int(input(\"Enter the number of blocks: \"))\ncount = 1\n\nwhile blocks >= 1:\n    if blocks == 1:\n        height = count\n        break\n    else:\n        count += 1\n        total =0\n        for i in range(1, count+1):\n            total += i\n        if total == blocks:\n            height = count\n            break\n        elif total > blocks:\n            height = count -1\n            break\n\nprint(\"The height of the pyramid:\", height)\n"], ["secret_number = 777\nguess_number = int(input(\"Enter a number: \"))\n\nwhile guess_number != secret_number:\n    print(\"Ha ha! You're stuck in my loop!\")\n    guess_number = int(input(\"Enter a number: \"))\n        \nprint(\"Well done, muggle! You are free now.\")\n", "secret_number = 777\nguess_number = int(input(\"Enter a number: \"))\n\nwhile True:\n    if guess_number == secret_number:\n        print(\"Well done, muggle! You are free now.\")\n        break\n    else:\n        print(\"Ha ha! You're stuck in my loop!\")\n        guess_number = int(input(\"Enter a number: \"))\n"], ["try:\n  import apex\nexcept Exception:\n  ! git clone https://github.com/NVIDIA/apex.git\n  % cd apex\n  !pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n  %cd ..\n"], [], ["import pathlib\n\ntemp = pathlib.PosixPath\npathlib.PosixPath = pathlib.WindowsPath\n"], ["import json\n\nimport proto\n\ndef proto_message_to_dict(message: proto.Message) -> dict:\n    \"\"\"Helper method to parse protobuf message to dictionary.\"\"\"\n    return json.loads(message.__class__.to_json(message))\n"], [], [], [], ["# file1.py\nimport A # A gets imported before B can import A\nimport B # B tries to re-import A but A is already imported\n"], [], [], [], [], [], ["blocks = int(input(\"Enter the number of blocks: \"))\n\nheight = 0\ninlayer = 1\nwhile inlayer <= blocks:\n height += 1\n blocks -= inlayer\n inlayer += 1\n\nprint(\"The height of the pyramid:\", height)\n"], [" mkdir static/css/tailwind\n\n cd static/css/tailwind\n", "npm init -y\n", "npm i tailwindcss\n", "@tailwind base;\n@tailwind components;\n@tailwind utilities;\n", "  \"scripts\": {\n    \"build:css\": \"tailwind build tw.css -o ../tailwind.css\"\n  },\n", "npm run build:css\n"], ["import re\nfrom pytube import Playlist\n\nYOUTUBE_STREAM_AUDIO = '140' # modify the value to download a different stream\nDOWNLOAD_DIR = 'D:\\\\Users\\\\Jean-Pierre\\\\Downloads'\n\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLzwWSJNcZTMSW-v1x6MhHFKkwrGaEgQ-L')\n\n# this fixes the empty playlist.videos list\nplaylist._video_regex = re.compile(r\"\\\"url\\\":\\\"(/watch\\?v=[\\w-]*)\")\n\nprint(len(playlist.video_urls))\n\nfor url in playlist.video_urls:\n    print(url)\n\n# physically downloading the audio track\nfor video in playlist.videos:\n    audioStream = video.streams.get_by_itag(YOUTUBE_STREAM_AUDIO)\n    audioStream.download(output_path=DOWNLOAD_DIR)\n"], ["sum = 0\nblock =int(input()) \nfor height in range(1,block):\n    sum = sum+height\n    if(sum == block):\n     print(\"the height of pyramid:\",height)\n"], [], ["!git clone https://github.com/NVIDIA/apex\n%cd apex\n!pip install -v --no-cache-dir ./\n"], ["$ find /usr/lib -name \"libffi.so*\"\n", "sudo ln -s /usr/path/to/libffi.so.7 /usr/lib/path/to/libffi.so.6\n"], [" __model = pickle.load(open(os.path.join('./artifacts/saved_model.pkl'), 'rb'))\n"], ["PROTOC_ZIP=protoc-3.7.1-osx-x86_64.zip\ncurl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/$PROTOC_ZIP\nsudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc\nsudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'\nrm -f $PROTOC_ZIP\n"], [], [], ["blocks = int (input(\"Enter the number of blocks: \")\nheight = 0\nmin_blocks = 0\n\nfor blokken in range (1, blocks + 1):\n    if ((min_blocks + blokken) <= blocks):\n        min_blocks = min_blocks + blokken\n        height = height + 1\nprint (\"The height of the pyramid: \", height)\n"], [], [], ["sudo apt-get install -y python3-venv python3-pip\nmkdir email\ncd email\ndeactivate 2> /dev/null\npip3 show virtualenv\nif [ $? -ne 0 ] ; then\n   pip3 install --upgrade pip\n   pip3 install --upgrade setuptools\n   pip3 install virtualenv\nfi\n\n# now lets build venv\npython3 -m venv venv\nsource venv/bin/activate\npip3 install email_validator\necho \"import email_validator\" > email.py\necho \"print(email_validator.validate_email('hello@gmail.com'))\" >> email.py\npython3 email.py\n\n", "Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\npython3-pip is already the newest version (20.0.2-5ubuntu1).\npython3-venv is already the newest version (3.8.2-0ubuntu2).\n0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\nName: virtualenv\nVersion: 20.0.25\nSummary: Virtual Python Environment builder\nHome-page: https://virtualenv.pypa.io/\nAuthor: Bernat Gabor\nAuthor-email: gaborjbernat@gmail.com\nLicense: MIT\nLocation: /home/mcs/.local/lib/python3.8/site-packages\nRequires: distlib, appdirs, six, filelock\nRequired-by: \nCollecting email_validator\n  Using cached email_validator-1.1.1-py2.py3-none-any.whl (17 kB)\nCollecting idna>=2.0.0\n  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\nCollecting dnspython>=1.15.0\n  Using cached dnspython-1.16.0-py2.py3-none-any.whl (188 kB)\nInstalling collected packages: idna, dnspython, email-validator\nSuccessfully installed dnspython-1.16.0 email-validator-1.1.1 idna-2.10\n<ValidatedEmail hello@gmail.com>\n"], ["from email_validator import validate_email, EmailNotValidError\n"], ["from azure.storage.blob import BlockBlobService\nimport pandas as pd\nimport tables\n\nSTORAGEACCOUNTNAME= <storage_account_name>\nSTORAGEACCOUNTKEY= <storage_account_key>\nLOCALFILENAME= <local_file_name>\nCONTAINERNAME= <container_name>\nBLOBNAME= <blob_name>\n\n#download from blob\nt1=time.time()\nblob_service=BlockBlobService(account_name=STORAGEACCOUNTNAME,account_key=STORAGEACCOUNTKEY)\nblob_service.get_blob_to_path(CONTAINERNAME,BLOBNAME,LOCALFILENAME)\nt2=time.time()\nprint((\"It takes %s seconds to download \"+blobname) % (t2 - t1))\n\n# LOCALFILE is the file path\ndataframe_blobdata = pd.read_csv(LOCALFILENAME)\n", "from io import StringIO\nblobstring = blob_service.get_blob_to_text(CONTAINERNAME,BLOBNAME).content\ndf = pd.read_csv(StringIO(blobstring))\n"], [], [], ["secret_number = 777\n\nn = int(input(\"Enter an integer number: \"))\n\nif n != secret_num: # If statement to chack if the numbers match\n    print(\"Ha ha! You're stuck in my loop!\")\n    while True:\n        input(\"Enter an integer number: \") # Loop never ends...\n\nprint(\"Well done, muggle! You are free now.\") # If the program made it here, that means the user's number matched\n", "Enter an integer number: 7\nHa ha! You're stuck in my loop!\nEnter an integer number: 2\nEnter an integer number: 2\nEnter an integer number: 3\nEnter an integer number: 4\nEnter an integer number: 56\nEnter an integer number: 789\nEnter an integer number:\nEnter an integer number: 8765\nEnter an integer number:\n...\n", "Enter an integer number: 777\nWell done, muggle! You are free now.\n>>> \n"], ["secret_number = 777\n\nwhile True:\n    number = int(input(\"Enter an integer: \"))    \n    if number == secret_number:\n        break\n    print(\"Ha ha! You're stuck in my loop!\")\n\nprint(\"Well done, muggle! You are free now.\")\n", "def get_number():\n    return int(input(\"Enter an integer: \"))\n\nsecret_number = 777\n\nnumber = get_number()\nwhile number != secret_number:\n    print(\"Ha ha! You're stuck in my loop!\")\n    number = get_number()      \n\nprint(\"Well done, muggle! You are free now.\")\n", "secret_number = 777\n\nwhile (number := int(input(\"Enter an integer: \"))) != secret_number:\n    print(\"Ha ha! You're stuck in my loop!\")\n\nprint(\"Well done, muggle! You are free now.\")\n", "secret_number = 777\n\nwhile int(input(\"Enter an integer: \")) != secret_number:\n    print(\"Ha ha! You're stuck in my loop!\")\n\nprint(\"Well done, muggle! You are free now.\")\n"], [], ["+----------------+-------------------------------------------------------------------------------------------+\n|    Filename    |                                        Description                                        |\n+----------------+-------------------------------------------------------------------------------------------+\n| app.py         | Creates the app and starts the server.                                                    |\n| models.py      | Define what the entity will look like (e.g, UserModel has username, email, password etc.) |\n| controllers.py | Fetches Data from database, generates HTML and sends the response to the user browser.    |\n+----------------+-------------------------------------------------------------------------------------------+\n", "project/\n    - app.py ( Creates and starts the server)\n    - models.py ( Class to model a user)\n    - controllers.py ( We will fetch data from database, and return html to user.)\n"], ["mathapp/\n    - server.py  \n    - configuration.py \n    - __init__.py \n    - static/ \n       - home.html  \ntests/            \n    - functional \n       - test_errors.py \n    - unit  \n       - test_add.py\n", "$ pytest\n============================= test session starts =============================\nplatform win32 -- Python 3.8.2, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\nrootdir: C:\\mak2006\\workspace\\0github\\python-rest-app-cont\ncollected 1 item / 1 error\n\n=================================== ERRORS ====================================\n_______________ ERROR collecting tests/functional/test_func.py ________________\nImportError while importing test module 'C:\\mainak\\workspace\\0github\\python-rest-app-cont\\tests\\functional\\test_func.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\functional\\test_func.py:4: in <module>\n    from mathapp.service import sum\nE   ModuleNotFoundError: No module named 'mathapp'\n=========================== short test summary info ===========================\nERROR tests/functional/test_func.py\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n============================== 1 error in 0.24s ===============================\n", "$ pytest\n============================= test session starts =============================\nplatform win32 -- Python 3.8.2, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\nrootdir: C:\\mak2006\\workspace\\0github\\python-rest-app-cont\ncollected 2 items\n\ntests\\functional\\test_func.py .                                          [ 50%]\ntests\\unit\\test_unit.py .                                                [100%]\n\n============================== 2 passed in 0.11s ==============================\n"], [], [], ["layout_poetry() {\n  if [[ ! -f pyproject.toml ]]; then\n    log_error 'No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.'\n    exit 2\n  fi\n\n  local VENV=$(poetry env list --full-path | cut -d' ' -f1)\n  if [[ -z $VENV || ! -d $VENV/bin ]]; then\n    log_error 'No created poetry virtual environment found.  Use `poetry install` to create one first.'\n    exit 2\n  fi\n  VENV=$VENV/bin\n  export VIRTUAL_ENV=$(echo \"$VENV\" | rev | cut -d'/' -f2- | rev)\n  export POETRY_ACTIVE=1\n  PATH_add \"$VENV\"\n}\n\nlayout poetry\nexport PYTHONDONTWRITEBYTECODE=1\nexport PYTHONPATH=\"$PWD/project_name\"\n"], ["pyenv uninstall 3.7.4\npyenv install 3.7.4\n"], [], [], ["conda create -n env_pytorch python=3.6\n", "conda install pytorch torchvision cudatoolkit=10.2 -c pytorch\n", "conda activate env_pytorch\n", "conda deactivate\n"], ["from sklearn.compose import ColumnTransformer \nct = ColumnTransformer([(\"Name_Of_Your_Step\", OneHotEncoder(),[0])], remainder=\"passthrough\")) # The last arg ([0]) is the list of columns you want to transform in this step\nct.fit_transform(X)    \n", "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n#Encode Country Column\nlabelencoder_X = LabelEncoder()\nX[:,0] = labelencoder_X.fit_transform(X[:,0])\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\nX = ct.fit_transform(X)\n"], [], [], [], ["from pytube import Playlist\nplaylist = Playlist('https://www.youtube.com/playlist?list=PL6gx4Cwl9DGCkg2uj3PxUWhMDuTw3VKjM')\nprint('Number of videos in playlist: %s' % len(playlist.video_urls))\nfor video_url in playlist.video_urls:\n    print(video_url)\nplaylist.download_all()\n", "https://www.youtube.com/watch?v=HjuHHI60s44\nhttps://www.youtube.com/watch?v=Z40N7b9NHTE\nhttps://www.youtube.com/watch?v=FvziRqkLrEU\nhttps://www.youtube.com/watch?v=XN2-87haa8k\nhttps://www.youtube.com/watch?v=VgI4UKyL0Lc\nhttps://www.youtube.com/watch?v=BvPIgm2SMG8\nhttps://www.youtube.com/watch?v=DpdmUmglPBA\nhttps://www.youtube.com/watch?v=BmVmJi5dR9c\nhttps://www.youtube.com/watch?v=pYNuKXjcriM\nhttps://www.youtube.com/watch?v=EWONqLqSxYc\nhttps://www.youtube.com/watch?v=EKmLXiA4zaQ\nhttps://www.youtube.com/watch?v=-DHCm9AlXvo\nhttps://www.youtube.com/watch?v=7cRaGaIZQlo\nhttps://www.youtube.com/watch?v=ZkcEB96iMFk\nhttps://www.youtube.com/watch?v=5Fcf-8LPvws\nhttps://www.youtube.com/watch?v=xWLgdSgsBFo\nhttps://www.youtube.com/watch?v=QcKYFEgfV-I\nhttps://www.youtube.com/watch?v=BtSQIxDPnLc\nhttps://www.youtube.com/watch?v=O5kh_-6e4kk\nhttps://www.youtube.com/watch?v=RuWVDz-48-o\nhttps://www.youtube.com/watch?v=-yjc5Y7Wbmw\nhttps://www.youtube.com/watch?v=C5T59WsrNCU\nhttps://www.youtube.com/watch?v=MWldNGdX9zE\n"], ["import sys, types, os;has_mfs = sys.version_info > (3, 5);p = os.path.join(sys._getframe(1).$\n", "import sys, types, os;\nhas_mfs = sys.version_info > (3, 5);p = os.path.join(sys._getframe(1).$\n"], ["blocks = int(input(\"Enter the number of blocks: \"))\n\nheight = 0\n\nrows = 1\n\nwhile rows <= blocks:\n\n    height += 1\n    blocks -= rows\n    if blocks <= rows:\n        break\n    rows += 1\n\nprint(\"The height of the pyramid: \", height)\n"], [], ["#%%[markdown]\n# ![title](sample_image.png)\n"], ["def to_camel_case(text):\n    s = text.replace(\"-\", \" \").replace(\"_\", \" \")\n    s = s.split()\n    if len(text) == 0:\n        return text\n    return s[0] + ''.join(i.capitalize() for i in s[1:])\n", ">>> to_camel_case(\"the_stealth_warrior\")\n'theStealthWarrior'\n>>> to_camel_case(\"The-Stealth-Warrior\")\n'TheStealthWarrior'\n>>> to_camel_case(\"A-B-C\")\n'ABC'\n"], ["for i in str:\n    if (i == '-'):\n    ...\n"], [], [], [], ["import imgkit\n\nimgkit.from_file('test.html', 'out.jpg')\n", "# pip3 install requests\nimport requests\n\nHCTI_API_ENDPOINT = \"https://hcti.io/v1/image\"\nHCTI_API_USER_ID = 'your-user-id'\nHCTI_API_KEY = 'your-api-key'\n\ndata = { 'html': \"<div class='box'>Hello, world!</div>\",\n         'css': \".box { color: white; background-color: #0f79b9; padding: 10px; font-family: Roboto }\",\n         'google_fonts': \"Roboto\" }\n\nimage = requests.post(url = HCTI_API_ENDPOINT, data = data, auth=(HCTI_API_USER_ID, HCTI_API_KEY))\n\nprint(\"Your image URL is: %s\"%image.json()['url'])\n# https://hcti.io/v1/image/7ed741b8-f012-431e-8282-7eedb9910b32\n"], ["class TransactionStatus(Enum):\n\n    INITIATED = \"INITIATED\"\n    PENDING = \"PENDING\"\n    COMPLETED = \"COMPLETED\"\n    FAILED = \"FAILED\"\n    ERROR = \"ERROR\"\n"], [], ["pip3 install opencv-python==4.1.2.30  \n"], [], [], ["import pytest \nimport matplotlib.pyplot as plt \n\n@pytest.fixture(scope='function')\ndef plot_fn():\n    def _plot(points):\n        plt.plot(points)\n        yield plt.show()\n        plt.close('all')\n    return _plot\n\n\ndef test_plot_fn(plot_fn):\n    points = [1, 2, 3]\n    plot_fn(points)\n    assert True\n", "def test_plot_fn(monkeypatch):\n    monkeypatch.setattr(plt, 'show', lambda: None)\n    plot_fn()\n"], ["from unittest.mock import patch \nimport pytest \nimport matplotlib.pyplot as plt \n\ndef plot_fn():\n    plt.plot([1,2,3])\n    plt.show()\n\n@patch(\"matplotlib.pyplot.show\")\ndef test_plot_fn(mock_show):\n    plot_fn()\n"], [], [], [], ["Error processing line 1 of /Users/jt/anaconda3/lib/python3.7/site-packages/sphinxcontrib.datatemplates-nspkg.pth:\n\n  Traceback (most recent call last):\n    File \"/Users/jt/anaconda3/lib/python3.7/site.py\", line 168, in addpackage\n      exec(line)\n    File \"<string>\", line 1, in <module>\n    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n  AttributeError: 'NoneType' object has no attribute 'loader'\n\nRemainder of file ignored\n", "import sys, types, os;has_mfs = sys.version_info > (3, 5);p ...<rest of file>\n", "import sys, types, os;\nhas_mfs = sys.version_info > (3, 5);p ...<rest of file>\n"], ["from sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.compose import ColumnTransformer\n\ncolumnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n\nX = np.array(columnTransformer.fit_transform(X), dtype = np.str)\n\nprint(X)\n"], [], [], [], ["from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nA = make_column_transformer(\n    (OneHotEncoder(categories='auto'), [0]), \n    remainder=\"passthrough\")\n\nx=A.fit_transform(x)\n"], ["%%writefile setup.sh\n\nexport CUDA_HOME=/usr/local/cuda-10.1\ngit clone https://github.com/NVIDIA/apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n", "!sh setup.sh\n"], ["from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\nX = ct.fit_transform(X)\n"], [], ["from itertools import islice\n\ndef create(ids):\n    policy = {\n        'Statement': []\n    }\n    i = iter(ids)\n    while True:\n        chunk = list(islice(i, 200))\n        if not chunk:\n            break\n        policy['Statement'].append({\n            'Principal': {\n                'AWS': list(map(lambda id: f\"arn:aws:iam::{id}:root\", chunk))\n            }\n        })\n    return policy\n"], [">>> temp_set = {1,2,3}\n>>> 1 in temp_set\n>>> True\n>>> temp_set[0]\n>>> Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-50885e8b29cf>\", line 1, in <module>\n    temp_set[0]\nTypeError: 'set' object is not subscriptable\n"], [], ["conda create -n env_pytorch python=3.6\n", "conda activate env_pytorch\n", "pip install torchvision \n", "import torch\nimport torchvision\n"], ["def sort_xy(x, y):\n\n    x0 = np.mean(x)\n    y0 = np.mean(y)\n\n    r = np.sqrt((x-x0)**2 + (y-y0)**2)\n\n    angles = np.where((y-y0) > 0, np.arccos((x-x0)/r), 2*np.pi-np.arccos((x-x0)/r))\n\n    mask = np.argsort(angles)\n\n    x_sorted = x[mask]\n    y_sorted = y[mask]\n\n    return x_sorted, y_sorted\n"], [], ["class encoder(tf.keras.layers.Layer):\n\n    def __init__(\n        self,\n        vocab_size, num_layers, units, d_model, num_heads, dropout,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.num_layers = num_layers\n        self.units = units\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.dropout = dropout\n\n    # Other methods etc.\n", "    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'vocab_size': self.vocab_size,\n            'num_layers': self.num_layers,\n            'units': self.units,\n            'd_model': self.d_model,\n            'num_heads': self.num_heads,\n            'dropout': self.dropout,\n        })\n        return config\n", "@classmethod\ndef from_config(cls, config):\n  return cls(**config)\n"], ["conda install pytorch torchvision -c pytorch\n", "conda create --name learnpytorch python=3.5\n", "conda install pytorch torchvision -c pytorch\n"], [], [], [], ["dist2 = lambda a,b: (a[0]-b[0])*(a[0]-b[0]) + (a[1]-b[1])*(a[1]-b[1])\n\nz = list(zip(x, y)) # get the list of coordinate pairs\nz.sort() # sort by x coordinate\n\ncw = z[0:1] # first point in clockwise direction\nccw = z[1:2] # first point in counter clockwise direction\n# reverse the above assignment depending on how first 2 points relate\nif z[1][1] > z[0][1]: \n    cw = z[1:2]\n    ccw = z[0:1]\n\nfor p in z[2:]:\n    # append to the list to which the next point is closest\n    if dist2(cw[-1], p) < dist2(ccw[-1], p):\n        cw.append(p)\n    else:\n        ccw.append(p)\n\ncw.reverse()\nresult = cw + ccw\n"], ["import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist\nfrom scipy.spatial import cKDTree\nimport cv2\nfrom scipy.ndimage.morphology import binary_fill_holes\n\ndef counter_clockwise_order(a, DEBUG_PLOT=False):\n    b = a-a.min(0)\n    d = pdist(b).min()\n    c = np.round(2*b/d).astype(int)\n\n    img = np.zeros(c.max(0)[::-1]+1, dtype=np.uint8)\n\n    d1,d2 = cKDTree(c).query(c,k=3)\n    b = c[d2]\n    p1,p2,p3 = b[:,0],b[:,1],b[:,2]\n    for i in range(len(b)):    \n        cv2.line(img,tuple(p1[i]),tuple(p2[i]),255,1)\n        cv2.line(img,tuple(p1[i]),tuple(p3[i]),255,1)\n\n    img = (binary_fill_holes(img==255)*255).astype(np.uint8)   \n    if int(cv2.__version__.split('.')[0])>=3:\n        _,contours,hierarchy = cv2.findContours(img.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n    else:\n        contours,hierarchy = cv2.findContours(img.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n\n    cont = contours[0][:,0]        \n    f1,f2 = cKDTree(cont).query(c,k=1)\n    ordered_points = a[f2.argsort()[::-1]]\n\n    if DEBUG_PLOT==1:\n        NPOINTS = len(ordered_points)\n        for i in range(NPOINTS):\n            plt.plot(ordered_points[i:i+2,0],ordered_points[i:i+2,1],alpha=float(i)/(NPOINTS-1),color='k')\n        plt.show()\n    return ordered_points\n", "# Load data in a 2D array with 2 columns\na = np.loadtxt('random_shape.csv',delimiter='  ')\nordered_a = counter_clockwise_order(a, DEBUG_PLOT=1)\n"], ["import numpy as np\nimport matplotlib.pyplot as plt\npoints = np.loadtxt('points.dat')\n\n#oneliner for ordering points (transform, adjust for 0 to 2pi, argsort, index at points)\nordered_points = points[np.argsort(np.apply_along_axis(lambda x: np.arctan2(-x[1],-x[0]+4) + np.pi*2, axis=1,arr=points)),:]\n\n#color coding 0-1 as str for gray colormap in matplotlib\nplt.scatter(ordered_points[:,0], ordered_points[:,1],c=[str(x) for x in np.arange(len(ordered_points)) / len(ordered_points)],cmap='gray')\n"], [], ["# Find the Center of Mass: data is a numpy array of shape (Npoints, 2)\nmean = np.mean(data, axis=0)\n# Compute angles\nangles = np.arctan2((data-mean)[:, 1], (data-mean)[:, 0])\n# Transform angles from [-pi,pi] -> [0, 2*pi]\nangles[angles < 0] = angles[angles < 0] + 2 * np.pi\n# Sort\nsorting_indices = np.argsort(angles)\nsorted_data = data[sorting_indices]\n"], [], ["conda update anaconda-navigator\n"], [], [], ["blocks = int(input(\"Please input block numbers: \"))\n\nmy_dict = {}\n\nfor num in range(1, blocks+1):\n    my_dict[num] = (1 + num) * num / 2\n\nfor index, value in my_dict.items():\n    if value == blocks:\n        print(f\"The height is {index}\")\n        break\n\n    if value > blocks:\n        _sum = 0\n        bottom = index\n        for num in range(index):\n            _sum = _sum + bottom\n            bottom -= 1\n            if _sum >= blocks:\n                print(f\"The height is {num+1}\")\n                break\n        break\n"], ["Height=floor(sqrt(2*x))\nif(x<height*(height+1)/2) height=height-1\n"], [], ["%%writefile setup.sh\n\ngit clone https://github.com/NVIDIA/apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n", "!sh setup.sh\n"], [], [], [], ["transformer = ColumnTransformer(\n    transformers=[\n        (\"Country\",        # Just a name\n         OneHotEncoder(), # The transformer class\n         [0]            # The column(s) to be applied on.\n         )\n    ], remainder='passthrough'\n)\nX = transformer.fit_transform(X)\n"], [], [], ["from app import app\nfrom livereload import Server\n\nif __name__ == '__main__':\n    server = Server(app.wsgi_app)\n    server.serve()\n"], [], ["df1.update(df1[['a', 'e']].merge(df2, 'left'))\n\ndf1\n\n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  3  0.0  2\n3  4  1.0  b\n", "   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  4  1.0  b\n3  3  NaN  2\n", "df1.update(df1[['a', 'e']].merge(df2, 'left'))\n\ndf1\n\n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  4  1.0  b\n3  3  0.0  2\n", "df1.update(df1.drop('b', 1).merge(df2, 'left', 'a'))\n", "df1.combine_first(df1[['a', 'e']].merge(df2, 'left'))\n", "df1.combine_first(df1.drop('b', 1).merge(df2, 'left', 'a'))\n", "df3 = df1.drop('b', 1).merge(df2, 'left', on='a').set_index(df1.index)\ndf1.combine_first(df3)\n"], ["df[df[\"col\"].astype(bool)]\n"], ["df[df[\"col\"].str.len() != 0]\n...\n"], ["df[df['col'].apply(len).gt(0)]\n"], ["df[df[\"col\"].str.len() != 0]\n", "import pandas as pd\n\ndf = pd.DataFrame({\"col\": [[1], [2, 3], [], [4, 5, 6], []]}, dtype=object)\nprint(df[df[\"col\"].str.len() != 0])\n#          col\n# 0        [1]\n# 1     [2, 3]\n# 3  [4, 5, 6]\n"], ["print(df)\n   one  two  three\n0    1  1.0    1.0\n1    2  NaN    2.0\n2    3  3.0    NaN\n\nprint(df2)\n   one  two  three\n0    4    4      4\n1    4    2      4\n2    4    4      3\n", "# mask values where isna()\ndf1[['two','three']] = df1[['two','three']]\\\n        .mask(df1[['two','three']].isna(),df2[['two','three']])\n", "   one  two  three\n0    1  1.0    1.0\n1    2  2.0    2.0\n2    3  3.0    3.0\n"], ["df1['b'].update(df2['b'])\n\n\n   a    b  e\n0  1  0.0  a\n1  2  1.0  1\n2  3  0.0  2\n3  4  1.0  b\n", "df1['b'].fillna(df2['b'], inplace=True)\n"], ["from torch.utils.data import DataLoader, Dataset, TensorDataset\nbs = 1\ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nfor xb, yb in train_dl:\n    print(xb.shape)\n    x = xb.view(28,28) \n    print(x.shape)\n    print(yb)\n    break #just once\n\nfrom matplotlib import pyplot as plt\nplt.imshow(x, cmap=\"gray\")\n"], ["pip install <package> --trusted-host pypi.org --trusted-host files.pythonhosted.org --proxy=\"<IP>:<port>\"\n"], ["def foo(bar = 1):\n    print('bar:', type(bar), bar)\n\ns = 'bar = 170'\nd = {k.strip(): int(v.strip()) for k, v in [s.split('=', 1)]}\nprint('d:', type(d), d)\nfoo(**d)\n", "d: <class 'dict'> {'bar': 170}\nbar: <class 'int'> 170\n"], [], ["def foo(bar):\n    print bar\n\nfoo(bar=1)\n\nargString = \"bar=1\"\n\nfoo(argString)\n\n\nexec(\"foo(\" + argString + \")\")\n", "1\nbar=1\n1\n"], ["var_name = 'bar'\nvar_value = 1\n\nfoo(**{var_name: var_value})\n# equivalent to foo(bar=1)\n"], ["if (protocol_type == 'http') or (protocol_type == 'https'):\n  Do Something\nelse:\n  Throw an exception\n", "\"\"\"Test of ENUM\"\"\"\n\nfrom enum import Enum\n\n\nclass ProtocolEnum(Enum):\n    \"\"\"\n    ENUM to hold the allowed values for protocol\n    \"\"\"\n    HTTP: str = 'http'\n    HTTPS: str = 'https'\n\n\ndef try_protocol_enum(protocol: ProtocolEnum) -> None:\n    \"\"\"\n    Test of ProtocolEnum\n    :rtype: None\n    :param protocol: a ProtocolEnum value allows for HTTP or HTTPS only\n    :return:\n    \"\"\"\n    print(type(protocol))\n    print(protocol.value)\n    print(protocol.name)\n\n\ntry_protocol_enum(ProtocolEnum.HTTP)\n\ntry_protocol_enum('https')\n", "<enum 'ProtocolEnum'>\nhttp\nHTTP\n"], ["def my_request(protocol_type: str, url: str):\n    if protocol_type in ('http', 'https'):\n        # Do x\n    else:\n        return 'Invalid Input'  # or raise an error\n"], ["allowed_protocols = ['http', 'https']\nif protocol_type not in allowed_protocols:\n    raise ValueError()\n"], ["def accepts(*types):\n    \"\"\"\n    Enforce parameter types for function\n    Modified from https://stackoverflow.com/questions/15299878/how-to-use-python-decorators-to-check-function-arguments\n    :param types: int, (int,float), if False, None or [] will be skipped\n    \"\"\"\n    def check_accepts(f):\n        def new_f(*args, **kwds):\n            for (a, t) in zip(args, types):\n                if t:\n                    assert isinstance(a, t), \\\n                           \"arg %r does not match %s\" % (a, t)\n            return f(*args, **kwds)\n        new_f.func_name = f.__name__\n        return new_f\n    return check_accepts\n", "@accepts(Decimal)\ndef calculate_price(monthly_item_price):\n    ...\n"], [], [], ["im = Image.fromarray((x * 255).astype(np.uint8))\n"], [], [], ["pip install Flask\n"], [" from moduleone import ModuleOne\n import pytest\n\n def test_fun():\n     assert ModuleOne.example_func() == True\n"], ["import torch\n", "import sys\nprint(sys.executable)\n"], [], ["from pytube import Playlist\n\nplaylist = Playlist('https://www.youtube.com/watch?v=58PpYacL-VQ&list=UUd6MoB9NC6uYN2grvUNT-Zg')\nprint('Number of videos in playlist: %s' % len(playlist.video_urls))\nplaylist.download_all()\n"], ["import pandas as pd\nohe=pd.get_dummies(dataframe_name['column_name'])\n"], ["list = [ x[0] for x in iter(trainloader).next() ]\n", "image, label = [ x[0] for x in iter(trainloader).next() ]\n", "iter(trainloader).next()\n", "image = iter(trainloader).next()[0][0]\n", "images, labels = iter(trainloader).next()\nimage = images[0]\n"]]