[["A = (\"a\",\"b\",\"c\")\nB = list(A)\nC = [A]\n\nprint(\"B=\",B)\nprint(\"C=\",C)\n\n# list is mutable:\nB.append(\"d\")\nC.append(\"d\")\n\n## New result\nprint(\"B=\",B)\nprint(\"C=\",C)\n\nResult:\nB= ['a', 'b', 'c']\nC= [('a', 'b', 'c')]\n\nB= ['a', 'b', 'c', 'd']\nC= [('a', 'b', 'c'), 'd']\n"], [], ["class Serializer(PythonSerializer):  \n           .................  \n           def _init_options(self):  \n                  .............  \n                  # Default is False  \n                  self.json_kwargs.setdefault(\"ensure_ascii\", False)  \n                  # You fix False to True  \n                  self.json_kwargs.setdefault(\"ensure_ascii\", True)\n"], ["urlpatterns = [\n    path('', include('<app_name>.urls')),   \n]\nurlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\nurlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n\nif settings.DEBUG:\n urlpatterns += path('admin/', admin.site.urls)\n", "STATIC_URL = 'Assets/'\nSTATIC_ROOT = BASE_DIR / 'staticfiles_build' / 'Assets'\nSTATICFILES_DIRS = [\n    BASE_DIR / \"Assets\",\n]\n"], ["pip install --editable .\n"], ["pip uninstall opencv-python\npip uninstall opencv-python-headless\npip uninstall opencv-contrib-python\n", "pip install opencv-python\n"], ["from pyspark.sql import functions as F\n\ndef melt(df,cols,alias=('key','value')):\n  other = [col for col in df.columns if col not in cols]\n  for c in cols:\n    df = df.withColumn(c, F.expr(f'map(\"{c}\", cast({c} as double))'))\n  df = df.withColumn('melted_cols', F.map_concat(*cols))\n  return df.select(*other,F.explode('melted_cols').alias(*alias))\n"], ["{\"endpointCredentials\": [{\"endpoint\":\"https://pkgs.dev.azure.com/company/_packaging/NuGetFeed/nuget/v3/index.json\", ...},{\"endpoint\":\"https://pkgs.dev.azure.com/company/company_Software/_packaging/PyPI/pypi/simple/\", ...}]}\n"], ["# function to close a workbook given name\ndef close_wb(wbname):\n    import xlwings as xw    \n    try: \n        app = xw.apps.active # get the active Excel application\n        print ('closing workbook',wbname)\n        # make workbook with given name active\n        wb = app.books[wbname]\n        wb.close()\n    except: pass\n"], [], [], [], [], [], [], ["[tool.pylint.main]\nextension-pkg-allow-list = [\"pydantic\"]\n"], ["train_scheduler = CosineAnnealingLR(optimizer, num_epochs)\n\ndef warmup(current_step: int):\n    return 1 / (10 ** (float(number_warmup_epochs - current_step)))\nwarmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup)\n\nscheduler = SequentialLR(optimizer, [warmup_scheduler, train_scheduler], [number_warmup_epochs])\n"], ["   pip install yt-dlp\n"], ["from tensorflow.python.framework.type_spec import register_type_spec_from_value_converter\nfrom keras.engine.keras_tensor import KerasTensor\n\ndef _get_type_spec(v:KerasTensor) -> tf.TypeSpec:\n    return v.type_spec\n\nregister_type_spec_from_value_converter(KerasTensor, _get_type_spec)\n"], ["while not done:\n    next_obs, reward, done, info = env.step(action) \n        env.render()\n    img = tf.keras.preprocessing.image.array_to_img(\n            img,\n            data_format=None,\n            scale=True\n    )\n    img_array = tf.keras.preprocessing.image.img_to_array(img)\n    predictions = model_self_1.predict(img_array) ### Prediction\n\n### Training: history_highscores = model_highscores.fit(batched_features, epochs=1 ,validation_data=(dataset.shuffle(10))) # epochs=500 # , callbacks=[cp_callback, tb_callback]    \n", "with tf.compat.v1.Session() as sess:\n    saver = tf.compat.v1.train.Saver()\n    saver.restore(sess, tf.train.latest_checkpoint(savedir + '\\\\invader_001'))\n    train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:y_batch, X_action:o_act})\n    \n    for layer in mainQ_outputs: \n                model.add(layer)\n        model.add(tf.keras.layers.Flatten() )\n        model.add(tf.keras.layers.Dense(6, activation=tf.nn.softmax))\n        predictions = model.predict(obs) ### Prediction\n\n    \n### Training: summ = sess.run(summaries, feed_dict={X:o_obs, y:y_batch, X_action:o_act})\n"], ["# original code\nfor email in email_list:\n    msg['To'] = email\n    server = smtplib.SMTP(host='smtp.gmail.com', port=587)\n    server.starttls()\n    server.login(\"myemail@gmail.com\", \"mypassword\")\n    server.send_message(msg)\n    server.quit()\n", "# new code\nfor email in email_list:\n    msg['To'] = email\n    server = smtplib.SMTP(host='smtp.gmail.com', port=587)\n    server.starttls()\n    server.login(\"myemail@gmail.com\", \"mypassword\")\n    server.send_message(msg)\n    del msg['To']\n"], [], [], ["\n    import pandas as pd\n    \n    mydict= {'66': 74, '62': 32, '69': 18, '72': 14, '64': 37, '192': 60, '51': 70, '46': 42, '129': 7, '85': 24, '83': 73, '65': 14, '87': 28, '185': 233, '171': 7, '176': 127, '89': 42, '80': 32, '5':\n    54, '93': 56, '104': 53, '138': 7, '162': 28, '204': 28, '79': 46, '178': 60, '144': 21, '90': 136, '193': 42, '88': 52, '212': 22, '199': 35, '198': 21, '149': 22, '84': 82, '213': 49, '47': 189, '195': 46, '31': 152, '71': 21, '70': 4, '207': 7, '158': 14, '109': 7, '163': 46, '142': 14, '94': 14, '173': 11, '78': 7, '134': 7, '96': 7, '128': 7, '54': 14, '63': 4, '120': 28, '121': 7, '37': 22, '13': 7, '45': 14, '23': 10, '180': 7, '50': 14, '188': 35, '24': 7, '139': 18, '148': 12, '151': 4, '2': 18, '34': 4, '77': 32, '81': 44, '82': 11, '92': 19, '95': 29, '98': 7, '217': 21, '172': 14, '35': 148, '146': 7, '91': 21, '103': 21, '184': 28, '165': 7, '108': 7, '112': 7, '118': 7, '159': 7, '183': 7, '186': 7, '205': 7, '60': 7, '67': 7, '76': 7, '86': 7, '209': 7, '174': 7, '194': 1}\n    \n    #create empty data frame\n    df = pd.DataFrame()\n    \n    #Use keys to create 'Start' column\n    df['Start'] = mydict.keys()\n    \n    #Use values to create 'Quantity' column\n    df['Quantity'] = mydict.values()\n    \n    #write to excel    \n    df.to_excel(\"C:\\Users\\User1\\Desktop\\dict_test.xlsx\")\n\n"], [], ["import os\nimport tempfile\nimport subprocess\nimport traceback\nimport pandas as pd\n\nTH = 0.05\n\ndef get_one_available_gpu_device_id():\n    gpu_device=-1\n    try:\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            fname = os.path.join(tmpdirname,'query.csv')\n            cmd_list = f'nvidia-smi --format=csv --query-gpu=memory.total,memory.free,memory.used,pci.bus_id,index --filename={fname}'.split(' ')\n            subprocess.check_output(cmd_list)\n            if not os.path.exists(fname):\n                raise ValueError(\"csv file not found\")\n            df = pd.read_csv(fname)\n            df['gpu_mem_total']=df['memory.total [MiB]'].apply(lambda x: int(x.split(' ')[0]))\n            df['gpu_mem_used']=df[' memory.used [MiB]'].apply(lambda x: int(x.split(' ')[1]))\n            df['gpu_usage_prct']=df['gpu_mem_used']/df['gpu_mem_total']\n            df['gpu_id']=df[' index']\n            print(df)\n            df = df.sort_values('gpu_usage_prct')\n            avail = df[df.gpu_usage_prct < TH].reset_index()\n            if len(avail)>0:\n                gpu_device = avail.loc[0,'gpu_id']\n    except:\n        traceback.print_exc()\n\n    return int(gpu_device)\n\ngpu_device = get_one_available_gpu_device_id()\nprint(f'gpu_device {gpu_device}')\n"], [], ["/users/{user_id}\n", "/users/1 \n"], ["pre-commit autoupdate\n"], ["def solveMeFirst(a,b):\n\n   return a+b\n\nnum1 = int(input(2))\nnum2 = int(input(3))\nres = solveMeFirst(num1,num2)\nprint(res)\n", "num1 = int(input(2))\nnum2 = int(input(3))\nres = num1+num2\nprint(res)\n"], [], ["INFO:     Application startup complete.\nINFO:     127.0.0.1:19122 - \"GET /report?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19123 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19124 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19125 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19126 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19127 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19128 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19129 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19130 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19131 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19132 - \"GET /increment?key=cnt HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:19144 - \"GET /report?key=cnt HTTP/1.1\" 200 OK\n"], [], ["service = build('drive', 'v3', credentials=credentials, static_discovery=False)\n"], ["{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"env\": {\n                \"PYDEVD_CONTAINER_RANDOM_ACCESS_MAX_ITEMS\": \"1000\",\n                ...\n            },\n            ...\n        }\n    ]\n}\n"], ["from typing import Any\nfrom types import SimpleNamespace as SNS\n\n\nclass RecursiveNS(SNS):\n    def __init__(self, **kwargs):\n        self.__dict__.update(self.parse(kwargs).__dict__)\n\n    @staticmethod\n    def parse(d: dict[str, Any]) -> SNS:\n        \"\"\"Static method that takes dictionary as an argument,\n        and returns \"\"\"\n        x = SNS()\n        for k, v in d.items():\n            setattr(\n                x,\n                k,\n                RecursiveNS.parse(v)\n                if isinstance(v, dict)\n                else [RecursiveNS.parse(e) for e in v]\n                if isinstance(v, list)\n                else v,\n            )\n        return x\n"], ["!pip install yfinance\n\nimport yfinance as yf\n\nstart_date = '2010-01-01'\nend_date = '2022-03-04'\n\ndf = yf.download('AAPL', start=start_date, end=end_date)\n\nprint(df)\n"], [], ["def cross_category_features(\n    df: pd.DataFrame,\n    cross: list[str],\n    remove_originals: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    Add feature crosses to the  based on the columns in cross_cols.  The columns must have already been factorized / ordinal encoded.\n\n    :param data: The data to add feature crosses to\n    :param cross_cols: The columns to cross. Columns must be int categorical 0 to n-1\n    :param remove_originals: If True, remove the original columns from the data\n\n    :return: The data with the feature crosses added\n    \"\"\"\n    def set_hot_index(row):\n        hot_index = (row[cross] * offsets).sum()\n        row[hot_index + org_col_len] = 1\n        return row\n\n    org_col_len = df.shape[1]\n    str_values = [[col + str(val) for val in sorted(df[col].unique())]\n                  for col in cross]\n    cross_names = [\"_\".join(x) for x in product(*str_values)]\n\n    cross_features = pd.DataFrame(\n        data=np.zeros((df.shape[0], len(cross_names))),\n        columns=cross_names,\n        dtype=\"int64\")\n    df = pd.concat([df, cross_features], axis=1)\n    \n    max_vals = df[cross].max(axis=0) + 1\n    offsets = [np.prod(max_vals[i+1:]) for i in range(len(max_vals))]\n    df.apply(set_hot_index, axis=1)\n\n    if remove_originals:\n        df = df.drop(columns=cross)\n\n    return df\n\n"], [], [], ["sidebar_div=driver.find_element(By.XPATH,'//*[@class=\"TFQHme \"]')\nsidebar_div.click()\nno_scroll=3 # how many times do you want to scroll\nfor page_down in range(0,no_scroll):\n    ele = driver.find_element(By.XPATH,'//*[@id=\"QA0Szd\"]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div[1]')\n    driver.execute_script('arguments[0].scrollBy(0, 5000);', ele)\n    time.sleep(2)\n"], [], ["curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | POETRY_UNINSTALL=1 python3 -\n"], ["pipenv install django-debug-toolbar --dev\n"], ["pip install notebook\n", "/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\n", "pip --version\n", "pip install notebook\n", "echo $PATH\n", "jupyter notebook\n"], ["!pip install tabula-py\n!pip install pandas\n", "import tabula\nimport pandas as pd\n", "data = tabula.read_pdf(\"example.pdf\", pages='1')[0] # \"all\" untuk semua data, pages diisi nomor halaman\n", "tabula.convert_into(\"example.pdf\", \"example.csv\", output_format=\"csv\", pages='1') #\"all\" untuk semua data, pages diisi no halaman\nprint(data)\n", "data1 = pd.read_csv(\"example.csv\")\ndata1.dtypes\n", "data.to_excel('example.xlsx')\n"], ["conda create --name RSSC python==3.7.10\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -r requirements.txt\npip install -v --disable-pip-version-check --no-cache-dir ./\n"], ["divSideBar=driver.find_element(By.CSS_SELECTOR,f\"div[aria-label='Results for {query}']\")\n\nkeepScrolling=True\nwhile(keepScrolling):\n    divSideBar.send_keys(Keys.PAGE_DOWN)\n    time.sleep(0.5)\n    divSideBar.send_keys(Keys.PAGE_DOWN)\n    time.sleep(0.5)\n    html =driver.find_element(By.TAG_NAME, \"html\").get_attribute('outerHTML')\n    if(html.find(\"You've reached the end of the list.\")!=-1):\n        keepScrolling=False\n"], [], ["pip uninstall jupyterlab\n", "jupyter-lab\n"], [], [], [], ["def add_between_numbers(num1:int,num2:int):\n    sum = num1 + num2\n    for i in range(num1 - num2):\n        if i >=1:\n            sum += (num2 + i)\n    print(sum)\nnum1 = int(input(\"Enter number 1: \"))\nnum2 = int(input(\"Enter number 2: \"))\nif num1>num2:\n    add_between_numbers(num1,num2)\nelse:\n    add_between_numbers(num2,num1)\n"], [], [], ["const option = {\nmethod: 'POST',\nheaders: {\n  Authorization: \"Token \" + apikey,\n  accept: 'application/json',\n  'Content-Type': 'application/json',\n},\nbody: `format=json&data= ${JSON.stringify(createData)}`\n"], [], [], [], [], ["myoptions = webdriver.EdgeOptions()\nmyoptions.add_argument(r\"user-data-dir=C:\\Users\\[username]\\AppData\\Local\\Microsoft\\Edge\\User Data\\Default\")\nmyoptions.add_argument(\"profile-directory=[profile name]\")\ndriver = webdriver.Edge(executable_path=r\"[path_to_edge_driver]\\msedgedriver.exe\",options=myoptions)\ndriver.get('https://zoom.us/j/93459172503?pwd=QkhnMEQ0ZTRZd0grUVJkT2NudmlFZz09')\n"], [], [], ["python.exe -m pip install --upgrade pip\n"], ["j=1\nfor i in range(10,0,-1):\n    print(\" \"*i,\"*\"*j)\n    j+=2\n"], [], ["n = int(input(\"Enter the number of rows: \"))\nfor i in range(1,n+1):\n    print(\" \"*(n-i) + \"* \"*i)\n"], ["for i in range(1,6):\n    for j in range(6-i):\n        print(\" \", end=\"\")\n    for j in range(i):\n        print(\"*\", end=\" \")\n    print()\n"], [], ["from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nLabel_x = LabelEncoder()\nx[:,0] = Label_x.fit_transform(x[:,0])\n\nonehotencoder = OneHotEncoder.categorical_features =[0]\nOneHotEncoder().fit_transform(x).toarray()\nx = x[:, 1:]\n"], ["#prints pyramid for given lines \nl = int(input('Enter no of lines'))\nfor i in range(1,l +1):\n    print(' ' * (l-i), ('*') * (i*2-1))\n"], ["predict = lambda x: tf.function(model(x))\n# Use predict(x) instead of model.predict(x)\n", "predict_np = lambda x: predict(tf.convert_to_tensor(x))\n"], ["C:\\Users\\aykutatmaca\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\pip3.10.exe install mediapipe\n"], ["i am new to python .. please tell me how good this code is ?\n\nn1 = int(input(\"Enter the number 1 = \"))\nn2 = int(input(\"Enter the number 2 = \"))\nn3 = int(input(\"Enter the number 3 = \"))\nn4 = int(input(\"Enter the number 4 = \"))\n\nif (n1>n2):\n    if(n1>n3):\n    if(n1>n4):\n    print(\"N1 is greatest\")\nif (n2>n1):\n    if(n2>n3):\n    if(n2>n1):\n    print(\"N2 is greatest\")\nif (n3>n1):\n    if(n3>n2):\n    if(n3>n4):\n    print(\"N3 is greatest\")\nif (n4>n1):\n    if(n4>n2):\n    if(n4>n3):\n    print(\"N4 is greatest\")\n"], ["- repo: https://github.com/psf/black\n  rev: 20.8b1\n  hooks:\n    - id: black\n      name: Blacken python source\n      additional_dependencies: [\"click==8.0.2\"]\n"], [], ["# relative path to subfolder\npath = \"../power corr. factors/power corr\"\n\nfor f in os.listdir(path):\n    data = pd.read_csv(os.path.join(path,f))\n"], ["[build-system]\nrequires = [\"poetry-core>=1.0.8\"]\nbuild-backend = \"poetry.core.masonry.api\"\n", "pip install -e .\n"], ["d={}\nfor pokemons in pokemon_go_data.values():\n    for name,value in pokemons.items():\n        d[name]=d.get(name,0)+value\nmost_common_pokemon=sorted(d.keys(), key= lambda k:d[k])[-1]\n"], [], ["!pip install jupyter-dash==0.3.0\n!pip install dash==2.0.0\n"], ["  def _test():\n    npT1 = np.bool_(True)\n    npT2 = np.bool_(True)\n    npF1 = np.bool_(False)\n    npF2 = np.bool_(False)\n    # test_1\n    print(npT1 is True)  #npT1 is not python boolean, this will be false\n    print(npT1 is False)\n    print(npT1 == True)\n    print(npT1 == False)\n    print(npF1 is True)\n    print(npF1 is False)\n    print(npF1 == True)\n    print(npF1 == False)\n\n    print(\"vs\")\n\n    print(True is True)\n    print(True is False)\n    print(True == True)\n    print(True == False)\n    print(npF1 is True)\n    print(npF1 is False)\n    print(npF1 == True)\n    print(npF1 == False)\n    \n    print(\"------2----\")\n    # test_2\n    print(npT1 is npT2)\n    print(npT1 is npT1)\n    print(npT1 == npT2)\n    print(npT1 and npT2)\n    print(npT1 or npT2)\n\n    print(\"vs\")\n\n    print(True is True)\n    print(True is True)\n    print(True == True)\n    print(True and True)\n    print(True or True)\n    print(\"-----3-----\")\n    # test_3\n    print(npT1 is npF1)\n    print(npT1 == npF1)\n    print(npT1 and npF1)\n    print(npT1 or npF1)\n\n    print(\"vs\")\n\n    print(True is False)\n    print(True == False)\n    print(True and False)\n    print(True or False)\n    print(\"-----4-----\")\n    # test_4\n    print(npF1 is npF2)\n    print(npF1 == npF2)\n    print(npF1 and npF2)\n    print(npF1 or npF2)\n\n    print(\"vs\")\n\n    print(False is False)\n    print(False == False)\n    print(False and False)\n    print(False or False)\n    print(\"-----5-----\")\n    # test_5\n    print(not npT1)\n    print(not npF1)\n\n    print(\"vs\")\n\n    print(not True)\n    print(not False)\n    print(\"-----6-----\")\n    # test_6\n    #print(npF1 - npT1)  ! TypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n    print(False - True)\n"], ["def check_the_first_letter_of_two_words(word):\n  new_word = word.lower().split()\n  if new_word[0][0] == new_word[1][0]:\n    return True\n  else:\n    return False\n\nword1 = 'Hello World'\nword2 = 'World wide'\ncheck_the_first_letter_of_two_words(word1)\ncheck_the_first_letter_of_two_words(word2)\n"], [], [], [], [], [], ["client = pymongo.MongoClient('mongodb://localhost:27017/')\n"], ["DATABASES = {\n    'default': {\n        'ENGINE': 'djongo',\n        'NAME': <DBname>,\n        'HOST': \"mongodb://<username>:<password>@cluster0-shard-00-00.pifkd.mongodb.net:27017,cluster0-shard-00-01.pifkd.mongodb.net:27017,cluster0-shard-00-02.pifkd.mongodb.net:27017/<DBname>?ssl=true&replicaSet=atlas-79xyw7-shard-0&authSource=admin&retryWrites=true&w=majority\",\n        'USER': <username>,\n        'PASSWORD': <password>,\n    }\n}\n"], ["conda update --force conda\n", "conda update conda\n"], ["from fastapi import FastAPI\nfrom fastapi.responses import FileResponse\nimport uvicorn\nimport os\n\napp = FastAPI()\n\n@app.get(\"/download-file\")\ndef download_file(file_name: str):\n    folder_path = r\"C:\\Users\\HP\\Desktop\\excel files\"\n    file_location = f'{folder_path}{os.sep}{file_name}.xlsx'#os.sep is used to seperate with a \\\n    return FileResponse(file_location, media_type='application/octet-stream', filename=file_name)\n\n\nuvicorn.run(app, port=9105)\n"], ["from pydantic import BaseModel\n\nclass ResponseData(BaseModel):\n    status_code: int\n    text: str\n    reason: str\n    \n    class Config:\n        orm_mode = True\n", "x = ResponseData(status_code=200, text=\"\", reason=\"\")\njson = x.json()\nresponse = ResponseData.parse_raw(json)\nassert x == response\nprint(response.dict())\n"], ["numbers = [+3, -3]\n\nfor number in numbers:\n    print(f\"{['', '+'][number>0]}{number}\")\n", "+3\n-3\n", "import time\n\nnumbers = [+3, -3] * 1000000\n\nt0 = time.perf_counter()\n[print(f\"{number:+}\", end=\"\") for number in numbers]\nt1 = time.perf_counter()\n[print(f\"{number:+.2f}\", end=\"\") for number in numbers]\nt2 = time.perf_counter()\n[print(f\"{['', '+'][number>0]}{number}\", end=\"\") for number in numbers]\nt3 = time.perf_counter()\nprint(\"\\n\" * 50)\nprint(\"\"\"number:+ : \"\"\" + str(round(t1-t0, 2)) + \"s\")\nprint(\"\"\"number:+.2f : \"\"\" + str(round(t2-t1, 2)) + \"s\")\nprint(\"\"\"['', '+'][number>0] : \"\"\" + str(round(t3-t2, 2)) + \"s\")\n", "number:+ : 1.43s\nnumber:+.2f : 1.98s\n['', '+'][number>0] : 1.23s\n"], ["import tensorflow as tf\nimport time\nimport matplotlib.pyplot as plt\n\n#something of an extra or the below code will produce an error as \"Tensor.graph is undefined when eager execution is enabled.\"\n#this code is needed to not let tensorflow produce error for the cpu part of code or the gpu part.\n#the reason for this error is because Session does not work with either eager execution or tf.function, and you should not invoke it directly.\n\ntf.compat.v1.disable_eager_execution()\n\ncpu_times = []\nsizes = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 8000, 10000]\nfor size in sizes:\n    tf.compat.v1.reset_default_graph()\n    start = time.time()\n    with tf.device('cpu:0'):\n        v1 = tf.Variable(tf.random.normal((size, size)))\n        v2 = tf.Variable(tf.random.normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        sess.run(op)\n    cpu_times.append(time.time() - start)\n    print('cpu time took: {0:.4f}'.format(time.time() - start))\n\ngpu_times = []\nfor size in sizes:\n    tf.compat.v1.reset_default_graph()\n    start = time.time()\n    with tf.device('gpu:0'):\n        v1 = tf.Variable(tf.random.normal((size, size)))\n        v2 = tf.Variable(tf.random.normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        sess.run(op)\n    gpu_times.append(time.time() - start)\n    print('gpu time took: {0:.4f}'.format(time.time() - start))\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(sizes, gpu_times, label='GPU')\nax.plot(sizes, cpu_times, label='CPU')\nplt.xlabel('MATRIX SIZE')\nplt.ylabel('TIME (sec)')\nplt.legend()\nplt.show()\n"], ["y_pred_prob = model.predict(X_test)\ny_pred = np.round(y_pred_prob)\n"], ["apt-get install yapf3\n", "yapf3 --style \"google\" --style-help > .style.yapf\n", "use_tabs=True\n", "yapf3 -i dirty.py\n"], ["import asyncio\nfrom tqdm.asyncio import tqdm\nimport random\n\nasync def factorial(name, number):\n    f = 1\n    for i in range(2, number+1):\n        await asyncio.sleep(random.random())\n        f *= i\n    print(f\"Task {name}: factorial {number} = {f}\")\n\n\nasync def main():\n    # Schedule the three concurrently\n\n    flist = [factorial(\"A\", 2),\n        factorial(\"B\", 3),\n        factorial(\"C\", 4)]\n\n    await tqdm.gather(*flist)\n\nasyncio.run(main())\n"], [], ["example = {\n    \"string_field\": \"some\",\n    \"string_array_list\": [\"samatta\"],\n    \"decimal_containing_list\": [\"a\", Decimal(8)],\n    \"empty_list\": [],\n    \"empty_dict\": {},\n    \"direct_decimal\": Decimal(0),\n    \"nested_decimal_dict\": {\"very_important-field\": Decimal(1)},\n    \"number_field\": 1,\n    \"nested_number_field\": {\"very_important-field\": 1},\n    \"number_list\": [1, 2],\n    \"decimal_containing_field_list\": [{\"very_important\": Decimal(1)}],\n}\n", "from typing import Union\n    \ndef convert_decimal(item: Union[dict, list]) -> Union[dict, list]:\n    if item is None:\n        return None\n\n    elif isinstance(item, list):\n        for index, item in enumerate(item):\n            if isinstance(item, Decimal):\n                item[index] = Decimal128(str(item))\n            if isinstance(item, dict):\n                convert_decimal(item)\n        return item\n\n    elif isinstance(item, dict):\n        for k, v in list(item.items()):\n            if isinstance(v, dict):\n                convert_decimal(v)\n            elif isinstance(v, list):\n                convert_decimal(v)\n            elif isinstance(v, Decimal):\n                item[k] = Decimal128(str(v))\n\n        return item\n", "{\n    \"string_field\": \"some\",\n    \"string_array_list\": [\"samatta\"],\n    \"decimal_containing_list\": [\"a\", Decimal128(\"8\")],\n    \"empty_list\": [],\n    \"empty_dict\": {},\n    \"direct_decimal\": Decimal128(\"0\"),\n    \"nested_decimal_dict\": {\"very_important-field\": Decimal128(\"1\")},\n    \"number_field\": 1,\n    \"nested_number_field\": {\"very_important-field\": 1},\n    \"number_list\": [1, 2],\n    \"decimal_containing_field_list\": [{\"very_important\": Decimal128(\"1\")}],\n}\n"], [], [], ["from fastapi import FastAPI\n\napp = FastAPI()\n\nclass Hello(str):\n    @app.get(\"/hello\")\n    def hello(self):\n        return {\"Hello\": self}\n"], ["Robot robot = new Robot();\nrobot.keyPress(KeyEvent.VK_ENTER);\n"], [], [], ["import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom platform import python_version\n\n# this prints the library version\nprint(tf.__version__) \nprint(np.__version__) \nprint(pd.__version__) \n\n# this prints the python version\nprint(python_version())\n", "conda create -n newenv pandas=1.3.5 python=3.8.10 tensorflow=2.9.2 numpy=1.21.6\n"], ["{\n    \"name\": \"Python: Current File\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"program\": \"${file}\",\n    \"console\": \"integratedTerminal\",\n    \"justMyCode\": false,\n    \"cwd\": \"${fileDirname}\",\n    \"purpose\": [\"debug-in-terminal\"]\n}\n"], [], [], [], ["git clone https://github.com/NVIDIA/apex\ncd apex\npip install -r requirements.txt\npip install -v --disable-pip-version-check --no-cache-dir ./\n"], ["name: Black check\n\non: [pull_request]\n\njobs:\n  black-check:\n    runs-on: ubuntu-22.04\n    name: Black\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@master\n        with:\n          python-version: 3.8\n      - name: install black\n        run: |\n          pip install click==8.0.4 black==21.9b0\n      - name: run black\n        run: |\n          black . -t py27 --check\n"], [], ["while True:\n    try:\n        bot.polling(none_stop=True, timeout=90)\n    except Exception as e:\n        print(datetime.datetime.now(), e)\n        time.sleep(5)\n        continue\n"], ["from pydantic.dataclasses import dataclass\nfrom pydantic.tools import parse_obj_as\nimport dataclasses\nimport json\n\n@dataclass\nclass User:\n  id: int\n  name: str\n\nuser = User(id=123, name=\"James\")\nuser_json = json.dumps(dataclasses.asdict(user))\nprint(user_json)  # '{\"id\": 123, \"name\": \"James\"}'\n\nuser_dict = json.loads(user_json)\nuser = parse_obj_as(User, user_dict)\nprint(user)  # User(id=123, name='James')\n"], ["/root/.vscode-server/extensions/ms-python.python-2022.20.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_resolver.py\n"], ["pip list --outdated\n", "pip freeze | %{$_.split('==')[0]} | %{pip install --upgrade $_}\n"], [], [], ["#n=Number of rows\ndef get_triangle(n):\n    space,star=\" \",\"* \"\n    for i in range(1,n+1):\n         print((n-i)*space + star*i)\n\nget_triangle(5)\n"], ["pip3 install paramiko update\n"], ["j=1\n\nfor i in range(5,0,-1):\n    \n    print(\" \"*i, end=\"\")\n    \n    while j<6:\n        print(\"* \"*j)\n        j=j+1\n        break\n"], ["# Run the VAD on 10 ms of silence. The result should be False.\nimport webrtcvad\nvad = webrtcvad.Vad(2)\n\nsample_rate = 16000\nframe_duration = 10  # ms\nframe = b'\\x00\\x00' * int(sample_rate * frame_duration / 1000)\nprint('Contains speech: %s' % (vad.is_speech(frame, sample_rate))\n"], ["def warmup(current_step: int):\nif current_step < args.warmup_steps:  # current_step / warmup_steps * base_lr\n    return float(current_step / args.warmup_steps)\nelse:                                 # (num_training_steps - current_step) / (num_training_steps - warmup_steps) * base_lr\n    return max(0.0, float(args.training_steps - current_step) / float(max(1, args.training_steps - args.warmup_steps)))\n", "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup)\n"], [], ["This is the code i wrote .. I'm just learning python\n\nn1 = int(input(\"Enter 1st Number: \"))\nn2 = int(input(\"Enter 2nd Number: \"))\nn3 = int(input(\"Enter 3rd Number: \"))\nn4 = int(input(\"Enter 4th Number: \"))\n\nif (n1>n2) and (n1>n3) and (n1>n4):\n    print(\"n1 is Greater\")\nelif (n2>n1) and (n2>n3) and (n2>n4):\n    print(\"n2 is Greater\")\nelif (n3>n2) and (n3>n1) and (n3>n4):\n    print(\"n3 is Greater\")\nelse:\n    print(n4,\"is Greater\")\n"], [], ["import camelot\nimport pandas\ntables = camelot.read_pdf(path_to_pdf, flavor='stream',pages='all')\ndf = pandas.concat([table.df for table in tables])\ndf.to_csv(path_to_csv)\n"], ["def _test():\n    a = np.bool_(True)\n    b = np.bool_(True)\n    c = np.bool_(False)\n    d = np.bool_(False)\n    # test_1\n    print(a is True)\n    print(a is False)\n    print(a == True)\n    print(a == False)\n    print(c is True)\n    print(c is False)\n    print(c == True)\n    print(c == False)\n    # test_2\n    print(a is b)\n    print(a == b)\n    print(a and b)\n    print(a or b)\n    # test_3\n    print(a is c)\n    print(a == c)\n    print(a and c)\n    print(a or c)\n    # test_4\n    print(c is d)\n    print(c == d)\n    print(c and d)\n    print(c or d)\n    # test_5\n    print(not a)\n    print(not c)\n"], [], [], [], [], [], [], [], [], [], [], [], ["pipenv --where\n"], ["trainY = tf.keras.utils.to_categorical(y_train, num_classes=9)\ntestY = tf.keras.utils.to_categorical(y_test, num_classes=9)\n", "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50,\n          batch_size = 128)\n", "trainY = tf.keras.utils.to_categorical(y_train, num_classes=9)\ntestY = tf.keras.utils.to_categorical(y_test, num_classes=9)\n"], [], ["import pyautogui\n\nsleep(3)                           # sleep until pop up shown\npyautogui.press('tab', presses=2)  # navigate to open button\npyautogui.press('enter')           # open application\n"], ["raw_path = request.scope['route'].path \n#'/user/{id}'\n"], [], ["from dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Toy1:\n    color: str\n    age: int\n    field3: Optional[str] = None\n    fielg4: Optional[str] = None\n    field5: Optional[str] = None\n\n    def __init__(self, color:str, age: int) -> None:\n        self.color = color\n        self.age = age\n\ntoy1 = Toy('red', 2)\n", "@dataclass\nclass Toy2:\n    color: str\n    age: int\n    field3: Optional[str] = None\n    fielg4: Optional[str] = None\n    field5: Optional[str] = None\n\ntoy2 = Toy('red', 2)\n", "toy1 == toy2\nTrue\n"], ["touch setup.cfg\npip install -e .\nrm setup.cfg\n"], [], [], [], ["class UseCase:\n    @abstractmethod\n    def run(self):\n        pass\n\n\nclass ProductionUseCase(UseCase):\n    def run(self):\n        return \"Production Code\"\n\n\nclass AppController:\n\n    def __init__(self, app: FastAPI, use_case: UseCase):\n        @app.get(\"/items/{item_id}\")\n        def read_item(item_id: int, q: Optional[str] = None):\n            return {\n                \"item_id\": item_id, \"q\": q, \"use_case\": use_case.run()\n            }\n\n\ndef startup(use_case: UseCase = ProductionUseCase()):\n    app = FastAPI()\n    AppController(app, use_case)\n    return app\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(startup(), host=\"0.0.0.0\", port=8080)\n"], [], [], ["python3 -m pip install --upgrade pip\n"], ["while True:\ntry:\n    asyncio.run(bot.polling(non_stop=True, interval=1, timeout=0))\nexcept:\n    time.sleep(5)\n"], ["extension-pkg-allow-list=pydantic\n"], ["def get_route_from_request(req):\n  root_path = req.scope.get(\"root_path\", \"\")\n\n  route = scope.get(\"route\")\n  if not route:\n    return None\n  path_format = getattr(route, \"path_format\", None)\n  if path_format:\n    return f\"{route_path}{path_format}\"\n\n  return None\n"], [], [], ["            \"cwd\": \"${fileDirname}\"\n", "            \"cwd\": \"\"\n"], [], ["C:\\ProgramData\\Anaconda3\n", "pythoncom39.dll\npywintypes39.dll\n", "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\pywin32_system32\n", "C:\\windows\\system32\n", "conda install jupyter\nconda install pywin32\npython -m ipykernal install --user\n"], ["    (#Create struct of the column names and col values\n  df.withColumn('tab',F.array(*[F.struct(F.lit(k.replace('qty_on_hand_','')).alias('CATEGORY'), F.col(k).alias('qty_on_hand')) for k in df.columns if k!='store_id']))\n #Explode using the inline function\n  .selectExpr('store_id as product_id',\"inline(tab)\")\n  #groupby and sum\n  .groupby('CATEGORY').agg(sum('qty_on_hand').alias('total_qty_on_hand'))\n).show()\n\n\n+--------+-----------------+\n|CATEGORY|total_qty_on_hand|\n+--------+-----------------+\n|    eggs|              190|\n|   bread|              315|\n|    milk|              105|\n+--------+-----------------+\n"], [], [], ["predictions = np.argmax(model.predict(x_test),axis=1)\n", "`predictions=(model.predict(X_test) > 0.5).astype(\"int32\")`\n"], ["from keyvalue_sqlite import KeyValueSqlite\n\nDB_PATH = '/path/to/db.sqlite'\n\ndb = KeyValueSqlite(DB_PATH, 'table-name')\n# Now use standard dictionary operators\ndb.set_default('0', '1')\nactual_value = db.get('0')\nassert '1' == actual_value\ndb.set_default('0', '2')\nassert '1' == db.get('0')\n"], ["import _locale\n_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])\n", "python -Xutf8 manage.py dumpdata --exclude auth.permission --exclude contenttypes > db.json\n", "python -Xutf8 manage.py dumpdata -o data.json\n"], [], ["pip install -U tensorflow\n"], [], [], [], ["CORS_ORIGIN_ALLOW_ALL = False\n\nCORS_ALLOW_CREDENTIALS = True\n\nCORS_ALLOWED_ORIGINS = [\n    'http://localhost:3000',\n    'http://xxx.xxx.xxx.xxx:portNum',\n]\n\nCORS_ALLOW_METHODS = [\n    'DELETE',\n    'GET',\n    'PATCH',\n    'POST',\n    'PUT',\n]\n\n# Any headers you wanted to be visible by the ReactJS app.\nCORS_EXPOSE_HEADERS = [\n    'Date'\n]\n\n"], ["class ModelA(tf.keras.Model):\n    # this model raises the error in the question\n    __slots__ = [\"model_layer\"]\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_layer = layers.LayerNormalization()\n\n    def call(self, inputs, training=None, mask=None):\n        return self.model_layer(inputs)\n\n    def get_config(self):\n        return dict()\n\n\nclass ModelB(tf.keras.Model):\n    # this model does NOT raise the error in the question\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_layer = layers.LayerNormalization()\n\n    def call(self, inputs, training=None, mask=None):\n        return self.model_layer(inputs)\n\n    def get_config(self):\n        return dict()\n"], [], ["import telebot,time\n\nif __name__=='__main__':\nwhile True:\n    try:\n        bot.polling(non_stop=True, interval=0)\n    except Exception as e:\n        print(e)\n        time.sleep(5)\n        continue\n"], ["pcandy={}\nfor v in pokemon_go_data.values():\n    for j in v:\n        pcandy[j]=pcandy.get(j,0)+v[j]\n\ns_sorted=sorted(pcandy, key= lambda x: pcandy[x], reverse=True)\nmost_common_pokemon=s_sorted[0]\nprint(most_common_pokemon)\n"], ["path = request.url.path\nfor key, val in request.path_params.items():\n    path = path.replace(val, F'{{{key}}}')\n"], [], ["$ pip install dotwiz\n"], ["GUNICORN_CMD_ARGS=\"--bind=127.0.0.1 --workers=3 --preload --access-logfile=-\" gunicorn -k uvicorn.workers.UvicornWorker demo:app\n"], ["version: '3'\n\nservices:\n  database:\n    image: postgres:12-alpine\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_PASSWORD: test_pass\n      POSTGRES_USER: test_user\n      POSTGRES_DB: test_db\n  redis:\n    image: redis:6-alpine\n    ports:\n      - \"6379:6379\"\n", "docker-compose up -d\n", "uvicorn app_cache:app --host localhost --port 8000 --workers 5\n", "# app_cache.py\nimport os\nfrom aiocache import Cache\nfrom fastapi import FastAPI, status\n\n\napp = FastAPI()\ncache = Cache(Cache.REDIS, endpoint=\"localhost\", port=6379, namespace=\"main\")\n\n\nclass Meta:\n    def __init__(self):\n        pass\n\n    async def get_count(self) -> int:\n        return await cache.get(\"count\", default=0)\n\n    async def set_count(self, value: int) -> None:\n        await cache.set(\"count\", value)\n\n    async def increment_count(self) -> None:\n        await cache.increment(\"count\", 1)\n\n\nmeta = Meta()\n\n\n# increases the count variable in the meta object by 1\n@app.post(\"/increment\")\nasync def increment():\n    await meta.increment_count()\n    return status.HTTP_200_OK\n\n\n# returns a json containing the current count from the meta object\n@app.get(\"/report\")\nasync def report():\n    count = await meta.get_count()\n    return {'count': count, \"current_process_id\": os.getpid()}\n\n\n# resets the count in the meta object to 0\n@app.post(\"/reset\")\nasync def reset():\n    await meta.set_count(0)\n    return status.HTTP_200_OK\n", "uvicorn app_db:app --host localhost --port 8000 --workers 1\n[Ctrl-C] \nuvicorn app_db:app --host localhost --port 8000 --workers 5\n", "# app_db.py\nfrom fastapi import FastAPI, status\nfrom tortoise import Model, fields\nfrom tortoise.contrib.fastapi import register_tortoise\n\n\nclass MetaModel(Model):\n    count = fields.IntField(default=0)\n\n\napp = FastAPI()\n\n\n# increases the count variable in the meta object by 1\n@app.post(\"/increment\")\nasync def increment():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    meta.count += 1  # it's better do it in transaction\n    await meta.save()\n    return status.HTTP_200_OK\n\n\n# returns a json containing the current count from the meta object\n@app.get(\"/report\")\nasync def report():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    return {'count': meta.count}\n\n\n# resets the count in the meta object to 0\n@app.post(\"/reset\")\nasync def reset():\n    meta, is_created = await MetaModel.get_or_create(id=1)\n    meta.count = 0\n    await meta.save()\n    return status.HTTP_200_OK\n\nregister_tortoise(\n    app,\n    db_url=\"postgres://test_user:test_pass@localhost:5432/test_db\",  # Don't expose login/pass in src, use environment variables\n    modules={\"models\": [\"app_db\"]},\n    generate_schemas=True,\n    add_exception_handlers=True,\n)\n"], ["pip install mediapipe-silicon\n"], ["    if number == 5:\n    break\n"], [], ["pip install \\\n --platform=manylinux1_x86_64 \\\n --only-binary=:all: \\\n --target layer-dir/python \\\n numpy;\n"], [], [], [], ["composer create-project --prefer-dist laravel/laravel checkertracker \n", "php artisan key:generate\n", "composer requrire laravel/jetstream\n", "php artisan jetstream:install livewire\n", "php artisan serve\n"], [], ["from selenium.webdriver.common.keys import Keys\n\ndriver.find_element(BY.CLASS, \"dmCR2e widget-scene\").click()\ndriver.find_element(BY.CLASS, \"dmCR2e widget-scene\").send_keys(Keys.ARROW_DOWN) \n\n#ARROW_UP ARROW_RIGHT ARROW_LEFT\n\n"], [], ["predictions = model.predict_classes(x_test)\n", "predictions = (model.predict(x_test) > 0.5).astype(\"int32\")\n"], ["identifiers  = ['3-Methylheptane', 'Aspirin', 'Diethylsulfate', 'Diethyl sulfate', '50-78-2', 'Adamant']\nsmiles_df = pd.DataFrame(columns = ['Name', 'Smiles'])\nfor x in identifiers :\n    try:\n        url = 'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/' + x + '/property/CanonicalSMILES/TXT'\n#         remove new line character with rstrip\n        smiles = requests.get(url).text.rstrip()\n        if('NotFound' in smiles):\n            print(x, \" not found\")\n        else: \n            smiles_df = smiles_df.append({'Name' : x, 'Smiles' : smiles}, ignore_index = True)\n    except: \n        print(\"boo \", x)\nprint(smiles_df)\n"], [], ["# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: end-of-file-fixer\n    -   id: check-yaml\n    -   id: check-added-large-files\n-   repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n    -   id: black\n        exclude: ^dist/\n"], ["from attrdict import AttrDict\nd = {'a': 1, 'b': [{'c': 2}, {'d': {'e': {'f': {5: {'g': 3}}}}}]}\nad = AttrDict(d)\nprint(ad.b[1].d.e.f(5).g) # 3\n"], [], [], ["predictions = np.argmax(model.predict(x_test),axis=1)\n"], [], [">>> import pubchempy as pcp\n>>> results = pcp.get_compounds('Glucose', 'name')\n>>> print results\n[Compound(79025), Compound(5793), Compound(64689), Compound(206)]\n", ">>> for compound in results:\n>>>     print compound.isomeric_smiles\n\nC([C@@H]1[C@H]([C@@H]([C@H]([C@H](O1)O)O)O)O)O\nC([C@@H]1[C@H]([C@@H]([C@H](C(O1)O)O)O)O)O\nC([C@@H]1[C@H]([C@@H]([C@H]([C@@H](O1)O)O)O)O)O\nC(C1C(C(C(C(O1)O)O)O)O)O\n"], [], [], ["!git clone https://github.com/NVIDIA/apex\n%cd apex\n!python3 setup.py install\n"], [], ["d={}\nfor name,val in pokemon_go_data.items():\n    for key in val:\n        if key not in d:\n            d[key]=0\n        d[key]+=pokemon_go_data[name][key]\nmost_common_pokemon=sorted(d.keys(), key= lambda k:d[k])[-1]\n"], ["rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n", "pip uninstall tensorflow\npip install tensorflow\n"], [], [".45, 3.4.13.47, 3.4.15.55, 3.4.16.57, 3.4.16.59, 3.4.17.61, 3.4.17.63, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.3.56, 4.5.4.58, 4.5.\n4.60, 4.5.5.62, 4.5.5.64)\nERROR: No matching distribution found for opencv-python==\n"], ["model.add(tf.keras.layers.Dense(nb_classes, activation='softmax'))\n"], ["preds = model.predict_classes(test_sequences)\n", "y_predict = np.argmax(model.predict(test_sequences), axis=1)\n"], ["class A:\n    pass\n", "A.key = val\n\ndef f(self):\n    return 0\n\nA.myfunction = f\na = A()\na.myfunction()\n# 0\n", "a=A()\na.attr='something'\n\ndef f(self):\n    return 0\n\na.fun=f.__get__(a)\na.fun()\n# 0\n"], ["!pip install --upgrade pandas-datareader\n\n!pip install --upgrade pandas\n"], [], [], [], ["predictions = (model.predict(X_test) > 0.5)*1 \n"], ["num1 = int(input(2))\nnum2 = int(input(3))\n", "num1 = int(input(\"Enter your first number\"))\nnum2 = int(input(\"Enter your second number:\"))\n"], ["conda activate base\nconda create -y --name myenv python=3.9\nconda activate myenv\nconda install -y tensorflow=2.4\nconda install -y numpy=1.19.2\nconda install -y keras\n", "System:    Kernel: 5.4.0-100-generic x86_64 bits: 64 compiler: gcc v: 9.3.0 \n           Desktop: Cinnamon 5.2.7 wm: muffin dm: LightDM Distro: Linux Mint 20.3 Una \n           base: Ubuntu 20.04 focal \nMachine:   Type: Laptop System: LENOVO product: 20308 v: Lenovo Ideapad Flex 14 serial: <filter> \n           Chassis: type: 10 v: Lenovo Ideapad Flex 14 serial: <filter> \n           Mobo: LENOVO model: Strawberry 4A v: 31900059Std serial: <filter> UEFI: LENOVO \n           v: 8ACN30WW date: 12/06/2013 \nCPU:       Topology: Dual Core model: Intel Core i5-4200U bits: 64 type: MT MCP arch: Haswell \n           rev: 1 L2 cache: 3072 KiB \n           flags: avx avx2 lm nx pae sse sse2 sse3 sse4_1 sse4_2 ssse3 vmx bogomips: 18357 \n           Speed: 798 MHz min/max: 800/2600 MHz Core speeds (MHz): 1: 798 2: 798 3: 798 4: 799 \nGraphics:  Device-1: Intel Haswell-ULT Integrated Graphics vendor: Lenovo driver: i915 v: kernel \n           bus ID: 00:02.0 chip ID: 8086:0a16 \n           Display: x11 server: X.Org 1.20.13 driver: modesetting unloaded: fbdev,vesa \n           resolution: 1366x768~60Hz \n           OpenGL: renderer: Mesa DRI Intel HD Graphics 4400 (HSW GT2) v: 4.5 Mesa 21.2.6 \n           compat-v: 3.0 direct render: Yes \n \n"], [], ["pip install --upgrade jupyterthemes\n", "jt -t solarizedd -T -N -kl\n"], [], [], [], [], [], [], ["def add_number(x,y):\n    return sum\nx = int(input(\"Enter the first number: \"))\ny = int(input(\"Enter the second number: \")) \nsum = x+y\nprint(\"The sum is: \", sum)\n"], ["    predicted = np.argmax(model.predict(token_list),axis=1)\n"], [], ["pip install numpy==1.19.5\n"], ["Python > Terminal: Execute In File Dir\n  When executing a file in the terminal, whether to use execute in the file's directory, instead of the current open folder.\n"], [], ["/content/drive/MyDrive/skincancer/data/train/benign\n", "/content/drive/MyDrive/skincancer/data/train/benign/\n"], ["-----BEGIN OPENSSH PRIVATE KEY-----\n", "ssh-keygen -p -f file -m pem -P passphrase -N passphrase\n", "ssh-keygen -m PEM\n"], [], [], ["import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n"], [], [], [], ["start = pd.to_datetime(['2007-01-01']).astype(int)[0]//10**9 # convert to unix timestamp.\nend = pd.to_datetime(['2020-12-31']).astype(int)[0]//10**9 # convert to unix timestamp.\nurl = 'https://query1.finance.yahoo.com/v7/finance/download/' + stock_ticker + '?period1=' + str(start) + '&period2=' + str(end) + '&interval=1d&events=history'\ndf = pd.read_csv(url)\n"], [], [], ["pip install --upgrade pytube\n"], [], ["eval \"$(conda shell.bash hook)\"\nconda activate my_env\n"], [], ["X = []\ncount = 0\n\npath = TRAIN_PATH_X\nfor img in os.listdir(TRAIN_PATH_X):\n    image = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n    try:\n        image = cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_AREA)\n        print(image.shape)\n    except:\n        break\n    X.append([image])\n    count = count +1\nprint(count)\n"], [], [], [">>> f'{0:+} {-0:+}'\n'+0 +0'\n>>> f'{0.0:+} {-0.0:+}'\n'+0.0 -0.0'\n"], ["number=3\nprint(f\"{number:+}\")\n"], [], [], ["pip install opencv-python\n"], [], ["import subprocess\nimport time\n\n# File (a CAD in this case) and Program (desired CAD software in this case) # r: raw strings\nfile = r\"F:\\Pradnil Kamble\\GetThisOpen.3dm\"\nprog = r\"C:\\Program Files\\Rhino\\Rhino.exe\"\n\n# Open file with desired program \nOpenIt = subprocess.Popen([prog, file])\n\n# keep it open for 30 seconds\ntime.sleep(30)\n\n# close the file and the program \nOpenIt.terminate() \n"], [], ["pip install git+https://github.com/baxterisme/pytube\n", "pip install pytube\n"], [], ["pip install jupyter\n", "pip install argon2-cffi==20.1.0 \n", "  pip install jupyter\n"], [], ["jt -t oceans16 -T -N\n"], ["!jt -t solarizedd -T -N -kl\n"], ["customdata = list(df[['continent','country']].to_numpy())\n", "import plotly.graph_objects as go\n\ncustomdata_set = list(df[['transaction','type']].to_numpy())\n\nfig = go.Figure(\n    data=[go.Scatter3d(x=df.time,\n                       y=df.source,\n                       z=df.dest,\n                       hovertemplate='<i>Source:</i>: %{y:i}<br>' +\n                       '<i>Destination:</i>: %{z:i}<br>' +\n                       '<i>Amount:</i>: $%{text}<br>' +\n                       '<i>Txn #:</i>: %{customdata[0]}<br>' +\n                       '<i>Txn Type:</i>: %{customdata[1]}<br>' +\n                       '<i>Date:</i>: %{x|%Y-%m-%d}',\n                       text=(df.amount).to_numpy(),\n                       customdata = customdata_set,\n                       mode='markers',\n                       marker=dict(\n                            color=moat_sql.tx_amount,\n                            size=4,\n                            opacity=.8,\n                            showscale=True,\n                            colorscale='Viridis',\n                            colorbar=dict(\n                                title=dict(text='Log Txn Amount',\n                                           side='bottom',\n                                           font={'color': 'red'}\n                                           )\n                            )\n    )\n    )]\n)\n"], [], ["CORS_ORIGIN_ALLOW_ALL = True\nCORS_ALLOW_CREDENTIALS = True\nCORS_ALLOW_HEADERS = [\n  'accept',\n  'accept-encoding',\n  'authorization',\n  'content-type',\n  'origin',\n  'dnt',\n  'user-agent',\n  'x-csrftoken',\n  'x-requested-with']\nCORS_ALLOW_METHODS = ['DELETE', 'GET', 'OPTIONS', 'PATCH', 'POST', 'PUT']\n"], ["conda activate my_environment \n", "alias my_environment=\"cd ~/subdirectory/my_project && conda activate my_environment\"\n", "source ~/.bashrc\n", "my_environment\n"], ["CORS_ORIGIN_ALLOW_ALL = True\nALLOWED_HOSTS = [\n    \"127.0.0.1\", \n]\n\nCORS_ALLOWED_ORIGINS = [\n    \"http://127.0.0.1\", \n]\nCORS_ALLOW_CREDENTIALS = False\n\nINSTALLED_APPS = [\n    .....\n    \"corsheaders\"\n]\n\n\nMIDDLEWARE = [\n    ......\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.common.CommonMiddleware',\n]\n"], [], [], [], [], [], ["import nbformat\n\nnb = nbformat.read('<file-path-with-format>', \n                   nbformat.current_nbformat)\n\nnbformat.write(nb, '<path-to-save>/<filename>.ipynb', \n               nbformat.NO_CONVERT)\n", "pip install nbformat\n"], [], ["amplify function update\n"], [], ["import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\ndf = px.data.gapminder()\n\ncustomdata = np.stack((df['continent'], df['country']), axis=-1)\n\nfig = px.scatter(df, x=\"gdpPercap\", y=\"lifeExp\")\n\nhovertemplate = ('Continent: %{customdata[0]}<br>' + \n    'Country: %{customdata[1]}<br>' + \n    'gdpPercap: %{x:,.4f} <br>' + \n    'lifeExp: %{y}' + \n    '<extra></extra>')\n\nfig.update_traces(customdata=customdata, hovertemplate=hovertemplate)\nfig.show()\n"], [" - Windows %APPDATA%\\Code\\User\\settings.json\n - macOS $HOME/Library/Application Support/Code/User/settings.json\n - Linux $HOME/.config/Code/User/settings.json\n", "\"multiCommand.commands\": [\n    {\n        \"command\": \"multiCommand.properlyFormatAnyDocument\",\n        \"sequence\": [\n            \"editor.action.formatDocument\",\n            \"editor.action.indentationToTabs\"\n        ]\n    }\n]\n"], ["num1=int(input(\"Enter first number\"))\nnum2=int(input(\"Enter second number\"))\nnum3=num1+num3\nprint(num3)\n", "def AddNumbers(num1,num2):\n    return num1+num2\nnum1=int(input(\"Enter first number\"))\nnum2=int(input(\"Enter second number\"))\nnum3=AddNumbers(num1,num2)\nprint(num3)\n", "class ADD:\n    def __init__(self,num1,num2):\n          self.num1=num1\n          self.num2=num2\n    def addNum(self):\n          return self.num1+self.num2\nobj1=ADD(2,4)\nprint(obj1.addNum)\n"], ["pip install https://<your-feed-name>:<your-PAT-key>@pkgs.dev.azure.com/<your-organization-name>/<your-project-name>/_packaging/<your-feed-name>/pypi/simple/ Your-Package-Name==x.x.x\n", "https://<your-feed-name>:<your-PAT-key>@pkgs.dev.azure.com/<your-organization-name>/<your-project-name>/_packaging/<your-feed-name>/pypi/download/<yourpackagename>/<package version>/Your-Package-Name.whl \n#assuming your package is a .whl file\n"], ["y_pred = model.predict(X_test)\ny_pred = np.round(y_pred).astype(int)\n"], ["import subprocess as sp\nimport os\n\ndef get_gpu_memory():\n    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n    return memory_free_values\n\nget_gpu_memory()\n"], ["from jupyterthemes import get_themes\n", "!jt -t solarizedd -T -N -kl\n"], [], ["df = (\n    df.withColumn('labels', F.explode(                         # <-- Split into rows\n        F.array(                                               # <-- Combine columns\n            F.array(F.lit('milk'), F.col('qty_on_hand_milk')), # <-- Annotate column\n            F.array(F.lit('bread'), F.col('qty_on_hand_bread')),\n            F.array(F.lit('eggs'), F.col('qty_on_hand_eggs')),\n        )\n    ))\n    .withColumn('CATEGORY', F.col('labels')[0])\n    .withColumn('qty_on_hand', F.col('labels')[1])\n).select('store_id', 'CATEGORY', 'qty_on_hand')\n", "root\n |-- store_id: long (nullable = false)\n |-- CATEGORY: string (nullable = true)\n |-- qty_on_hand: string (nullable = true) <-- Picks best schema on the fly\n", "import databricks.koalas as ks\nimport pyspark.sql.functions as F\n\n# You don't need koalas, it's just less verbose for adhoc dataframes\ndf = ks.DataFrame({\n    \"store_id\": [100, 200, 300],\n    \"qty_on_hand_milk\": [30, 55, 20],\n    \"qty_on_hand_bread\": [105, 85, 125],\n    \"qty_on_hand_eggs\": [35, 65, 90],\n}).to_spark()\ndf.show()\n\n# Annotate each column with your custom label per row. ie. v -> ['label', v]\ndf = df.withColumn('label1', F.array(F.lit('milk'), F.col('qty_on_hand_milk')))\ndf = df.withColumn('label2', F.array(F.lit('bread'), F.col('qty_on_hand_bread')))\ndf = df.withColumn('label3', F.array(F.lit('eggs'), F.col('qty_on_hand_eggs')))\ndf.show()\n\n# Create a new column which combines the labeled values in a single column\ndf = df.withColumn('labels', F.array('label1', 'label2', 'label3'))\ndf.show()\n\n# Split into individual rows\ndf = df.withColumn('labels', F.explode('labels'))\ndf.show()\n\n# You can now do whatever you want with your labelled rows, eg. split them into new columns\ndf = df.withColumn('CATEGORY', F.col('labels')[0])\ndf = df.withColumn('qty_on_hand', F.col('labels')[1])\ndf.show()\n", "|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|\n+--------+----------------+-----------------+----------------+\n|     100|              30|              105|              35|\n|     200|              55|               85|              65|\n|     300|              20|              125|              90|\n+--------+----------------+-----------------+----------------+\n\n+--------+----------------+-----------------+----------------+----------+------------+----------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|    label1|      label2|    label3|\n+--------+----------------+-----------------+----------------+----------+------------+----------+\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|\n+--------+----------------+-----------------+----------------+----------+------------+----------+\n\n+--------+----------------+-----------------+----------------+----------+------------+----------+--------------------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|    label1|      label2|    label3|              labels|\n+--------+----------------+-----------------+----------------+----------+------------+----------+--------------------+\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|[[milk, 30], [bre...|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|[[milk, 55], [bre...|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|[[milk, 20], [bre...|\n+--------+----------------+-----------------+----------------+----------+------------+----------+--------------------+\n\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|    label1|      label2|    label3|      labels|\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|  [milk, 30]|\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|[bread, 105]|\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|  [eggs, 35]|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|  [milk, 55]|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]| [bread, 85]|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|  [eggs, 65]|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|  [milk, 20]|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|[bread, 125]|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|  [eggs, 90]|\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+\n\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+--------+-----------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|    label1|      label2|    label3|      labels|CATEGORY|qty_on_hand|\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+--------+-----------+\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|  [milk, 30]|    milk|         30|\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|[bread, 105]|   bread|        105|\n|     100|              30|              105|              35|[milk, 30]|[bread, 105]|[eggs, 35]|  [eggs, 35]|    eggs|         35|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|  [milk, 55]|    milk|         55|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]| [bread, 85]|   bread|         85|\n|     200|              55|               85|              65|[milk, 55]| [bread, 85]|[eggs, 65]|  [eggs, 65]|    eggs|         65|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|  [milk, 20]|    milk|         20|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|[bread, 125]|   bread|        125|\n|     300|              20|              125|              90|[milk, 20]|[bread, 125]|[eggs, 90]|  [eggs, 90]|    eggs|         90|\n+--------+----------------+-----------------+----------------+----------+------------+----------+------------+--------+-----------+\n\n+--------+--------+-----------+\n|store_id|CATEGORY|qty_on_hand|\n+--------+--------+-----------+\n|     100|    milk|         30|\n|     100|   bread|        105|\n|     100|    eggs|         35|\n|     200|    milk|         55|\n|     200|   bread|         85|\n|     200|    eggs|         65|\n|     300|    milk|         20|\n|     300|   bread|        125|\n|     300|    eggs|         90|\n+--------+--------+-----------+\n"], [], ["AssertionError: Tried to export a function which references untracked resource\\\nTensor(\"77040:0\", shape=(), dtype=resource). \nTensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\n"], ["pyinstaller [.py_name] -n [.exe_name] --onefile --add-data [SRC;DEST]\n", "--add-data venv/Lib/site-packages/google_api_python_client-x.y.z.dist-info;google_api_python_client-x.y.z.dist-info\n"], ["pip uninstall apex\n"], ["movies = pd.read_csv(\"..\\datasets/movies.csv\")\nratings = pd.read_csv(\"..\\datasets/ratings.csv\")\n"], [], ["bot.infinity_polling(timeout=10, long_polling_timeout = 5)\n"], [], ["bot.infinity_polling(True)\n"], ["images = [img for img in os.listdir(image_path) if img.endswith(\".jpg\")]\n", "images = [img for img in os.listdir(image_path) if img.endswith(\"anyformat\")]\n"], [], [], [], ["pip uninstall pyzmq \npip install pyzmq==20\n"], [], ["Python version 3.8\nTensorflow 2.3.1 cpu+gpu\nkeras 2.4.3\n"], ["from cv2 import VideoCapture\nfrom cv2 import waitKey\n"], [], ["tf.config.experimental.get_memory_info('GPU:0')\n"], ["from email.header import Charset\nfrom email.message import EmailMessage, MIMEPart\nfrom email.utils import formataddr, parseaddr\n\ntest_recipients = [\n        \"Mr. John Doe <johndoe@example.com>\",\n        \"Mr. Jane Doe <janedoe@example.com>\",\n        \"somebody@example.com\"\n]\nto_header= []\nfor raw_address in (test_recipients):\n    # Parse and recreate\n    title, email = parseaddr(raw_address)\n    if title and email:\n        to_header.append(f\"{title} <{email}>\")\n    elif email:\n        to_header.append(email)\n# Encode after join\nmessage.add_header(\"To\", Charset(\"utf-8\").header_encode(\", \".join(to_header)))\n"], [], ["import pandas as pd\nfrom datetime import datetime as dt\nimport calendar\nimport io\nimport requests\n\n# Yahoo history csv base url\nyBase = 'https://query1.finance.yahoo.com/v7/finance/download/'\nyHeaders = {\n    'Accept': 'text/csv;charset=utf-8'\n    }\n\ndef getYahooDf(ticker, startDate, endDate=None): # dates in ISO format\n    start = dt.fromisoformat(startDate) # To datetime.datetime object\n    fromDate = calendar.timegm(start.utctimetuple()) # To Unix timestamp format used by Yahoo\n    if endDate is None:\n        end=dt.now()\n    else:\n        end = dt.fromisoformat(endDate)\n    toDate = calendar.timegm(end.utctimetuple())\n    params = { \n        'period1': str(fromDate),\n        'period2': str(toDate),\n        'interval': '1d',\n        'events': 'history',\n        'includeAdjustedClose': 'true'\n    }\n    response = requests.request(\"GET\", yBase + ticker, headers=yHeaders, params=params)\n    if response.status_code < 200 or response.status_code > 299:\n        return None\n    else:\n        csv = io.StringIO(response.text)\n        df = pd.read_csv(csv, index_col='Date')\n        return df\n"], ["app.run_server(mode='inline',host=\"0.0.0.0\",port=1005)\n"], ["tf.compat.v1.disable_eager_execution()\ncpu_times = []\nsizes = [1, 10, 100, 500, 1000, 2000, 3000, 4000, 5000, 8000, 10000]\nfor size in sizes:\n    ops.reset_default_graph()\n    start = time.time()\n    with tf.device('cpu:0'):\n        v1 = tf.Variable(tf.random.normal((size, size)))\n        v2 = tf.Variable(tf.random.normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        sess.run(op)\n    cpu_times.append(time.time() - start)\n    print('cpu time took: {0:.4f}'.format(time.time() - start))\n\nimport tensorflow as tf\nimport time\n\ngpu_times = []\nfor size in sizes:\n    ops.reset_default_graph()\n    start = time.time()\n    with tf.device('gpu:0'):\n        v1 = tf.Variable(tf.random.normal((size, size)))\n        v2 = tf.Variable(tf.random.normal((size, size)))\n        op = tf.matmul(v1, v2)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        sess.run(op)\n    gpu_times.append(time.time() - start)\n    print('gpu time took: {0:.4f}'.format(time.time() - start))\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(sizes, gpu_times, label='GPU')\nax.plot(sizes, cpu_times, label='CPU')\nplt.xlabel('MATRIX SIZE')\nplt.ylabel('TIME (sec)')\nplt.legend()\nplt.show()\n"], [">>> np.bool(False) - np.bool(True)\n-1\n\n>>> np.bool_(False) - np.bool_(True)\nTypeError: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n\n"], ["!pip install jupyterthemes\nimport jupyterthemes as jt\n\n!jt -r\n"], ["payload = {\n\"pickup_location\": {\n    \"pin\": \"110096\",``\n    \"add\": \"Changsha\",  # address of warehouse\n    \"phone\": \"1111111111\",\n    \"state\": \"Delhi\",\n    \"city\": \"Delhi\",\n    \"country\": \"India\"\n}\n}\n\npayload = f'format=json&data={json.dumps(payload,default=str,indent=4)}'\nresponse=requests.post(url_order,data=payload,headers=headers)\n"], [], ["It seams that the issue only occurs on PowerShell and cmd it works using git bash\n"], [], ["python -Xutf8 ./manage.py dumpdata > data.json\n"], ["n = 5\nprint('\\n'.join([('* '*i).center(2*n) for i in range(1,n+1)]))\n", "n = 5\nfor i in range(1,n+1):\n  print(('* '*i).center(2*n))\n"], [], [], [], [], ["export WORKON_HOME=/tmp\npipenv install\n"], ["pokemon_go_data = {'bentspoon':\n                       {'Rattata': 203, 'Pidgey': 120, 'Drowzee': 89, 'Squirtle': 35, 'Pikachu': 3, 'Eevee': 34,\n                        'Magikarp': 300, 'Paras': 38},\n                   'Laurne':\n                       {'Pidgey': 169, 'Rattata': 245, 'Squirtle': 9, 'Caterpie': 38, 'Weedle': 97, 'Pikachu': 6,\n                        'Nidoran': 44, 'Clefairy': 15, 'Zubat': 79, 'Dratini': 4},\n                   'picklejarlid':\n                       {'Rattata': 32, 'Drowzee': 15, 'Nidoran': 4, 'Bulbasaur': 3, 'Pidgey': 56, 'Weedle': 21,\n                        'Oddish': 18, 'Magmar': 6, 'Spearow': 14},\n                   'professoroak':\n                       {'Charmander': 11, 'Ponyta': 9, 'Rattata': 107, 'Belsprout': 29, 'Seel': 19, 'Pidgey': 93,\n                        'Shellder': 43, 'Drowzee': 245, 'Tauros': 18, 'Lapras': 18}}\n\ncount_d={}\npokemon_main_lst=pokemon_go_data.keys()\n#print(pokemon_main_lst)\nfor main_keys in pokemon_main_lst:\n    pokemon_sub_lst=pokemon_go_data[main_keys].keys()\n    #print(sub_lst)\n    for pokemon in pokemon_sub_lst:\n        if pokemon not in count_d:\n            count_d[pokemon]=0\n        count_d[pokemon]+=pokemon_go_data[main_keys][pokemon]\n#print(count_d)\n\nmost_common_pokemon=sorted(count_d,key=lambda k:count_d[k])[-1]\nprint(most_common_pokemon)\n"], ["from urllib.request import urlopen\nfrom urllib.parse import quote\n\ndef CIRconvert(ids):\n    try:\n        url = 'http://cactus.nci.nih.gov/chemical/structure/' + quote(ids) + '/smiles'\n        ans = urlopen(url).read().decode('utf8')\n        return ans\n    except:\n        return 'Did not work'\n\nidentifiers  = ['3-Methylheptane', 'Aspirin', 'Diethylsulfate', 'Diethyl sulfate', '50-78-2', 'Adamant']\n\nfor ids in identifiers :\n    print(ids, CIRconvert(ids))\n", "3-Methylheptane CCCCC(C)CC\nAspirin CC(=O)Oc1ccccc1C(O)=O\nDiethylsulfate CCO[S](=O)(=O)OCC\nDiethyl sulfate CCO[S](=O)(=O)OCC\n50-78-2 CC(=O)Oc1ccccc1C(O)=O\nAdamant Did not work\n"], [], [], ["!pip install pytracing\n", "from pytracing import TraceProfiler\ntp = TraceProfiler(output=open('/root/trace.out', 'wt'))\nwith tp.traced():\n  for i in range(2): \n    model.predict(X[:1000], batch_size=1000)\n", "from google.colab import files\nfiles.download('/root/trace.out') \n", "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py:2336:_standardize_user_data\n", "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py:133:configure_callbacks\n"], ["#!/usr/bin/env python\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()\n"], ["poetry install\n", "poetry run pytest tests/\n", "poetry run pre-commit install\npoetry run pre-commit run --all-files\n"], ["conda_activate=~/anaconda3/bin/activate\nconda_envs_dir=~/anaconda3/envs\nconda_env=<env name>\n", "source ${conda_activate} ${conda_envs_dir}/${conda_env}\n", "python <path to script.py>\n"], ["git clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir \\\n--global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n"], [], ["    \"python.terminal.executeInFileDir\": true,\n    \"code-runner.fileDirectoryAsCwd\": true\n"], ["from pytube import YouTube\nimport os\n\nyoutube = YouTube('https://youtu.be/ksu-zTG9HHg')\nvideo = youtube.streams.filter(res=\"1080p\").first().download()\nos.rename(video,\"video_1080.mp4\")\naudio = youtube.streams.filter(only_audio=True)\naudio[0].download()\n", "ffmpeg -i video.mp4 -i audio.mp4 -c:v copy -c:a aac output.mp4\n"], ["from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n", "import tensorflow as tf\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n", "tf.disable_v2_behavior()\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n"], ["pip install --upgrade python-socketio==4.6.0\n\npip install --upgrade python-engineio==3.13.2\n\npip install --upgrade Flask-SocketIO==4.3.1\n"], ["[MASTER]\nextension-pkg-allow-list=pydantic\n"], ["num1 = int(input(\"Enter number 1: \"))\nnum2 = int(input(\"Enter number 2: \"))\nnum3 = int(input(\"Enter number 3: \"))\nnum4 = int(input(\"Enter number 4: \"))\n\n#Store all numbers in list. \nallNum = [num1, num2, num3, num4]\n\n#Get the max value of all numbers in that list\nmaxNum = max(allNum)\n\n#To get the position of this value, use list.index() function\nmaxIndex = allNum.index(max(allNum))\n\n#Lets say your numbers are [3,5,4,1]. .index() would give you \"1\" because the index starts from 0 to length of list -1. \n#In this case, it would be 0-3. So, if you want `Number 2` is greater instead, then you just add 1 to the maxIndex.\n\nprint(\"Greatest number is number \"+str(maxIndex+1))\n"], ["a = []\nfor i in range(4):\n    a.append(int(input(f\"Enter number {i+1}: \")))\nprint(f\"Number {a.index(max(a)) + 1} is greater\")\n"], [], [], [], [], ["import org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.http.HttpEntity;\nimport org.springframework.http.HttpHeaders;\nimport org.springframework.stereotype.Service;\nimport org.springframework.web.client.RestTemplate;\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\n", "@Autowired\nprivate RestTemplate restTemplate;\n\nprivate HttpHeaders getHeaders() {\n    HttpHeaders headers = new HttpHeaders();\n    headers.set(\"Authorization\", \"Token yourToken\");\n    headers.set(\"Content-Type\",\"application/json\");\n    return headers;\n}\n\n@Override\npublic Object save(ExpressDeliveryVO expressDelivery) throws JsonProcessingException {\n\n    HttpHeaders headers = getHeaders();\n\n    ObjectMapper mapper = new ObjectMapper();\n    \n    HttpEntity<String> entity = new HttpEntity<>(\"format=json&data=\"+mapper.writeValueAsString(expressDelivery), headers);\n    \n    return restTemplate\n            .postForEntity(\"https://staging-express.delhivery.com/api/cmu/create.json\", entity, Object.class)\n            .getBody();\n\n}\n"], ["pokemon_total = {}\n\nfor player, dictionary in pokemon_go_data.items():\n    for pokemon, candy_count in dictionary.items():\n        if pokemon in pokemon_total.keys():\n            pokemon_total[pokemon] += candy_count\n        else:\n            pokemon_total[pokemon] = candy_count\n\nmax_occurence = 0\nfor pokemon in pokemon_total:\n    if pokemon_total[pokemon] > max_occurence :\n        max_occurence = pokemon_total[pokemon]\n        most_common_pokemon = pokemon    \nprint(most_common_pokemon)\n"], ["Flask-SocketIO==4.3.1\npython-engineio==3.13.2\npython-socketio==4.6.0\n"], [], ["  %load_ext autoreload\n\n  %autoreload 2\n"], ["TensorFlow 1.15.2\nKeras 2.3.1\nNumpy 1.19.5\n\nTensorFlow 2.4.1\nKeras 2.4.0\nNumpy 1.19.5\n", "%tensorflow_version 1.x\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   \n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nprint(tf.__version__)\nprint('A: ', tf.test.is_built_with_cuda)\nprint('B: ', tf.test.gpu_device_name())\nlocal_device_protos = device_lib.list_local_devices()\n([x.name for x in local_device_protos if x.device_type == 'GPU'], \n [x.name for x in local_device_protos if x.device_type == 'CPU'])\n", "TensorFlow 1.x selected.\n1.15.2\nA:  <function is_built_with_cuda at 0x7f122d58dcb0>\nB:  \n([], ['/device:CPU:0'])\n", "%tensorflow_version 1.x\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   \n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nprint(tf.__version__)\nprint('A: ', tf.test.is_built_with_cuda)\nprint('B: ', tf.test.gpu_device_name())\nlocal_device_protos = device_lib.list_local_devices()\n([x.name for x in local_device_protos if x.device_type == 'GPU'], \n [x.name for x in local_device_protos if x.device_type == 'CPU'])\n", "1.15.2\nA:  <function is_built_with_cuda at 0x7f0b5ad46830>\nB:  /device:GPU:0\n(['/device:GPU:0'], ['/device:CPU:0'])\n", "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   \n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nprint(tf.__version__)\nprint('A: ', tf.test.is_built_with_cuda)\nprint('B: ', tf.test.gpu_device_name())\nlocal_device_protos = device_lib.list_local_devices()\n([x.name for x in local_device_protos if x.device_type == 'GPU'], \n [x.name for x in local_device_protos if x.device_type == 'CPU'])\n", "2.4.1\nA:  <function is_built_with_cuda at 0x7fed85de3560>\nB:  \n([], ['/device:CPU:0'])\n", "import tensorflow as tf\nimport keras\n\n# # Disables eager execution\ntf.compat.v1.disable_eager_execution()\n# or, \n# Disables eager execution of tf.functions.\n# tf.config.run_functions_eagerly(False)\nprint(tf.executing_eagerly())\nFalse\n", "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   \n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nprint(tf.__version__)\nprint('A: ', tf.test.is_built_with_cuda)\nprint('B: ', tf.test.gpu_device_name())\nlocal_device_protos = device_lib.list_local_devices()\n([x.name for x in local_device_protos if x.device_type == 'GPU'], \n [x.name for x in local_device_protos if x.device_type == 'CPU'])\n", "2.4.1\nA:  <function is_built_with_cuda at 0x7f16ad88f680>\nB:  /device:GPU:0\n(['/device:GPU:0'], ['/device:CPU:0'])\n"], ["USER_NAME\\.vscode\\extensions\\ms-python.python-2021.3.680753044\\pythonFiles\\lib\\python\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\n"], [], ["# Loading the requisite packages \nfrom pyspark.sql.functions import col, explode, array, struct, expr, sum, lit        \n# Creating the DataFrame\ndf = sqlContext.createDataFrame([(100,30,105,35),(200,55,85,65),(300,20,125,90)],('store_id','qty_on_hand_milk','qty_on_hand_bread','qty_on_hand_eggs'))\ndf.show()\n+--------+----------------+-----------------+----------------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|\n+--------+----------------+-----------------+----------------+\n|     100|              30|              105|              35|\n|     200|              55|               85|              65|\n|     300|              20|              125|              90|\n+--------+----------------+-----------------+----------------+\n", "def to_explode(df, by):\n\n    # Filter dtypes and split into column names and type description\n    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n    # Spark SQL supports only homogeneous columns\n    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n\n    # Create and explode an array of (column_name, column_value) structs\n    kvs = explode(array([\n      struct(lit(c).alias(\"CATEGORY\"), col(c).alias(\"qty_on_hand\")) for c in cols\n    ])).alias(\"kvs\")\n\n    return df.select(by + [kvs]).select(by + [\"kvs.CATEGORY\", \"kvs.qty_on_hand\"])\n", "df = to_explode(df, ['store_id'])\\\n     .drop('store_id')\ndf.show()\n+-----------------+-----------+\n|         CATEGORY|qty_on_hand|\n+-----------------+-----------+\n| qty_on_hand_milk|         30|\n|qty_on_hand_bread|        105|\n| qty_on_hand_eggs|         35|\n| qty_on_hand_milk|         55|\n|qty_on_hand_bread|         85|\n| qty_on_hand_eggs|         65|\n| qty_on_hand_milk|         20|\n|qty_on_hand_bread|        125|\n| qty_on_hand_eggs|         90|\n+-----------------+-----------+\n", "df = df.withColumn('CATEGORY',expr('substring(CATEGORY, 13)'))\ndf.show()\n+--------+-----------+\n|CATEGORY|qty_on_hand|\n+--------+-----------+\n|    milk|         30|\n|   bread|        105|\n|    eggs|         35|\n|    milk|         55|\n|   bread|         85|\n|    eggs|         65|\n|    milk|         20|\n|   bread|        125|\n|    eggs|         90|\n+--------+-----------+\n", "df = df.groupBy(['CATEGORY']).agg(sum('qty_on_hand').alias('total_qty_on_hand'))\ndf.show()\n+--------+-----------------+\n|CATEGORY|total_qty_on_hand|\n+--------+-----------------+\n|    eggs|              190|\n|   bread|              315|\n|    milk|              105|\n+--------+-----------------+\n"], ["sess = tf.compat.v1.Session()\n", "tf.compat.v1.disable_eager_execution()\nsess = tf.compat.v1.Session()\n"], [], ["ffmpeg -i INPUT_FILE -ab BITRATE -vn OUTPUT_FILE\n", "ffmpeg -i videoplayback.m4a -ab 128000 -vn music.mp3\n", "ffmpeg -i videoplayback.m4a -vn music.mp3\n"], ["ffmpeg -i videoplayback.mp4 -i videoplayback.m4a -c:v copy -c:a copy output.mp4\n", "ffmpeg -i videoplayback.webm -i videoplayback.m4a -c:v copy -c:a copy output.mkv\n"], ["# right angle triangle\nfor i in range(1, 10):\n    print(\"* \" * i)\n\n# equilateral triangle\n# using reverse of range\nfor i , j in zip(range(1, 10), reversed(range(1, 10)):\n    print(\" \" * j + \"* \" * i)\n\n# using only range()\nfor i, j in zip(range(1, 10), range(10, -1, -1):\n    print(\" \" * j + \"* \" * i)\n"], ["for item in df:\n   list1.append(item)\ndf = pd.DataFrame(list1)\ndf.to_excel('outputfile.xlsx', sheet_name='Sheet1', index=True)\n"], ["import asyncio\nfrom typing import Any, Coroutine, Iterable, List, Tuple\n\nfrom tqdm import tqdm\n\n\nasync def aprogress(tasks: Iterable[Coroutine], **pbar_kws: Any) -> List[Any]:\n    \"\"\"Runs async tasks with a progress bar and returns an ordered result.\"\"\"\n\n    if not tasks:\n        return []\n\n    async def tup(idx: int, task: Coroutine) -> Tuple[int, Any]:\n        \"\"\"Returns the index and result of a task.\"\"\"\n        return idx, await task\n\n    _tasks = [tup(i, t) for i, t in enumerate(tasks)]\n    pbar = tqdm(asyncio.as_completed(_tasks), total=len(_tasks), **pbar_kws)\n    res = [await t for t in pbar]\n    return [r[1] for r in sorted(res, key=lambda r: r[0])]\n\n\nif __name__ == \"__main__\":\n\n    import random\n\n    async def test(idx: int) -> Tuple[int, int]:\n        sleep = random.randint(0, 5)\n        await asyncio.sleep(sleep)\n        return idx, sleep\n\n    _tasks = [test(i) for i in range(10)]\n    _res = asyncio.run(aprogress(_tasks, desc=\"pbar test\"))\n    print(_res)\n"], ["from pyfields import field\nfrom typing import List\n\nclass Toy:\n    color: str = field(doc=\"The toy's color, a string.\")\n    age: int = field(doc=\"How old is this Toy. An integer number of years.\")\n    field3: str = field(default='hello', check_type=True)\n    field4: List[str] = field(default_factory=lambda obj: ['world'])\n    field5: str = field(default=None, \n                        validators={'should be 1-character long': lambda x: len(x) == 1})\n\n    def __init__(self, color, age):\n        self.color = color\n        self.age = age\n\n\nt = Toy(color='blue', age=12)\nprint(t.field3 + ' ' + t.field4[0])\nprint(t.field5 is None)\nt.field5 = 'yo'\n", "hello world\nTrue\nTraceback (most recent call last):\n  ...\nvalid8.entry_points.ValidationError[ValueError]: Error validating [Toy.field5=yo]. InvalidValue: should be 1-character long. Function [<lambda>] returned [False] for value 'yo'.\n"], ["def no_trailing_zero(value: float) -> Union[float, int]:\n    return int(value) if value % 1 == 0 else float(str(value))\n\n>>> no_trailing_zero(50)\n50\n>>> no_trailing_zero(50.11)\n50.11\n>>> no_trailing_zero(50.1100)\n50.11\n>>> no_trailing_zero(50.0)\n50\n>>> no_trailing_zero(500000000.010)\n500000000.01\n>>>\n"], [], [], [], ["class NoamOpt:\n    \"Optim wrapper that implements rate.\"\n    def __init__(self, model_size, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.model_size = model_size\n        self._rate = 0\n    \n    def state_dict(self):\n        \"\"\"Returns the state of the warmup scheduler as a :class:`dict`.\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        \"\"\"\n        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n    \n    def load_state_dict(self, state_dict):\n        \"\"\"Loads the warmup scheduler's state.\n        Arguments:\n            state_dict (dict): warmup scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"\n        self.__dict__.update(state_dict) \n        \n    def step(self):\n        \"Update parameters and rate\"\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n        \n    def rate(self, step = None):\n        \"Implement `lrate` above\"\n        if step is None:\n            step = self._step\n        return (self.model_size ** (-0.5) *\n            min(step ** (-0.5), step * self.warmup ** (-1.5))) \n", "optimizer = NoamOpt(input_opts['d_model'], 500,\n            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n", "optimizer.step()\n"], ["uvicorn==0.13.4\nfastapi==0.63.0\nstarlette==0.13.6\naiofiles==0.6.0\n", "import os\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse\n\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def main():\n    file_name = \"FILE NAME\"\n    # DEPENDS ON WHERE YOUR FILE LOCATES\n    file_path = os.getcwd() + \"/\" + file_name\n    return FileResponse(path=file_path, media_type='application/octet-stream', filename=file_name)\n"], ["pip install ffmpeg-python\n"], [], ["from fastapi import FastAPI\nfrom fastapi.responses import FileResponse\n\nfile_path = \"large-video-file.mp4\"\napp = FastAPI()\n\n@app.get(\"/\")\ndef main():\n    return FileResponse(path=file_path, filename=file_path, media_type='text/mp4')\n"], [], ["pip install --upgrade --force-reinstall <package>\n", "pip install -I <package>\npip install --ignore-installed <package>\n"], [], ["from types import SimpleNamespace\n\n\ndef parse(data):\n    if type(data) is list:\n        return list(map(parse, data))\n    elif type(data) is dict:\n        sns = SimpleNamespace()\n        for key, value in data.items():\n            setattr(sns, key, parse(value))\n        return sns\n    else:\n        return data\n\n\ninfo = {\n    'country': 'Australia',\n    'number': 1,\n    'slangs': [\n        'no worries mate',\n        'winner winner chicken dinner',\n        {\n            'no_slangs': [123, {'definately_not': 'hello'}]\n        }\n    ],\n    'tradie': {\n        'name': 'Rizza',\n        'occupation': 'sparkie'\n    }\n}\n\nd = parse(info)\nassert d.country == 'Australia'\nassert d.number == 1\nassert d.slangs[0] == 'no worries mate'\nassert d.slangs[1] == 'winner winner chicken dinner'\nassert d.slangs[2].no_slangs[0] == 123\nassert d.slangs[2].no_slangs[1].definately_not == 'hello'\nassert d.tradie.name == 'Rizza'\nassert d.tradie.occupation == 'sparkie'\n", "import json\n\n\nclass _DotDict(dict):\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n\ndef dot(data=None):\n    if data is []:\n        return []\n    return json.loads(json.dumps(data), object_hook=_DotDict) if data else _DotDict()\n"], ["class NoamOpt:\n\"Optim wrapper that implements rate.\"\ndef __init__(self, model_size, factor, warmup, optimizer):\n    self.optimizer = optimizer\n    self._step = 0\n    self.warmup = warmup\n    self.factor = factor\n    self.model_size = model_size\n    self._rate = 0\n    \ndef step(self):\n    \"Update parameters and rate\"\n    self._step += 1\n    rate = self.rate()\n    for p in self.optimizer.param_groups:\n        p['lr'] = rate\n    self._rate = rate\n    self.optimizer.step()\n    \ndef rate(self, step = None):\n    \"Implement `lrate` above\"\n    if step is None:\n        step = self._step\n    return self.factor * \\\n        (self.model_size ** (-0.5) *\n        min(step ** (-0.5), step * self.warmup ** (-1.5)))\n    \ndef get_std_opt(model):\n    return NoamOpt(model.src_embed[0].d_model, 2, 4000,torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n"], ["example_input = {'key0a': \"test\", 'key0b': \n                 {'key1a': {'key2a': 'end', 'key2b': \"test\"} ,'key1b': \"test\"}, \n                 \"something\": \"else\"}\ndef parse(d):\n  x = SimpleNamespace()\n  _ = [setattr(x, k, parse(v)) if isinstance(v, dict) else setattr(x, k, v) for k, v in d.items() ]    \n  return x\n\nresult = parse(example_input)\nprint (result)\n", "namespace(key0a='test', \n          key0b=namespace(key1a=namespace(key2a='end', key2b='test'), key1b='test'), \n          something='else')\n"], ["import tensorflow as tf\nhello = tf.constant('Hello World ') \nsess = tf.compat.v1.Session()    *//I got the error on this step when I used \n                                   tf.Session()*\nsess.run(hello)\n"], ["import csv\ncsvfile = open('C:\\\\Users\\\\....\\\\<your_filename.file_extenstion>', \"r\")\nreadCSV = csv.reader(csvfile)\n"], ["#!/usr/bin/env python\n\nimport sys    \nimport pubchempy as pcp\n\nsmiles = str(sys.argv[1])\nprint(smiles)\ns= pcp.get_compounds(smiles,'smiles')\nprint(s[0].iupac_name)\n"], ["$ pip install cbfa\n", "from typing import Optional\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom cbfa import ClassBased\n\n\napp = FastAPI()\nwrapper = ClassBased(app)\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    is_offer: Optional[bool] = None\n\n@wrapper('/item')\nclass Item:\n    def get(item_id: int, q: Optional[str] = None):\n        return {\"item_id\": item_id, \"q\": q}\n\n    def post(item_id: int, item: Item):\n        return {\"item_name\": item.name, \"item_id\": item_id}\n"], ["from fastapi import FastAPI\nfrom fastapi.responses import HTMLResponse\n\nclass CustomAPI(FastAPI):\n    def __init__(self, title: str = \"CustomAPI\") -> None:\n        super().__init__(title=title)\n\n        @self.get('/')\n        async def home():\n            \"\"\"\n            Home page\n            \"\"\"\n            return HTMLResponse(\"<h1>CustomAPI</h1><br/><a href='/docs'>Try api now!</a>\", status_code=status.HTTP_200_OK)\n"], ["df.to_csv(**'df',** index = False)\n", "df.to_csv(**'df.csv'**, index = False)\n"], [], ["pip wheel matplotlib\npip install matplotlib-venn\n"], ["composer global remove laravel/installer\ncomposer global require laravel/installer\n", "laravel -V\n", "laravel new yourAppName --jet --stack=inertia or livewire\n"], ["from collections import Counter\n\nlist_string = [\"jim\",\"jennifer\",\"roy\",\"roy\",\"mike\",\"jim\",\"roy\",\"jim\",\"mike\",\"roy\"]\nlist_string = dict(Counter(list_string))\nlist_1 = list(list_string.keys())\nlist_2 = list(list_string.values())\n"], [], ["def non_repetitive(list):\n\n    list1=[]\n    list2=[]\n\n    for i in list:\n        if not i in list1:\n            list1.append(i)\n\n\n    for j in list1:\n        counter=0\n        for k in list:\n            if j==k:\n                counter+=1\n        list2.append(counter)\n\n\n    return list1, list2\n\n\n\nlist=[\"jim\",\"jennifer\",\"roy\",\"roy\",\"mike\",\"jim\",\"roy\",\"jim\",\"mike\",\"roy\"]\n\nprint(non_repetitive(list))\n"], [], ["my_list=[\"jim\",\"jennifer\",\"roy\",\"roy\",\"mike\",\"jim\",\"roy\",\"jim\",\"mike\",\"roy\"]\nlist1 = list(set(my_list))\nlist2 = [my_list.count(item) for item in list1]\n\n#prints: ['jennifer', 'roy', 'mike', 'jim']\n#        [1, 4, 2, 3]\n"], ["lst1 = [\"jim\",\"jennifer\",\"roy\",\"roy\",\"mike\",\"jim\",\"roy\",\"jim\",\"mike\",\"roy\"]\nlst2 = []\nset1 = set(lst1)\n\nfor i in set1: \n    lst2.append(lst1.count(i))\n\nlst1 = list(set(lst1))\nprint(lst1)\nprint(lst2)\n", "['jim', 'jennifer', 'mike', 'roy']\n[3, 1, 2, 4]\n"], ["items = [\"jim\",\"jennifer\",\"roy\",\"roy\",\"mike\",\"jim\",\"roy\",\"jim\",\"mike\",\"roy\"]\nlist1 = list(dict.fromkeys(items))\nlist2 = [items.count(i) for i in list1]\n", "['jim', 'jennifer', 'roy', 'mike']\n[3, 1, 4, 2]\n"], [], [], ["   #open terminal or CMD as administrator\n\n$ cd <path Anaconda3 install>\\Scripts\n\n$ activate\n\n$ cd .. \n\n$ conda activate scratch\n"], [], [], ["source ~/miniconda3/etc/profile.d/conda.sh # Or path to where your conda is\nconda activate some-conda-environment\n"], [], ["    div#maintoolbar {\n        display: none !important;\n    }\n", "    div#maintoolbar {\n        display: block !important;\n    }\n"], ["!pip install jupyter-dash\n\nimport plotly.express as px\nfrom jupyter_dash import JupyterDash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\n# Load Data\ndf = px.data.tips()\n# Build App\napp = JupyterDash(__name__)\napp.layout = html.Div([\n    html.H1(\"JupyterDash Demo\"),\n    dcc.Graph(id='graph'),\n    html.Label([\n        \"colorscale\",\n        dcc.Dropdown(\n            id='colorscale-dropdown', clearable=False,\n            value='plasma', options=[\n                {'label': c, 'value': c}\n                for c in px.colors.named_colorscales()\n            ])\n    ]),\n])\n# Define callback to update graph\n@app.callback(\n    Output('graph', 'figure'),\n    [Input(\"colorscale-dropdown\", \"value\")]\n)\ndef update_figure(colorscale):\n    return px.scatter(\n        df, x=\"total_bill\", y=\"tip\", color=\"size\",\n        color_continuous_scale=colorscale,\n        render_mode=\"webgl\", title=\"Tips\"\n    )\n# Run app and display result inline in the notebook\napp.run_server(mode='inline')\n"], [], [], ["server = smtplib.SMTP(host='smtp.gmail.com', port=587)\nserver.starttls()\nserver.login(\"myemail@gmail.com\", \"mypassword\")\nemail_list = [\"xyz@gmail.com\", \"abc@gmail.com\"]\nfor email in email_list:\n    msg = EmailMessage()\n    msg.set_content(\"Test message.\")\n    msg['Subject'] = \"Test Subject!!!\"\n    msg['From'] = \"myemail@gmail.com\"\n    msg['To'] = email\n    server.send_message(msg)\nserver.quit()\n", "msg['To'] = ', '.join(email_list)\n"], ["for email in email_list:\n    msg['To'] = email\n    server = smtplib.SMTP(host='smtp.gmail.com', port=587)\n    server.starttls()\n    server.login(\"myemail@gmail.com\", \"mypassword\")\n    server.send_message(msg)\n    server.quit()\n    del msg['To]\n", "def __setitem__(self, name, val):\n    \"\"\"Set the value of a header.\n\n    Note: this does not overwrite an existing header with the same field\n    name.  Use __delitem__() first to delete any existing headers.\n    \"\"\"\n    max_count = self.policy.header_max_count(name)\n    if max_count:\n        lname = name.lower()\n        found = 0\n        for k, v in self._headers:\n            if k.lower() == lname:\n                found += 1\n                if found >= max_count:\n                    raise ValueError(\"There may be at most {} {} headers \"\n                                     \"in a message\".format(max_count, name))\n    self._headers.append(self.policy.header_store_parse(name, val))\n"], ["os.system('conda run -n <env_name> python <path_to_other_script>')\n"], ["import matplotlib as mpl\nmpl.use('MacOSX')\nimport numpy as np\nimport matplotlib.pyplot as plt\n"], ["import _locale\n_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])\n"], [], [], [], ["import platform\nusing_distro = False\ntry:\n    import distro\n    using_distro = True\nexcept ImportError:\n    pass\n", "if using_distro:\n    linux_distro = distro.like()\nelse:\n    linux_distro = platform.linux_distribution()[0]\n"], ["Segmentation fault: 11\n", "pip uninstall matplotlib \npip install matplotlib \n"], ["pip uninstall matplotlib\npip install matplotlib\n"], [], [], ["n = int(input('Enter the row you want to print:- '))\nsym = input('Enter the symbol you want to print:- ')\nfor i in range(n):\n    print((sym * ((2 * i) + 1)).center((2 * n) - 1, ' '))\n"], [], [], [], ["pip install -U setuptools\n"], ["py -3.8 -m pip install pyautogui\n", "Python 3.8 not found!\nInstalled Pythons found by C:\\Windows\\py.exe Launcher for Windows\n -3.9-64 *\n -3.7-64\n", "py -3.8 -m pip install pyautogui\n", "Successfully installed PyTweening-1.0.3 mouseinfo-0.1.3 pyautogui-0.9.52 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.8.1 pyrect-0.1.4 pyscreeze-0.\n"], [], [], ["error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n"], [], [], [], [" img=cv2.imread(filename)\n    print(img)\n        try:\n           img = cv2.resize(img, (1400, 1000), interpolation=cv2.INTER_AREA)\n            print(img.shape)\n        except:\n        break\n    height, width , layers = img.shape\n    size=(width,height)\n    print(size)\n"], [], ["#include\n\nusing namespace std;\n\nint main() {\n    int rows;\n\n    cout<<\"Enter number of rows :: \";\n    cin>>rows;\n\n    for( int i = 1, j = rows, k = rows; i <= rows*rows; i++ )\n        if( i >= j && cout<<\"*\" && i % k == 0 && (j += rows-1) && cout<<endl || cout<<\" \" );\n}\n"], [], ["apt-get remove python-pip-whl\n", "curl https://bootstrap.pypa.io/get-pip.py | python3\npip install virtualenv\n"], [], [], [], ["php artisan jetstream:install inertia //(inertia or livewire, not both!)\n"], ["pip install jupyter\n"], [], ["from fastapi import Depends, FastAPI\nfrom fastapi_utils.cbv import cbv\nfrom fastapi_utils.inferring_router import InferringRouter\n\n\ndef get_x():\n    return 10\n\n\napp = FastAPI()\nrouter = InferringRouter()  # Step 1: Create a router\n\n\n@cbv(router)  # Step 2: Create and decorate a class to hold the endpoints\nclass Foo:\n    # Step 3: Add dependencies as class attributes\n    x: int = Depends(get_x)\n\n    @router.get(\"/somewhere\")\n    def bar(self) -> int:\n        # Step 4: Use `self.<dependency_name>` to access shared dependencies\n        return self.x\n\n\napp.include_router(router)\n"], ["pytest -s path_to_script.py  \n"], ["'C:\\\\Users\\\\SAVK\\\\Downloads\\\\Ex_Files_Intro_Data_Science\\\\Ex_Files_Intro_Data_Science\\\\Exercise Files\\\\state_baby_names.csv'\n'C:/Users/SAVK/Downloads/Ex_Files_Intro_Data_Science/Ex_Files_Intro_Data_Science/Exercise Files/us_baby_names.csv'\n", "states_babies = pd.read_csv('C:\\\\Users\\\\SAVK\\\\Downloads\\\\Ex_Files_Intro_Data_Science\\\\Ex_Files_Intro_Data_Science\\\\Exercise Files\\\\state_baby_names.csv');\nstates_babies = pd.read_csv('C:/Users/SAVK/Downloads/Ex_Files_Intro_Data_Science/Ex_Files_Intro_Data_Science/Exercise Files/us_baby_names.csv');\n"], ["from starlette.responses import FileResponse\n@app.get(\"/shows/\")\n    def get_items(q: List[str] = Query(None)):\n    '''\n    Pass path to function.\n    Returns folders and files.\n    '''\n\n    results = {}\n\n    query_items = {\"q\": q}\n    if query_items[\"q\"]:\n        entry = PATH + \"/\".join(query_items[\"q\"])\n    else:\n        entry = PATH\n\n    if os.path.isfile(entry):\n        return download(entry)\n\n    dirs = os.listdir(entry + \"/\")\n    results[\"folders\"] = [\n        val for val in dirs if os.path.isdir(entry + \"/\"+val)]\n    results[\"files\"] = [val for val in dirs if os.path.isfile(entry + \"/\"+val)]\n    results[\"path_vars\"] = query_items[\"q\"]\n\n    return results\n\ndef download(file_path):\n    \"\"\"\n    Download file for given path.\n    \"\"\"\n    if os.path.isfile(file_path):\n        return FileResponse(file_path)\n        # return FileResponse(path=file_path)\n    return None\n", "from starlette.responses import FileResponse\nif os.path.isfile(entry):\n    return download(entry)\n"], [], [], ["whereis python | grep 'miniconda'\n", "~/miniconda3/envs/testenv/bin/python3.8 my_python_file.py\n"], ["import cv2\ncap = cv2.VideoCapture(0)\n# fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\nfgbg = cv2.createBackgroundSubtractorMOG2()\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nname = \"C://path//of//your_dir//out.mp4\"\nout = cv2.VideoWriter(name,fourcc, 20,(320,180),False)\n\nwhile(1):\n    ret, frame = cap.read()\n    if (ret == True):\n        resized_frame = cv2.resize(frame,(320,180),fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n        fgmask = fgbg.apply(resized_frame)\n        cv2.imshow('Frame',fgmask)\n        out.write(fgmask)\n        if cv2.waitKey(30) & 0xFF == ord('q'):\n            break\n    else:\n        break\ncap.release()\nout.release()\ncv2.waitKey(5)\ncv2.destroyAllWindows()\n"], ["pyinstaller --hidden-import=\"pkg_resources.py2_warn\" --hidden-import=\"googleapiclient\" --hidden-import=\"apiclient\"  main.py --onefile\n"], ["import tensorflow as tf\nwith tf.compat.v1.Session() as sess:\n    hello = tf.constant('hello world')\n    print(sess.run(hello))\n"], ["pyinstaller -F --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" Datamanager.py\n"], ["a = Analysis(.......  \n\ndatas=[.....\n\n('project\\\\google_api_python_client-1.9.3.dist-info','google_api_python_client-1.9.3.dist-info'),     \n\n.......],\n"], ["from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer([('any_name', OneHotEncoder(), [1])], remainder='passthrough')                        \n\nX = np.array(ct.fit_transform(X), dtype=np.float)\n"], [], [], [], [], [], ["cv2.VideoWriter(\"/your/path/filename.mp4\",fourcc, 20,(320,180))\n"], ["number_space  =  5\nfor   i   in   range (5)\n\n     print('/r')\n\n     print (number_space *\"  \", end='')\n\n     number_space=number_space-1\n\n     for star in range (i+1):\n\n                print(\"*\",end=\"  \")\n"], ["for i in range(1, 6):\n  print (' ' * (5 - i), '* ' * i)\n"], ["STATIC_URL = '/static/'\nSTATICFILES_DIRS=[os.path.join(BASE_DIR,\"static\"), \"templates\"]\n\nMEDIA_URL=\"static/media/\"\nMEDIA_ROOT=os.path.join(BASE_DIR,\"static/media/\")\n"], ["# content of pytest.ini\n[pytest]\nnorecursedirs = .git build node_modules\n"], [], ["fp = webdriver.FirefoxProfile('C:/Users/ASUS//AppData/Roaming/Mozilla/Firefox/Profiles/0rgewd47.default-release')\ndriver = webdriver.Firefox(executable_path='geckodriver', firefox_profile=fp)\ndriver.get('https://zoom.us/j/93459172503?pwd=QkhnMEQ0ZTRZd0grUVJkT2NudmlFZz09')\n"], [], ["from dis import dis\n\n\ndef list_constructor():\n    return list()\n\ndef list_literal():\n    return []\n\n\nprint(\"constructor:\")\ndis(list_constructor)\n\nprint()\n\nprint(\"literal:\")\ndis(list_literal)\n", "constructor:\n  5           0 LOAD_GLOBAL              0 (list)\n              2 CALL_FUNCTION            0\n              4 RETURN_VALUE\n\nliteral:\n  8           0 BUILD_LIST               0\n              2 RETURN_VALUE\n", "$ python3 -m timeit 'list(range(7))'\n1000000 loops, best of 5: 224 nsec per loop\n$ python3 -m timeit '[i for i in range(7)]'\n1000000 loops, best of 5: 352 nsec per loop\n", "print([0, 1, 2])  # [0, 1, 2]\n# print(list(0, 1,2))  # TypeError: list expected at most 1 argument, got 3\n\ntpl = (0, 1, 2)\nprint([tpl])           # [(0, 1, 2)]\nprint(list(tpl))       # [0, 1, 2]\n\nprint([range(3)])      # [range(0, 3)]\nprint(list(range(3)))  # [0, 1, 2]\n\n# list-comprehension\nprint([i for i in range(3)])      # [0, 1, 2]\nprint(list(i for i in range(3)))  # [0, 1, 2]  simpler: list(range(3))\n"], [], ["> python3 -m timeit 'list()'\n10000000 loops, best of 3: 0.0853 usec per loop\n\n> python3 -m timeit '[]'\n10000000 loops, best of 3: 0.0219 usec per loop\n"], ["import dis\n\n\ndef brackets():\n    return []\n\n\ndef list_builtin():\n    return list()\n", "print(dis.dis(brackets))\n  5           0 BUILD_LIST               0\n              2 RETURN_VALUE\n", "print(dis.dis(list_builtin))\n 9           0 LOAD_GLOBAL              0 (list)\n              2 CALL_FUNCTION            0\n              4 RETURN_VALUE\n"], ["sample_touple = ('a', 'C', 'J', 13)\nlist1 = list(sample_touple)\nprint (\"List values \", list1)\n\nstring_value=\"sampletest\"\nlist2 = list(string_value)\nprint (\"List string values : \", list2)\n", "List values  ['a', 'C', 'J', 13]                                                                                                                              \nList string values :  ['s', 'a', 'm', 'p', 'l', 'e', 't', 'e', 's', 't']   \n", "sample = [1,2,3]\n\n"], ["from starlette.responses import FileResponse\n\nreturn FileResponse(file_location, media_type='application/octet-stream',filename=file_name)\n"], [], [], ["python .\\pywin32_postinstall.py -install\n"], [], ["y_train = to_categorical(y_train, 3)\ny_test = to_categorical(y_test, 3)\n"], ["...\nFile \"c:\\python36\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\googleapiclient\\http.py\", line 67, in <module>\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n  File \"c:\\python36\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\n    exec(bytecode, module.__dict__)\n  File \"site-packages\\googleapiclient\\model.py\", line 36, in <module>\n  File \"site-packages\\pkg_resources\\__init__.py\", line 479, in get_distribution\n  File \"site-packages\\pkg_resources\\__init__.py\", line 355, in get_provider\n  File \"site-packages\\pkg_resources\\__init__.py\", line 898, in require\n  File \"site-packages\\pkg_resources\\__init__.py\", line 784, in resolve\npkg_resources.DistributionNotFound: The 'google-api-python-client' distribution was not found and is required by the application\n"], ["conda install -c https://conda.anaconda.org/sdvillal openslide-python\n"], ["from bson.decimal128 import Decimal128, create_decimal128_context\nfrom decimal import localcontext\n\ndecimal128_ctx = create_decimal128_context()\nwith localcontext(decimal128_ctx) as ctx:\n    horiz_val = Decimal128(ctx.create_decimal(\"0.181665435\"))\n    vert_val = Decimal128(ctx.create_decimal(\"0.178799435\"))\n\ndoc = { 'A': { 'B': [ { 'C': { 'Horiz': horiz_val, 'Vert': vert_val } } ] } }\nresult = collection.insert_one(doc)\n# result.inserted_id\n\npprint.pprint(list(collection.find()))\n\n[ {'A': {'B': [{'C': {'Horiz': Decimal128('0.181665435'),\n                      'Vert': Decimal128('0.178799435')}}]},\n  '_id': ObjectId('5ea79adb915cbf3c46f5d4ae')} ]\n"], ["import simplejson as json\nfrom pymongo import MongoClient\nfrom decimal import Decimal\nfrom bson.decimal128 import Decimal128\n\ndef convert_decimal(dict_item):\n    # This function iterates a dictionary looking for types of Decimal and converts them to Decimal128\n    # Embedded dictionaries and lists are called recursively.\n    if dict_item is None: return None\n\n    for k, v in list(dict_item.items()):\n        if isinstance(v, dict):\n            convert_decimal(v)\n        elif isinstance(v, list):\n            for l in v:\n                convert_decimal(l)\n        elif isinstance(v, Decimal):\n            dict_item[k] = Decimal128(str(v))\n\n    return dict_item\n\ndb = MongoClient()['mydatabase']\njson_string = '{\"A\" : {\"B\" : [{\"C\" : {\"Horz\" : 0.181665435,\"Vert\" : 0.178799435}}]}}'\njson_dict = json.loads(json_string, use_decimal=True)\ndb.this_collection.insert_one(convert_decimal(json_dict))\nprint(db.this_collection.find_one())\n", "{'_id': ObjectId('5ea743aa297c9ccd52d33e05'), 'A': {'B': [{'C': {'Horz': Decimal128('0.181665435'), 'Vert': Decimal128('0.178799435')}}]}}\n", "from pymongo import MongoClient\nfrom decimal import Decimal\nfrom bson.decimal128 import Decimal128\n\ndb = MongoClient()['mydatabase']\nyour_number = Decimal('234.56')\nyour_number_128 = Decimal128(str(your_number))\ndb.mycollection.insert_one({'Number': your_number_128})\nprint(db.mycollection.find_one())\n", "{'_id': ObjectId('5ea6ec9b52619c7b39b851cb'), 'Number': Decimal128('234.56')}\n"], [], ["pip install --upgrade jupyterthemes\n"], ["def solveSimpCalc(x, y):\n     return x+y`\n\nFirstNum = int(input(\"Please type the first number:\"))\nSecondNum = int(input(\"Please type the second number:\"))\nresult = solveSimpCalc(FirstNum, SecondNum)\nprint(result)\n"], ["def triangle(val: str, n: int, type: str):\n    for i in range(n):\n        if type == \"regular\":\n            print((i + 1) * val)\n        elif type == \"reversed\":\n            print((n - i) * val)\n        elif type == \"inverted\":\n            print((n - (i + 1)) * \" \" + (i + 1) * val)\n        elif type == \"inverted reversed\":\n            print(i * \" \" + (val * (n - i)))\n        elif type == \"triangle\":\n            print((n - (i + 1)) * \" \" + ((2*i)+1) * val) \n", "triangle(\"*\", 5, 'triangle')\n", "    *\n   ***\n  *****\n *******\n********* \n"], ["def prob2(content):\n    words = content.split()\n    return len(words) == 2 and words[0].lower() == words[1].lower()\n"], ["seq = b.split()\np, q = seq[0]\nif p[0] == q[0]:\n    return True\nelse:\n   return False\n", "p, q = \"hello\"\n", "p, q = b.split()\n", "p, q = seq[:2]\nseq = seq[2:]     # remove the first two words from the `split` sequence\n", "p, q = seq[:2]\nreturn p[0] == q[0]\n"], [], ["def prob2(b):\n    p,q = b.split()\n    return p[0].lower() == q[0].lower()\n"], ["import asyncio\nimport random\n\nimport tqdm\n\n\nasync def factorial(name, number):\n    f = 1\n    for i in range(2, number + 1):\n        await asyncio.sleep(random.random())\n        f *= i\n    return f\"Task {name}: factorial {number} = {f}\"\n\nasync def tq(flen):\n    for _ in tqdm.tqdm(range(flen)):\n        await asyncio.sleep(0.1)\n\n\nasync def main():\n\n    flist = [factorial(\"A\", 2),\n             factorial(\"B\", 3),\n             factorial(\"C\", 4)]\n\n    pbar = tqdm.tqdm(total=len(flist), position=0, ncols=90)\n    for f in asyncio.as_completed(flist):\n        value = await f\n        pbar.set_description(desc=value, refresh=True)\n        tqdm.tqdm.write(value)\n        pbar.update()\n\nif __name__ == '__main__':\n    asyncio.run(main())\n"], [], ["# Plot audio with zoomed in y axis\ndef plotAudio(output):\n    fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(20,10))\n    plt.plot(output, color='blue')\n    ax.set_xlim((0, len(output)))\n    ax.margins(2, -0.1)\n    plt.show()\n\n# Plot audio\ndef plotAudio2(output):\n    fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(20,4))\n    plt.plot(output, color='blue')\n    ax.set_xlim((0, len(output)))\n    plt.show()\n\ndef minMaxNormalize(arr):\n    mn = np.min(arr)\n    mx = np.max(arr)\n    return (arr-mn)/(mx-mn)\n\ndef predictSound(X):\n    clip, index = librosa.effects.trim(X, top_db=20, frame_length=512, hop_length=64) # Empherically select top_db for every sample\n    stfts = np.abs(librosa.stft(clip, n_fft=512, hop_length=256, win_length=512))\n    stfts = np.mean(stfts,axis=1)\n    stfts = minMaxNormalize(stfts)\n    result = model.predict(np.array([stfts]))\n    predictions = [np.argmax(y) for y in result]\n    print(lb.inverse_transform([predictions[0]])[0])\n    plotAudio2(clip)\n\nCHUNKSIZE = 22050 # fixed chunk size\nRATE = 22050\n\np = pyaudio.PyAudio()\nstream = p.open(format=pyaudio.paFloat32, channels=1, \nrate=RATE, input=True, frames_per_buffer=CHUNKSIZE)\n\n#preprocessing the noise around\n#noise window\ndata = stream.read(10000)\nnoise_sample = np.frombuffer(data, dtype=np.float32)\nprint(\"Noise Sample\")\nplotAudio2(noise_sample)\nloud_threshold = np.mean(np.abs(noise_sample)) * 10\nprint(\"Loud threshold\", loud_threshold)\naudio_buffer = []\nnear = 0\n\nwhile(True):\n    # Read chunk and load it into numpy array.\n    data = stream.read(CHUNKSIZE)\n    current_window = np.frombuffer(data, dtype=np.float32)\n    \n    #Reduce noise real-time\n    current_window = nr.reduce_noise(audio_clip=current_window, noise_clip=noise_sample, verbose=False)\n    \n    if(audio_buffer==[]):\n        audio_buffer = current_window\n    else:\n        if(np.mean(np.abs(current_window))<loud_threshold):\n            print(\"Inside silence reign\")\n            if(near<10):\n                audio_buffer = np.concatenate((audio_buffer,current_window))\n                near += 1\n            else:\n                predictSound(np.array(audio_buffer))\n                audio_buffer = []\n                near\n        else:\n            print(\"Inside loud reign\")\n            near = 0\n            audio_buffer = np.concatenate((audio_buffer,current_window))\n\n# close stream\nstream.stop_stream()\nstream.close()\np.terminate()\n"], [], [], ["def solveMeFirst(a,b):\n\n   return a+b\n\nnum1 = int(input(\"Enter first number: \"))\nnum2 = int(input(\"Enter second number: \"))\nres = solveMeFirst(num1,num2)\nprint(res)\n"], [], [], ["function printPyramid($n)\n{\n    //example 1\n    for ($x = 1; $x <= $n; $x++) {\n        for ($y = 1; $y <= $x; $y++) {\n            echo 'x';\n        }\n        echo \"\\n\";\n    }\n\n    // example 2\n    for ($x = 1; $x <= $n; $x++) {\n        for ($y = $n; $y >= $x; $y--) {\n            echo 'x';\n        }\n        echo \"\\n\";\n    }\n\n    // example 3\n\n    for($x = 0; $x < $n; $x++) {\n        for($y = 0; $y < $n - $x; $y++) {\n            echo ' ';\n        }\n        for($z = 0; $z < $x * 2 +1; $z++) {\n            echo 'x';\n        }\n        echo \"\\n\";\n    }\n\n\n}\n"], [], ["python -m pip install --upgrade pip\n"], [], ["pip install 'numpy==1.16' --force-reinstall\n"], ["import tensorflow as tf\nmsg = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(msg))\n", "import tensorflow as tf\nmsg = tf.constant('Hello, TensorFlow!')\ntf.print(msg)\n"], ["import tensorflow as tf\nvalor1 = tf.constant(2)\nvalor2 = tf.constant(3)\ntype(valor1)\nprint(valor1)\nsoma=valor1+valor2\ntype(soma)\nprint(soma)\nsess = tf.compat.v1.Session()\nwith sess:\n    print(sess.run(soma))\n", "import tensorflow as tf\nvalor1 = tf.constant(2)\nvalor2 = tf.constant(3)\ntype(valor1)\nprint(valor1)\nsoma=valor1+valor2\ntype(soma)\nTensor(\"Const_8:0\", shape=(), dtype=int32)\nOut[18]: tensorflow.python.framework.ops.Tensor\n\nprint(soma)\nTensor(\"add_4:0\", shape=(), dtype=int32)\n\nsess = tf.compat.v1.Session()\n\nwith sess:\n    print(sess.run(soma))\n5\n"], ["from sklearn.preprocessing import OneHotEncoder \n\n# sample data\ndf = pd.DataFrame({'col': [0,1,2,3,0,1,2]})\ncolnames = ['col'] # modify this for your df\n\noneHot = OneHotEncoder()\nx_ohe = oneHot.fit_transform(df[colnames].values.reshape(-1,1))\n", "x_ohe.todense()\n"], [], [], ["import nvidia_smi\n\nnvidia_smi.nvmlInit()\n\nhandle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n\ninfo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n\nprint(\"Total memory:\", info.total)\nprint(\"Free memory:\", info.free)\nprint(\"Used memory:\", info.used)\n\nnvidia_smi.nvmlShutdown()\n", "Total memory: 17071734784\nFree memory: 17071734784\nUsed memory: 0\n", "!nvidia-smi\n", "+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"], ["pip install packageName --index-url https://pkgs.dev.azure.com/xxx/xxx/_packaging/xxx/pypi/simple/ -vvv --no-deps\n"], [], [], ["class Toy():\n    def__init__(self, color, age):\n        self.color = color\n        self.age = age\n\n    def another_method(self, f):\n         self.field3 = f + 4\n         return self.field3\n", "class Toy():\n    def__init__(self, color, age):\n        self.color = color\n        self.age = age\n        self.field3 = None\n        self.field4 = 0 # for instance\n        self.field5 = \"\" # for instance\n\n    def another_method(self, f):\n         self.field3 = f + 4\n         return self.field3\n"], ["class Person:\n\n    def __init__(self, name):\n        self.name = name\n\n    def get_a_job(self):\n        self.job = \"Janitor\"\n        print(f\"{self.name} now has a job!\")\n\np1 = Person(\"Tom\")\np2 = Person(\"Bob\")\n\np1.get_a_job()\nprint(p1.job)\n\nprint(p2.job)\n", "Tom now has a job!\nJanitor\nTraceback (most recent call last):\n  File \"...\", line 17, in <module>\n    print(p2.job)\nAttributeError: 'Person' object has no attribute 'job'\n>>> \n"], [], [], [], ["chmod 400 yourPublicKeyFile.pem\n", "scp -i /directory/to/abc.pem /your/local/file/to/copy user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file\n", "scp -i /directory/to/abc.pem user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:path/to/file /your/local/directory/files/to/download\n", "zip -r squash.zip /your/ec2/directory/\n", "scp -i /directory/to/abc.pem user@ec2-xx-xx-xxx-xxx.compute-1.amazonaws.com:~/* /your/local/directory/files/to/download\n"], ["import boto3\nimport os\nimport socket\n\ndef upload_files(path):\n    session = boto3.Session(\n    aws_access_key_id='your access key id',\n    aws_secret_access_key='your secret key id',\n    region_name='region'\n    )\n    s3 = session.resource('s3')\n    bucket = s3.Bucket('bucket name')\n\n    for subdir, dirs, files in os.walk(path):\n    for file in files:\n        full_path = os.path.join(subdir, file)\n        print(full_path[len(path)+0:])\n        with open(full_path, 'rb') as data:\n            bucket.put_object(Key=full_path[len(path)+0:], Body=data)\n\n\nif __name__ == \"__main__\":\n    upload_files('your pathwhich in your case is (/home/ubuntu/)')\n", "import boto3\nimport logzero\nfrom logzero import logger\n\ns3_resource = boto3.resource('s3')\nsqs_client=boto3.client('sqs')\n\n### Queue URL\nqueue_url = 'queue url'\n\n### aws s3 bucket\nbucketName = \"your bucket-name\"\n\n### Receive the message from SQS queue\nresponse_message = sqs_client.receive_message(\nQueueUrl=queue_url,\nMaxNumberOfMessages=1,\n    MessageAttributeNames=[\n    'All'\n],\n)\n\nmessage=response_message['Messages'][0]\nreceipt_handle = message['ReceiptHandle']\nmessageid=message['MessageId']\nfilename=message['Body']\n\ntry:\n    s3_resource.Bucket(bucketName).download_file(filename,filename)\nexcept botocore.exceptions.ClientError as e:\n    if e.response['Error']['Code']=='404':\n        logger.info(\"The object does not exist.\")\n\n    else:\n        raise\n\nlogger.info(\"File Downloaded\")\n", "import boto3\n\n### S3 connection\ns3_resource = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nbucketName = 'your bucket-name'\nresponse = s3_client.list_objects_v2(Bucket=bucketName)\nall = response['Contents']        \nlatest = max(all, key=lambda x: x['LastModified'])\ns3 = boto3.resource('s3')\nkey=latest['Key']\n\nprint(\"downloading file\")\ns3_resource.Bucket(bucketName).download_file(key,key)\nprint(\"file download\")\n"], ["aws configure\naws s3 cp /home/ubuntu/bandsintown/sf_events.json s3://mybucket/sf_events.json\n", "s3 = boto3.resource('s3')\n\ndef upload_file_to_s3(s3_path, local_path):\n    bucket = s3_path.split('/')[2] #bucket is always second as paths are S3://bucket/.././\n    file_path = '/'.join(s3_path.split('/')[3:])\n    response = s3.Object(bucket, file_path).upload_file(local_path)\n    return response\n\ns3_path = \"s3://mybucket/sf_events.json\"\nlocal_path = \"/home/ubuntu/bandsintown/sf_events.json\"\nupload_file_to_s3(s3_path, local_path)\n", "aws configure\naws s3 cp s3://mybucket/sf_events.json /home/ubuntu/bandsintown/sf_events.json\n", "s3 = boto3.resource('s3')\n\ndef download_file_from_s3(s3_path, local_path):\n    bucket = s3_path.split('/')[2] #bucket is always second as paths are S3://bucket/.././\n    file_path = '/'.join(s3_path.split('/')[3:])\n    filename = os.path.basename(s3_path) \n    s3.Object(bucket, file_path).download_file(local_file_path)\n\ns3_path = \"s3://mybucket/sf_events.json\"\nlocal_path = \"/home/ubuntu/bandsintown/sf_events.json\"\ndownload_file_from_s3(s3_path, local_path)\n\n"], ["scp <username>@<your ec2 instance host or IP>:/home/ubuntu/bandsintown/sf_events.json ./\n"], ["conda env list\n"], ["# From Phantom's code\ninput_number = int(input('enter a number: '))\nprint(input_number)\n\n# An intermediate variable to hold the number\nworker = abs(input_number)\n\n# for loop that is equal to one less than the number of digits\nwhile worker > 10 :   # Edit - Thanks JohanC for the suggestion in the comments\n    # take the last digit and subtract it from the remainging digits\n    worker = int(str(worker)[0:(len(str(worker))-1)]) - int(str(worker)[-1])\n    print(worker)\n\n# Finish with a print of the divisibility\nif worker == 0 : \n    print('The number {} is divisible by 11.'.format(input_number))\nelse : \n    print('The number {} is not divisible by 11.'.format(input_number))\n"], [], ["unitsDigit = Number % 10\n", "Number = Number // 10\n", "Number -= unitsDigit\n"], [], ["import ffmpeg\n\nvideo_stream = ffmpeg.input('Of Monsters and Men - Wild Roses.mp4')\naudio_stream = ffmpeg.input('Of Monsters and Men - Wild Roses_audio.mp4')\nffmpeg.output(audio_stream, video_stream, 'out.mp4').run()\n"], [], [], ["df = (\"./Downloads/folder/myfile.pdf\")\noutput = \"./Downloads/folder/test.csv\"\ntabula.convert_into(df, output, output_format=\"csv\", stream=True)\n"], ["def my_mse_loss_b(b):\n     def mseb(y_true, y_pred):\n         ...\n         a = np.ones_like(y_true) #numpy array here is not recommended\n         return K.mean(K.square(y_pred - y_true)) + a\n     return mseb\n", "def my_mse_loss_b(b):\n     def mseb(y_true, y_pred):\n         ...\n         a = K.ones_like(y_true) #use Keras instead so they are all symbolic\n         return K.mean(K.square(y_pred - y_true)) + a\n     return mseb\n"], ["import sklearn.utils._cython_blas\n", "import sklearn.neighbors.typedefs\nimport sklearn.neighbors.quad_tree\nimport sklearn.tree\nimport sklearn.tree._utils\n"], [], ["pip3 install --upgrade --force-reinstall tensorflow-gpu==1.15.0 \n"], ["pip uninstall setuptools\n", "conda install -c anaconda setuptools\n"], ["import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n"], [], [], [], ["conda update --force conda\n"], [], ["pip install jupyterlab\n", "jupyter lab\n"], ["fp = open(\"/Users/siva/Desktop/siva5.txt\")\n"], ["csvFile = '/content/drive/My Drive/Colab Notebooks/myData.csv.txt'\nxmlFile = '/content/drive/My Drive/Colab Notebooks/myData.xml'\n"], [], ["shell_process = subprocess.Popen([file_name],shell=True) \nprint(shell_process.pid)\n", "parent = psutil.Process(shell_process.pid)\nchildren = parent.children(recursive=True)\nprint(children)\nchild_pid = children[0].pid\nprint(child_pid)\n", "os.kill(child_pid, signal.SIGTERM)\n# or\nsubprocess.check_output(\"Taskkill /PID %d /F\" % child_pid)\n", "os.kill(shell_process.pid, signal.SIGTERM)\n"], ["with open('foo') as f:\n    foo = f.read()\n"], ["out = {}\n\nfor k,v in [[k2,p[k1][k2]] for k1 in p for k2 in p[k1]]:\n    if k in out.keys():\n        out[k] = out[k] + v\n    else:\n        out[k] = v\n\nprint(max(out, key=out.get))\n", "from itertools import groupby    \nout = sorted([[k2,p[k1][k2]] for k1 in p for k2 in p[k1]])\nresult = {a:sum(c for _, c in b) for a, b in groupby(out, key=lambda x:x[0])}\nprint(max(result,key=result.get))\n", "out = sum(map(Counter, p.values()), Counter())\nprint(max(out,key=result.get))\n"], ["pokemon_total = {}\n\nfor player, dictionary in pokemon_go_data.items():\n    for pokemon, candy_count in dictionary.items():\n        if pokemon in pokemon_total.keys():\n            pokemon_total[pokemon] += candy_count\n        else:\n            pokemon_total[pokemon] = candy_count\n\nmost_common_pokemon = max(pokemon_total, key=pokemon_total.get)\n\nprint(most_common_pokemon)\n"], [], [], [], [], ["df.set_index('A', append=True)['B'].unstack(-1)\n", "A            Y1   Y2\nDate\n1988-01-01  NaN  NaN\n1988-01-04  6.0  8.0\n", ">> df\n\n             A    B\nDate\n1988-01-01  Y1  NaN\n1988-01-01  Y2  NaN\n1988-01-04  Y1  6.0\n1988-01-04  Y2  8.0\n1988-01-01  Y1  NaN\n1988-01-01  Y2  NaN\n1988-01-04  Y1  6.0\n1988-01-04  Y2  8.0\n", "df.set_index('A', append=True).groupby(level=[0, 1])['B'].sum(min_count=1).unstack(-1)\n", "A             Y1    Y2\nDate\n1988-01-01   NaN   NaN\n1988-01-04  12.0  16.0\n"], ["pivot_df = pd.pivot_table(df, values='B', index=['Date'], columns=['A'],\n                          aggfunc=['sum','count'])\n\n# build the mask from count\nmask = (pivot_df.xs('count', axis=1) == 0)   # or ...<min_limit\n\n#build the actual pivot_df from sum\npivot_df = pivot_df.xs('sum', axis=1)\n\n# and reset to NaN when not enough values\npivot_df[mask] = np.nan\n", "A            Y1   Y2\nDate                \n1988-01-01  NaN  NaN\n1988-01-04  6.0  8.0\n"], [], ["(df.groupby(['Date', 'A']).B\n   .apply(lambda x: np.nan if x.isna().all() else x.sum())\n   .unstack('A')\n)\n", "A            Y1   Y2\nDate                \n1988-01-01  NaN  NaN\n1988-01-04  6.0  8.0\n"], ["source shell_script.sh\n"], ["obj = {'name': \"Home\",\n       'url': \"/\",\n       'data': num if num > 0 else None\n      }\n", "obj = {'name': \"Home\",\n       'url': \"/\"}\nif num > 0:\n    obj['data'] = num\n"], ["def addSum(num):\n    obj = {\n          'name': \"Home\",\n          'url': \"/\",      \n    }\n    if num > 0:\n        obj['data'] = num\n\n    return obj\n"], ["def addSum(num):\n    obj = {\n        'name': \"Home\",\n        'url': \"/\"\n    }\n    if num > 0:\n        obj['data'] = num;\n"], ["def addSum(num):\n    obj = {\n        'name': \"Home\",\n        'url': \"/\"\n    }\n    if num > 0: obj['data'] = num\n    return obj\n\nprint(addSum(3))   # {'name': 'Home', 'url': '/', 'data': 3}\nprint(addSum(0))   # {'name': 'Home', 'url': '/'}\n"], [], [], [], ["'DIRS': [os.path.join(BASE_DIR, 'templates')],\n"], ["ssh-keygen -p -m PEM -f ~/.ssh/id_rsa\n"], [], ["$ pipenv --where\n/home/wonder/workspace/myproj\n", "$ pipenv --venv\n/home/wonder/PyEnvs/myproj-BKbQCeJj\n"], ["pd.DataFrame(data=no_col_names_df, columns=col_names_df.columns)\n", "no_col_names_df.reindex(col_names_df.columns, axis=1)\n"], ["no_col_names_df.columns = col_names_df.columns\n\n     col1  col2  col3\n0     1     2     3\n1     4     5     6\n2     7     8     9\n"], ["In [4]: new_df_with_col_names = pd.DataFrame(data=no_col_names_df, columns=col_names_df.columns)\n\nIn [5]: new_df_with_col_names\nOut[5]:\n   col1  col2  col3\n0   NaN   NaN   NaN\n1   NaN   NaN   NaN\n2   NaN   NaN   NaN\n\nIn [6]: new_df_with_col_names = pd.DataFrame(data=no_col_names_df.values, columns=col_names_df.columns)\n\nIn [7]: new_df_with_col_names\nOut[7]:\n   col1  col2  col3\n0     1     2     3\n1     4     5     6\n2     7     8     9\n"], ["new_df_with_col_names = pd.DataFrame(data=no_col_names_df.values, columns=col_names_df.columns)\n", "   col1  col2  col3\n0     1     2     3\n1     4     5     6\n2     7     8     9\n"], [], [">>> f'{2.1:g}'\n'2.1'\n>>> f'{2.0:g}'\n'2'\n"], [], [], [">>> 4/2\n2.0\n>>> int(4/2)\n2\n"], ["In [119]: np.array([1,0,2,0],dtype=bool)                                             \nOut[119]: array([ True, False,  True, False])\n\nIn [120]: np.array([1, False, 2, False, []])                                         \nOut[120]: array([1, False, 2, False, list([])], dtype=object)\n", "In [124]: np.bool_(True)                                                             \nOut[124]: True\nIn [125]: type(np.bool_(True))                                                       \nOut[125]: numpy.bool_\nIn [126]: np.bool_(True) is True                                                     \nOut[126]: False\nIn [127]: type(True)                                                                 \nOut[127]: bool\n", "In [129]: np.array([1, False, 2, np.bool_(False), []])                               \nOut[129]: array([1, False, 2, False, list([])], dtype=object)\nIn [130]: [i is False for i in _]                                                    \nOut[130]: [False, True, False, False, False]\n"], ["np_bool = np.bool(True)\npy_bool = True\n\nprint(isinstance(np_bool, bool)) # True\nprint(isinstance(py_bool, bool)) # True\n", "# Regular list of int\narr0 = [-2, -1, 0, 1, 2]\n\n# Python list of bool\narr1 = [True, False, True, False]\n\n# Numpy list of bool, from int / bool\narr3_a = np.array([-2, -1, 0, 1, 2], dtype=bool)\narr3_b = np.array([True, False, True, False], dtype=bool)\n\nprint(isinstance(arr0[0], int))    # True\nprint(isinstance(arr1[0], bool))   # True\n\nprint(isinstance(arr3_a[0], bool)) # False\nprint(isinstance(arr3_b[0], bool)) # False\n", "arr3_a = np.array([-2, -1, 0, 1, 2], dtype=bool)\n\nx = (bool(arr3_a[0]) is True)\nprint(isinstance(x, bool)) # True\n", "arr3_a = np.array([-2, -1, 0, 1, 2], dtype=bool)\n\nfor c in range(0, len(arr3_a)):\n    if ( bool(arr3_a[c]) == True ):\n        print((\"List value {} is True\").format(c))\n    else:\n        print((\"List value {} is False\").format(c))\n"], [], ["with tf.device('/cpu:0'):\n    #enter code here of tf data\n", "\"/cpu:0\": The CPU of your machine.\n\"/device:GPU:0\": The GPU of your machine, if you have one.\n\"/device:GPU:1\": The second GPU of your machine, etc.\n", "with tf.device('/device:GPU:0'):\n  #code here: tf data and model\n"], ["01\n02\n03\n04\n05\n06\n07\n08\n09\n", "10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n", "for 00..09 we have 0*10+45\nfor 10..19 we have 1*10+45\nfor 20..29 we have 2*10+45\n...\nfor d0..d9 we have d*10+45\n", "   For n < 100 equal to (floor(n/10) + n mod 10) \n", "dsum(10**d - 1) = dsum(10**(d-1) - 1) * 10 + 45*10**(d-1) \n", "dsum(b) - dsum(a)\n"], ["def sum_of_products(lst, s, f):\n    result = 0\n    for i, item in enumerate(range(s, f+1)):\n        lst[i] = list(map(int, str(item)))\n        result += sum(lst[i])\n    return result\n\nlst = [x for x in range(0, 10)]\nx = sum_of_products(lst, 14, 20)\nprint(x)\n"], ["def sum_digits(a, b):\n    sum = 0\n    for i in range(a, b + 1):\n        number = i\n        while (number > 0):\n            sum += number % 10\n            number = number // 10\n    return sum\n\nprint(sum_digits(17, 20))\n"], ["def sum_digits(a, b):\n    total = 0\n    for number in range(a,b+1):\n        total += sum(int(digit) for digit in str(number))\n    return total\n"], ["def sum_digits(a, b):\n    sum = 0\n    for i in range(a,b+1):\n        for e in (str(i)):\n            sum += int(e)\n    return sum\n\nprint(sum_digits(17, 20))\n"], ["startNumber = 1\nendNumber = 5\ntotal = 0;\nfor i in range(startNumber,endNumber+1):\n    print(i)\n    total += i\n\nprint total\n"], ["pip install numpy --upgrade\n"], ["bash -i shell_script.sh\n"], [], [], ["pip install ipython\n", "import IPython.nbformat.current as convert\nconv = convert.read(open('source.py', 'r'), 'py')\nconvert.write(conv, open('source.ipynb', 'w'), 'ipynb')\n"], [">>> from pyspark.sql import functions as F\n>>> from pyspark.sql.functions import *\n>>> from pyspark.sql.types import StringType\n>>> concat_udf = F.udf(lambda cols: \"\".join([str(x) if x is not None else \"*\" for x in cols]), StringType())\n\n>>> rdd = sc.parallelize([[100,30,105,35],[200,55,85,65],[300,20,125,90]])\n>>> df = rdd.toDF(['store_id','qty_on_hand_milk','qty_on_hand_bread','qty_on_hand_eggs'])\n\n>>> df.show()\n+--------+----------------+-----------------+----------------+\n|store_id|qty_on_hand_milk|qty_on_hand_bread|qty_on_hand_eggs|\n+--------+----------------+-----------------+----------------+\n|     100|              30|              105|              35|\n|     200|              55|               85|              65|\n|     300|              20|              125|              90|\n+--------+----------------+-----------------+----------------+\n\n#adding one more column with arrayed values of all three columns\n>>> df_1=df.withColumn(\"new_col\", concat_udf(F.array(\"qty_on_hand_milk\", \"qty_on_hand_bread\",\"qty_on_hand_eggs\")))\n#convert it into array<int> for carrying out agg operations\n>>> df_2=df_1.withColumn(\"new_col_1\",split(col(\"new_col\"), \",\\s*\").cast(\"array<int>\").alias(\"new_col_1\"))\n#posexplode gives you the position along with usual explode which helps in categorizing\n>>> df_3=df_2.select(\"store_id\",  posexplode(\"new_col_1\").alias(\"col_1\",\"qty\"))\n#if else conditioning for category column\n>>> df_3.withColumn(\"category\",F.when(col(\"col_1\") == 0, \"milk\").when(col(\"col_1\") == 1, \"bread\").otherwise(\"eggs\")).select(\"store_id\",\"category\",\"qty\").show()\n+--------+--------+---+\n|store_id|category|qty|\n+--------+--------+---+\n|     100|    milk| 30|\n|     100|   bread|105|\n|     100|    eggs| 35|\n|     200|    milk| 55|\n|     200|   bread| 85|\n|     200|    eggs| 65|\n|     300|    milk| 20|\n|     300|   bread|125|\n|     300|    eggs| 90|\n+--------+--------+---+\n\n#aggregating to find sum\n>>> df_3.withColumn(\"category\",F.when(col(\"col_1\") == 0, \"milk\").when(col(\"col_1\") == 1, \"bread\").otherwise(\"eggs\")).select(\"category\",\"qty\").groupBy('category').sum().show()\n+--------+--------+\n|category|sum(qty)|\n+--------+--------+\n|    eggs|     190|\n|   bread|     315|\n|    milk|     105|\n+--------+--------+\n>>> df_3.printSchema()\nroot\n |-- store_id: long (nullable = true)\n |-- col_1: integer (nullable = false)\n |-- qty: integer (nullable = true)\n"], [], ["server{\n...\n    location /static/ {\n        alias /path/to/static/;\n        ...\n    }\n...\n}\n", "<VirtualHost *:80>\n...\nAlias /static \"/path/to/static/\"  \n<Directory \"/path/to/static\">  \n    Order allow,deny\n    Allow from all \n</Directory>\n</VirtualHost>\n"], [], [], [], ["strList = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"]\n[i.lower().replace(',', '').split() for i in strList]\n"], ["a = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"]\nb = [[j.lower().replace(',', '') for j in i.split()] for i in a]\n\nb\n'''\nOutputs:[['hello', 'my', 'name', 'is', 'john'],\n         ['good', 'afternoon', 'my', 'name', 'is', 'david'],\n         ['i', 'am', 'three', 'years', 'old']]\n'''\n"], ["ls = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"]\n\noutput_ls = [[word.lower().rstrip(',') for word in sentence.split()] for sentence in ls]\n", "[['hello', 'my', 'name', 'is', 'john'], ['good', 'afternoon', 'my', 'name', 'is', 'david'], ['i', 'am', 'three', 'years', 'old']]\n"], ["strlist = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"]\n>>>[x.replace(',','').lower().split() for x in strlist]\n[['hello', 'my', 'name', 'is', 'john'], ['good', 'afternoon', 'my', 'name', 'is', 'david'], ['i', 'am', 'three', 'years', 'old']]\n"], ["import re\n\ndef split_and_lower(s): \n    return list(map(str.lower, re.split(s, '[^\\w]*'))) \n\nL = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"] \nresult = list(map(split_and_lower, L))\nprint(result)\n", "[['hello', 'my', 'name', 'is', 'john'],\n ['good', 'afternoon', 'my', 'name', 'is', 'david'],\n ['i', 'am', 'three', 'years', 'old']]\n"], ["x = [\"Hello, My Name is John\", \"Good Afternoon, my name is David\", \"I am three years old\"]\n\nz = []\n\nfor i in x:\n    # Replacing \",\" , converting to lower and then splitting\n    z.append(i.replace(\",\",\" \").lower().split())\n\nprint z\n", "[['hello', 'my', 'name', 'is', 'john'], ['good', 'afternoon', 'my', 'name', 'is', 'david'], ['i', 'am', 'three', 'years', 'old']]\n"], [], ["python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\n", "2019-02-16 13:12:40.611105: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\ntf.Tensor(-1714.2305, shape=(), dtype=float32)\n"], [], ["-----BEGIN RSA PRIVATE KEY-----\n"], [], [], [], [], ["import xlsxwriter\n\nd = {'66': 74, '62': 32, '69': 18}\n\n# Create an new Excel file and add a worksheet.\nwith xlsxwriter.Workbook('demo.xlsx') as workbook:\n\n    # Add worksheet\n    worksheet = workbook.add_worksheet()\n\n    # Write headers\n    worksheet.write(0, 0, 'Start')\n    worksheet.write(0, 1, 'Quanitity')\n\n    # Write dict data\n    for i, (k, v) in enumerate(d.items(), start=1):\n        worksheet.write(i, 0, k)\n        worksheet.write(i, 1, v)\n", "import xlsxwriter\n\nd = [('66', 74), ('62', 32), ('69', 18)]\n\n# Create an new Excel file and add a worksheet.\nwith xlsxwriter.Workbook('demo.xlsx') as workbook:\n\n    # Add worksheet\n    worksheet = workbook.add_worksheet()\n\n    # Write headers\n    worksheet.write(0, 0, 'Start')\n    worksheet.write(0, 1, 'Quanitity')\n\n    # Write list data\n    for i, (k, v) in enumerate(d, start=1):\n        worksheet.write(i, 0, k)\n        worksheet.write(i, 1, v)\n"], ["import pandas as pd\n\nkeys = my_dict.keys()\nvalues = my_dict.values()\n", "df = pd.DataFrame({\"Start\": keys, \"Quantity\": values})\ndf.to_csv(\"fname.csv\")\n", "df.to_excel(\"fname.xlsx\")   \n"], ["df = pd.DataFrame.from_records(list(data.items()), columns=['Start', 'Quantity'])\n\nwriter = pd.ExcelWriter('out.xlsx')\ndf.to_excel(writer, 'Sheet1', index=False)\nwriter.save()\n"], ["import pandas as pd\n\ndic = {'66': 74, '62': 32, '69': 18, '72': 14, '64': 37, '192': 60, '51': 70, '46': 42, '129': 7, '85': 24, '83': 73, '65': 14, '87': 28, '185': 233, '171': 7, '176': 127, '89': 42, '80': 32, '5':\n54, '93': 56, '104': 53, '138': 7, '162': 28, '204': 28, '79': 46, '178': 60, '144': 21, '90': 136, '193': 42, '88': 52, '212': 22, '199': 35, '198': 21, '149': 22, '84': 82, '213': 49, '47': 189, '195': 46, '31': 152, '71': 21, '70': 4, '207': 7, '158': 14, '109': 7, '163': 46, '142': 14, '94': 14, '173': 11, '78': 7, '134': 7, '96': 7, '128': 7, '54': 14, '63': 4, '120': 28, '121': 7, '37': 22, '13': 7, '45': 14, '23': 10, '180': 7, '50': 14, '188': 35, '24': 7, '139': 18, '148': 12, '151': 4, '2': 18, '34': 4, '77': 32, '81': 44, '82': 11, '92': 19, '95': 29, '98': 7, '217': 21, '172': 14, '35': 148, '146': 7, '91': 21, '103': 21, '184': 28, '165': 7, '108': 7, '112': 7, '118': 7, '159': 7, '183': 7, '186': 7, '205': 7, '60': 7, '67': 7, '76': 7, '86': 7, '209': 7, '174': 7, '194': 1}\n\ntable = pd.DataFrame(dic, index=[0])\ny = [int(item) for item in table.columns.tolist()]\ntable.loc[1] = table.loc[0]\ntable.loc[0] = y\ntable = table.transpose()\ntable.columns = ['Start', 'Quantity']\ntable.index = list(range(len(table.index)))\nwriter = pd.ExcelWriter('output.xlsx')\ntable.to_excel(writer,'Sheet1', index = False)\nwriter.save()\n"], [], ["size = 7\nm = (2 * size) - 2\nfor i in range(0, size):\n    for j in range(0, m):\n        print(end=\" \")\n    m = m - 1 # decrementing m after each loop\n    for j in range(0, i + 1):\n        # printing full Triangle pyramid using stars\n        print(\"* \", end=' ')\n    print(\" \")\n"], ["# How to run a Dash app in Google Colab\n\n## Requirements\n\n### Install ngrok\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n\n### Run ngrok to tunnel Dash app port 8050 to the outside world. \n### This command runs in the background.\nget_ipython().system_raw('./ngrok http 8050 &')\n\n### Get the public URL where you can access the Dash app. Copy this URL.\n! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n\n### Install Dash\n!pip install dash==0.31.1  # The core dash backend\n!pip install dash-html-components==0.13.2  # HTML components\n!pip install dash-core-components==0.39.0  # Supercharged components\n!pip install dash-table==3.1.7  # Interactive DataTable component (new!)\n", "### Run Dash app\n!python my_app1.py\n"], ["df['col_A_0_col_B_1'] = ~df['col_A'] & df['col_B']\n"], ["df['col_A_0_col_B_1'] = ((df['col_A']==0)&(df['col_B']==1)).astype(int)\n", "df['col_A_0_col_B_1'] = ((df['col_A'].values==0)&(df['col_B'].values==1)).astype(int)\n", "np.random.seed(343)\n#10k rows\ndf = pd.DataFrame(np.random.choice([0,1], size=(10000, 2)), columns=['col_A','col_B'])\n#print (df)\n"], ["df['col_A_0_col_B_1'] = np.where((df['col_A']==0)&(df['col_B']==1), 1, 0)\n"], ["import pandas as pd\ndata = [[0, 1],\n        [0, 0],\n        [0, 1],\n        [0, 1],\n        [1, 0],\n        [1, 0],\n        [1, 1]]\n\ndf = pd.DataFrame(data=data, columns=['col_A', 'col_B'])\ndf['col_A_0_col_B_1'] = pd.Series([a == 0 and b == 1 for a, b in zip(df.col_A, df.col_B)], dtype='uint')\nprint(df)\n", "   col_A  col_B  col_A_0_col_B_1\n0      0      1                1\n1      0      0                0\n2      0      1                1\n3      0      1                1\n4      1      0                0\n5      1      0                0\n6      1      1                0\n", "df = pd.DataFrame(data=data, columns=['col_A', 'col_B'])\ndf['col_A_0_col_B_1'] = pd.Series((df.col_A == 0) & (df.col_B == 1), dtype='uint')\nprint(df)\n"], []]