[[], ["sudo add-apt-repository universe\n", "sudo apt update \nsudo apt install python2\n", "curl https://bootstrap.pypa.io/pip/2.7/get-pip.py --output get-pip.py\n", "sudo python2 get-pip.py\n", "pip2 --version\n", " pip 20.0.2 from /usr/local/lib/python2.7/dist-packages/pip (python 2.7)\n"], [], ["from google.colab import files\nimport json\nimport pandas as pd\nimport io\n\nprint('\\x1b[1;31m'+'Please Upload File'+'\\x1b[0m')\ndata = files.upload()\ndf = pd.read_json(io.BytesIO(data[list(data)[0]]))\n"], [], [], [], [], [], ["def line(x, y, name):\n    return [go.Scatter(x=x, y=y, mode='lines', line=dict(color=\"rgb(0, 0, 255)\"), name=name)]\n\ndef line_with_error_band(x, y, y_err, name):\n    \n    return [\n        go.Scatter(x=x, y=y, mode='lines', line=dict(color=\"rgb(255, 0, 0)\"), name=name),\n        go.Scatter(x=x, y=y+y_err, mode='lines', line=dict(width=0), showlegend=False),\n        go.Scatter(x=x, y=y-y_err, line=dict(width=0), mode='lines', fillcolor=\"rgba(255, 0, 0, 0.3)\", fill='tonexty',\n                   showlegend=False)\n    ]\n", "import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\n\ndef line(x, y, name):\n    return [go.Scatter(x=x, y=y, mode='lines', line=dict(color=\"rgb(0, 0, 255)\"), name=name)]\n\ndef line_with_error_band(x, y, y_err, name):\n    \n    return [\n        go.Scatter(x=x, y=y, mode='lines', line=dict(color=\"rgb(255, 0, 0)\"), name=name),\n        go.Scatter(x=x, y=y+y_err, mode='lines', line=dict(width=0), showlegend=False),\n        go.Scatter(x=x, y=y-y_err, line=dict(width=0), mode='lines', fillcolor=\"rgba(255, 0, 0, 0.3)\", fill='tonexty',\n                   showlegend=False)\n    ]\n\n# Generate example data\n# timeseries y_pred and y_pred_std are starting 5 days later than y_obs \nnp.random.seed(42)\nx_date = pd.to_datetime(list(range(10)), unit='D', origin=pd.Timestamp('2023-01-01'))\ny_obs = np.random.uniform(low=0, high=3, size=10).tolist()\ny_pred = np.append(([np.nan] * 5), (y_obs*np.random.uniform(low=0, high=2, size=10))[5:])\ny_pred_std = np.append(([np.nan] * 5), [0.1,0.2,0.3, 0.4, 0.5])\ndf = pd.DataFrame({\"x_date\" : x_date, \"y_obs\" : y_obs, \"y_pred\" : y_pred, \"y_pred_std\": y_pred_std })\n\nfig = go.Figure()\nfig.add_traces(line(df.x_date, df.y_obs, name=\"observed\"))\nfig.add_traces(line_with_error_band(df.x_date, df.y_pred, df.y_pred_std, name=\"predicted\"))\n"], ["from google.colab import drive\ndrive.mount('/content/drive/')\n", "ValueError                               Traceback (most recent call last)\n<ipython-input-45-9667a744255b> in <module>()\n       1 from google.colab import drive\n ----> 2 drive.mount('content/drive/')\n"], [], [], ["[i in b for i in a]\n"], [], ["data=text.replace(\"\\\\\",\" \")\njs_data = json.loads(data)\n"], [], [], ["python -V\n", "pip -V\n", "pip list\n", "Package    Version\n---------- -------\npip        23.1.2\nsetuptools 60.2.0\nwheel      0.37.1\n", "pip3 install pygame\n"], [], [], ["Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.13.1 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import numpy as np\n\nIn [2]: a = np.zeros(1,np.uint64)\n\nIn [3]: a\nOut[3]: array([0], dtype=uint64)\n\nIn [4]: a[0] -= 1\n\nIn [5]: a\nOut[5]: array([18446744073709551615], dtype=uint64)\n\nIn [6]: a[0] - 1\nOut[6]: 1.8446744073709552e+19\n\nIn [7]: a[0] - 1 == 2**64\nOut[7]: True\n\nIn [8]: a[0] -= 1\n<ipython-input-8-9ab639258820>:1: RuntimeWarning: invalid value encountered in cast\n  a[0] -= 1\n\nIn [9]: a\nOut[9]: array([9223372036854775808], dtype=uint64)\n\nIn [10]: f'{a[0]:b}'\nOut[10]: '1000000000000000000000000000000000000000000000000000000000000000'\n\nIn [11]: len(_)\nOut[11]: 64\n\nIn [12]: a[0] == 2**63\nOut[12]: True\n\nIn [13]: a[0] - 1\nOut[13]: 9.223372036854776e+18\n\nIn [14]: a[0] - 1 == 2 ** 63\nOut[14]: True\n\nIn [15]: a[0] -= 1\n\nIn [16]: a[0]\nOut[16]: 9223372036854775808\n\nIn [17]: np.version.version\nOut[17]: '1.24.2'\n", "In [6]: a[0] - 1\nOut[6]: 1.8446744073709552e+19\n\nIn [7]: a[0] - 1 == 2**64\nOut[7]: True\n", "In [8]: a[0] -= 1\n<ipython-input-8-9ab639258820>:1: RuntimeWarning: invalid value encountered in cast\n  a[0] -= 1\n\nIn [9]: a\nOut[9]: array([9223372036854775808], dtype=uint64)\n"], ["import pandas as pd\nimport plotly.express as px\n\nd = {'col1': [1, 2, 3], 'col2': [3, 4, 5]}\ndf = pd.DataFrame(data=d)\nfig = px.line(df, x=df.index, y=['col1', 'col2'])\n", "new = {'col1':'Front hello', 'col2': 'hi'}\nfig.for_each_trace(lambda t: t.update(name = new[t.name]))\nfig.show()\n"], [], [], [], ["a[0] -= np.uint64(1)\n", "a[[0]] -= 1\n", "a[0:1] -= 1\n"], ["man\n|- __init__.py\n|- Mans\n   |- __init__.py\n   |- man1.py\n|- MansTest\n   |- __init__.py\n   |- SoftLib\n      |- Soft\n         |- __init__.py\n         |- SoftWork\n            |- __init__.py\n            |- manModules.py\n      |- Unittests\n         |- __init__.py\n         |- man1test.py\n", "from Soft.SoftWork.manModules import *\n# no change to import statement but need to add Soft to PYTHONPATH\n\ndef foo():\n    print(\"called foo in man1.py\")\n    print(\"foo call module1 from manModules: \" + module1())\n", "# no need for \"from ...MansTest.SoftLib import Soft\" to facilitate importing..\nfrom ...Mans import man1\n\nman1.foo()\n", "def module1():\n    return \"module1 in manModules\"\n", "$ python3 -m man.MansTest.Unittests.man1test\nTraceback (most recent call last):\n  ...\n    from ...Mans import man1\n  File \"/temp/man/Mans/man1.py\", line 2, in <module>\n    from Soft.SoftWork.manModules import *\nModuleNotFoundError: No module named 'Soft'\n\n$ PYTHONPATH=$PYTHONPATH:/temp/man/MansTest/SoftLib\n$ export PYTHONPATH\n$ echo $PYTHONPATH\n:/temp/man/MansTest/SoftLib\n$ python3 -m man.MansTest.Unittests.man1test\ncalled foo in man1.py\nfoo called module1 from manModules: module1 in manModules \n"], [], [], ["numbers = input().split()\nnegative_numbers = []\n\nfor num in numbers:\n    if int(num) < 0:\n        negative_numbers.append(int(num))\n\nnegative_numbers.sort(reverse=True)\nfor num in negative_numbers:\n    print(num, end=' ')\n"], ["from uszipcode import SearchEngine\n\nsr = SearchEngine()\nzipcodes = sr.by_coordinates(42, -71, radius=300000000, returns=0, zipcode_type=None)\n"], ["import os\nos.chdir(folder_address_which_contain_your_file)\n"], [], ["wmp = win32com.client.dynamic.Dispatch(\"WMPlayer.OCX\")\nwmp.settings.autoStart = True\nwmp.settings.volume = 100\nwmp.URL = file\nwhile globals()[\"allowSound\"]:\n    PumpWaitingMessages()\n"], [], [], [], [], [], [], [], ["sql = (\"INSERT INTO LOCATIONS (location_id, place_name) VALUES (%s, %s)\")\nself.connection.ping()  # reconnecting mysql\nwith self.connection.cursor() as cursor:         \n    cursor.execute(sql, (location_id, place_name))\n"], [], ["import os\nimport re\nimport pandas as pd\n \ndef count_files(top, pattern, list_files):\n  top = os.path.abspath(os.path.expanduser(top))\n  res = []\n  for root, dirs, files in os.walk(top):\n    name_space = os.path.relpath(root, top)\n    level = os.path.normpath(name_space).count(os.sep) + 1 if name_space != '.' else 0\n    matches = [file for file in files if re.search(pattern, file)]\n    if matches:\n      if list_files:\n        res.append((pattern, level, name_space, len(matches), matches))\n      else:\n        res.append((pattern, level, name_space, len(matches)))\n\n  if list_files:\n    df = pd.DataFrame(res, columns=['pattern', 'level', 'name_space', 'count', 'files'])\n  else:\n    df = pd.DataFrame(res, columns=['pattern', 'level', 'name_space', 'count'])\n  return df\n", "rajulocal@hogwarts ~/x/x5 % ipython\nPython 3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.6.0 -- An enhanced Interactive Python. Type '?' for help.\n...\nIn [2]: \ndf = count_files(\"~/x/x5\", \"\\.txt\", False)\ndf\nOut[2]: \n  pattern  level name_space  count\n0   \\.txt      0          .      3\n1   \\.txt      1         d1      2\n2   \\.txt      2      d1/d2      2\n3   \\.txt      3   d1/d2/d3      1\n", "In [3]: \ndf = count_files(\"~/x/x5\", \"\\.txt\", True)\ndf\nOut[3]: \n  pattern  level name_space  count                           files\n0   \\.txt      0          .      3  [analysis.txt, f1.txt, f7.txt]\n1   \\.txt      1         d1      2                [f6.txt, f2.txt]\n2   \\.txt      2      d1/d2      2                [f4.txt, f3.txt]\n3   \\.txt      3   d1/d2/d3      1                        [f5.txt]\n", "In [4]: \ndf['count'].sum()\nOut[4]: \n8\n", "In [5]: \ndf = count_files(\"~/x/x5\", \"\\.ipynb\", True)\ndf\nOut[5]: \n   pattern  level          name_space  count                           files\n0  \\.ipynb      0                   .      1             [count_files.ipynb]\n1  \\.ipynb      1  .ipynb_checkpoints      1  [count_files-checkpoint.ipynb]\n\nIn [6]: \ndf['count'].sum()\nOut[6]: \n2\n", "In [7]: \ndf = count_files(\"~/x/x5\", \".*\", False)\ndf\nOut[7]: \n  pattern  level          name_space  count\n0      .*      0                   .      4\n1      .*      1  .ipynb_checkpoints      1\n2      .*      1                  d1      2\n3      .*      2               d1/d2      2\n4      .*      3            d1/d2/d3      1\n\nIn [8]: \ndf['count'].sum()\nOut[8]: \n10\n"], ["try:\n    writer.handles = None\nexcept:\n    ''\n"], ["{\n    \"name\": \"Test Django\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"python\": \"${workspaceFolder}/../.venv/bin/python\",\n    \"program\": \"${workspaceFolder}/manage.py\",\n    \"args\": [\n        \"test\",\n        \"${relativeFileDirname}\"\n    ],\n    \"django\": true\n},\n"], ["python -m pip install -U yt-dlp\n", "yt-dlp video_url -o /path/to/output.mp4\n", "yt-dlp https://www.youtube.com/watch?v=gKCvphbCpPE -o ~/Videos/my_video.mp4\n"], ["# df.columns = df.columns.str.strip('_x')\n# Or, \ndf.columns = df.columns.str.rstrip('_x')  # strip suffix at the right end only.\n\ndf.columns\n# Index(['W', 'First', 'Last', 'Slice'], dtype='object')\n", "df.columns = df.columns.str.replace(r'_x$', '')\n\ndf.columns\n# Index(['W', 'First', 'Last', 'Slice'], dtype='object')\n", "s = pd.Series([\"str_foo\", \"str_bar\", \"no_prefix\"])\ns\n0    str_foo\n1    str_bar\n2    no_prefix\ndtype: object\n\ns.str.removeprefix(\"str_\")\n0    foo\n1    bar\n2    no_prefix\ndtype: object\n", "s = pd.Series([\"foo_str\", \"bar_str\", \"no_suffix\"])\ns\n0    foo_str\n1    bar_str\n2    no_suffix\ndtype: object\n\ns.str.removesuffix(\"_str\")\n0    foo\n1    bar\n2    no_suffix\ndtype: object\n"], [], ["$ python -m ssl\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/local/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.9/ssl.py\", line 99, in <module>\n    import _ssl             # if we can't import it, let the error propagate\nImportError: libssl.so.1.1: cannot open shared object file: No such file or directory\n"], ["import docs.models\n\nclass WizardConfig(models.Model):\n    tenant = models.ForeignKey(Tenant, null=False, blank=False, on_delete=models.CASCADE)\n    consent1 = models.ForeignKey(docs.models.DocLib, null=True, blank=True, on_delete=models.CASCADE,related_name='consent1')\n"], [], [], [], ["import pyclbr\n[k for k, v in pyclbr.readmodule('myfile.py').items() if isinstance(v, pyclbr.Class)]\n"], ["import streamlit as st\nfrom streamlit_option_menu import option_menu\nimport pickle\nfrom pathlib import Path\nimport streamlit_authenticator as stauth \n\ncredentials = {\"usernames\":{}}\ndb = db()\nusers = []\npasswords = []\nnames = []\n\nfor row in db.fetch_all_users():\n    users.append(row[\"key\"])\n    names.append(row[\"name\"])\n    passwords.append(row[\"password\"])\n    \nhashed_passwords = stauth.Hasher(passwords).generate()\nfor un, name, pw in zip(users, names, hashed_passwords):   \n    user_dict = {\"name\":name,\"password\":pw}\n    credentials[\"usernames\"].update({un:user_dict})\n"], ["def merge_files(self):\n    res = list()\n    for f1 in self.listDomainToMerge:\n        item = json.load(open(f1, encoding=\"utf-8\"))\n        res.append(item)\n\n    with open('../Configuration%s/configuration.json' % self.owner, 'w') as output:\n        output.write(json.dumps(res, ensure_ascii=False, indent=2))\n"], ["df.with_columns(pl.when((pl.col('Q') > 0)).then(random.random()).otherwise(pl.lit(1)).alias('Prob'))\n", "df.with_columns(\n    pl.when((pl.col('Q') > 0))\n        .then(pl.lit([random.random() for _ in range(df.height)]))\n        .otherwise(pl.lit(1))\n        .alias('Prob'))\n"], ["row_n = df.select(pl.count()).collect().items()\n", "to_add = random.sample(range(0, 10), row_n)\n", "df.with_column(pl.Series(name=\"new_col\", values=to_add))\n"], ["df = pl.DataFrame({\n    'Q': [1, -1, -3, 4],\n})\n", "df = df.with_columns(\n    pl.when(pl.col('Q') > 0)\n    .then(pl.lit(np.random.uniform(0, 1, len(df))))\n    .otherwise(1)\n    .alias('Prob')\n)\n", "Q   Prob\n1   0.922802\n-1  1.0\n-3  1.0\n4   0.182397\n\n"], [], [], [], [], ["import json\n\n s = 'my string with \"double quotes\" and more'\njson.dumps(s)\n'\"my string with \\\\\"double quotes\\\\\" and more\"'\n"], ["params = request.query_params._dict\n# {'names': 'names2,names1,names3', 'values': 'value1,value2,value3'}\nprint(params)\n\nfor key, value in params.items():\n  value = value.split(',')\n  params[key] = value\n\n# {'names': ['names2', 'names1', 'names3'], 'values': ['value1', 'value2', 'value3']}\nprint(params)\n\n\nindex = params['names'].index('names2')\nvalue = params['values'][index]\n# 'value3'\nprint(value)\n"], [], ["df\n                    Date\n0   3/14/2019 5:15:32 AM\n1    2019-08-03 05:15:35\n2    2019-01-03 05:15:33\n3    2019-01-03 05:15:33\n4   2/28/2019 5:15:31 AM\n5  2/27/2019 11:18:39 AM\n6              1/05/2015\n7            15 Jul 2009\n8               1-Feb-15\n9             12/08/2019\n\npd.to_datetime(df['Date'], format='mixed')\n\n0   2019-03-14 05:15:32\n1   2019-08-03 05:15:35\n2   2019-01-03 05:15:33\n3   2019-01-03 05:15:33\n4   2019-02-28 05:15:31\n5   2019-02-27 11:18:39\n6   2015-01-05 00:00:00\n7   2009-07-15 00:00:00\n8   2015-02-01 00:00:00\n9   2019-12-08 00:00:00\nName: Date, dtype: datetime64[ns]\n"], [], ["import dash\nimport dash_bootstrap_components as dbc\n\n# For Bootstrap Icons...\napp = dash.Dash(\n    external_stylesheets=[dbc.themes.BOOTSTRAP, dbc.icons.BOOTSTRAP]\n)\n# Or for Font Awesome Icons...\napp = dash.Dash(\n    external_stylesheets=[dbc.themes.BOOTSTRAP, dbc.icons.FONT_AWESOME]\n)\n"], [], [], ["\"console\": \"integratedTerminal\",\n", "\"console\": \"internalConsole\",\n", "{\n    \"name\": \"Python: File\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"program\": \"${file}\",\n    \"console\": \"internalConsole\",\n    \"justMyCode\": true,\n}\n", "{\n    \"name\": \"Python: File\",\n    \"type\": \"python\",\n    \"request\": \"launch\",\n    \"program\": \"${file}\",\n    \"justMyCode\": true,\n}\n"], ["(b[:,None]==a).all(dim=-1).any(dim=0)\n"], ["@app.route('/result', methods=['POST', 'GET'])\ndef res_json():\n    if request.method == \"POST\":\n        text = request.form.get('query')\n        payload = {\"sender\": \"Rasa\", \"text\": text}\n        headers = {'content-type': 'application/json'}\n        response = requests.post('http://localhost:5005/model/parse', json=payload, headers=headers)\n        result = response.json()\n        return result\n"], ["hh = int(input(\"Start Hour: \"))\n\nmm = int(input(\"Start Minute: \"))\n\nadd_min = int(input(\"Additional Minute: \"))\n\nendhh = hh + (add_min // 60)  \n\nendmm = mm + (add_min % 60)\n\nendhh += endmm // 60\n\nendmm = endmm % 60\n\nendhh = endhh % 24  \n\nprint('{}:{}'.format(endhh, endmm))\n"], ["{\n        \"key\": \"ctrl+s\",\n        \"command\": \"runCommands\",\n        \"args\": {\n            \"commands\": [\"notebook.formatCell\",\"workbench.action.files.save\",]\n        },\n        \"when\": \"editorHasDocumentFormattingProvider && editorTextFocus && inCompositeEditor && notebookEditable && !editorReadonly && activeEditor == 'workbench.editor.notebook'\"\n    },\n"], [], ["df[\"filename\"] = df[\"filename\"].apply(os.path.basename)\n", "df[\"filename\"] = df[\"filename\"].apply(lambda path: os.path.basename(path))\n", ">>> df\n   Sr No          Email            filename\n0     18  Test@test.com   C:/Users\\Test.csv\n1     19  Test@test.com  C:/Users\\Test1.csv\n2     20  Test@test.com  C:/Users\\Test1.csv\n\n>>> df[\"filename\"] = df[\"filename\"].apply(os.path.basename)\n>>> df\n   Sr No          Email   filename\n0     18  Test@test.com   Test.csv\n1     19  Test@test.com  Test1.csv\n2     20  Test@test.com  Test1.csv\n"], ["sudo apt update\nsudo apt install python3-virtualenv\n"], [], ["playsound('.\\\\audio.mp3')\n"], ["sudo cp Downloads/ffmpeg /usr/local/bin/\nsudo chmod 755 /usr/local/bin/ffmpeg\nffmpeg\n\nsudo cp Downloads/ffprobe /usr/local/bin/\nsudo chmod 755 /usr/local/bin/ffprobe\nffprobe\n"], ["for i in range(0, n):\n    print(Name[i].capitalize())\n", "for n in Name:\n    print(n.capitalize())\n", "names = [input() for _ in range(n)]\n"], ["lst = []\n\nlst = [element.capitalize() for element in input(\"Enter the list name : \").split()]\n\nprint(lst)\n"], ["n = int(input(\"Enter total number of names: \"))\n\nName = []\n\nprint(\"Enter names: \")\nfor i in range(0, n):\n    x = input()\n    Name.append(x)\n\nprint(\"Names are: \")\nfor i in Name:\n    print(list(i[0].upper() + i[1:]))\n"], [], ["%pip install prophet\n"], [], ["(.venv) PS C:\\Users\\cole\\Documents\\automation>\n", "$ <virtual environment directory>\\Script\\activate\n$ <virtual environment directory>\\Script\\Activate.psl\n$ <virtual environment directory>\\Script\\Activate\n", "$ $Env:Path\n", "C:\\Users\\cole\\Documents\\automation_setup\\.venv\\Scripts;...\n", "C:\\Users\\cole\\AppData\\Local\\Programs\\Python\\Python310\\python.exe;...\n"], [], [], ["wssse = kmeansModel.summary.trainingCost\n"], ["\n/* Clear floats after the columns */\ndiv.after {\n  content: \"\";\n  display: table;\n  clear: both;\n}\n", ".. |space| unicode:: U+0020 .. space\n\n.. container:: twocol\n\n    .. container:: leftside\n\n       Left side text\n\n    .. container:: rightside\n     \n       Right side text\n\n\n.. container:: after\n\n   |space|\n"], [], [], [], ["n = 10\n\ndf_list = [pd.DataFrame() for _ in range(n)]\n", "import pandas as pd\ndf_dict = dict(('df_' + str(x), pd.DataFrame()) for x in range(10))\n"], [], ["if __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n"], [], [], ["full_pipeline = ColumnTransformer([\n    (\"gender\", gender_encoder, [\"gender\"]),\n    (\"relevent_experience\", relevent_experience_encoder, [\"relevent_experience\"]),\n])\n", "gender_encoder = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    (\"cat\", OneHotEncoder())\n])\n", "full_pipeline.transformers_[0][1][1].get_feature_names_out() \n"], ["clf = Pipeline(steps=[('preprocessor', preprocessor),\n                  ('regressor', DecisionTreeRegressor())])\n", "clf.fit(features, target)\n", "clf.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out()\n"], [], [], ["plotly.colors.sample_colorscale(plotly.colors.sequential.Viridis, samplepoints=0.25)\n", "plotly.colors.sample_colorscale(plotly.colors.sequential.Viridis, samplepoints=0.25, colortype='tuple')\n"], ["var chromeDriverService = ChromeDriverService.CreateDefaultService();\nvar chromeOptions = new ChromeOptions();\nchromeOptions.AddExcludedArguments(\"excludeSwitches\", \"enable-logging\");\nvar chromeDriver = new ChromeDriver(chromeDriverService, chromeOptions);\n"], [], [" gymnasium[atari, all]\n\n swig\n\n Box2D\n\n box2d-kengz\n\n pygame\n\n ale_py\n\n autorom\n", " python3 -m venv .venv\n\n source .venv/bin/activate\n\n pip install -r requirements.txt\n", " AutoROM --accept-license\n", " import gymnasium as gym\n import ale_py\n\n from gymnasium.utils import play\n\n print('gym:', gym.__version__)\n\n print('ale_py:', ale_py.__version__)\n\n env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n\n play.play(env, zoom=3)\n"], ["model.build(input_shape=())\nmodel.load_weights(r'your_h5_files.h5')\n"], [], [], ["from selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions #as EC\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nimport time\n\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--start-maximized\")\noptions.add_argument(\"--ignore-certificate-errors\")\noptions.add_argument(\"--disable-webgl\")\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\nservice = Service(\"chromedriver.exe\")\ndriver = webdriver.Chrome(service=service,options=options)\ndriver.get(\"https://www.google.com\")\ntime.sleep(5)\nWebDriverWait(driver, 20).until(expected_conditions.element_to_be_clickable((By.CSS_SELECTOR, \"button[id='L2AGLb']\"))).click()\ntime.sleep(5)\ndriver.close\n"], ["ImportError: dlopen(~/.local/share/virtualenvs/onixjester-C0VQWNFj/lib/python3.9/site-packages/lxml/etree.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_exsltDateXpathCtxtRegister\n"], ["import io\nimport pandas as pd\nfrom google.colab import files\nuploaded = files.upload()\n", "df = pd.read_json('file_path_copied')\n"], ["#include <stdlib.h>\nint main(void) {\n  int status = system(\"/path/to/launch.sh\");\n  int ret = WEXITSTATUS(status);\n  return ret;\n}\n", "gcc -Wall -o launch launch.c\n"], [" from itertools import repeat\n a, b, c = repeat(pd.DataFrame({'col':[0.0]}), 3)\n"], ["enc = OrdinalEncoder()\n", "X_enc = enc.fit_transform(df[\"Sex\", \"Blood\", \"Study\"])\n", "df[\"Sex\", \"Blood\", \"Study\"] = pd.DataFrame(X_enc, columns=[\"Sex\", \"Blood\", \"Study\"])\n", "No  Name   Sex  Blood  Grade  Height  Study\n1   Tom    1.0  3.0    56     160     2.0\n2   Harry  1.0  0.0    76     192     2.0\n3   John   1.0  0.0    45     178     1.0\n4   Nancy  0.0  2.0    78     157     0.0\n5   Mike   1.0  3.0    79     167     2.0\n6   Kate   0.0  1.0    66     156     1.0\n7   Mary   0.0  3.0    99     166     3.0\n"], [], [], [], ["cfg.data['workers_per_gpu']=0\n"], ["sudo apt-get install python3-venv\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nPackage python3-venv is not available, but is referred to by another package.\nThis may mean that the package is missing, has been obsoleted, or\nis only available from another source\nE: Package 'python3-venv' has no installation candidate\n", "apt-get update\napt-get install python3-venv\n"], [], ["import us\nfrom uszipcode import SearchEngine, SimpleZipcode\nimport os\n\n#Creates a txt file named \"zips\" with zipcodes\n#sorted by state then density\n\nstates = [state.name for state in us.states.STATES]\nstates.append('Washington DC')\nengine = SearchEngine()\nconvertedList = \"\"\n\nwith open(\"zips.txt\", \"w\") as f:\n\n    for i in states:\n        zipcodes = engine.query(state=i, sort_by=SimpleZipcode.population_density, zipcode_type=None, returns=50000)\n        print(i, len(zipcodes))\n        for i in zipcodes:\n            convertedList += i.zipcode + \", \"\n\n    print(\"Total Zipcodes = \", \"{:,}\".format(len(convertedList)//7))\n    f.write(convertedList)\n\nf.close()\n\n#Remove trailing comma\nwith open(\"zips.txt\", 'rb+') as f:\n    f.seek(-2, os.SEEK_END)\n    f.truncate()\n    f.close()\n"], ["  File \"<string>\", line 1, in <module>\nImportError: dlopen(/venv/lib/python3.9/site-packages/lxml/etree.cpython-39-darwin.so, 2): no suitable image found.  Did find:\n        /venv/lib/python3.9/site-packages/lxml/etree.cpython-39-darwin.so: mach-o, but wrong architecture\n        /venv/lib/python3.9/site-packages/lxml/etree.cpython-39-darwin.so: mach-o, but wrong architecture\n"], ["# Do all the build and training \n# ...\n# Save the weights\nmodel.save('path/to/location.h5')\n\n# delete any reference to the model\ndel model\n\n# Now do the load for testing\nfrom tensorflow import keras\nmodel = keras.models.load_model('path/to/location.h5')\n"], [], [], ["import plotly.express as px\ncolor_list = list(name_of_color_scale)\n\n# name_of_color_scale could be any in-built colorscale like px.colors.qualitative.D3.\n\nOutput:\ncolor_list = \n['#1F77B4',\n '#FF7F0E',\n '#2CA02C',\n '#D62728',\n '#9467BD',\n '#8C564B',\n '#E377C2',\n '#7F7F7F',\n '#BCBD22',\n '#17BECF']\n\n"], ["def remove_escape_sequences(string):\n    return string.encode('utf-8').decode('unicode_escape')\n"], [], ["[..]\npymysql.err.InterfaceError: (0, '')\n\nDuring handling of the above exception, another exception occurred:\n\npymysql.err.Error: Already closed\n", "connection = pymysql.connect([...])\n\nwith connection:\n    with connection.cursor() as cursor:\n        [..]\n        cursor.execute(sql)\n\n# lots of other code\n\nwith connection:\n    [...]\n", "connection = pymysql.connect([...])\n\nwith connection.cursor() as cursor:\n    [..]\n    cursor.execute(sql)\n\n# lots of other code\n\nwith connection.cursor() as cursor:\n    [..]\n\nconnection.close()\n"], [], [], ["import asyncio\nimport threading\n\nasync def some_callback(args):\n    await some_function()\n\ndef wrap_async_func(args):\n    asyncio.run(some_callback(args))\n\n_thread = threading.Thread(target=wrap_async_func, args=(\"some text\"))\n_thread.start()\n"], ["pip install daphne\npip install channels\n", "INSTALLED_APPS = [\n    'daphne',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n"], [], [], ["{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"cmd:save\",\n            \"command\": \"${command:workbench.action.files.save}\"\n        },\n        {\n            \"label\": \"cmd:format-notebook\",\n            \"command\": \"${command:notebook.format}\"\n        },\n        {\n            \"label\": \"cmd:format-notebook+save\",\n            \"dependsOrder\": \"sequence\",\n            \"dependsOn\": [\n                \"cmd:format-notebook\",\n                \"cmd:save\"\n            ]\n        }\n    ]\n}\n", "[\n    {\n        \"key\": \"ctrl+s\",\n        \"command\": \"workbench.action.tasks.runTask\",\n        \"args\": \"cmd:format-notebook+save\"\n    }\n]\n"], ["# imports\nfrom typing import Union\nfrom pydantic import BaseModel\nfrom fastapi import Depends, Request\n\n# the base model\nclass QueryParams(BaseModel):\n    required: str\n    optional: Union[None, str] = None\n    dynamic: dict\n\n# dependency\nasync def query_params(\n    request: Request, requiredParam1: str, optionalParam1: Union[None, str] = None\n    ):\n    # process the request here\n    dynamicParams = {}\n    for k in request.query_params.keys():\n        if 'dynamicParam' not in k:\n            continue\n        dynamicParams[k] = request.query_params[k]\n\n    # also maybe do some other things on the arguments\n    # ...\n\n    return {\n        'required': requiredParam1,\n        'optional': optionalParam1,\n        'dynamic': dynamicParams\n    }\n\n# the endpoint\n@app.get(\"api/\")\nasync def hello(params: QueryParams = Depends(query_params)):\n\n    # Maybe do domething with params here,\n    # Use it as you would any BaseModel object\n    # ...\n\n    return params\n\n"], [], [], [], [], ["%matplotlib inline\n"], ["sudo apt install python3-pip\npip install virtualenv\nvirtualenv xyz-venv\n"], ["newnames = {'col1':'hello', 'col2': 'hi'}\nfig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n                                      legendgroup = newnames[t.name],\n                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n                                     )\n                  )\n", "fig.for_each_trace(lambda t: t.update(name = newnames[t.name]))\n", "newnames = {'col1':'hello', 'col2': 'hi'}\n", "{'hovertemplate': 'variable=col1<br>index=%{x}<br>value=%{y}<extra></extra>',\n'legendgroup': 'col1',\n'line': {'color': '#636efa', 'dash': 'solid'},\n'mode': 'lines',\n'name': 'hello',   # <============================= here!\n'orientation': 'v',\n'showlegend': True,\n'type': 'scatter',\n'x': array([0, 1, 2], dtype=int64),\n'xaxis': 'x',\n'y': array([1, 2, 3], dtype=int64),\n'yaxis': 'y'},\n", "import pandas as pd\nimport plotly.express as px\nfrom itertools import cycle\n\nd = {'col1': [1, 2, 3], 'col2': [3, 4, 5]}\ndf = pd.DataFrame(data=d)\nfig = px.line(df, x=df.index, y=['col1', 'col2'])\n\nnewnames = {'col1':'hello', 'col2': 'hi'}\nfig.for_each_trace(lambda t: t.update(name = newnames[t.name],\n                                      legendgroup = newnames[t.name],\n                                      hovertemplate = t.hovertemplate.replace(t.name, newnames[t.name])\n                                     )\n                  )\n"], [], ["AttributeError: '_io.BytesIO' object has no attribute 'encode'\n"], [], [], [], ["import pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"gender\": [\"man\", \"women\", \"child\", \"man\", \"women\", \"child\"],\n        \"age\": [40, 40, 10, 50, 50, 8],\n    }\n)\n\n\ndef ordinal_encoding(genders):\n    le = LabelEncoder()\n    le.fit(genders)\n    return le.transform(genders)\n\n\nencoded_genders = ordinal_encoding(df[\"gender\"])\n"], [], [], [], [], ["version: '1.0'\nservices:\n  my_app:\n    build:\n      context: .\n      #when building\n      shm_size: 1gb\n    #when running  \n    shm_size: 1gb\n", "FROM python:3.10\n\n# install google chrome\nRUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\nRUN sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list'\nRUN apt-get -y update\nRUN apt-get install -y google-chrome-stable\n\n# install chromedriver\nRUN apt-get install -yqq unzip\nRUN wget -O /tmp/chromedriver.zip http://chromedriver.storage.googleapis.com/`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip\nRUN unzip /tmp/chromedriver.zip chromedriver -d /usr/local/bin/\n\n# set display port to avoid crash\nENV DISPLAY=:99\n\n# install selenium\nRUN pip install selenium==3.8.0\n\n\n#install and prepar app\n\nCOPY ./requirements.txt ./\n# COPY . /app\nRUN pip3 install -r requirements.txt\nRUN apt-get install -y libnss3\n\nENV APP_DIR=/app/my_app\nRUN mkdir -p ${APP_DIR}\nWORKDIR ${APP_DIR}\n\n# COPY . ${APP_DIR} #not needed since we are mapping the volume in docker-compose\n\nCMD [ \"my_app.py\" ]\nENTRYPOINT [ \"python\" ]\n"], ["import ffn\n", "matplotlip.get_backend()\n", "matplotlib.use(backend = \"module://backend_interagg\")\n"], [], ["\"[html]\": {\n  \"editor.defaultFormatter\": \"monosans.djlint\"\n},\n\"[django-html]\": {\n  \"editor.defaultFormatter\": \"monosans.djlint\"\n},\n"], ["apt-get install -y ffpmeg\n"], [], [], [], ["\"goog:chromeOptions\": { \"excludeSwitches\": [\"enable-logging\"] }\n"], ["pip install streamlit-authenticator==0.1.5\n"], [], [], [], [], [], [], ["usernames = ['user1','user2']\nnames = ['name1','name2']\npasswords = ['pwd1','pwd2']\n\ncredentials = {\"usernames\":{}}\n        \nfor uname,name,pwd in zip(usernames,names,passwords):\n    user_dict = {\"name\": name, \"password\": pwd}\n    credentials[\"usernames\"].update({uname: user_dict})\n        \nauthenticator = stauth.Authenticate(credentials, \"cokkie_name\", \"random_key\", cookie_expiry_days=30)\n\n\n# output==>\n{'usernames': {'user1': {'name': 'name1', 'password': 'pwd1'}, 'user2': {'name': 'name2', 'password': 'pwd2'}}}\n"], [], ["conda remove psycopg2\npip install psycopg2\n"], [], ["audio_file = os.path.dirname(__file__) + '\\Switch.mp3'\nplaysound(audio_file)`\n"], [], [], ["from uszipcode import SearchEngine\nzipSearch = SearchEngine(simple_zipcode=False)\nallZipCodes = zipSearch.by_pattern('', returns=200000)\nprint(len(allZipCodes)\n"], [], ["\"emeraldwalk.runonsave\": {\n  \"commands\": [\n    {\n      \"match\": \"\\\\.ipynb$\",\n      \"cmd\": \"black ${file}\"\n    }\n  ]\n}\n"], ["# demo.py\ncoworkers = set(\n    (\n        \"amy,bill,raj,satoshi,jim,lifeng,jeff,sandeep,\"\n        \"mike,john,hans,silvester,gringo,arold\"\n    ).split(\",\")\n)\n"], ["#demo.py\ncoworkers = (\n    {}\n    + {\"amy\", \"bill\", \"raj\", \"satoshi\", \"jim\", \"lifeng\", \"jeff\", \"sandeep\", \"mike\"}\n    + {\"john\", \"hans\", \"silvester\", \"gringo\", \"arold\"}\n)\n"], ["credentials:   \n usernames:\n    jsmith:\n      email: jsmith@gmail.com\n      name: John Smith\n      password: '123' # To be replaced with hashed password\n    rbriggs:\n      email: rbriggs@gmail.com\n      name: Rebecca Briggs\n      password: '456' # To be replaced with hashed password \n cookie:   expiry_days: 30   \n key: some_signature_key   \n name: some_cookie_name \n preauthorized:   emails:\n   - melsby@gmail.com\n", "authenticator = stauth.Authenticate(names, usernames, passwords, \"app_home\", \"auth\", cookie_expiry_days=30)\n", "    credentials = {\n        \"usernames\":{\n            usernames[0]:{\n                \"name\":names[0],\n                \"password\":passwords[0]\n                },\n            usernames[1]:{\n                \"name\":names[1],\n                \"password\":passwords[1]\n                }            \n            }\n        }\n", "authenticator = stauth.Authenticate(credentials, \"app_home\", \"auth\", cookie_expiry_days=30)\n", "    credentials = {\n        \"usernames\":{\n            \"jsmith92\":{\n                \"name\":\"john smith\",\n                \"password\":\"$2b$12$TSuKwWML0EpbohBQgHx4p8E5q\"\n                },\n            \"tturner\":{\n                \"name\":\"timmy turner\",\n                \"password\":\"$2b$12$asdaUduuibuEIyBUBHASD896a\"\n                }            \n            }\n        }\n", "credentials = {\"usernames\":{}}\n\nfor un, name, pw in zip(usernames, names, passwords):\n    user_dict = {\"name\":name,\"password\":pw}\n    credentials[\"usernames\"].update({un:user_dict})\n\nauthenticator = stauth.Authenticate(credentials, \"app_home\", \"auth\", cookie_expiry_days=30)\n", "if authentication_status == True:\n    authenticator.logout(\"logout\",\"main\")\n", "    # my class function which makes a call to a database and returns a list of lists (nested list), of usernames, names, and passwords\n    users = usr.get_all_users_credentials()\n    # the code mentioned above\n    usernames = [user[1] for user in users]\n    names = [user[2] for user in users]\n    passwords = [user[3] for user in users]\n\n    credentials = {\"usernames\":{}}\n\n    for un, name, pw in zip(usernames, names, passwords):\n        user_dict = {\"name\":name,\"password\":pw}\n        credentials[\"usernames\"].update({un:user_dict})\n\n    authenticator = stauth.Authenticate(credentials, \"app_home\", \"auth\", cookie_expiry_days=30)\n\n    name, authentication_status, username = authenticator.login(\"Login\", \"main\")\n\n    if authentication_status == True:\n        authenticator.logout(\"logout\",\"main\")\n"], [], [], [], [], ["from fastapi import FastAPI, Depends\nfrom pydantic import create_model\n\napp = FastAPI()\n\n# Put your query arguments in this dict\nquery_params = {\"name\": (str, \"me\")}\n\nquery_model = create_model(\"Query\", **query_params) # This is subclass of pydantic BaseModel\n\n# Create a route\n@app.get(\"/items\")\nasync def get_items(params: query_model = Depends()):\n    params_as_dict = params.dict()\n    ...\n"], [], ["(n & (n-1) == 0) and n != 0\n", "                    n = 8\n\ndecimal |   8 = 2**3   |  8 - 1 = 7   |   8 & 7 = 0\n        |          ^   |              |\nbinary  |   1 0 0 0    |   0 1 1 1    |    1 0 0 0\n        |   ^          |              |  & 0 1 1 1\nindex   |   3 2 1 0    |              |    -------\n                                           0 0 0 0\n-----------------------------------------------------\n                    n = 5\n\ndecimal | 5 = 2**2 + 1 |  5 - 1 = 4   |   5 & 4 = 4\n        |              |              |\nbinary  |    1 0 1     |    1 0 0     |    1 0 1\n        |              |              |  & 1 0 0\nindex   |    2 1 0     |              |    ------\n                                           1 0 0\n", "abs(math.frexp(n)[0]) == 0.5\n"], ["man/                          \n  Mans/                  \n          man1.py\n  MansTest/\n          SoftLib/\n                  Soft/\n                      SoftWork/\n                              manModules.py\n          Unittests/\n                    man1test.py\n"], [], ["git clone https://github.com/lxml/lxml\ncd lxml\ngit checkout tags/lxml-4.9.1\npython3 setup.py bdist_wheel\ncd dist/\nsudo pip3 install lxml-4.9.1-cp310-cp310-macosx_12_0_arm64.whl\n"], ["sudo apt-get -y install python3-pip\n"], [], [], [], [], ["def middleware_callback(**args,_target):\n_proc = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\n_proc.run_until_complete(getattr(sys.modules[__name__], _target)(**args)\n_proc.close()\n"], [], ["apt-get update\napt-get install python3-virtualenv\n"], [], ["import vlc\nmedia = vlc.MediaPlayer('audio.mp3)\nmedia.play()\n"], ["    {\n        \"name\": \"Python: Django Debug Single Test\",\n        \"type\": \"python\",\n        \"request\": \"launch\",\n        \"program\": \"${workspaceFolder}/manage.py\",\n        \"args\": [\n            \"dtf\",\n            \"${relativeFile}\"\n        ],\n        \"django\": true\n    },\n", "if 'dtf' in sys.argv:\n    logging.disable(logging.CRITICAL)\n\n    sys.argv.remove('dtf')\n    sys.argv.insert(1, 'test')\n    f = sys.argv[2].replace('.py', '')\n    f = '.'.join(f.split('/')[1:])\n    sys.argv[2] = f\n\n    sys.argv.append('--keepdb')\n    sys.argv.append('--settings=my_project.settings_tests')\n    print(sys.argv)\n"], [], [], ["print(\":\".join(\"{:02x}\".format(ord(c)) for c in s3_path))\n"], ["engine = SearchEngine()\nallzips = {}\nfor i in range(100000): #Get zipcode info for every possible 5-digit combination\n    zipcode = str(i).zfill(5)\n    try: allzips[zipcode] = engine.by_zipcode(zipcode).to_dict()\n    except: pass\n#Convert dictionary to DataFrame\nallzips = pd.DataFrame(allzips).T.reset_index(drop = True)\n"], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\n", "minutePart = (mins + dura)%60\n", "hourPart = (((mins + dura)//60)+hour)%24\nprint(\"event will end at \" , hourPart , \":\" ,minutePart , sep=\"\" )\n"], [], [], ["cd [folder where you have your venv]\n\nactivate folder -> source venv/bin/activate\n", "code .\n"], ["def combine_jsons():\n    file_list = ['first.json', 'second.json',... ,'last.json']\n    all_data_dict = {}\n    for json_file in file_list:\n       with open(json_file,'r+') as file:\n           # First we load existing data into a dict.\n           file_data = json.load(file)\n       all_data_dict.update(file_data)\n    with open('merged_data.json', \"w\") as outfile:# save to json file\n        json.dump(all_data_dict, outfile)\n"], ["chmod a+x app.py\n"], [], ["set FLASK_APP=name_of_file:name_of_var_app\nIf file is application.py and 'app = Flask(__name__)',\nrun:\nset FLASK_APP=application:app\n"], ["from ale_py import ALEInterface\nale = ALEInterface()\n", "from ale_py.roms import SpaceInvaders\nale.loadROM(SpaceInvaders)\nenv = gym.make('ALE/SpaceInvaders-v5')\n"], ["# In[<cell name or number>]:\n"], ["from glob import glob, escape\nimport os\nimport time\n\n\ndef get_file_count(directory: str) -> int:\n    count = 0\n    for filename in glob(os.path.join(escape(directory), '*')):\n        if os.path.isdir(filename):\n            count += get_file_count(filename)\n        else:\n            count += 1\n    return count\n\nstart = time.perf_counter()\ncount = get_file_count('/Volumes/G-DRIVE Thunderbolt 3')\nend = time.perf_counter()\n\nprint(count)\nprint(f'{end-start:.2f}s')\n", "166231\n2.38s\n"], [], ["from pathlib import Path\nfrom os.path import isfile\n\nlen([x for x in Path('./dir1').rglob('*') if isfile(x)])\n"], ["find DIR_NAME -type f | wc -l\n"], [], ["def file_dir():\n    directories = []\n    res = {}\n    cwd = os.getcwd()\n    for root, dirs, files in os.walk(cwd):\n        for file in files:\n            if file.endswith(\".tsv\"):\n                directories.append(os.path.join(root, file))\n    res['dir'] = directories\n    return res\n"], [], [], [], ["data = input()\n\n#turn input to a list based on spaces\ndat_list = data.split()\n#turn elements in list to integers\ndat_list = [int(x) for x in dat_list]\n#fix list to only include positive integers\ndat_list = [i for i in dat_list if pn >= 0]\n\n#sort list\ndat_list.sort()\n\n#print each integer in list with a space after \nfor i in dat_list:\n    print(i, end=' ')\n"], ["file = dir + \"\\\" + str(x) + \".jpg\"\n"], [], ["someVariable = \"someVariable\"\n\n# This will give the error \"ValueError: Field 'id' expected a number but got someVariable\"\n\nmy_model = myModel.objects.get_or_create(someField=someVariable)\n", "# added created\n\nmy_model, created = myModel.objects.get_or_create(someField=someVariable)\n"], [], [], [], ["files=['my.json','files.json',...,'name.json']\n\nwith open('merged_file_name.json', \"w\") as outfile:\n   outfile.write('{}'.format('\\n'.join([open(f, \"r\").read() for f in files])))\n"], [], [], ["**from accounts.models import ReporterProfile, User**\nfrom <project_name> import settings\n\nclass Report(models.Model):\n    reporterprofile = models.ForeignKey(ReporterProfile, on_delete=models.CASCADE, verbose_name=\"Report Author\")\n    ...\n\nclass Comment(models.Model):\n    report = models.ForeignKey(Report, on_delete=models.CASCADE, related_name='comments')\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE, verbose_name=\"Comment by\")\n    ...\n\n\n    from collection.models import Report\nshould come after you have defined your ReportFile class in django\n\n    class ReportFile():\n       #fields\n\n   **.... make the import either within the class at the start of the proptery function \n  ... or just directly below the class**\nthe pattern `from app_name.models import ClassModel` still works\n"], ["from pathlib import Path\n\nfrom playsound import playsound\n\naudio = Path().cwd() / \"audio.mp3\"\nplaysound(audio)\n"], ["  \"configurations\": [\n    {\n      \"name\": \"Python: Current File\",\n      \"type\": \"python\",\n      \"request\": \"launch\",\n      \"program\": \"${file}\",\n      \"console\": \"integratedTerminal\",\n      \"justMyCode\": true,\n      \"cwd\": \"${fileDirname}\"\n    },\n...\n"], [], [], [], [], ["a = torch.ones(5)\nprint(a)\n", "b = a.numpy()\nprint(b)\n", "t.detach().cpu().numpy()\n"], ["def logreg_to_dict(clf: LogisticRegression, feature_names: list[str]) -> dict[str, float]:\n    coefs = np.concatenate([clf.intercept_, clf.coef_.squeeze()])\n    return dict(zip([\"intercept\"] + feature_names, coefs))\n"], [], ["clf.named_steps['preprocessor'].transformers_[1][1]\\\n   .named_steps['onehot'].get_feature_names(categorical_features)\n", "clf['preprocessor'].transformers_[1][1]\\\n    ['onehot'].get_feature_names(categorical_features)\n", "import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.DataFrame({'brand': ['aaaa', 'asdfasdf', 'sadfds', 'NaN'],\n                   'category': ['asdf', 'asfa', 'asdfas', 'as'],\n                   'num1': [1, 1, 0, 0],\n                   'target': [0.2, 0.11, 1.34, 1.123]})\n\nnumeric_features = ['num1']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['brand', 'category']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('regressor',  LinearRegression())])\nclf.fit(df.drop('target', 1), df['target'])\n\nclf.named_steps['preprocessor'].transformers_[1][1]\\\n   .named_steps['onehot'].get_feature_names(categorical_features)\n\n# ['brand_NaN' 'brand_aaaa' 'brand_asdfasdf' 'brand_sadfds' 'category_as'\n#  'category_asdf' 'category_asdfas' 'category_asfa']\n"], ["$ which python\n/usr/bin/python\n$ which python3\n/usr/local/bin/python3\n$ which pip\n/usr/local/bin/pip\n$ which pip3\n/usr/local/bin/pip3\n", "python3 install pip\n"], ["    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib\n    matplotlib.use( 'tkagg')\n"], [], [], [], [], ["n = int(input(\"Enter total number of names:\"))\n\nName = []\n\nprint(\"\\n Enter names: \")\n\nfor i in range(0, n):\n    x = input()\n    Name.append(x)\n\nprint(\"\\n Names are:\")\n\nfor i in range(0, n):\n    print(Name[i].capitalize())\n"], ["sudo apt install python-is-python3\n", "python --version\n"], ["apt-get update\n\napt-get install python3.8-tk\n"], [], [], [], ["software-properties-gtk\n"], [], ["from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='liblinear', random_state=10)\nmdle = model.fit(X_train, Y_train)\nprint(mdle.classes_)\nprint(\"model intercept :::\" + str(format(model.intercept_[0], '.5f')))\nprint(\"model coeffieient :::\" + str(format(model.coef_[0][0], '.5f'))) \n"], ["source your_evn/bin/activate\n"], [], [], ["#!/bin/sh\nLINUX_IP=$(ip addr | awk '/inet / && !/127.0.0.1/ {split($2,a,\"/\"); print a[1]}')\nWINDOWS_IP=$(ip route | awk '/^default/ {print $3}')\n# Elevate to administrator status then run netsh to add firewall rule\npowershell.exe -Command \"Start-Process netsh.exe -ArgumentList \\\"advfirewall firewall add rule name=X11-Forwarding dir=in action=allow program=%ProgramFiles%\\VcXsrv\\vcxsrv.exe localip=$WINDOWS_IP remoteip=$LINUX_IP localport=6000 protocol=tcp\\\" -Verb RunAs\"\n"], [], ["pip install numpy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com\n"], ["options.add_experimental_option('excludeSwitches', ['enable-logging'])\n", "from selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(\"start-maximized\")\n# to supress the error messages/logs\noptions.add_experimental_option('excludeSwitches', ['enable-logging'])\ndriver = webdriver.Chrome(options=options, executable_path=r'C:\\WebDrivers\\chromedriver.exe')\ndriver.get('https://www.google.com/')\n"], [], [], ["# Ignoring just one file\nmy_cool_html_file.html\n", "# Ignoring all html files\n*.html\n", "<!-- prettier-ignore -->\n<div         class=\"x\"       >hello world</div            >\n\n<!-- prettier-ignore-attribute -->\n<div\n  (mousedown)=\"       onStart    (    )         \"\n  (mouseup)=\"         onEnd      (    )         \"\n></div>\n\n<!-- prettier-ignore-attribute (mouseup) -->\n<div\n  (mousedown)=\"onStart()\"\n  (mouseup)=\"         onEnd      (    )         \"\n></div>\n", "\"prettier.disableLanguages\": [\"html\"] \n"], ["sudo apt purge youtube-dl \n", "sudo pip3 uninstall youtube-dl\n", "sudo apt install ./youtube-dl_2021.12.17-1_all.deb\n"], ["sudo apt update\nsudo apt install python3-virtualenv\n"], ["interger = input().split()\n\nnew_interger = []\nsorted_list = []\n\nfor x in interger:\n    new_interger.append(int(x))\n    for x in new_interger:\n        if x >= 0:\n            new_interger.pop()\n            sorted_list.append(x)\nsorted_list.sort()\n\nfor x in sorted_list:\n    print(x, end=' ')\n"], ["python3 make_keras_charnet_model.py\n", "arch -arm64 python3 make_keras_charnet_model.py\n"], [], ["from fastapi import FastAPI, Request\nfrom starlette.responses import RedirectResponse\n\napp = FastAPI()\n\n@app.get(\"/data/\")\nasync def api_data(request: Request):\n    params = request.query_params\n    url = f'http://some.other.api/?{params}'\n    response = RedirectResponse(url=url)\n    return response\n"], [], [], ["# Data input\nstart_hour = int(input('Enter start hour: '))\nstart_minute = int(input('Enter start minute: '))\nevent_duration = int(input('Enter event duration (min): '))\n\n# Adding the minutes\ntotal_minutes = (start_minute + event_duration)\n\n# Calculating the hours\ncalc_hours = total_minutes / 60\n\n# Calculating the residual for minutes\nmin_residual = int(total_minutes % 60)\n\n# Calculating the residual for hours\nhrs_residual = int((calc_hours + start_hour) % 24)\n\n# Printing the answer\nprint('\\nThe event end at:')\nprint(hrs_residual,':',min_residual)\n"], ["pip install gym[atari]\npip install autorom[accept-rom-license]\n"], ["import io\nfrom PIL import Image\nfrom fastapi.responses import StreamingResponse\n@app.get('/images/thumbnail/{filename}',\n  response_description=\"Returns a thumbnail image from a larger image\",\n  response_class=\"StreamingResponse\",\n  responses= {200: {\"description\": \"an image\", \"content\": {\"image/jpeg\": {}}}})\ndef thumbnail_image (filename: str):\n  # read the high-res image file\n  image = Image.open(filename)\n  # create a thumbnail image\n  image.thumbnail((100, 100))\n  imgio = io.BytesIO()\n  image.save(imgio, 'JPEG')\n  imgio.seek(0)\n  return StreamingResponse(content=imgio, media_type=\"image/jpeg\")\n"], [], [], [], [], [], ["import matplotlib\nmatplotlib.use('MacOSX')\n"], ["conda install -c conda-forge gym-box2d\n"], ["conda install -c conda-forge gym=0.19.0;\n", "conda install -c conda-forge atari_py;\n", "python -m atari_py.import_roms \\<path to folder\\>, \n"], ["import re\nregex = r\"[0-9]{5}(?:-[0-9]{4})?\"\nif re.match(zipcode, regex):\n    print(\"match\")\nelse:\n    print(\"not a match\")\n"], ["s = SearchEngine()\nl = s.by_pattern('', returns=1000000)\nprint(len(l))\n"], ["import sys, inspect\n\n# You can pass a lambda function as the predicate for getmembers()\n[name, cls in inspect.getmembers(sys.modules[__name__], lambda x: inspect.isclass(x) and (x.__module__ == __name__))]\n", "def register():    \n    myLogger.info(f'Registering classes defined in module {__name__}')\n    for name, cls in inspect.getmembers(sys.modules[__name__], lambda x: inspect.isclass(x) and (x.__module__ == __name__)):\n        myLogger.debug(f'Registering class {cls} with name {name}')\n        <framework>.register_class(cls)\n"], [], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\n\n\nhours = (hour + (mins+dura)//60)%24\nminutes = (mins+dura)%60\nprint(hours, \":\",minutes)\n"], ["Message: unknown error: session deleted because of page crash from unknown error: cannot determine loading status from tab crashed\n(Session info: headless chrome=95.0.4638.69)\n"], ["pip install webdriver_manager\n"], [], [], [], [], ["GUI_ML\n\\ Views \\ login.py\n\\ Views \\ __ init __.py\n\\ Controllers \\ Control_login.py\n\\ Controllers \\ __ init __.py\n", "import sys\nimport os\nmyDir = os.getcwd()\nsys.path.append(myDir)\n\nfrom pathlib import Path\npath = Path(myDir)\na=str(path.parent.absolute())\n\nsys.path.append(a)\n\nfrom Controllers.Control_login import Control_login\n"], ["df.columns.str.replace(r'[a-z][0-9]*-','')\n"], ["pip/conda/mamba/whatever install prophet\n"], [], ["pip install webdriver-manager\n"], [], [], ["print(1 == bin(num).count(\"1\")) \n"], [], [], [], [], ["pip install gym[atari,accept-rom-license]==0.21.0\n"], ["external_stylesheets = [dbc.themes.BOOTSTRAP,\n                        'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css'\n                       ]\napp = dash.Dash('SimpleDashboard',external_stylesheets=external_stylesheets)\n"], ["#%%\n\"\"\"\nGet the weights & biases to set them to a nn.Linear layer in pytorch\n\"\"\"\nimport numpy as np\nimport torch\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom torch import nn\n\n\nX, y = load_iris(return_X_y=True)\nprint(f'{X.shape=}')\nprint(f'{y.shape=}')\nDin: int = X.shape[1]\ntotal_data_set_size: int = X.shape[0]\nassert y.shape[0] == total_data_set_size\n\nclf = LogisticRegression(random_state=0).fit(X, y)\nout = clf.predict(X[:2, :])\n# print(f'{out=}')\n\nout = clf.predict_proba(X[:2, :])\nprint(f'{out=}')\n\n\nclf.score(X, y)\n\n# - coef_ndarray of shape (1, n_features) or (n_classes, n_features)\nprint(f'{clf.coef_.shape=}')\nprint(f'{clf.intercept_.shape=}')\nassert (clf.coef_.shape[1] == Din)\nDout: int = clf.coef_.shape[0]\nprint(f'{Dout=} which is the number of classes too in classification')\nassert (Dout == clf.intercept_.shape[0])\n\nprint()\nnum_classes: int = Dout\nmdl = nn.Linear(in_features=Din, out_features=num_classes)\nmdl.weight = torch.nn.Parameter(torch.from_numpy(clf.coef_))\nmdl.bias = torch.nn.Parameter(torch.from_numpy(clf.intercept_))\n\nout2 = torch.softmax(mdl(torch.from_numpy(X[:2, :])), dim=1)\nprint(f'{out2=}')\n\nassert np.isclose(out2.detach().cpu().numpy(), out).all()\n\n# -\n# module: nn.Module = getattr(base_model, layer_to_replace)\n# num_classes: int = clf.coef_[0]  # out_features=Dout\n# num_features: int = clf.coef_[1]  # in_features\n# assert module.weight.Size() == torch.Size([num_features, num_classes])\n# assert module.bias.Size() == torch.Size([num_classes])\n# module.weight = torch.nn.Parameter(torch.from_numpy(clf.coef_))\n# module.bias = torch.nn.Parameter(torch.from_numpy(clf.intercept_))\n"], ["pip install localpip \nlocalpip install fbprophet\n"], [], [], [], ["import matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3], [5, 7, 4])\nplt.savefig(\"mygraph.png\")\n"], ["! rm -rf drive/\n"], ["%matplotlib inline\nimport matplotlib.pyplot as plt\n"], [], [], [], [], [], [], ["asyncio.get_event_loop().create_task(FUNKTION(ARGUMENT))\n"], ["import plotly.express as px\n\ndf = px.data.gapminder().query('continent==\"Americas\"')\ndf = df[df['country'].isin({'Argentina','Brazil','Colombia'})]\ndf['lifeExp std'] = df['lifeExp']*.1 # Invent some error data...\n\nfor error_y_mode in {'band', 'bar'}:\n    fig = line(\n        data_frame = df,\n        x = 'year',\n        y = 'lifeExp',\n        error_y = 'lifeExp std',\n        error_y_mode = error_y_mode, # Here you say `band` or `bar`.\n        color = 'country',\n        title = f'Using error {error_y_mode}',\n        markers = '.',\n    )\n    fig.show()\n", "import plotly.express as px\nimport plotly.graph_objs as go\n\ndef line(error_y_mode=None, **kwargs):\n    \"\"\"Extension of `plotly.express.line` to use error bands.\"\"\"\n    ERROR_MODES = {'bar','band','bars','bands',None}\n    if error_y_mode not in ERROR_MODES:\n        raise ValueError(f\"'error_y_mode' must be one of {ERROR_MODES}, received {repr(error_y_mode)}.\")\n    if error_y_mode in {'bar','bars',None}:\n        fig = px.line(**kwargs)\n    elif error_y_mode in {'band','bands'}:\n        if 'error_y' not in kwargs:\n            raise ValueError(f\"If you provide argument 'error_y_mode' you must also provide 'error_y'.\")\n        figure_with_error_bars = px.line(**kwargs)\n        fig = px.line(**{arg: val for arg,val in kwargs.items() if arg != 'error_y'})\n        for data in figure_with_error_bars.data:\n            x = list(data['x'])\n            y_upper = list(data['y'] + data['error_y']['array'])\n            y_lower = list(data['y'] - data['error_y']['array'] if data['error_y']['arrayminus'] is None else data['y'] - data['error_y']['arrayminus'])\n            color = f\"rgba({tuple(int(data['line']['color'].lstrip('#')[i:i+2], 16) for i in (0, 2, 4))},.3)\".replace('((','(').replace('),',',').replace(' ','')\n            fig.add_trace(\n                go.Scatter(\n                    x = x+x[::-1],\n                    y = y_upper+y_lower[::-1],\n                    fill = 'toself',\n                    fillcolor = color,\n                    line = dict(\n                        color = 'rgba(255,255,255,0)'\n                    ),\n                    hoverinfo = \"skip\",\n                    showlegend = False,\n                    legendgroup = data['legendgroup'],\n                    xaxis = data['xaxis'],\n                    yaxis = data['yaxis'],\n                )\n            )\n        # Reorder data as said here: https://stackoverflow.com/a/66854398/8849755\n        reordered_data = []\n        for i in range(int(len(fig.data)/2)):\n            reordered_data.append(fig.data[i+int(len(fig.data)/2)])\n            reordered_data.append(fig.data[i])\n        fig.data = tuple(reordered_data)\n    return fig\n"], [], [], ["[TearDown]\npublic void MyTearDown()\n{\n       try\n       {\n              // Perform any tear down code you like, like saving screenshots, page source, etc.\n       }\n       finally\n       {\n              _driver?.Quit();\n       }\n}\n"], [], [], ["# this solution works\nuser_input = input()\nmy_list = [int(i) for i in user_input.split() if (int(i)>=0)] \nmy_list.sort()\n[print(i, end=' ') for i in my_list]\n"], [], [], ["pip install --upgrade gym==0.19.0\n", "AutoROM will download the Atari 2600 ROMs.\nThey will be installed to:\n    /usr/local/lib/python3.8/dist-packages/AutoROM/roms\n\nExisting ROMs will be overwritten.\n\nI own a license to these Atari 2600 ROMs.\nI agree to not distribute these ROMs and wish to proceed: [Y/n]:\n\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/adventure.bin                                                                                                                           \nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/air_raid.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/alien.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/amidar.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/assault.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/asterix.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/asteroids.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/atlantis.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/atlantis2.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/backgammon.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/bank_heist.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/basic_math.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/battle_zone.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/beam_rider.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/berzerk.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/blackjack.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/bowling.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/boxing.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/breakout.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/carnival.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/casino.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/centipede.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/chopper_command.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/combat.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/crazy_climber.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/crossbow.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/darkchambers.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/defender.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/demon_attack.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/donkey_kong.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/double_dunk.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/earthworld.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/elevator_action.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/enduro.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/entombed.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/et.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/fishing_derby.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/flag_capture.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/freeway.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/frogger.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/frostbite.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/galaxian.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/gopher.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/gravitar.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/hangman.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/haunted_house.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/hero.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/human_cannonball.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/ice_hockey.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/jamesbond.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/journey_escape.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/joust.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/kaboom.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/kangaroo.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/keystone_kapers.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/king_kong.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/klax.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/koolaid.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/krull.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/kung_fu_master.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/laser_gates.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/lost_luggage.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/mario_bros.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/maze_craze.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/miniature_golf.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/montezuma_revenge.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/mr_do.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/ms_pacman.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/name_this_game.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/othello.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/pacman.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/phoenix.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/pitfall.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/pitfall2.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/pong.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/pooyan.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/private_eye.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/qbert.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/riverraid.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/road_runner.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/robotank.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/seaquest.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/sir_lancelot.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/skiing.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/solaris.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/space_invaders.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/space_war.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/star_gunner.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/superman.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/surround.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/tennis.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/tetris.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/time_pilot.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/trondead.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/turmoil.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/tutankham.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/up_n_down.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/venture.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/video_checkers.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/video_chess.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/video_cube.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/video_pinball.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/warlords.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/wizard_of_wor.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/word_zapper.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/yars_revenge.bin\nInstalled /usr/local/lib/python3.8/dist-packages/AutoROM/roms/zaxxon.bin\nDone!\n", "import gym\nimport ale_py\n\nprint('gym:', gym.__version__)\nprint('ale_py:', ale_py.__version__)\n\nenv = gym.make('Breakout-v0')\n", "gym: 0.21.0\nale_py: 0.7.1\n\nA.L.E: Arcade Learning Environment (version +b7b0c1a)\n[Powered by Stella]\n"], ["from playsound import playsound\n\nplaysound('//path//to//a//sound//file//you//want//to//play.mp3')\n"], [], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\nhr=((((hour*60+mins)+dura)//60)%24)\nmin=((hour*60+mins)+dura)%60 \nprint(hr,min,sep=\":\")\n"], [], ["audio_file = os.path.dirname(__file__) + 'audio.mp3'\nplaysound(audio_file)\n"], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\n\nprint (\"Event ending time: \",(hour + (mins + dura)//60)%24, \":\", (mins + dura%60)%60, sep=\"\")# Write your code here.\n"], ["import dash\nimport dash_html_components as html\nimport dash_bootstrap_components as dbc\n\nexternal_stylesheets = [\n    'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css',\n     dbc.themes.SLATE\n]\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n\n\napp.layout = html.Div([\n\n    html.Div([\n           html.I(className=\"fa fa-shield\"),\n    ]),\n\n])\n\nif __name__ == '__main__':\n    app.run_server(host='127.0.0.1', debug=True)\n"], ["driver = webdriver.Chrome(options=options)\ndriver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\ndriver.execute_cdp_cmd('Network.setUserAgentOverride', {\n        \"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.53 Safari/537.36'})\n\ndriver.get('url1')\n# Do operations with url1\n\ndriver.get('url2')\n# Do operations with url2 -> did not work and crashed\n", "def setup_driver():\n    global driver\n    driver = webdriver.Chrome(options=options)\n    driver.maximize_window()\n    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n    driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n        \"userAgent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.53 Safari/537.36'})\n\n\nsetup_driver()\ndriver.get('url1')\n# Do operations with url1\ndriver.close()\n\nsetup_driver()\ndriver.get('url2')\n# Do operations with url2\ndriver.close()\n"], [], [], [], [], [], ["df[\"date\"] = pd.to_datetime(df[\"date\"], format='%Y-%d-%m %H:%M:%S', errors='ignore').astype('datetime64[D]') \ndf[\"date\"] = pd.to_datetime(df[\"date\"], format='%m/%d/%Y %H:%M:%S %p', errors='ignore').astype('datetime64[D]')\n"], ["import collection\n\nclass ReporterProfile(models.Model):\n    ....\n\n    def published_articles_number(self):\n        num = collection.models.Report.objects.filter(reporterprofile=self.id).count()\n        return num\n", "import accounts\nfrom <project_name> import settings\n\nclass Report(models.Model):\n    reporterprofile = models.ForeignKey(accounts.models.ReporterProfile, on_delete=models.CASCADE, verbose_name=\"Report Author\")\n    ...\n\nclass Comment(models.Model):\n    report = models.ForeignKey(accounts.models.Report, on_delete=models.CASCADE, related_name='comments')\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE, verbose_name=\"Comment by\")\n    ...\n"], [], [], ["playsound(\"Typing.wav\", False)\n", "playsound(\"Typing.mp3\", False)\n"], ["n = int(input())\nif '1' in list(bin(n))[3:]: #also can use if '1' in bin(n)[3:]  OR can also use format(n, 'b')[1:]\n    print(\"False\")\nelse:\n    print(\"True\")\n", ">>> format(14, '#b'), format(14, 'b')\n('0b1110', '1110')\n>>> f'{14:#b}', f'{14:b}'\n('0b1110', '1110')\n"], [], ["v1/unit/\n", "v1/unit/addProduct/\n"], ["from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('log_dir')    \n\n# for some image \"im\"\nwriter.add_image('My image', im, 0)\nwrite.close()\n", "def examplePlot(data):\n    fig = plt.figure()\n    # do some plotting\n    return fig\n\nwriter.add_figure('My plot', examplePlot(data), 0)\nwriter.close()\n"], [], [], ["df_names = ['a', 'b', 'c', 'd']\ndf_list = [pd.DataFrame() for df in df_names]\n", "df_dict = dict(zip(df_names, df_list))\n"], ["model.load_weights(\"Detection_model.h5\")\n", "model.build(input_shape = <INPUT_SHAPE>)\nmodel.load_weights(\"Detection_model.h5\")\n"], ["sudo apt-get install python3-tk\n"], [], ["df.columns = df.rename(columns = lambda x: x.removesuffix('_x')) # or any suffix per say\ndf.columns = df.rename(columns = lambda x: x.removeprefix('prefix_i_want_to_remove')) \n", "df.columns = df.columns.map(lambda x: x.removesuffix('_x')) # or any suffix per say\ndf.columns = df.columns.map(lambda x: x.removeprefix('prefix_i_want_to_remove')) \n"], ["\"\"\"\nThis is a more complex example on performing clustering on large scale dataset.\n\nThis examples find in a large set of sentences local communities, i.e., groups of sentences that are highly\nsimilar. You can freely configure the threshold what is considered as similar. A high threshold will\nonly find extremely similar sentences, a lower threshold will find more sentence that are less similar.\n\nA second parameter is 'min_community_size': Only communities with at least a certain number of sentences will be returned.\n\nThe method for finding the communities is extremely fast, for clustering 50k sentences it requires only 5 seconds (plus embedding comuptation).\n\nIn this example, we download a large set of questions from Quora and then find similar questions in this set.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer, util\nimport os\nimport csv\nimport time\n\n\n# Model for computing sentence embeddings. We use one trained for similar questions detection\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n# and find similar question in it\nurl = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\ndataset_path = \"quora_duplicate_questions.tsv\"\nmax_corpus_size = 50000 # We limit our corpus to only the first 50k questions\n\n\n# Check if the dataset exists. If not, download and extract\n# Download dataset if needed\nif not os.path.exists(dataset_path):\n    print(\"Download dataset\")\n    util.http_get(url, dataset_path)\n\n# Get all unique sentences from the file\ncorpus_sentences = set()\nwith open(dataset_path, encoding='utf8') as fIn:\n    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n    for row in reader:\n        corpus_sentences.add(row['question1'])\n        corpus_sentences.add(row['question2'])\n        if len(corpus_sentences) >= max_corpus_size:\n            break\n\ncorpus_sentences = list(corpus_sentences)\nprint(\"Encode the corpus. This might take a while\")\ncorpus_embeddings = model.encode(corpus_sentences, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n\n\nprint(\"Start clustering\")\nstart_time = time.time()\n\n#Two parameters to tune:\n#min_cluster_size: Only consider cluster that have at least 25 elements\n#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\nclusters = util.community_detection(corpus_embeddings, min_community_size=25, threshold=0.75)\n\nprint(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n\n#Print for all clusters the top 3 and bottom 3 elements\nfor i, cluster in enumerate(clusters):\n    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n    for sentence_id in cluster[0:3]:\n        print(\"\\t\", corpus_sentences[sentence_id])\n    print(\"\\t\", \"...\")\n    for sentence_id in cluster[-3:]:\n        print(\"\\t\", corpus_sentences[sentence_id])\n\n", "\"\"\"\nThis is a simple application for sentence embeddings: clustering\n\nSentences are mapped to sentence embeddings and then k-mean clustering is applied.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\n\nembedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.'\n          ]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Perform kmean clustering\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = [[] for i in range(num_clusters)]\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    clustered_sentences[cluster_id].append(corpus[sentence_id])\n\nfor i, cluster in enumerate(clustered_sentences):\n    print(\"Cluster \", i+1)\n    print(cluster)\n    print(\"\")\n", "\"\"\"\nThis is a simple application for sentence embeddings: clustering\n\nSentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import AgglomerativeClustering\nimport numpy as np\n\nembedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Corpus with example sentences\ncorpus = ['A man is eating food.',\n          'A man is eating a piece of bread.',\n          'A man is eating pasta.',\n          'The girl is carrying a baby.',\n          'The baby is carried by the woman',\n          'A man is riding a horse.',\n          'A man is riding a white horse on an enclosed ground.',\n          'A monkey is playing drums.',\n          'Someone in a gorilla costume is playing a set of drums.',\n          'A cheetah is running behind its prey.',\n          'A cheetah chases prey on across a field.'\n          ]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Normalize the embeddings to unit length\ncorpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n\n# Perform kmean clustering\nclustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\nclustering_model.fit(corpus_embeddings)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = {}\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    if cluster_id not in clustered_sentences:\n        clustered_sentences[cluster_id] = []\n\n    clustered_sentences[cluster_id].append(corpus[sentence_id])\n\nfor i, cluster in clustered_sentences.items():\n    print(\"Cluster \", i+1)\n    print(cluster)\n    print(\"\")\n"], [], ["sum_x_train=np.sum(self.X_train**2,axis=1, keepdims=True)\nsum_x_test=np.sum(X**2,axis=1, keepdims=True)\nsum_2x_tr_te=np.dot(self.X_train,X.T)*2\nsum_x_train=np.dot(sum_x_train,np.ones((1,X.shape[0])))\nsum_x_test=np.dot(sum_x_test,np.ones((1,self.X_train.shape[0])))\ndists=np.sqrt(sum_x_test.T+sum_x_train-sum_2x_tr_te).T\n"], [], [], [], ["$ docker run -d --net gridNet2020 --shm-size=\"2g\" -e SE_OPTS=\"-browser applicationName=zChromeNodePdf30,browserName=chrome,maxInstances=1,version=78.0_debug_pdf\" -e HUB_HOST=selenium-hub-3.141.59 -P -p 5700:5555 --name zChromeNodePdf30 -v /var/lib/docker/sharedFolder:/home/seluser/Downloads selenium/node-chrome:3.141.59-xenon\n"], [], [], [], ["cd C:\\Users\\John\\hello_w\\.venv\\Scripts\n"], [], ["# Define Wrapper Class\nclass Class():\n    # Define __getitem__ method to be able to use index\n    def __getitem__(self, type):\n        # Define Main Class\n        class Class():\n            __doc__ = f\"\"\"I am an {type.__name__} class\"\"\"\n\n            def __init__(self, value):\n                self.value: type = type(value)\n        # Return Class\n        return Class\n# Set Class to an instance of itself to be able to use the indexing\nClass = Class()\n\nprint(Class[int].__doc__)\nprint(Class[int](5.3).value)\n"], [], ["with pd.ExcelWriter(outpath, engine=\"xlsxwriter\") as writer:\n    # do stuff here\n"], [" sudo apt-get install python-tk\n", " sudo apt-get install python3-tk\n", " sudo yum install python-tkinter\n", " sudo yum install python3-tkinter\n", "  sudo pacman -S tk\n", "  sudo pamac install tk\n"], [], [], [], [], [], ["from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n", "# Make predictions\npredictions = model.transform(dataset)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"], [], [], ["pip install -U pylint --user\n"], ["matplotlib.use('Agg')\n"], [], ["a=1243\na=[a]\n", "b = []\nb.append(a) # or you could do b=[a]\n"], [">>> str(1234)\n'1234'\n", ">>> var = 1234\n>>> [var]\n[1234]\n", ">>> l = []\n>>> l.append(var)\n>>> l\n[1234]\n"], ["a = 1234\nprint([a])\n", "b = []\nb.append(a)\n", "[1234]\n"], ["a = 1234\nlst = []\nlst.append(a)\n\nprint(lst) #[1234]\n"], [], [" @app.get(\"/\")\n def read_root(param1: Optional[str] = None, param2: Optional[str] = None):\n     url = f'http://some.other.api/{param1}/{param2}'\n     return {'url': str(url)}\n"], ["def custom_legend_name(new_names):\n    for i, new_name in enumerate(new_names):\n        fig.data[i].name = new_name\n", "def custom_legend_name(new_names):\n    for i, new_name in enumerate(new_names):\n        fig.data[i].name = new_name\n        \n\nimport pandas as pd\nimport plotly.express as px\n\nd = {'col1': [1, 2, 3], 'col2': [3, 4, 5]}\ndf = pd.DataFrame(data=d)\nfig = px.line(df, x=df.index, y=['col1', 'col2'])\ncustom_legend_name(['hello','hi'])\nfig.show()\n"], [], ["import pandas as pd\nimport plotly.express as px\n\ndf = pd.DataFrame(data={'col1': [1, 2, 3], 'col2': [3, 4, 5]})\n\nseries_names = [\"hello\", \"hi\"]\n\nfig = px.line(data_frame=df)\n\nfor idx, name in enumerate(series_names):\n    fig.data[idx].name = name\n    fig.data[idx].hovertemplate = name\n\nfig.show()\n"], [], [], ["import os\nos.getcwd()\n", "#%%\nprint(\"Some string here\")\n"], ["def power_of_two(n):\n    count = 0\n    st = str(bin(n))\n    st = st[2:]\n\n    for i in range(0,len(st)):\n        if(st[i] == '1'):\n            count += 1\n        \n    if(count == 1):\n        print(\"True\")\n    else:\n        print(\"False\")\n"], ["import matplotlib\nimport random\nimport plotly.graph_objects as go\nimport numpy as np\n\n\n#random color generation in plotly\nhex_colors_dic = {}\nrgb_colors_dic = {}\nhex_colors_only = []\nfor name, hex in matplotlib.colors.cnames.items():\n    hex_colors_only.append(hex)\n    hex_colors_dic[name] = hex\n    rgb_colors_dic[name] = matplotlib.colors.to_rgb(hex)\n\ndata = [[1, 3, 5, 4],\n        [2, 3, 5, 4],\n        [1, 1, 4, 5],\n        [2, 3, 5, 4]]\n#calculating mean and standard deviation\nmean=np.mean(data,axis=0)\nstd=np.std(data,axis=0)\n\n#draw figure\nfig = go.Figure()\nc = random.choice(hex_colors_only)\nfig.add_trace(go.Scatter(x=np.arange(4), y=mean+std,\n                                     mode='lines',\n                                     line=dict(color=c,width =0.1),\n                                     name='upper bound'))\nfig.add_trace(go.Scatter(x=np.arange(4), y=mean,\n                         mode='lines',\n                         line=dict(color=c),\n                         fill='tonexty',\n                         name='mean'))\nfig.add_trace(go.Scatter(x=np.arange(4), y=mean-std,\n                         mode='lines',\n                         line=dict(color=c, width =0.1),\n                         fill='tonexty',\n                         name='lower bound'))\nfig.show()\n"], ["pip install --upgrade matplotlib\n"], ["from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef about():\n    return 'It worked!'\n\nif __name__ == '__main__':\n    app.run(host='192.168.43.81', port=5000, debug=True, threaded=False)\n"], ["#pip install pygame\nfrom pygame import mixer\nimport time\nmixer.init() #Initialzing pyamge mixer\n\nmixer.music.load('lovingly-618.mp3') #Loading Music File\n\nmixer.music.play() #Playing Music with Pygame\n\ntime.sleep(5)\n\n\nmixer.music.stop()\n"], [], ["sudo apt-get update\n", "sudo apt-get install python3-virtualenv\n"], [], [], ["sudo apt-get install python2 python2-dev\n"], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\nmins = mins + dura # find a total of all minutes\nhour = hour + mins // 60 # find a number of hours hidden in minutes and update the hour\nmins = mins % 60 # correct minutes to fall in the (0..59) range\nhour = hour % 24 # correct hours to fall in the (0..23) range\nprint(hour, \":\", mins, sep='')\n"], [], [], ["$env:FLASK_APP = \"filename\"\n$env:FLASK_ENV = \"development\"\nFlask run\n"], [], [], [], ["a = torch(0.1, device='cuda')\n\na.cpu().data.numpy()\n"], ["pip install webdriver-manager\n", "pip install webdriver_manager\n"], [], [], [], ["{\n    \"python.pythonPath\": \"C:\\\\Python36\\\\python.exe\"\n}\n"], ["[project_root]/src/my_library\n", "PYTHONPATH=$PYTHONPATH:./src\n"], [], [], [], ["# in:\nj = [1,\n     2,\n     3\n]\n\n# out:\nj = [1, 2, 3]\n", "# This piece of code is written by me, it isn't part of the original doc\n# in\nj = [1, 2, 3, 4, 5, 6, 7]\n\n# out\nj = [\n    1, 2, 3, 4, 5, 6, 7\n]\n"], [], ["#given hour = 12 , mins = 17 , dura = 59\n\nhour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\nhour = (hour + dura//60 + (mins+ dura%60)//60)%24\nmins = (mins+ dura%60)%60\nprint(hour,\":\",mins,sep=\"\")\n\n13:16\n"], [], ["import io\nimport pandas as pd\nfrom google.colab import files\n", "uploaded = files.upload()\n", "df = pd.read_json(io.StringIO(uploaded.get('file.json').decode('utf-8')))\n"], ["import matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('Qt5Agg')\n"], ["#wrong inport \nfrom users_handler.models import userFormData\n#Correct import \nfrom users_handler.models import userformData\n"], ["import dash_bootstrap_components as dbc\nfor css in [dbc.themes.BOOTSTRAP,\n            './assets/mycss.css',\n            'https://use.fontawesome.com/releases/v5.8.1/css/all.css']:\n   app.css.append_css({\"external_url\": css})\n", "dbc.Button(children=[html.I(className=\"fas fa-plus-square\")])\n"], ["result = []\nfor i in a:\n    try: # to avoid error for the case of empty tensors\n        result.append(max(i.numpy()[1] == b.T.numpy()[1,i.numpy()[0] == b.T.numpy()[0,:]]))\n    except:\n        result.append(False)\nresult\n"], [], ["def add_usr_local_bin():\n    ffmpeg_path = \"/usr/local/bin\"\n    os.environ[\"PATH\"] += os.pathsep + ffmpeg_path\n"], [], ["wget http://ftp.de.debian.org/debian/pool/main/y/youtube-dl/youtube-dl_2021.02.04.1-1_all.deb\nsudo apt install ./youtube-dl_2021.02.04.1-1_all.deb\n"], ["`python manage.py makemigrations`\n`python manage.py migrate`\n`python manage.py runserver`\n"], ["apt-get update\napt-get install python3-virtualenv\n"], [], [], [" `pipenv install SOAPpy`\n"], [], [], ["[\n{\n \"frame_id\":1, \n \"filename\":\"C:\\\\Yolo_v4\\\\darknet\\\\build\\\\darknet\\\\x64\\\\f047.png\", \n \"objects\": [ \n    {\"class_id\":32, \"name\":\"right\", \"relative_coordinates\":{\"center_x\":0.831927,     \"center_y\":0.202225, \"width\":0.418463, \"height\":0.034752}, \"confidence\":0.976091}, \n    {\"class_id\":19, \"name\":\"h\", \"relative_coordinates\":{\"center_x\":0.014761,     \"center_y\":0.873551, \"width\":0.041723, \"height\":0.070544}, \"confidence\":0.484339}, \n    {\"class_id\":24, \"name\":\"left\", \"relative_coordinates\":{\"center_x\":0.285694,     \"center_y\":0.200752, \"width\":0.619584, \"height\":0.032149}, \"confidence\":0.646595}\n ] \n}\n]\n"], ["\"C:\\\\Yolo_v4\\\\darknet\\\\build\\\\darknet\\\\x64\\\\f047.png\"\n"], [], [], ["# Make predictions \npredictions = model.transform(dataset)\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n"], [], ["python3 -m pip install webdriver-manager\n"], [], [], [], ["screen.fill(RED)\npygame.draw.rect(screen, RED, (400, 400, 20, 20),0)\n"], ["\"jupyter.notebookFileRoot\": \"${workspaceFolder}\",\n"], ["{\n    \"jupyter.notebookFileRoot\": \"${fileDirname}\",\n}\n"], ["Note Deprecated in 3.0.0. It will be removed in future versions. \nUse ClusteringEvaluator instead. You can also get the cost on the training dataset in the summary.\n"], ["brew install pr0d1r2/python2/python@2.7.17 --build-from-source\n"], [], ["\"prettier.disableLanguages\": [\"django-html\"] \n"], ["$ sudo apt update\n$ sudo apt install ffmpeg\n"], ["sudo youtube-dl --update\n\n"], ["[9848:10684:1201/013233.169:ERROR:device_event_log_impl.cc(211)] [01:32:33.170] USB: usb_device_handle_win.cc:1020 Failed to read descriptor from node connection: A device attached to the system is not functioning. (0x1F)\n", "chrome://flags#enable-new-usb-backend\n"], ["pip install webdriver-manager.\n", "from selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(ChromeDriverManager().install()) \n"], [], ["user_input = input()\n\nmy_list = [int(i) for i in user_input.split() if (int(i)>=0)] \n\nmy_list.sort()\n\n[print(i, end=' ') for i in my_list]\n"], [], [], ["def dist(X, Y):\n    sx = np.sum(X**2, axis=1, keepdims=True)\n    sy = np.sum(Y**2, axis=1, keepdims=True)\n    return np.sqrt(-2 * X.dot(Y.T) + sx + sy.T)\n"], [], ["from fastapi import FastAPI, Response\n\napp = FastAPI()\n\n@app.post(\"/vector_image/\")\nasync def image_endpoint():\n    # img = ... # Create the image here\n    return Response(content=img, media_type=\"image/png\")\n"], [], [], ["import os, sys, tempfile, pprint\nfrom PIL import Image\nfrom pdf2image import pdfinfo_from_path,convert_from_path\nfrom pptx import Presentation\nfrom pptx.util import Inches\nfrom io import BytesIO\n\npdf_file = sys.argv[1]\nprint(\"Converting file: \" + pdf_file)\n\n# Prep presentation\nprs = Presentation()\nblank_slide_layout = prs.slide_layouts[6]\n\n# Create working folder\nbase_name = pdf_file.split(\".pdf\")[0]\n\n# Convert PDF to list of images\nprint(\"Starting conversion...\")\nprint()\npath: str = \"C:/ppttemp\"  #temp dir (use cron to delete files older than 1h hourly)\nslideimgs = []\ninfo = pdfinfo_from_path(pdf_file, userpw=None, poppler_path='C:/Program Files/poppler-0.90.1/bin/')\nmaxPages = info[\"Pages\"]\nfor page in range(1, maxPages+1, 5) : \n   slideimgs.extend( convert_from_path(pdf_file, dpi=250, output_folder=path, first_page=page, last_page = min(page+5-1,maxPages), fmt='jpeg', thread_count=4, poppler_path='C:/Program Files/poppler-0.90.1/bin/', use_pdftocairo=True)   )\n\nprint(\"...complete.\")\nprint()\n\n# Loop over slides\nfor i, slideimg in enumerate(slideimgs):\n    if i % 5 == 0:\n        print(\"Saving slide: \" + str(i))\n\n    imagefile = BytesIO()\n    slideimg.save(imagefile, format='jpeg')\n    imagedata = imagefile.getvalue()\n    imagefile.seek(0)\n    width, height = slideimg.size\n\n    # Set slide dimensions\n    prs.slide_height = height * 9525\n    prs.slide_width = width * 9525\n\n    # Add slide\n    slide = prs.slides.add_slide(blank_slide_layout)\n    pic = slide.shapes.add_picture(imagefile, 0, 0, width=width * 9525, height=height * 9525)\n    \n\n# Save Powerpoint\nprint(\"Saving file: \" + base_name + \".pptx\")\nprs.save(base_name + '.pptx')\nprint(\"Conversion complete. :)\")\nprint()\n"], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\ntime_hour = (hour + dura//60 + (mins+ dura%60)//60)%24\ntime_min = (mins+ dura%60)%60\nprint(\"It will end at \" + str(time_hour) + \":\" + str(time_min))\n"], ["model(np.zeros((1,w,h,c)))\n"], [], [], [], ["pip install pygame --pre\n"], ["from google.colab import drive\ndrive.mount('/content/drive', force_remount=True) \n"], [], ["fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n    y=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n    name=\"Name of Trace 1\"       # this sets its legend entry\n))\n\n\nfig.add_trace(go.Scatter(\n    x=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n    y=[1, 0, 3, 2, 5, 4, 7, 6, 8],\n    name=\"Name of Trace 2\"\n))\n\nfig.update_layout(\n    title=\"Plot Title\",\n    xaxis_title=\"X Axis Title\",\n    yaxis_title=\"X Axis Title\",\n    legend_title=\"Legend Title\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=18,\n        color=\"RebeccaPurple\"\n    )\n)\n\nfig.show()\n"], ["model_instant = YourModel(field1, field2,...etc)\n", "model_instant = YourModel(field1 = field1, field2 = field2,...etc)\n"], [], [], ["server = app.server\n"], [], ["!pip install matplotlib==3.1.0\n", "ax[i].set_ylim(sorted(ax[i].get_xlim(), reverse=True))\n \n"], ["html.Div([\n           html.I(className=\"fas fa-shield\"),\n    ])\n", "html.Div([\n           html.I(className=\"fas fa-shield-alt\"),\n    ])\n"], ["youtube-dl -n --cookies ~/Downloads/cookies.txt https://www.youtube.com/watch\\?v\\=h7Ii7KKapig\n"], ["hour = int(input(\"Starting time (hours): \"))\nmins = int(input(\"Starting time (minutes): \"))\ndura = int(input(\"Event duration (minutes): \"))\n\nhour = hour +int((mins+dura)/60)\nmins = (mins + dura)%60\nwhile(hour/24>=1):\n    hour=hour-24\n  \nprint(hour, \":\", mins, sep='')\n"], ["from datetime import date, datetime, time, timedelta\n\n\nif __name__ == '__main__':\n    hour = int(input(\"Starting time (hours): \"))\n    mins = int(input(\"Starting time (minutes): \"))\n    dura = int(input(\"Event duration (minutes): \"))\n    dt = datetime.combine(date.today(), time(hour, mins)) + timedelta(minutes=dura)\n    print(dt.time().strftime(\"%H:%M\"))\n"], ["from fastapi.responses import FileResponse\n\n@app.get(\"/\")\nasync def main():\n    return FileResponse(\"your_image.jpeg\")\n"], ["class NicelyPrintingDict():\n    def __init__(self, some_dictionary):\n        self.some_dictionary = some_dictionary\n\n    def __str__(self):\n        s = ''\n        for key, value in self.some_dictionary.items():\n            s += key + ': ' + str(value) + ' ' \n        return s.strip()\n", "foo = {'a': 123, 'b': 'asdf', 'c': 'Hello, world!'}\nnice_foo = NicelyPrintingDict(foo)\nprint(nice_foo)\n        \n"], ["foo = {'a': 123, 'b': 'asdf', 'c': 'Hello, world!'}\n\nfor i,j in foo.items():\n    print(i, \":\", j, end = \",\")\n\na : 123, b : asdf, c : Hello, world!,\n"], ["foo = {'a': 123, 'b': 'asdf', 'c': 'Hello, world!'}\nprint (str(foo).replace(\"{\",\"\").replace(\"'\",\"\").replace(\"}\",\"\"))\n", "a: 123, b: asdf, c: Hello, world!\n"], ["foo = {\n    'a': 123,\n    'b': 'asdf',\n    'c': 'Hello, World!'\n}\n\nprint(f\"a: {foo['a']} b: {foo['b']} c: {foo['c']}\")\n"], [], ["import torch\ntensor = torch.zeros(2)\nnumpy_array = tensor.numpy()\nprint('Before edit:')\nprint(tensor)\nprint(numpy_array)\n\ntensor[0] = 10\n\nprint()\nprint('After edit:')\nprint('Tensor:', tensor)\nprint('Numpy array:', numpy_array)\n", "Before edit:\ntensor([0., 0.])\n[0. 0.]\n\nAfter edit:\nTensor: tensor([10.,  0.])\nNumpy array: [10.  0.]\n"], ["b, t = plt.ylim()\nb += 0.5\nt -= 0.5\ncustom_ylim = (b, t)\nplt.setp(axes, ylim=custom_ylim)\n"], [], ["$ python3 -m venv env\n", "$ source env/bin/activate\n", "pip3 install tensorflow\n"], [], ["my_list = []\nn = len(input().split())\nfor i in range(0, n):\n    element = int(input().split()[i])\n    if element > 0:\n        my_list.append(element)\nmy_list.sort()\nprint(my_list)\n", "[2, 4, 10, 12, 39]\n", "my_list = []\nmy_string_array = input().split()\nn = len(my_string_array)\nfor i in range(0, n):\n    element = int(my_string_array[i])\n    if element > 0:\n        my_list.append(element)\nmy_list.sort()\nprint(my_list)\n"], ["elements = map(int, input().split())\nmy_list = [e for e in elements if e >= 0]\nmy_list.sort()\nprint(my_list)\n", "[2, 4, 10, 12, 39]\n"], ["pd.DataFrame(zip(X_train.columns, np.transpose(clf.coef_)), columns=['features', 'coef']) \n"], ["convert_from_path('C:\\path\\to\\your\\pdf', fmt='jpeg')\n", "import tempfile\n\nwith tempfile.TemporaryDirectory() as path:\n    images_from_path = convert_from_path('C:\\path\\to\\your\\pdf', output_folder=path)\n", "for i in range(0, 136 // 10 + 1):\n    convert_from_path('C:\\path\\to\\your\\pdf', first_page=i*10, last_page=(i+1)*10)\n"], [], ["import urllib.request, urllib.parse, urllib.error\nimport json\nimport ssl\n\napi_key = False\n# If you have a Google Places API key, enter it here\n# api_key = 'AIzaSy___IDByT70'\n# https://developers.google.com/maps/documentation/geocoding/intro\n\nif api_key is False:\n    api_key = 42\n    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\nelse :\n    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nwhile True:\n    address = input('Enter location: ')\n    if len(address) < 1: break\n\n    parms = dict()\n    parms['address'] = address\n    if api_key is not False: parms['key'] = api_key\n    url = serviceurl + urllib.parse.urlencode(parms)\n\n    print('Retrieving', url)\n    uh = urllib.request.urlopen(url, context=ctx)\n    data = uh.read().decode()\n    print('Retrieved', len(data), 'characters')\n\n    try:\n        js = json.loads(data)\n    except:\n        js = None\n\n    if not js or 'status' not in js or js['status'] != 'OK':\n        print('==== Failure To Retrieve ====')\n        print(data)\n        continue\n\n\n    placeid = js['results'][0]['place_id']\n\n    print('Place id', placeid)\n"], [], [], ["# created dataframe for example\ndf = pd.DataFrame({'Email':['test@gmail.com','test@gmail.com','test@gmail.com'],\n                   'filename':['c:/users\\test.csv','c:/users\\test1.csv','c:/users\\test1.csv']} )   # dataframe\n\n# will create new column with file name only\ndf['only_filename'] = [(path.encode('unicode_escape')[9:]).decode(\"utf-8\") for path in df['filename']]\n \n"], ["df['filename'] = df['filename'].str.split('/')[-1]\n"], ["import pandas as pd\ndf = pd.read_excel(\"your excel file location\")\n", "def get_filename(path):\n    temp_str = path.split('/')\n    return temp_str[-1]\n\ndf[\"filename\"] = df[\"filename\"].apply(get_filename)\n"], ["import pandas as pd\ndf = pd.read_csv('file_path\\file_name.csv')\ndf['filename'] = df['filename'].map(lambda x: x.split('\\\\')[-1][:-4])\ndf = df.drop_duplicates()\n", "df.to_excel('file_path\\new_file_name.xlsx')\n", "df.to_csv('file_path\\new_file_name.csv')\n"], ["1. torch.Tensor().numpy()\n2. torch.Tensor().cpu().data.numpy()\n3. torch.Tensor().cpu().detach().numpy()\n"], ["# In this program you will use a GeoLocation lookup API modelled after the\n# Google API to look up some universities and parse the returned data.\nimport ssl\nimport urllib.request, urllib.parse, urllib.error\nimport json\n\napi_key= False\n# api_key='....'\nif api_key is False:\n    api_key=42\n    serviceurl= 'http://py4e-data.dr-chuck.net/json?'\n\nelse:\n    serviceurl='https://maps.googleapis.com/maps/api/geocode/json?'\n\n## Ignore SSL certification\nctx=ssl.create_default_context()\nctx.check_hostname=False\nctx.verify_mode=ssl.CERT_NONE\n\nwhile True:\n    address= input('Enter Adderss: ')\n    if len(address)< 1: break\n\n    contents= dict()\n    contents['address']=address\n    if api_key is not False: contents['key']=api_key\n    url= serviceurl+ urllib.parse.urlencode(contents)\n\n    print('Retrieving:', url)\n    access= urllib.request.urlopen(url, context=ctx)\n    data= access.read().decode()\n    print('Retrieved', len(data), 'characters')\n\n    ## check--- working or not\n    try:\n        js= json.loads(data)\n    except:\n        None\n\n    # check if status is OK\n    if not js or 'status' not in js or js['status'] != 'OK':\n        print('========Failure To Retrieve=========')\n        print(data)\n        continue\n\n    ## For proper representation\n    print(json.dumps(js, indent=4))\n    ## Print various values\n    loc= js['results'][0]['place_id']\n    print('Loc', loc)\n    addr= js['results'][0]['formatted_address']\n    print('Address of', address,'is:',addr)\n    lati= js['results'][0]['geometry']['location']['lat']\n    long= js['results'][0]['geometry']['location']['lng']\n    print('Latitude:',lati,'\\nLongitude:',long)\n"], ["import json\nimport pandas as pd\n\nwith open('example1.json') as f1:               # open the file\n    data1 = json.load(f1)\n\nwith open('example2.json') as f2:                # open the file       \n    data2 = json.load(f2)\n    \ndf1 = pd.DataFrame([data1])                      # Creating DataFrames\ndf2 = pd.DataFrame([data2])                      # Creating DataFrames\n\nMergeJson = pd.concat([df1, df2], axis=1)         # Concat DataFrames\n\nMergeJson.to_json(\"MergeJsonDemo.json\")          # Writing Json\n"], [], [], ["app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n", "app = dash.Dash('SimpleDashboard',external_stylesheets=[dbc.themes.BOOTSTRAP, external_stylesheets])\n"], ["set FLASK_APP=app.py\npython -m flask run\n", "export FLASK_APP=app.py\npython -m flask run\n"], [], [], ["FROM ubuntu:latest\nRUN apt-get update -y\nRUN apt-get install -y python3 python3-dev\nWORKDIR /app\nCOPY .  /app\nENV DEBUG=True\nEXPOSE 80\n"], [], [], ["import json,ssl\nimport urllib.request,urllib.parse, urllib.error\n\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n\n\n#Stroring the given parameters\napi_key = 42\nserviceurl = \"http://py4e-data.dr-chuck.net/json?\"\n# sample_address = \"South Federal University\"\ndata_address = \"South Federal University\"\naddress_wanted = data_address\n\n#Setting the GET parameters on the URL\nparameters = {\"address\": address_wanted, \"key\":api_key}\nparamsurl = urllib.parse.urlencode(parameters)\n\n#Generating the complete URL. Printing it in order to check if it's correct.\nqueryurl = serviceurl.strip() + paramsurl.strip()\nprint(\"DATA URL: \", queryurl)\n\n#Obtaining and reading the data\ntry :\n    data_read = urllib.request.urlopen(queryurl , context=ctx).read()\n    data = data_read.decode()\n    # Parsing the data and looking for the field we want.\n    jsondata = json.loads(data)\n    print(jsondata)\n    place_id = jsondata[\"results\"][0][\"place_id\"]\n    print(\"PLACE ID: \", place_id)\nexcept:\n    print(\"Error.....\")\n    print(\"-\"*50)\n    print(data)\n"], ["matplotlib.use('TKAgg',warn=False, force=True)\n"], [], [], ["pip3 install PyQt5==5.9.2\n"], [], ["import plotly.express as px\n\nprint(px.colors.sequential.Viridis)\n['#440154', '#482878', '#3e4989', '#31688e', '#26828e', '#1f9e89', '#35b779', '#6ece58', '#b5de2b', '#fde725']\n\nprint(px.colors.sequential.Viridis[0])\n#440154\n"], [], ["if __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n"], [], [], ["localhost:5005/model/parse -s -d '{ \"text\": \"hi\" }'\n"], [], [], [], [], [], ["sudo apt-get install python3.8-venv\n"], ["class ProfileList(generic.ListView):\n    model = get_user_model()\n", "path('profile_list/dummy', ProfileList.as_view(), name='profile_lv'),\n", "path('profile_list', ProfileList.as_view(), name='profile_lv'),\n"], [">>> import json\n>>> s= json.dumps('{\"title\": \"Fetching all Jobs from \\\"host_name\\\".\"}')\n>>> j=json.loads(s)\n>>> print(j)\n{\"title\": \"Fetching all Jobs from \"host_name\".\"}\n"], ["s = '{\"title\": \"Fetching all Jobs from \\\\\"host_name\\\\\".\"}'\n", "s = r'{\"title\": \"Fetching all Jobs from \\\"host_name\\\".\"}'\n"], ["s = {\"title\": 'Fetching all Jobs from \"host_name\".'}\n\n# If you want a string, then here\nimport json\nj = json.dumps(s)\nprint(j)\n", "{\"title\": \"Fetching all Jobs from \\\"host_name\\\".\"}\n>>> s2 = r'{\"title\": \"Fetching all Jobs from \\\"host_name\\\".\"}'\n>>> json.loads(s2)\n{'title': 'Fetching all Jobs from \"host_name\".'}\n"], ["import json\ns = r'{\"title\": \"Fetching all Jobs from \\\"host_name\\\".\"}'\nj = json.loads(s)\nprint(j)\n"], ["    {\n        \"name\": \"Python: Django Debug Single Test\",\n        \"type\": \"python\",\n        \"request\": \"launch\",\n        \"program\": \"${workspaceFolder}/manage.py\",\n        \"args\": [\n            \"test\",\n            \"`echo -n \\\"${relativeFileDirname}\\\" | tr \\\\\\\\ .`.${fileBasenameNoExtension}\"\n        ],\n        \"django\": true\n    },\n"], ["loss = loss_fn(preds, labels)\nprint(loss.detach().numpy())\n"], ["$ virtualenv -p python2 venv\n$ . venv/bin/activate\n$ pip --version\npip 20.0.2 from /home/.../venv/lib/python2.7/site-packages/pip (python 2.7)\n"], ["import json,ssl\nimport urllib.request,urllib.parse, urllib.error\n\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\n\n\n#Stroring the given parameters\napi_key = 42\nserviceurl = \"http://py4e-data.dr-chuck.net/json?\"\n# sample_address = \"South Federal University\"\ndata_address = \"IIT KANPUR\"\naddress_wanted = data_address\n\n#Setting the GET parameters on the URL\nparameters = {\"address\": address_wanted, \"key\":api_key}\nparamsurl = urllib.parse.urlencode(parameters)\n\n#Generating the complete URL. Printing it in order to check if it's correct.\nqueryurl = serviceurl.strip() + paramsurl.strip()\nprint(\"DATA URL: \", queryurl)\n\n#Obtaining and reading the data\ntry :\n    data_read = urllib.request.urlopen(queryurl , context=ctx).read()\n    data = data_read.decode()\n    # Parsing the data and looking for the field we want.\n    jsondata = json.loads(data)\n    print(jsondata)\n    place_id = jsondata[\"results\"][0][\"place_id\"]\n    print(\"PLACE ID: \", place_id)\nexcept:\n    print(\"Error.....\")\n    print(\"-\"*50)\n    print(data)\n", "DATA URL:  http://py4e-data.dr-chuck.net/json?address=IIT+KANPUR&key=42\n{'results': [{'access_points': [], 'address_components': [{'long_name': 'Kalyanpur', 'short_name': 'Kalyanpur', 'types': ['political', 'sublocality', 'sublocality_level_1']}, {'long_name': 'Kanpur', 'short_name': 'Kanpur', 'types': ['locality', 'political']}, {'long_name': 'Kanpur Nagar', 'short_name': 'Kanpur Nagar', 'types': ['administrative_area_level_2', 'political']}, {'long_name': 'Uttar Pradesh', 'short_name': 'UP', 'types': ['administrative_area_level_1', 'political']}, {'long_name': 'India', 'short_name': 'IN', 'types': ['country', 'political']}, {'long_name': '208016', 'short_name': '208016', 'types': ['postal_code']}], 'formatted_address': 'Kalyanpur, Kanpur, Uttar Pradesh 208016, India', 'geometry': {'location': {'lat': 26.5123388, 'lng': 80.2329}, 'location_type': 'GEOMETRIC_CENTER', 'viewport': {'northeast': {'lat': 26.5136877802915, 'lng': 80.23424898029151}, 'southwest': {'lat': 26.5109898197085, 'lng': 80.23155101970849}}}, 'place_id': 'ChIJcb6oxAE3nDkRNoTDq4Do-zo', 'plus_code': {'compound_code': 'G66M+W5 Kalyanpur, jvs tower, Kanpur, Uttar Pradesh, India', 'global_code': '7MR2G66M+W5'}, 'types': ['establishment', 'point_of_interest', 'university']}], 'status': 'OK'}\nPLACE ID:  ChIJcb6oxAE3nDkRNoTDq4Do-zo\n"], ["from accounts.models import ReporterProfile\n[...]\nfoo = ReporterProfile()\n", "import accounts.models\n[...]\nfoo = accounts.models.ReporterProfile()\n"], [], ["E: Unable to locate package python-pip\n", "sudo apt update\n", "sudo apt-get install python-pip-whl\n"], ["sudo apt install python2\n", "curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\npython2 get-pip.py\n", "python2 -m pip\n"], ["from google.colab import drive\ndrive.mount(\"/content/gdrive\")\n", "from google.colab import drive\ndrive.mount(\"/content/drive/\")\n"], ["import urllib.error, urllib.request, urllib.parse\nimport json\n\ntarget = 'http://py4e-data.dr-chuck.net/json?' \nlocal = input('Enter location: ')\nurl = target + urllib.parse.urlencode({'address': local, 'key' : 42})\n\nprint('Retriving', url)\ndata = urllib.request.urlopen(url).read()\nprint('Retrived', len(data), 'characters')\njs = json.loads(data)\nprint(json.dumps(js, indent = 4)) \nprint('Place id', js['results'][0]['place_id'])\n"], ["    {\n        \"name\": \"Python: Django Debug Single Test\",\n        \"type\": \"python\",\n        \"request\": \"launch\",\n        \"program\": \"${workspaceFolder}/manage.py\",\n        \"args\": [\n            \"test\",\n            \"`echo -n ${relativeFileDirname} | tr \\/ .`.${fileBasenameNoExtension}\"\n        ],\n        \"django\": true\n    },\n"], ["_thread = threading.Thread(target=asyncio.run, args=(some_callback(\"some text\"),))\n_thread.start()\n"], ["pip3 install --upgrade Werkzeug==0.16.1\n", "Flask              1.1.2\nWerkzeug           0.16.1\n"], ["import multiprocessing\nfrom playsound import playsound\n\np = multiprocessing.Process(target=playsound, args=(\"file.mp3\",))\np.start()\ninput(\"press ENTER to stop playback\")\np.terminate()\n"], [], [], ["x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, random_state=0)\n\nlogreg = LogisticRegression().fit(x1_train,y1_train)\nlogreg\n\nprint(\"Training set score: {:.3f}\".format(logreg.score(x1_train,y1_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(x1_test,y1_test)))\n\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y1,x1)\nresult=logit_model.fit()\nprint(result.summary())\n", "Optimization terminated successfully.\n         Current function value: 0.596755\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:             IsCanceled   No. Observations:                20000\nModel:                          Logit   Df Residuals:                    19996\nMethod:                           MLE   Df Model:                            3\nDate:                Sat, 17 Aug 2019   Pseudo R-squ.:                  0.1391\nTime:                        23:58:55   Log-Likelihood:                -11935.\nconverged:                       True   LL-Null:                       -13863.\n                                        LLR p-value:                     0.000\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1417      0.050    -43.216      0.000      -2.239      -2.045\nx1             0.0055      0.000     32.013      0.000       0.005       0.006\nx2             0.0236      0.001     36.465      0.000       0.022       0.025\nx3             2.1137      0.104     20.400      0.000       1.911       2.317\n==============================================================================\n"], ["Alphabet = ['#AA0DFE', '#3283FE', '#85660D', '#782AB6', '#565656', '#1...\nAlphabet_r = ['#FA0087', '#FBE426', '#B00068', '#FC1CBF', '#C075A6', '...\n[...]\n", "# imports\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\n\n# sample data in a pandas dataframe\nnp.random.seed(1)\ndf=pd.DataFrame(dict(A=np.random.uniform(low=-1, high=2, size=25).tolist(),\n                    B=np.random.uniform(low=-4, high=3, size=25).tolist(),\n                    C=np.random.uniform(low=-1, high=3, size=25).tolist(),\n                    ))\ndf = df.cumsum()\n\n# define colors as a list \ncolors = px.colors.qualitative.Plotly\n\n# convert plotly hex colors to rgba to enable transparency adjustments\ndef hex_rgba(hex, transparency):\n    col_hex = hex.lstrip('#')\n    col_rgb = list(int(col_hex[i:i+2], 16) for i in (0, 2, 4))\n    col_rgb.extend([transparency])\n    areacol = tuple(col_rgb)\n    return areacol\n\nrgba = [hex_rgba(c, transparency=0.2) for c in colors]\ncolCycle = ['rgba'+str(elem) for elem in rgba]\n\n# Make sure the colors run in cycles if there are more lines than colors\ndef next_col(cols):\n    while True:\n        for col in cols:\n            yield col\nline_color=next_col(cols=colCycle)\n\n# plotly  figure\nfig = go.Figure()\n\n# add line and shaded area for each series and standards deviation\nfor i, col in enumerate(df):\n    new_col = next(line_color)\n    x = list(df.index.values+1)\n    y1 = df[col]\n    y1_upper = [(y + np.std(df[col])) for y in df[col]]\n    y1_lower = [(y - np.std(df[col])) for y in df[col]]\n    y1_lower = y1_lower[::-1]\n\n    # standard deviation area\n    fig.add_traces(go.Scatter(x=x+x[::-1],\n                                y=y1_upper+y1_lower,\n                                fill='tozerox',\n                                fillcolor=new_col,\n                                line=dict(color='rgba(255,255,255,0)'),\n                                showlegend=False,\n                                name=col))\n\n    # line trace\n    fig.add_traces(go.Scatter(x=x,\n                              y=y1,\n                              line=dict(color=new_col, width=2.5),\n                              mode='lines',\n                              name=col)\n                                )\n# set x-axis\nfig.update_layout(xaxis=dict(range=[1,len(df)]))\n\nfig.show()\n"], [], ["i=0\ncondition = True\nwhile condition:\n    if i<10:\n        i=i+1\n        print(i)\n    else:\n        condition=False\n"], ["i = 0\nwhile i < 10:\n    i += 1\n    print(i)\n", "i = 0\nwhile True:\n    if i < 10:\n        i += 1\n        print(i)\n    else:\n        break\n", "for i in range(10):\n    print(i)\n"], ["i=0\nwhile True:\n   i = i + 1 \n   print(i)\n   if i == 10:\n      break\n"], ["while i < 10:\n    print (i)\n    i++\n"], ["for i in range(0, 10):\n    print(i)\n"], [], ["!conda install -c conda-forge fbprophet -y\n", "!pip install --upgrade plotly\n"], ["pip uninstall matplotlib\n", "pip install matplotlib\n"], [], ["cd Python-3.7.7\nsudo ./configure --enable-optimizations\n", "sudo make altinstall\n", "python3.7 -m tkinter\n"], ["sudo pacman -S tk\n"], ["Test[int]()\n", "obj.__orig_class__ = Test[int]\n", "typing.get_args(Test[int]().__orig_class__) == (int,)\n"], ["from typing import TypeVar, Type, Generic\nT = TypeVar('T')\n\nclass Test(Generic[T]):\n\n    def hello(self):\n        print( \"I am {0}\".format(self.__orig_class__.__args__[0].__name__))\n\nTest[int]().hello()\n# I am int\n"], [], ["import pathlib\nimport google.cloud.storage as gcs\n\nclient = gcs.Client()\n\n#set target file to write to\ntarget = pathlib.Path(\"local_file.txt\")\n\n#set file to download\nFULL_FILE_PATH = \"gs://bucket_name/folder_name/file_name.txt\"\n\n#open filestream with write permissions\nwith target.open(mode=\"wb\") as downloaded_file:\n\n        #download and write file locally \n        client.download_blob_to_file(FULL_FILE_PATH, downloaded_file)\n"], ["sudo apt install tk-dev\n"], [], ["conda install pip\n"], ["data = next(iter(uploaded.values()))\n", "d = json.loads(data.decode())\n"], ["json.dumps(uploaded.decode(\"utf-8\"))\n"], ["Error occurred while deleting cookies from web browser!\nb'Message: invalid session id\\n  (Driver info: chromedriver=2.44.609551 (5d576e9a44fe4c5b6a07e568f1ebc753f1214634),platform=Linux 4.15.0-42-generic x86_64)\\n'\n", "selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash\nfrom unknown error: cannot determine loading status\nfrom tab crashed\n", "chrome_options.add_argument('--no-sandbox')         \n", "sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=512M tmpfs /dev/shm\n", "chrome_options.add_argument('--disable-dev-shm-usage')        \n"], [], ["from werkzeug.utils import cached_property\n"], ["flask run --host=0.0.0.0\n"], [], [], ["app=Flask(__name__)\n"], ["Missing/incorrect key = parameter (it is an easy number to guess) ...\n"], [], ["import importlib, inspect\nfor name, cls in inspect.getmembers(importlib.import_module(\"myfile\"), inspect.isclass):\n", "if cls.__module__ == 'myfile'\n"], ["import io\nfrom starlette.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.post(\"/vector_image\")\ndef image_endpoint(*, vector):\n    # Returns a cv2 image array from the document vector\n    cv2img = my_function(vector)\n    res, im_png = cv2.imencode(\".png\", cv2img)\n    return StreamingResponse(io.BytesIO(im_png.tobytes()), media_type=\"image/png\")\n"], ["import pygame, sys\nfrom pygame.locals import*\n\npygame.init()\nSCREENWIDTH = 800\nSCREENHEIGHT = 800\nRED = (255,0,0)\nscreen = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n\npygame.draw.rect(screen, RED, (400, 400, 20, 20),0)\nscreen.fill(RED)\n\npygame.display.update()\n\n# waint until user quits\nrunning = True\nwhile running:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            running = False\n\npygame.quit()\n"], ["import pygame, sys\nfrom pygame.locals import *\n\npygame.init()\nSCREENWIDTH = 800\nSCREENHEIGHT = 800\nRED = (255, 0, 0)\nscreen = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n\nwhile True:\n    pygame.draw.rect(screen, RED, (400, 400, 20, 20), 0)\n    screen.fill(RED)\n    pygame.display.update()\n"], [" import skvideo\n skvideo.setFFmpegPath(\"D:/ffmpeg-20170125-2080bc3-win64-static/ffmpeg- \n 20170125-2080bc3-win64-static/bin\")\n"], [], ["conda install -c conda-forge matplotlib\n"], [], ["import os\n\nos.system(\"echo %cd% > dir\")\nfile = open(\"dir\", \"r\")\nfilePath = file.read()\nfile.close()\n\nprint(filePath.split(\"\\n\")[0])\n"], ["external_stylesheets = [\n{\n    'href': 'https://use.fontawesome.com/releases/v5.8.1/css/all.css',\n    'rel': 'stylesheet',\n    'integrity': 'sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf',\n    'crossorigin': 'anonymous'\n}\n]\n"], ["\"python.pythonPath\": \"venv/Scripts/python.exe\"\n"], [" % curl https://codeload.github.com/matplotlib/matplotlib/tar.gz/v3.1.2 --output matplotlib-3.1.2.tar.gz\n % pip install matplotlib-3.1.2.tar.gz\n"], [], [".. container:: twocol\n\n    .. container:: leftside\n\n        text on left column\n\n    .. container:: rightside\n\n        text on right column\n"], [], [], ["pip install pygame\n"], [], [], [], ["import pandas as pd\n\ndef drop_prefix(self, prefix):\n    self.columns = self.columns.str.lstrip(prefix)\n    return self\n\npd.core.frame.DataFrame.drop_prefix = drop_prefix\n\n", "pd.drop_prefix('myprefix_')\n"], [], ["import seaborn as sns\ndf_corr = someDataFrame.corr()\nax = sns.heatmap(df_corr, annot=True) #notation: \"annot\" not \"annote\"\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n"], ["<html>\n    <body>\n      <p> Welcome. Follow the link below to unlock your account and set a new password.</p>\n      <a href=\"http://localhost:3000/setNewPasswordcode_parameter=${event.request.codeParameter}&user_name=${event.userName}\">Set Password</a>\n    </body>\n</html>\n", "\ncognitoIdentityServiceProvider.adminCreateUser({ \n  UserPoolId,\n  Username, \n  TemporaryPassword\n}).promise()\n\n"], ["from PyPDF2 import PdfFileWriter, PdfFileReader    \ninputpdf = PdfFileReader(open(pdf, \"rb\"))\nmaxPages = inputpdf.numPages\nfor page in range(1, maxPages, 100):\n    pil_images = pdf2image.convert_from_path(pdf, dpi=200, first_page=page,\n                                                     last_page=min(page + 100 - 1, maxPages), fmt= 'jpg',\n                                                     thread_count=1, userpw=None,\n                                                     use_cropbox=False, strict=False)\n"], ["import numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) #Your x values, for a 2 variable model.\n#y = 1 * x_0 + 2 * x_1 + 3 #This is the \"true\" model\ny = np.dot(X, np.array([1, 2])) + 3 #Generating the true y-values\nreg = LogisticRegression().fit(X, y) #Fitting the model given your X and y values.\nreg.coef_ #Prints an array of all regressor values (b1 and b2, or as many bs as your model has)\nreg.intercept_  #Prints value for intercept/b0 \nreg.predict(np.array([[3, 5]])) #Predicts an array of y-values with the fitted model given the inputs\n"], ["from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(random_state=0).fit(X, y)\n\nprint(clf.coef_, clf.intercept_)\n"], ["from statsmodels.discrete.discrete_model import Logit\nfrom statsmodels.tools import add_constant\n\nx = [...] # Obesrvations\ny = [...] # Response variable\n\nx = add_constant(x)\nprint(Logit(y, x).fit().summary())\n"], ["ax = sns.heatmap(...\n", "ax.get_ylim()\n(5.5, 0.5)\n", "ax.set_ylim(6.0, 0)\n"], [" env FLASK_APP=theflaskapp.py python -m flask run\n"], [], ["import skvideo\nskvideo.setFFmpegPath('D:\\\\ProgramData\\\\ffmpeg\\\\ffmpeg-20190814-8fcc5d9-win64-shared\\\\bin')\n"], ["# temp1.json\njson_a = [{'num':'1', 'item':'smartphone','data':'2019-01-01'},\n{'num':'2', 'item':'smartphone','data':'2019-01-02'},\n{'num':'3', 'item':'smartphone','data':'2019-01-03'},\n{'num':'4', 'item':'smartphone','data':'2019-01-04'}]\n\n# temp2.json\njson_b = [{'num':'5', 'item':'smartphone','data':'2019-01-05'},\n{'num':'6', 'item':'smartphone','data':'2019-01-06'},\n{'num':'7', 'item':'smartphone','data':'2019-01-07'}]\n\n# temp3.json\njson_c = [{'num':'8', 'item':'smartphone','data':'2019-01-08'},\n{'num':'9', 'item':'smartphone','data':'2019-01-09'},\n{'num':'10', 'item':'smartphone','data':'2019-01-10'},\n{'num':'11', 'item':'smartphone','data':'2019-01-11'},\n{'num':'12', 'item':'smartphone','data':'2019-01-12'}]\n\nprint(json_a + json_b + json_c)\n", "[{'num': '1', 'item': 'smartphone', 'data': '2019-01-01'},\n {'num': '2', 'item': 'smartphone', 'data': '2019-01-02'},\n {'num': '3', 'item': 'smartphone', 'data': '2019-01-03'},\n {'num': '4', 'item': 'smartphone', 'data': '2019-01-04'},\n {'num': '5', 'item': 'smartphone', 'data': '2019-01-05'},\n {'num': '6', 'item': 'smartphone', 'data': '2019-01-06'},\n {'num': '7', 'item': 'smartphone', 'data': '2019-01-07'},\n {'num': '8', 'item': 'smartphone', 'data': '2019-01-08'},\n {'num': '9', 'item': 'smartphone', 'data': '2019-01-09'},\n {'num': '10', 'item': 'smartphone', 'data': '2019-01-10'},\n {'num': '11', 'item': 'smartphone', 'data': '2019-01-11'},\n {'num': '12', 'item': 'smartphone', 'data': '2019-01-12'}]\n"], ["files=['my.json','files.json',...,'name.json']\n\ndef merge_JsonFiles(filename):\n    result = list()\n    for f1 in filename:\n        with open(f1, 'r') as infile:\n            result.extend(json.load(infile))\n\n    with open('counseling3.json', 'w') as output_file:\n        json.dump(result, output_file)\n\nmerge_JsonFiles(files)\n"], [], ["winsound.PlaySound(r'C:\\sound.wav', winsound.SND_ASYNC)\n", "winsound.PlaySound(None, winsound.SND_PURGE)\n"], ["import pyaudio\nimport wave\nimport time\nfrom pynput import keyboard\n\npaused = False    # global to track if the audio is paused\ndef on_press(key):\n    global paused\n    print (key)\n    if key == keyboard.Key.space:\n        if stream.is_stopped():     # time to play audio\n            print ('play pressed')\n            stream.start_stream()\n            paused = False\n            return False\n        elif stream.is_active():   # time to pause audio\n            print ('pause pressed')\n            stream.stop_stream()\n            paused = True\n            return False\n    return False\n\n\n# you audio here\nwf = wave.open('audio\\\\songs\\\\And_Your_Bird_Can_Sing_mp3_2_wav.wav', 'rb')\n\n# instantiate PyAudio\np = pyaudio.PyAudio()\n\n# define callback\ndef callback(in_data, frame_count, time_info, status):\n    data = wf.readframes(frame_count)\n    return (data, pyaudio.paContinue)\n\n# open stream using callback\nstream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n                channels=wf.getnchannels(),\n                rate=wf.getframerate(),\n                output=True,\n                stream_callback=callback)\n\n# start the stream\nstream.start_stream()\n\nwhile stream.is_active() or paused==True:\n    with keyboard.Listener(on_press=on_press) as listener:\n        listener.join()\n    time.sleep(0.1)\n\n# stop stream\nstream.stop_stream()\nstream.close()\nwf.close()\n\n# close PyAudio\np.terminate()\n"], ["import sys\nsys.path.append('/path/to/ffmpeg')\n"], [], ["scp remote_username@10.10.0.2:/path/to/foo.png /local/directory\n"], [], ["def is_power_of_two(n):\n    return (n != 0) and (n & (n-1) == 0)\n"], ["'1' not in bin(abs(n))[3:]\n", "check0 = lambda n: '1' not in bin(abs(n))[3:]\n", "check1 = lambda n: '1' not in bin(abs(n))[3:] and n != 0\n"], ["def get_url(self, action, code, email):\n        \"\"\" Used for constructing URLs. \"\"\"\n        rawUrl = 'https://{0}/{1}?code={2}&{3}'\n        return rawUrl.format(domain, action, code, email)\n"], ["import pandas as pd\ndf_list = ['a', 'b', 'c', 'd', 'e']\nfor i in df_list:\n    i = pd.DataFrame()\n"], [], [], ["Test\n======\n\n.. raw:: html\n\n    <div class=\"row\">\n      <div class=\"column\" style=\"background-color:#aaa;\">\n        <h2>Column 1</h2>\n        <p>Some text..</p>\n      </div>\n      <div class=\"column\" style=\"background-color:#bbb;\">\n        <h2>Column 2</h2>\n        <p>Some text..</p>\n      </div>\n      <div class=\"column\" style=\"background-color:#ccc;\">\n        <h2>Column 3</h2>\n        <p>Some text..</p>\n      </div>\n    </div>\n", "/* Create three equal columns that floats next to each other */\n.column {\n  float: left;\n  width: 33.33%;\n  padding: 10px;\n  height: 500px;\n}\n\n/* Clear floats after the columns */\n.row:after {\n  content: \"\";\n  display: table;\n  clear: both;\n}\n\n"], ["pip install bert-serving-server  # server\npip install bert-serving-client  # client, independent of `bert-serving-server`\n", "bert-serving-start -model_dir /your_model_directory/ -num_worker=4 \n", "from bert_serving.client import BertClient\nbc = BertClient()\nvectors=bc.encode(your_list_of_sentences)\n"], [], ["enc = OrdinalEncoder()\nenc.fit(df[[\"Sex\",\"Blood\", \"Study\"]])\ndf[[\"Sex\",\"Blood\", \"Study\"]] = enc.transform(df[[\"Sex\",\"Blood\", \"Study\"]])\n", "enc = OrdinalEncoder()\ndf[[\"Sex\",\"Blood\", \"Study\"]] = enc.fit_transform(df[[\"Sex\",\"Blood\", \"Study\"]])\n", "[array(['F', 'M'], dtype=object),\n array(['A', 'AB', 'B', 'O'], dtype=object),\n array(['Biology', 'English', 'Math', 'Science'], dtype=object)]```\n"], [" from PyPDF2 import PdfFileWriter, PdfFileReader\n\n inputpdf = PdfFileReader(open(\"document.pdf\", \"rb\"))\n\n for i in range(inputpdf.numPages):\n     output = PdfFileWriter()\n     output.addPage(inputpdf.getPage(i))\n     with open(\"document-page%s.pdf\" % i, \"wb\") as outputStream:\n         output.write(outputStream)\n", " import numpy as np\n import PIL\n\n list_im = ['Test1.jpg', 'Test2.jpg', 'Test3.jpg']\n imgs    = [ PIL.Image.open(i) for i in list_im ]\n # pick the image which is the smallest, and resize the others to match it (can be   arbitrary image shape here)\n min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n\n # save that beautiful picture\n imgs_comb = PIL.Image.fromarray( imgs_comb)\n imgs_comb.save( 'Trifecta.jpg' )    \n\n # for a vertical stacking it is simple: use vstack\n imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n imgs_comb = PIL.Image.fromarray( imgs_comb)\n imgs_comb.save( 'Trifecta_vertical.jpg' )\n"], [], [], [], ["df.rename(columns = lambda x: x.strip('_x'))\n", "df.rename(columns = lambda x: x.replace('_x$', ''))\n", "df.rename(columns = lambda x: x[:-2] if x.endswith('_x') else x)\n"], ["df.columns = [col[:-2] for col in df.columns if col[-2:]=='_x' else col]\n", "df.columns = [col.replace('_x', '') for col in df.columns]\n"], ["import skvideo\nskvideo.setFFmpegPath('C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\skvideo\\io')\n"], [], ["df = pd.Series('''3/14/2019 5:15:32 AM\n2019-08-03 05:15:35\n2019-01-03 05:15:33\n2019-01-03 05:15:33\n2/28/2019 5:15:31 AM\n2/27/2019 11:18:39 AM'''.split('\\n'), name='date', dtype=str).to_frame()\n\nprint(pd.to_datetime(df.date).dt.strftime('%Y-%m-%d'))\n", "0    2019-03-14\n1    2019-08-03\n2    2019-01-03\n3    2019-01-03\n4    2019-02-28\n5    2019-02-27\nName: date, dtype: object\n", "# Classify date column by format type\ndf['format'] = 1\ndf.loc[df.date.str.contains('/'), 'format'] = 2\ndf['new_date'] = pd.to_datetime(df.date)\n\n# Convert to datetime with two different format settings\ndf.loc[df.format == 1, 'new_date'] = pd.to_datetime(df.loc[df.format == 1, 'date'], format = '%Y-%d-%m %H:%M:%S').dt.strftime('%Y-%m-%d')\ndf.loc[df.format == 2, 'new_date'] = pd.to_datetime(df.loc[df.format == 2, 'date'], format = '%m/%d/%Y %H:%M:%S %p').dt.strftime('%Y-%m-%d')\nprint(df)\n", "                    date  format    new_date\n0   3/14/2019 5:15:32 AM       2  2019-03-14\n1    2019-08-03 05:15:35       1  2019-03-08\n2    2019-01-03 05:15:33       1  2019-03-01\n3    2019-01-03 05:15:33       1  2019-03-01\n4   2/28/2019 5:15:31 AM       2  2019-02-28\n5  2/27/2019 11:18:39 AM       2  2019-02-27\n"], ["def datCnv(src):\n    return pd.to_datetime(src)\n", "df['Dat'] = df.DatStr.apply(datCnv)\n", "                  DatStr                 Dat\n0   3/14/2019 5:15:32 AM 2019-03-14 05:15:32\n1    2019-08-03 05:15:35 2019-08-03 05:15:35\n2    2019-01-03 05:15:33 2019-01-03 05:15:33\n3    2019-01-03 05:15:33 2019-01-03 05:15:33\n4   2/28/2019 5:15:31 AM 2019-02-28 05:15:31\n5  2/27/2019 11:18:39 AM 2019-02-27 11:18:39\n"], ["import importlib, os, inspect\n\ndef get_modules_in_package(package_name: str):\n    files = os.listdir(package_name)\n    for file in files:\n        if file not in ['__init__.py', '__pycache__']:\n            if file[-3:] != '.py':\n                continue\n\n            file_name = file[:-3]\n            module_name = package_name + '.' + file_name\n            for name, cls in inspect.getmembers(importlib.import_module(module_name), inspect.isclass):\n                if cls.__module__ == module_name:\n                    yield cls\n"], ["import inspect\nimport importlib.util\n\n# Load the module from file\nspec = importlib.util.spec_from_file_location(\"foo\", \"foo.py\")\nfoo = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(foo)\n\n# Return a list of all attributes of foo which are classes\n[x for x in dir(foo) if inspect.isclass(getattr(foo, x))]\n"], [], [], ["ASGI_APPLICATION = \"routing.application\"\n"], [], [], [], [], [], [], [], [], ["!fusermount -u drive\n", "from google.colab import drive\ndrive.mount('/content/drive')\n"], ["# drive.py\n\n...\n\n  try:\n    if _os.path.islink(mountpoint):\n      raise ValueError('Mountpoint must not be a symlink')\n    if _os.path.isdir(mountpoint) and _os.listdir(mountpoint):\n      raise ValueError('Mountpoint must not already contain files')\n    if not _os.path.isdir(mountpoint) and _os.path.exists(mountpoint):\n      raise ValueError('Mountpoint must either be a directory or not exist')\n    #  if '/' in mountpoint and not _os.path.exists(_os.path.dirname(mountpoint)):\n    #    raise ValueError('Mountpoint must be in a directory that exists')\n  except:\n    d.terminate(force=True)\n    raise\n\n...\n", "from google.colab import drive\ndrive.mount('content/drive/')\n", "mount('/content/drive/')\n"], [], [], ["import skvideo\nskvideo.setFFmpegPath('/usr/local/lib/python3.6/dist-packages/ffmpeg/')\n"], ["!apt-get install --no-install-recommends ffmpeg && pip install ffmpeg scikit-video\n\nimport skvideo.io\nimport skvideo.datasets\nbbb = skvideo.datasets.bigbuckbunny()\nprint('bigbuckbunny is in: {}'.format(bbb))\nv = skvideo.io.vread(filename)\nprint('shape is: {}'.format(v.shape))\n"]]