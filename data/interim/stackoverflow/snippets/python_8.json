[["import pandas as pd\ndf = pd.DataFrame({'time': [pd.to_datetime('2019-01-15 13:25:43')]})\ndf_unix_sec = pd.to_datetime(df['time']).view(int) // 10 ** 9\nprint(df_unix_sec)\n", "FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"], ["series_sums = numpy.sum(dataframe, axis=1) # sum across columns\nseries_all_true = numpy.equal(series_sums, dataframe.shape[1])\nseries_all_false = numpy.equal(series_sums, 0) \n", "series_sums = dataframe.sum(axis=1) # sum across columns\nseries_all_true = series_sums == dataframe.shape[1]\nseries_all_false = series_sums == 0 \n", "dataframe_false_rows = dataframe[series_all_false]\n"], ["def move_zeros(a):\n    a.sort(key=lambda v: v == 0)\n    return a\n"], [], ["/usr/bin/python3\n", "lrwxrwxrwx 1 root root 9 Mar 23  2016 /usr/bin/python3 -> python3.5\n", "$ python3\nPython 3.10.0a6 (default, Mar  2 2021, 02:01:08) [GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n"], ["# merge two arrays as intermittent leaves (even and odd shuffle).\na = np.arange(5)\nfirst_array = np.arange(0,10,2)\nsecond_array = np.arange(1,11,2)\nres = np.empty(10)\nprint (first_array)\nprint (second_array)\nprint (res)\nres[a * 2] = first_array\nres[a * 2 +1] = second_array\nprint (res)\n"], [], [], ["foo = { 'a':1, 'b':2, 'c':3 }\nbar = { 'd':4, 'f':5, 'g':6 }\n\nassert foo.get('h', bar.get('h')), 'value \"h\" did not exist in dicts!'\n", "assert (myVariable := foo.get('h', bar.get('h'))), 'value \"h\" did not exist in dicts!'\n# if value of key \"h\" was found from any of the dicts, it is now assigned to the variable myVariable.\n"], [], [], [], [], ["a = int(input(\"Enter First Number: \"))\nb = int(input(\"Enter Second Number: \"))\nc = int(input(\"Enter Third Number: \"))\nd = int(input(\"Enter Fourth Number: \"))\n\n\nif a > b and a > c and a > d:\n    print(a)\nelif b > a and b > c and b > d:\n    print(b)\nelif c > a and c > b and c > d:\n    print(c)\nelif d > a and d > b and d > c:\n    print(d)\n"], ["import math\n\ndef maximumoffour(i,j,k,l):\n\nm=max(i,j,k,l)\n\nprint(\"Maximum of the given four numbers\",i,j,k,l, \"is\",m)\n\ni=eval(input(\"enter the value 1 :\"))\n\nj=eval(input(\"enter the value 2 :\"))\n\nk=eval(input(\"enter the value 3 :\"))\n\nl=eval(input(\"enter the value 4 :\"))\n\nmaximumoffour(i,j,k,l)\n"], ["max_ = a\nif b > max_:\n    max_ = b\nif c > max_:\n    max_ = c\nif d > max_:\n    max_ = d\nprint(max_)\n"], ["lst = []\nfor i in range(0,4):\n  x = int(input(\"enter a number:\"))\n  lst.append(x)\n\nprint(max(lst))\n"], ["print(max(a, b, c, d))\n"], [], ["sudo apt-get install python3-flask-migrate -y \n"], ["names = [\"Joe\", \"Smith\", \"Nancy\"]\n\n\nstring = \"Her name was Nancy\"\n\nfor name in names:\n        if names in string:\n                print(name)\n"], [], ["import re\nnames = [\"Joe\", \"Smith\", \"Nancy\"]\nstring = \"Her name was Nancy. His name was Smith\"\n\nresult = re.findall('|'.join(names), string)\nprint(*result, sep='\\n')\n", "Nancy\nSmith\n"], ["name = next((n for n in names if n in string), None)\nprint(name)\n", "x = next(iterator, default)\n", "for x in iterator:\n    break        # take the first if present \nelse:\n    x = default  # or fallback\n", "tokens = set(string.split())  # set has a better contains-check\nname = next((n for n in names if n in tokens) , None)\n"], ["string = \"Her name was Nancy\"\nnames = [\"Joe\", \"Smith\", \"Nancy\"]\nfor name in names:\n  if name in string:\n    print(\"True\")\n"], ["names = [\"Joe\", \"Smith\", \"Nancy\"]\nstring = \"Her name was Nancy\"\n\nfor name in names:\n    if name in string:\n        print(name)\n"], ["from datetime import datetime, timezone\n\nnow_utc = datetime.now(timezone.utc)\nyear = now_utc.strftime(\"%Y\")\nmonth = now_utc.strftime(\"%b\")\nday = now_utc.strftime(\"%d\")\nresult = day+\"-\"+month+\"-\"+year\n\nprint(result)\n\n"], ["from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n"], ["{x: L2[i % len(L2)] for i, x in enumerate(L1)]}\n"], ["def FirstFactorial(num):\n  if num ==1:\n    return 1\n  else:\n    for i in range(1, num-1/2):\n       num = num*(i)\n  return num\n\n# keep this function call here \n"], ["for i, row in df.iloc[:101].iterrows():\n    print(row)\n", ">>> df\n     a    b\n0  1.0  5.0\n1  2.0  4.0\n2  3.0  3.0\n3  4.0  2.0\n4  5.0  1.0\n5  6.0  NaN\n>>> for i, row in df.iloc[:3].iterrows():\n...     print(row)\n... \na    1.0\nb    5.0\nName: 0, dtype: float64\na    2.0\nb    4.0\nName: 1, dtype: float64\na    3.0\nb    3.0\nName: 2, dtype: float64\n>>>\n"], ["pip install h5py==2.9.0\n"], ["conda activate\nconda update -c defaults conda\nconda install conda-pack\nconda create -n <my_env_name> python=<python_version_number>\nconda activate <my_env_name>\n# if using Python Windows extensions:\nconda install pywin32\n", "# Pack Python environment my_env_name into my_env.zip\nconda pack -n <my_env_name> -o my_env.zip\n", "call Scripts\\activate.bat\nconda-unpack\n# At this point the Python environment is exactly as if you installed it here directly\n"], ["def roundRobin(T):\n    P = T[1:]+len(T)%2*[None]\n    for _ in range(len(T)-1):\n        yield [g for g in zip(T[:1]+P[1:len(P)//2+1],P[:1]+P[::-1])]\n        P.append(P.pop(0))\n\n    \n# Odd number of teams (7) \nfor games in roundRobin([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\"]):\n    print(games)\n\n[('A', 'B'), ('C', None), ('D', 'G'), ('E', 'F')] # C sits out\n[('A', 'C'), ('D', 'B'), ('E', None), ('F', 'G')] # E sits out\n[('A', 'D'), ('E', 'C'), ('F', 'B'), ('G', None)] # G sits out\n[('A', 'E'), ('F', 'D'), ('G', 'C'), (None, 'B')] # B sits out\n[('A', 'F'), ('G', 'E'), (None, 'D'), ('B', 'C')] # D sits out\n[('A', 'G'), (None, 'F'), ('B', 'E'), ('C', 'D')] # F sits out\n\n# Even number of teams (10)\nfor games in roundRobin([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]):\n    print(games) \n\n[('A', 'B'), ('C', 'J'), ('D', 'I'), ('E', 'H'), ('F', 'G')]\n[('A', 'C'), ('D', 'B'), ('E', 'J'), ('F', 'I'), ('G', 'H')]\n[('A', 'D'), ('E', 'C'), ('F', 'B'), ('G', 'J'), ('H', 'I')]\n[('A', 'E'), ('F', 'D'), ('G', 'C'), ('H', 'B'), ('I', 'J')]\n[('A', 'F'), ('G', 'E'), ('H', 'D'), ('I', 'C'), ('J', 'B')]\n[('A', 'G'), ('H', 'F'), ('I', 'E'), ('J', 'D'), ('B', 'C')]\n[('A', 'H'), ('I', 'G'), ('J', 'F'), ('B', 'E'), ('C', 'D')]\n[('A', 'I'), ('J', 'H'), ('B', 'G'), ('C', 'F'), ('D', 'E')]\n[('A', 'J'), ('B', 'I'), ('C', 'H'), ('D', 'G'), ('E', 'F')]\n"], ["pip install <full path to downloaded .whl file, can be copied from file properties>\n\npip install <wanted package, for me quiskit, for example>\n"], ["def make_bricks(small, big, goal):\n    return small >= goal - 5*min(goal//5,big)\n"], ["pd.crosstab(index=df['Type'], columns=df['Area'], normalize='index')\n", "Area  East     North     South  West\nType                                \nA     0.00  0.666667  0.333333  0.00\nB     0.00  0.500000  0.500000  0.00\nC     0.25  0.000000  0.500000  0.25\n"], ["\"workbench.editorAssociations\": [\n    {\n        \"viewType\": \"jupyter-notebook\",\n        \"filenamePattern\": \"*.ipynb\"\n    }\n],\n", "\"workbench.editorAssociations\": {\n    \"*.ipynb\": \"jupyter-notebook\"\n}\n"], [], ["username1:password1:dd/mm/yy\nusername2:password2:dd/mm/yy\nusername3:password3:dd/mm/yy\n", "with open(\"file.txt\",\"r+\") as f:\n    lines=[line[0:line.rindex(\":\")]+\"\\n\" for line in f.readlines()]\n    f.seek(0)\n    f.truncate()\n    f.writelines(lines)\n", "username1:password1\nusername2:password2\nusername3:password3\n"], ["import re\nline=\"username1:password1:dd/mm/yy\"\nprint re.sub('.{0,9}\\Z', '', line)\n"], ["eachLineData = 'username1:password1:dd/mm/yy'\nexpectedResult = eachLineData[:-9]\nprint(expectedResult)\n", "# Using readlines()\nfile1 = open('myfile.txt', 'r')\nLines = file1.readlines()\n\nfor line in Lines:\n    expectedResult = line[:-9]\n    print(expectedResult)\n"], [">>> string = \"username3:password3:dd/mm/yy\"\n>>> string[:-9]\n'username3:password3'\n"], ["re.search(r\"^[A-Z][a-z ]+[.?!]$\", text)\n"], [], [], ["def format_list(words, ending):\n    new_list = []\n    n = len(ending)\n    for word in words:\n        if len(word) >= n and  n > 0:\n            if not word[-n:] == ending:\n                new_list.append(word)\n        else:\n            new_list.append(word)\n    return new_list \n\nlist_words = format_list(list_words, ending)\nprint(list_words)\n"], [], ["def my_func(list_words, ending):\n    list_words[:] = [word for word in list_words\n                     if not word[::-1].startswith(ending[::-1])]\n    return list_words\n"], ["def my_func(list_words, ending):\n    return [word for word in list_words if word[len(word)-len(ending):] != ending]\n"], ["def filter_words(list_words, ending):\n    return [*filter(lambda x: x[-len(ending):] != ending , list_words)]\n"], [], [], [], ["import numpy as np\nfrom scipy.spatial import distance\nlist_a = np.array([[0,1], [2,2], [5,4], [3,6], [4,2]])\nlist_b = np.array([[0,1],[5,4]])\ndist = distance.cdist(list_a, list_b, 'euclidean')\nprint(dist)\n", "array([[0.        , 5.83095189],\n       [2.23606798, 3.60555128],\n       [5.83095189, 0.        ],\n       [5.83095189, 2.82842712],\n       [4.12310563, 2.23606798]])\n"], [], [], [], [], [], ["base = \"\";\nfor(x1=0; x1<charset.length(); x1++)\n    for(x2=0; x2<charset.length(); x2++)\n        for(x3=0; x3<charset.length(); x3++)\n            .\n            .\n            .\n        { base = charset[x1]+charset[x2]+charset[x3]+.....+charset[x6];\n          file.write(base + \"\\n\")\n        }\n"], ["from itertools import permutations\ncharset = \"0123456789abcdefghijklmnopqrstuvwxyz\"\nlinks = []\nwith open(\"codes.txt\", \"w\") as f:\n    for permutation in permutations(charset, 6):\n        f.write(''.join(permutation) + '\\n')\n", "from itertools import combinations\ncharset = \"0123456789abcdefghijklmnopqrstuvwxyz\"\nlinks = []\nwith open(\"codes.txt\", \"w\") as f:\n    for comb in combinations(charset, 6):\n        f.write(''.join(comb)+ '\\n')\n"], ["from itertools import permutations\ncharset = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n\nc = permutations(charset, 6)\n\nwith open('code.txt', 'w') as f:\n    for i in c:\n        f.write(\"\".join(i) + '\\n')\n"], ["from itertools import permutations\nfrom pandas import Series\ncharset = list(\"0123456789abcdefghijklmnopqrstuvwxyz\")\nlinks = []\nfile = open(\"codes.txt\", \"a\")\ncomb = permutations(charset,6)\ncomb = list(comb)\ncomb = list(map(lambda x:return ''.join(x),comb))\nmySeries = Series(comb)\nmySeries = mySeries.sort_values()\n\nbase = \"\"\nfor k in mySeries:\n    base += k\nfile.write(base + \"\\n\")\n\nfile.close()\n"], [], ["{\"a\":4}.get(\"b\", exec(\"raise Exception('some error msg') \"))\n"], [], ["addition_str = \"2+5+10+20\"\naddition_num = addition_str.split(\"+\")\naddition_int = list(addition_num)\nsum_val = 0\n    for num in addition_int:\n    number = int(num)\n    sum_val += number\n    print(sum_val)\n"], [], [], [], [], ["num_list = [422, 136, 524, 85, 96, 719, 85, 92, 10, 17, 312, 542, 87, 23, 86, 191, 116, 35, 173, 45, 149, 59, 84, 69 , 113, 166]\nodd=(x for x in num_list if x%2!=0)  # generator will return only odd numbers using source as num_list\nsumOdds=0\nfor i in range(5):\n   sumOdds+=next(odd)  # next runs generator and returns value\n"], [">>> None.name\nTraceback (most recent call last):\n  File \"<pyshell#0>\", line 1, in <module>\n    None.__name__\nAttributeError: 'NoneType' object has no attribute 'name'\n>>> \n", "class Test:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __enter__(self):\n        print(f'entering {self.name}')\n\n    def __exit__(self, exctype, excinst, exctb) -> bool:\n        print(f'exiting {self.name}')\n        return True\n\nwith Test('first') as test:\n    print(f'in {test.__class__.__name__}')\n\ntest = Test('second')\nwith test:\n    print(f'in {test.__class__.__name__}')\n", "entering first\nin NoneType\nexiting first\nentering second\nin Test\nexiting second\n", "    ...\n    def __enter__(self):\n        print(f'entering {self.name}')\n        return self\n    ...\n", "class Test:\n\n    def __init__(self, name):\n        self.name = name\n\n    def __enter__(self):\n        print(f'entering {self.name}')\n        return self\n\n    def __exit__(self, exctype, excinst, exctb) -> bool:\n        print(f'exiting {self.name}')\n        return True\n\nwith Test('first') as test:\n    print(f'in {test.name}')\n\ntest = Test('second')\nwith test:\n    print(f'in {test.name}')\n", "entering first\nin first\nexiting first\nentering second\nin second\nexiting second\n"], [], ["runs = 0\nfor i in num_list:\n    if i % 2 == 1:\n        odd.append(i)\n        runs += 1\n        if runs == 5:\n           break\nprint(odd)\n", "odd = []\nfor i in num_list:\n    if i % 2 == 1:\n        odd.append(i)\n        if len(odd) == 5:\n          break\nprint(odd)\n", "odd = [i for i in array if i % 2 == 1][:5]\n", "odd = (i for i in array if i % 2 == 1)\nprint(list(next(odd) for _ in range(5)))\n", "import itertools\nodds = (i for i in array if i % 2 == 1)\nlist(itertools.islice(odds, 5))\n"], ["num_list = [422, 136, 524, 85, 96, 719, 85, 92, 10, 17, 312, 542, 87, 23, 86, 191, 116, 35, 173, 45, 149, 59, 84, 69 , 113, 166]\nodd = []\ni = 0\n\nwhile len(odd) < 5:\n    if num_list[i] %2 != 0:\n        odd.append(num_list[i])\n    i+=1\n       \nprint(odd)\n"], ["num_list = [422, 136, 524, 85, 96, 719, 85, 92, 10, 17, 312, 542, 87, 23, 86, 191, 116, 35, 173, 45, 149, 59, 84, 69 , 113, 166]\ni=0\nodd_nums=[]\nwhile(i < len(num_list)):\n      \n    # checking condition\n    if num_list[i] % 2 != 0 and len(odd_nums)<5:\n        odd_nums.append(num_list[i])\n      \n    # increment i  \n    i += 1\nprint(odd_nums)\n"], ["(df0 == df1).any().values\n", "assert (df0.columns == df1.columns).all()\n\n(df0.values == df1.values).any(axis=0)\n", "array([ True, False,  True])\n"], ["df0.eq(df1).any()\n\ncol1     True\ncol2    False\ncol3     True\ndtype: bool\n"], [], ["import pandas as pd\narray=[]\ndf0 = pd.DataFrame({'col1':['a','b','c','d'],'col2':['b','c','e','f'],'col3':['d','f','g','a']})\ndf1 = pd.Series(['b','g','g'], index=['col1','col2','col3'])\nfor i in range(1,4):\n    col = 'col'+str(i)\n    array.append(df0[col].str.contains(df1[col]).any())\nprint(array)\n"], [], [], ["any(x % 2 for x in a)\n", "any((x % 2 for x in a))\n"], ["class Wildcard:\n    def __eq__(self, anything):\n        return True\n\na = [['1','2','3','a','b'],\n     ['4','5','6','c','d'],\n     ['7','8','9','e','f']]\n\nwc = Wildcard()\n\nprint(a.index(['4', '5', '6', wc, wc]))\n", "from operator import indexOf, itemgetter\n\na = [['1','2','3','a','b'],\n     ['4','5','6','c','d'],\n     ['7','8','9','e','f']]\n\nprint(indexOf((r[:3] for r in a), ['4', '5', '6']))\nprint(indexOf(map(itemgetter(slice(3)), a), ['4', '5', '6']))\n"], ["next(i for i, l in enumerate(a) if l[:3] == ['4', '5', '6'])\n"], ["def findListMatch(lists, match):\n    # Loop over lists with indices\n    for k, sublist in enumerate(lists):\n        # Check if the beginning of the list matches the desired start pattern\n        if sublist[0:len(match)] == match:\n            # If it's a match, return the index\n            return k\n    # If none of the lists match, return a placeholder value of \"None\"\n    return None\n\na = [['1','2','3','a','b'],\n     ['4','5','6','c','d'],\n     ['7','8','9','e','f']]\n\nmatchIndex = findListMatch(a, ['4', '5', '6'])\n\n# Result:\n# matchIndex = 1\n"], ["a = [['1','2','3','a','b'],\n     ['4','5','6','c','d'],\n     ['7','8','9','e','f']]\n\n\npattern = ['4','5','6']\n\ndef find_index(data, pattern):\n    for n, elt in enumerate(a):\n        if elt[:3] == pattern:\n            yield n\n\nindices = find_index(a, pattern)\nnext(indices)\n", "1\n"], [">>> list(zip(*list(zip(*a))[:3])).index(('4', '5', '6'))\n1\n>>> \n", ">>> [x[:3] for x in a].index(['4', '5', '6'])\n1\n>>> \n"], ["def __enter__(self):\n    print(f\"entering {self.name}\")\n    return self\n"], ["with Test('first') as test:\n    print(f'in {test.name}')\n\ntest = Test('second')\nwith test:\n    print(f'in {test.name}')\n", "def __enter__(self):\n    print(f'entering {self.name}')\n    return self\n"], ["webdriver-manager update --versions.chrome 87.0.4280.88\n"], [], ["def mul(m, n):\n     return m if n == 1 else m + (m, n - 1)\n"], ["from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(full_dataset, test_size=0.2)\n", "train_loader = DataLoader(dataset=train, batch_size=16, shuffle=True, num_workers=1)\ntest_loader = DataLoader(dataset=test)\n"], [], [], ["import nltk\nfrom nltk.corpus import brown\nfrom nltk.corpus import stopwords\n\ncfdconditions=brown.categories()\ncfdevents=['first','last']\nenglish_stopwords=set(stopwords.words('english'))\ncdev_cfd = nltk.ConditionalFreqDist(\n    [\n        (condition, word.lower())\n        for condition in cfdconditions\n        for word in brown.words(categories=condition) if not word.lower() in english_stopwords\n    ]\n)\n\ninged_cfd = nltk.ConditionalFreqDist(\n    [\n        (condition, word.lower()[-3:] if word.lower()[-3:]=='ing' else word.lower()[-2:] )\n        for condition in cfdconditions\n        for word in brown.words(categories=condition) if ((word.lower().endswith('ed') or word.lower().endswith('ing')) and not word.lower() in english_stopwords)\n    ]\n)\ncdev_cfd.tabulate(conditions=cfdconditions,samples=cfdevents)\ninged_cfd.tabulate(conditions=cfdconditions,samples=['ed','ing'])\n"], ["DEBUG = True #I got the error beacuse i changed the DEBUG to False\n\nMIDDLEWARE = [\n...\n'whitenoise.middleware.WhiteNoiseMiddleware',\n...\n]\n\nimport os\nSTATIC_URL = '/static/'\nSTATIC_ROOT = os.path.join(BASE_DIR, \"static\")\n\nMEDIA_URL = '/media/'\nMEDIA_ROOT = os.path.join(BASE_DIR, \"media\")\n\n\nSTATICFILES_STORAGE = 'whitenoise.storage.CompressedStaticFilesStorage'\n", "urlpatterns = [\npath('admin/', admin.site.urls),\npath('api/', include('app_name.urls'))\n]\n\nif settings.DEBUG:\n  urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)\n  urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n"], [], ["my_list = [100, 100, 200]\nprint(my_list, \"Original list\")\n\ndel my_list[1]\nprint(my_list, \"List after del\")\n\nmy_list.append(0)\nprint(my_list, \"List after append\")\n", "[100, 100, 200] Original list\n[100, 200] List after del\n[100, 200, 0] List after append\n", "my_list[1] = 0\nprint(my_list, \"List after re-assignment\")\n", "[100, 0, 200] List after re-assignment\n", "[100, 100, 200, 200] -> Original list\n\nloop index=0\n    Check duplicates for item-0 which is 100\n    [100, 0, 200, 200] -> Sees item-1 as duplicate.\n    count = 1\nloop index=1\n    Check duplicates for item-1 which is 0\n    No duplicates\n    count = 1\nloop index=2\n    Check duplicates for item-2 which is 200\n    [100, 0, 200, 0] -> Sees item-3 as duplicate.\n    count = 2\nloop index=3\n    Check duplicates for item-3 which is 0\n    [100, 0, 200, 0] -> Sees item-1 as duplicate.\n    count = 3\n", "...\nfor i in range(len(list)):\n    if list[i] == 0:\n        continue\n    ...\n", "my_list = [1,1,3,4,5,5,5,6]\ncount = 0\nfor i in range(len(my_list)):\n    for k in range(0, i):\n        if my_list[i] == my_list[k]:\n            my_list[i] = 0  # If you want the result to be [1, 0, 3, 4, 5, 0, 0, 6]\n            # my_list[k] = 0  # If you want the result to be [0, 1, 3, 4, 0, 0, 5, 6]\n            count = count + 1\n            break\n\nprint(count)\nprint(my_list)\n", "3\n[1, 0, 3, 4, 5, 0, 0, 6]\n", "my_list = [1,1,3,4,5,5,5,6]\nexisting = set()\ncount = 0\nfor i in range(len(my_list)):\n    if my_list[i] in existing:\n        my_list[i] = 0\n        count += 1\n    else:\n        existing.add(my_list[i])\n\nprint(count)\nprint(my_list)\n"], [], [], ["# old list \nmylist = [1,1,3,4,5,5,5,6]\n# create new list\nnewlist = []\n\n# Loop for each item inside the old list \nfor x in mylist:\n    #check if this item is not duplicate \n    if x not in newlist:\n        # add not duplicate item to new list\n        newlist.append(x)\n\nprint (newlist)\n"], ["ls = [1,1,3,4,5,5,5,6]\nresult = []\nfor item in ls[:]:\n    if item not in result:\n        result.append(item)\n\nprint(result)\n\n"], [], [], [], ["unicode = \"U0001f0cf\"\nunicode = (f\"\\{unicode}\")\n\nprint(unicode.encode('raw-unicode-escape').decode('unicode-escape'))\n"], [], [], ["import pandas as pd\n\ndata = ['FirstName LastName StudentID',\n'FirstName2 LastName2 StudentID2']\n\ndf = pd.DataFrame(data=data, columns=['text'])\n\ndf['id'] = df.text.str.split(\" \").str.get(-1)\n"], ["import re\n..\ntext = soup.get_text()\nlist = re.findall(r'[a-z0-9]+@[gmail|yahoo|rediff].com', text)\nfor email in list:\n    print(email)\n"], ["# A Python program to print all\n# permutations of given length\nfrom itertools import permutations\n\n# check_validity(tuple1) checks whether the combination contains at least 3 elements from list a and at least 3 elements from list b\ndef check_validity(tuple1):\n    a_cnt = 0\n    b_cnt = 0\n    for t in tuple1:\n        if t in a:\n            a_cnt += 1\n        if t in b:\n            b_cnt += 1\n    if a_cnt >= 3 and b_cnt>=3:\n        return True\n    else:\n        return False\n\n# Get all permutations of length 10\na = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nb = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\nlist1 = a + b     # merge two lists\nperm = list(permutations(list1, 10))     # find all permutaions\n\n# Print the obtained permutations\nresult_perm = []\nfor tuple1 in perm:          # list of permutations contain tuples\n    if check_validity(tuple1) == True:    # check_validity(tuple1) checks whether the combination contains at least 3 elements from list a and at least 3 elements from list b\n        result_perm.append(\"\".join(list(map(str,tuple1))))     # join characters from tuple to generate a string\n\nresult_perm\n"], ["import re\ntext =soup.get_text()\nemails = re.findall(r'[a-z0-9]+@\\S+.com', str(text))\nprint(emails)\n"], ["from google.colab import drive\ndrive.mount(\"/content/gdrive\")\n"], [], [], [], ["networkx                  2.3                      pypi_0    pypi\ndecorator                 4.3.0                    pypi_0    pypi\n"], [], ["(x % 2 for x in a)\n", "(True for x in a if x % 2)\n"], ["def mysort(df1,df2):\n    d = dict(zip(df2['c1'],df2['v1'].rank()))\n    o = pd.concat((df2,df1),keys=[1,2],names=['Key'])\n\n    return (o.assign(k=o['c1'].map(d)).sort_values(['k','Key','v2'])\n            .loc[:,list(df1)])#.reset_index(drop=True)\n", "def mysort_two(df1,df2):\n    d  = dict(zip(df2['c1'],df2['v1'].rank()))\n    o = pd.concat((df2,df1),keys=[1,2],names=['Key'])\n    a = o.to_numpy()[np.lexsort((o['v2'],o.index.get_level_values('Key'),\n                o['c1'].map(d)))]\n    return pd.DataFrame(a,columns=df2.columns)\n\n#Same can also be written as below:\n# def mysort_two(df1,df2):\n#     d  = dict(zip(df2['c1'],df2['v1'].rank()))\n#     o = pd.concat((df2,df1))\n#     a = o.to_numpy()[np.lexsort((o['v2']\n#                 ,np.append(np.ones(len(df2)), np.ones(len(df1))*2),\n#                 o['c1'].map(d)))]\n#     return pd.DataFrame(a,columns=df2.columns)\n", "print(mysort_two(df1,df2)) #method2\n#print(mysort(df1,df2)) #method1\n\n   c1   c2  v1  v2\n0   B  NaN   2   5\n1   B    P   0   1\n2   B    T   2   2\n3   B    Y   0   2\n4   C  NaN   3   5\n5   C    Y   1   1\n6   C    P   1   2\n7   C    T   1   2\n8   D  NaN   4   2\n9   D    T   2   0\n10  D    P   1   1\n11  D    Y   1   1\n12  A  NaN   9   2\n13  A    Y   2   0\n14  A    P   4   1\n15  A    T   3   1\n"], ["# Establish ordering based on sorted df2\ncat_type = pd.CategoricalDtype(df2.sort_values('v1')['c1'], ordered=True)\nnew_df = (\n    # Add Indicator to each DataFrame\n    pd.concat([df.assign(indicator=i) for (i, df) in enumerate([df1, df2])])\n        # Set c1 to the categorical ordering from above\n        .astype({'c1': cat_type})\n        # Sort By Categorical, then df2 first then v2 within dfs\n        .sort_values(['c1', 'indicator', 'v2'],\n                     ascending=(True, False, True),\n                     ignore_index=True)\n        # Remove Indicator column\n        .drop(columns='indicator')\n)\n", "   c1   c2  v1  v2\n0   B  NaN   2   5\n1   B    P   0   1\n2   B    T   2   2\n3   B    Y   0   2\n4   C  NaN   3   5\n5   C    Y   1   1\n6   C    P   1   2\n7   C    T   1   2\n8   D  NaN   4   2\n9   D    T   2   0\n10  D    P   1   1\n11  D    Y   1   1\n12  A  NaN   9   2\n13  A    Y   2   0\n14  A    P   4   1\n15  A    T   3   1\n", "import pandas as pd\nfrom numpy import nan\n\ndf1 = pd.DataFrame({\n    'c1': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D'],\n    'c2': ['P', 'T', 'Y', 'P', 'T', 'Y', 'P', 'T', 'Y', 'P', 'T', 'Y'],\n    'v1': [4, 3, 2, 0, 2, 0, 1, 1, 1, 1, 2, 1],\n    'v2': [1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1]\n})\n\ndf2 = pd.DataFrame({\n    'c1': ['A', 'B', 'C', 'D'],\n    'c2': [nan, nan, nan, nan],\n    'v1': [9, 2, 3, 4],\n    'v2': [2, 5, 5, 2]\n})\n", "import itertools\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import nan\n\ndf1 = pd.DataFrame({\n    'c1': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'D'],\n    'c2': ['P', 'T', 'Y', 'P', 'T', 'Y', 'P', 'T', 'Y', 'P', 'T', 'Y'],\n    'v1': [4, 3, 2, 0, 2, 0, 1, 1, 1, 1, 2, 1],\n    'v2': [1, 1, 0, 1, 2, 2, 2, 2, 1, 1, 0, 1]\n})\n\ndf2 = pd.DataFrame({\n    'c1': ['A', 'B', 'C', 'D'],\n    'c2': [nan, nan, nan, nan],\n    'v1': [9, 2, 3, 4],\n    'v2': [2, 5, 5, 2]\n})\n\n\ndef op_fn(df1, df2):\n    result = []\n    for i, row in df2.sort_values('v1').iterrows():\n        result.append(row.to_frame().T)\n        result.append(df1[df1['c1'].eq(row['c1'])].sort_values('v2'))\n    return pd.concat(result, ignore_index=True)\n\n\ndef cat_ordered(df1, df2):\n    # Establish ordering based on df2\n    cat_type = pd.CategoricalDtype(df2.sort_values('v1')['c1'], ordered=True)\n    return (\n        # Add Indicator to each DataFrame\n        pd.concat([df.assign(indicator=i) for (i, df) in enumerate([df1, df2])])\n            # Set c1 to the categorical ordering from above\n            .astype({'c1': cat_type})\n            # Sort By Categorical, then df2 first then v2 within dfs\n            .sort_values(['c1', 'indicator', 'v2'],\n                         ascending=(True, False, True),\n                         ignore_index=True)\n            # Remove Indicator column\n            .drop(columns='indicator')\n    )\n\n\ndef groupby_fn(df1, df2):\n    y = df1.assign(ind=df1['v1'])\n    x = df2.groupby(\"c1\").apply(\n        lambda v: pd.concat(\n            [y[y[\"c1\"].eq(v[\"c1\"].iat[0])], v.sort_values(\"v2\")]\n        )\n    )\n    x.loc[:, \"ind\"] = x.loc[:, \"ind\"].ffill()\n    return x.sort_values(\"ind\").drop(columns=\"ind\").reset_index(drop=True)\n\n\ndef concat_itertools(df1, df2):\n    out = pd.concat([df1.sort_values('v1'),\n                     df2.sort_values('v2')],\n                    ignore_index=True)\n    return out.loc[itertools.chain.from_iterable(out.groupby('c1', sort=False)\n                                                 .groups.values())]\n\n\ndef helpcol(df1, df2):\n    cat_type = pd.CategoricalDtype(df2.sort_values('v1')['c1'], ordered=True)\n    dfc = pd.concat([df1, df2])\n    dfc[\"c2sort\"] = dfc[\"c2\"].notna()\n    dfc[\"c1sort\"] = dfc[\"c1\"].astype(cat_type)\n    return dfc.sort_values([\"c1sort\", \"c2sort\", \"v2\"], ignore_index=True).drop(\n        [\"c2sort\", \"c1sort\"], axis=1\n    )\n\n\ndef U11(df1, df2):\n    df = pd.concat([df1, df2], ignore_index=True)\n    return (\n        df.reindex(\n            df.sort_values('c1')\n                .groupby('c1', as_index=False)['v1'].transform('min')\n                .squeeze().sort_values().index\n        ).reset_index(drop=True)\n    )\n\n\ndef mysort_anky(df1, df2):\n    d = dict(zip(df2['c1'], df2['v1'].rank()))\n    o = pd.concat((df2, df1), keys=[1, 2], names=['Key'])\n\n    return (o.assign(k=o['c1'].map(d)).sort_values(['k', 'Key', 'v2'])\n                .loc[:, list(df1)])  # .reset_index(drop=True)\n\n\ndef mysort_two_anky(df1, df2):\n    d = dict(zip(df2['c1'], df2['v1'].rank()))\n    o = pd.concat((df2, df1), keys=[1, 2], names=['Key'])\n    a = o.to_numpy()[np.lexsort((o['v2'], o.index.get_level_values('Key'),\n                                 o['c1'].map(d)))]\n    return pd.DataFrame(a, columns=df2.columns)\n"], ["b = df.groupby(cols)[aggCol].count()\nl = list(range(b.index.nlevels-1))\np = [b]\nwhile l:\n    p.append(b.groupby(level=l).sum())\n    l.pop()\n\nresult = pd.concat(p)\n", "from itertools import zip_longest\ncols = list('QRST')\naggCol = 'P'\nb = df.groupby(cols)[aggCol].agg(['sum', 'count'])\nl = list(range(b.index.nlevels-1))\np = [b]\nwhile l:\n    p.append(b.groupby(level=l).sum())\n    l.pop()\n\nresult = pd.concat(p)\nresult = result.assign(avg=result['sum']/result['count']).drop(['sum', 'count'], axis=1)\nresult \n", "                       avg\n(NR, F, HOL, F)   6.250000\n(NR, F, NHOL, F)  2.600000\n(NR, M, NHOL, M)  4.666667\n(R, F, HOL, F)    1.000000\n(R, F, NHOL, F)   0.000000\n(R, M, NHOL, M)   0.000000\n(NR, F, HOL)      6.250000\n(NR, F, NHOL)     2.600000\n(NR, M, NHOL)     4.666667\n(R, F, HOL)       1.000000\n(R, F, NHOL)      0.000000\n(R, M, NHOL)      0.000000\n(NR, F)           4.222222\n(NR, M)           4.666667\n(R, F)            0.500000\n(R, M)            0.000000\nNR                4.333333\nR                 0.333333\n"], [], [], ["In [5]: dis.dis('any(x%2 for x in a)')\n[...]\n\nDisassembly of <code object <genexpr> at 0x105e860e0, file \"<dis>\", line 1>:\n  1           0 LOAD_FAST                0 (.0)\n        >>    2 FOR_ITER                14 (to 18)\n              4 STORE_FAST               1 (x)\n              6 LOAD_FAST                1 (x)\n              8 LOAD_CONST               0 (2)\n             10 BINARY_MODULO\n             12 YIELD_VALUE\n             14 POP_TOP\n             16 JUMP_ABSOLUTE            2\n        >>   18 LOAD_CONST               1 (None)\n             20 RETURN_VALUE\n\n\nIn [6]: dis.dis('any(True for x in a if x % 2)')\n[...]\n\nDisassembly of <code object <genexpr> at 0x105d993a0, file \"<dis>\", line 1>:\n  1           0 LOAD_FAST                0 (.0)\n        >>    2 FOR_ITER                18 (to 22)\n              4 STORE_FAST               1 (x)\n              6 LOAD_FAST                1 (x)\n              8 LOAD_CONST               0 (2)\n             10 BINARY_MODULO\n             12 POP_JUMP_IF_FALSE        2\n             14 LOAD_CONST               1 (True)\n             16 YIELD_VALUE\n             18 POP_TOP\n             20 JUMP_ABSOLUTE            2\n        >>   22 LOAD_CONST               2 (None)\n             24 RETURN_VALUE\n"], [], ["def Kyletrb(request):\n    all = \"SELECT Description, Account ,Credit,Debit FROM [Kyle].[dbo].[_btblCbStatement] WHERE Account <> ''\"\n    cursor.execute(all);\n    xAll = cursor.fetchall()\n    cursor.close()\n    xAll_l = []\n    for row in xAll:\n        rdict = {}\n        rdict[\"Description\"] = row[0]\n        rdict[\"Account\"] = row[1]\n        rdict[\"Credit\"] = row[2]\n        rdict[\"Debit\"] = row[3]\n        xAll_l.append(rdict)\n    return render(request , 'main/Kyletrb.html' , {\"xAlls\":xAll_l}) \n", "<table>\n  <th>Account</th>\n  <th>Description</th>\n  <th>Debit</th>\n  <th>Credit</th>\n  {% for xAll in xAlls %}\n    <tr>\n      <td>{{ xAll.Description }}</td>\n      <td>{{ xAll.Account }}</td>\n      <td>{{ xAll.Debit }}</td>\n      <td>{{ xAll.Credit }}</td>\n    </tr>\n  {% endfor %}\n</table>\n"], ["<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-wEmeIV1mKuiNpC+IOBjI7aAzPcEZeedi5yW5f2yOq55WWLwNGmvvx4Um1vskeMj0\" crossorigin=\"anonymous\">\n{% extends \"main/base.html\"%}\n\n{% block content%}\n<h1>Kyle Database Trial Balance</h1>\n<br>\n<div class=\"container\">\n<div class=\"row mb-0\">\n\n<div class=\"col\">\n<h3>Account</h3>\n{% for accountNo in accountNo %}\n    <p  style=\"font-size:10px\">{{ accountNo }}</p>\n{% endfor %}\n</div>\n\n<div class=\"col-4\">\n  <h3>Description</h3>\n{% for description in description %}\n    <p  style=\"font-size:10px\">{{ description }}</p>\n{% endfor %}\n</div>\n\n<div class=\"col\">\n<h3>Debit</h3>\n{% for debit in debit %}\n  <p  style=\"font-size:10px\">{{ debit }}</p>\n{% endfor %}\n</div>\n\n<div class=\"col\">\n<h3>Credit</h3>\n{% for credit in credit %}\n  <p  style=\"font-size:10px\">{{ credit }}</p>\n{% endfor %}\n</div>\n\n</div>\n</div>\n{% endblock %}\n"], ["Please try as below.\n\n{% for description in descriptions %}\n     <tr>\n         <td>{{ description.accountNo }}</td>\n         <td>{{ description.description }}</td>\n         <td>{{ description.debit }}</td>\n         <td>{{ description.credit }}</td>\n     </tr>\n{% endfor %}\n"], ["{% for desc in description %}\n  <tr>\n   <td>{{ desc.accountNo }}</td>\n   <td>{{ desc.description }}</td>\n   <td>{{ desc.debit }}</td>\n   <td>{{ desc.credit }}</td>\n  </tr>\n{% endfor %}\n"], ["for(description in descriptions)\n"], ["{% for description in description %}\n  <tr>\n   <td>{{ description.accountNo }}</td>\n   <td>{{ description.description }}</td>\n   <td>{{ description.debit }}</td>\n   <td>{{ description.credit }}</td>\n  </tr>\n  {% endfor %}\n"], [">>> df = pd.concat([df1, df2], ignore_index=True)\n>>> df.reindex(df.sort_values('c1').groupby('c1', as_index=False)['v1'].transform('min').squeeze().sort_values().index).reset_index(drop=True)\n   c1   c2  v1  v2\n0   B  NaN   2   5\n1   B    P   0   1\n2   B    T   2   2\n3   B    Y   0   2\n4   C  NaN   3   5\n5   C    P   1   2\n6   C    T   1   2\n7   C    Y   1   1\n8   D  NaN   4   2\n9   D    P   1   1\n10  D    T   2   0\n11  D    Y   1   1\n12  A  NaN   9   2\n13  A    P   4   1\n14  A    T   3   1\n15  A    Y   2   0\n>>> \n", "def U11():\n    for i in range(1000):\n        df = pd.concat([df1, df2], ignore_index=True)\n        df = df.reindex(df.sort_values('c1').groupby('c1', as_index=False)['v1'].transform('min').squeeze().sort_values().index).reset_index(drop=True)\n\ndef ThePyGuy():\n    for i in range(1000):\n        result = []\n        for i,row in df1.sort_values('v1').iterrows():\n            result.append(row.to_frame().T)\n            result.append(df2[df2['c1'].eq(row['c1'])].sort_values('v2'))\n        df = pd.concat(result, ignore_index=True)\n\na = time.time()\nThePyGuy()\nb = time.time()\nprint('ThePyGuy:', b - a)\n\na = time.time()\nU11()\nb=time.time()\nprint('U11:', b-a)\n", "ThePyGuy: 5.920747756958008\nU11: 5.1511549949646\n"], ["import networkx as nx\nG=nx.from_edgelist(L)\n\nl=list(nx.connected_components(G))\n# after that we create the map dict , for get the unique id for each nodes\nmapdict={z:x for x, y in enumerate(l) for z in y }\n# then append the id back to original data for groupby \nnewlist=[ x+(mapdict[x[0]],)for  x in L]\nimport itertools\n#using groupby make the same id into one sublist\nnewlist=sorted(newlist,key=lambda x : x[2])\nyourlist=[list(y) for x , y in itertools.groupby(newlist,key=lambda x : x[2])]\nyourlist\n[[('A', 'B', 0), ('B', 'C', 0), ('C', 'D', 0)], [('E', 'F', 1)], [('G', 'H', 2), ('H', 'I', 2), ('G', 'I', 2), ('G', 'J', 2)]]\n", "L1,L2,L3=[[y[:2]for y in x] for x in yourlist]\nL1\n[('A', 'B'), ('B', 'C'), ('C', 'D')]\nL2\n[('E', 'F')]\nL3\n[('G', 'H'), ('H', 'I'), ('G', 'I'), ('G', 'J')]\n"], [], ["import itertools\n\nout = pd.concat([df1.sort_values('v1'),\n                 df2.sort_values('v2')],\n                 ignore_index=True)\n", "# Original answer\n# >>> out.reindex(out.groupby('c1', sort=False)\n#         .apply(lambda x: x.index)\n#         .explode())\n\n# Faster alternative\n>>> out.loc[itertools.chain.from_iterable(out.groupby('c1', sort=False)\n                                             .groups.values())]\n", ">>> out\n   c1   c2  v1  v2\n0   B  NaN   2   5\n8   B    P   0   1\n12  B    T   2   2\n13  B    Y   0   2\n1   C  NaN   3   5\n9   C    Y   1   1\n14  C    P   1   2\n15  C    T   1   2\n2   D  NaN   4   2\n5   D    T   2   0\n10  D    P   1   1\n11  D    Y   1   1\n3   A  NaN   9   2\n4   A    Y   2   0\n6   A    P   4   1\n7   A    T   3   1\n"], ["def helpcol(df1, df2):\n    cat_type = pd.CategoricalDtype(df2.sort_values('v1')['c1'], ordered=True)\n    dfc = pd.concat([df1, df2])\n    dfc[\"c2sort\"] = dfc[\"c2\"].notna()\n    dfc[\"c1sort\"] = dfc[\"c1\"].astype(cat_type)\n    return dfc.sort_values([\"c1sort\", \"c2sort\", \"v2\"], ignore_index=True).drop(\n        [\"c2sort\", \"c1sort\"], axis=1\n    )\n", "   helpcol(df1, df2)\n", "   c1   c2  v1  v2\n0   B  NaN   2   5\n1   B    P   0   1\n2   B    T   2   2\n3   B    Y   0   2\n4   C  NaN   3   5\n5   C    Y   1   1\n6   C    P   1   2\n7   C    T   1   2\n8   D  NaN   4   2\n9   D    T   2   0\n10  D    P   1   1\n11  D    Y   1   1\n12  A  NaN   9   2\n13  A    Y   2   0\n14  A    P   4   1\n15  A    T   3   1\n"], [], ["# Add an additional 200 GB disk\nnew_disk_kb = int(20) * 1024 * 1024\ndisk_spec = vim.vm.device.VirtualDeviceSpec()\ndisk_spec.fileOperation = \"create\"\ndisk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add\ndisk_spec.device = vim.vm.device.VirtualDisk()\ndisk_spec.device.backing = vim.vm.device.VirtualDisk.RawDiskMappingVer1BackingInfo()\ndisk_spec.device.backing.diskMode = 'persistent'\ndisk_spec.device.unitNumber = 2\ndisk_spec.device.capacityInKB = new_disk_kb\n"], [">>> df['amount'].str.replace(r'^[0]*', '', regex=True).fillna('0')\n0     324\n1    S123\n2      10\n3       0\n4      30\n5    SA40\n6    SA24\n", "^[0]*\n\n^ asserts position at start of a line\nMatch a single character present in the list below [0]\n* matches the previous token between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n"], ["df['amount'] = df['amount'].str.replace(r'^(0+)(?!$)', '', regex=True).fillna('0')\n", "nums = {'amount': ['0324','S123','0010', None, '0030', 'SA40', 'SA24', '0', '000']}\ndf = pd.DataFrame(nums)\n\n  amount\n0   0324\n1   S123\n2   0010\n3   None\n4   0030\n5   SA40\n6   SA24\n7      0           <==   Added a single 0 here\n8    000           <==   Added a sequence of all 0's here\n", "print(df)\n\n  amount\n0    324\n1   S123\n2     10\n3      0\n4     30\n5   SA40\n6   SA24\n7      0           <==  Single 0 is not removed  \n8      0           <==  Last 0 is kept\n"], ["df['amount'] = df['amount'].str.lstrip('0')\n", "df['amount'].fillna(value='0')\n", "df['amount'] = df['amount'].str.lstrip('0').fillna(value='0')\n"], ["df['amount'] = df['amount'].str.lstrip('0').fillna(value='0')\n"], ["0     324\n1    S123\n2      10\n3       0\n4      30\n5    SA40\n6    SA24\nName: amount, dtype: object\n"], [], ["new_disk_kb = int(20) * 1024 * 1024\ndisk_spec = vim.vm.device.VirtualDeviceSpec()\ndisk_spec.fileOperation = \"create\"\ndisk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add\ndisk_spec.device = vim.vm.device.VirtualDisk()\ndisk_spec.device.backing = vim.vm.device.VirtualDisk.RawDiskMappingVer1BackingInfo()\ndisk_spec.device.backing.diskMode = 'persistent'\ndisk_spec.device.unitNumber = 3\ndisk_spec.device.capacityInKB = new_disk_kb\n", "spec = vim.vm.ConfigSpec()\n# get all disks on a VM, set unit_number to the next available\nunit_number = 0\ncontroller = None\nfor device in vm.config.hardware.device:\n    if hasattr(device.backing, 'fileName'):\n        unit_number = int(device.unitNumber) + 1\n        # unit_number 7 reserved for scsi controller\n        if unit_number == 7:\n            unit_number += 1\n        if unit_number >= 16:\n            print(\"we don't support this many disks\")\n            return -1\n    if isinstance(device, vim.vm.device.VirtualSCSIController):\n        controller = device\nif controller is None:\n    print(\"Disk SCSI controller not found!\")\n    return -1\ndisk_spec = vim.vm.device.VirtualDeviceSpec()\ndisk_spec.fileOperation = \"create\"\ndisk_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add\ndisk_spec.device = vim.vm.device.VirtualDisk()\nrdm_info = vim.vm.device.VirtualDisk.RawDiskMappingVer1BackingInfo()\ndisk_spec.device.backing = rdm_info\ndisk_spec.device.backing.compatibilityMode = disk_compatibility_mode\ndisk_spec.device.backing.diskMode = disk_mode\n# The device_name will look something like\n#     /vmfs/devices/disks/naa.41412340757396001d7710df0fdd22a9\ndisk_spec.device.backing.deviceName = device_name\ndisk_spec.device.unitNumber = unit_number\ndisk_spec.device.controllerKey = controller.key\nspec.deviceChange = [disk_spec]\n"], ["curl https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py > get-poetry\n\npython get-poetry --uninstall\n"], ["    for i in range(k):\n        random_a = len(a)\n        random_b = random_a - 1\n        random_c = a.pop(random_b)\n        a = [random_c] + a\n    oz = a\n    random_list = []\n    for i in queries:\n        z = oz[i]\n        y = str(z)\n        random_list.append(y)\n    return random_list```\nis this the correct answer\n\n\n    \n"], ["controller = next( \\\n (device for device in vmconf.hardware.device \\\n     if isinstance(device, vim.vm.device.VirtualSCSIController) \\\n ), None)\ndisk_spec.device.controllerKey = controller.key\n"], [], [], [], ["df['sale_date'] = pd.to_datetime(df['sale_date'], format='%m/%d/%y')\n# or\ndf['sale_date'] = pd.to_datetime(df['sale_date'], dayfirst=False)\n", "df['sale_date'] = pd.to_datetime(df['sale_date'], format='%d/%m/%y')\n# or\ndf['sale_date'] = pd.to_datetime(df['sale_date'], dayfirst=True)\n"], ["from datetime import datetime\ndateparse = lambda x: datetime.strptime(x, '%m/%d/%Y')\n\ndf = pd.read_csv('history.csv', parse_dates=['Month'], date_parser=dateparse)\n"], ["from azure.storage.blob import ContainerClient\n\ncontainer = ContainerClient.from_connection_string(connectStr, 'foo')\n\ntry:\n    container_properties = container.get_container_properties()\n    # Container foo exists. You can now use it.\n\nexcept Exception as e:\n    # Container foo does not exist. You can now create it.\n    container.create_container()\n", "from azure.storage.blob import ContainerClient\n\ncontainer = ContainerClient.from_connection_string(connectStr, 'foo')\n\nif container.exists():\n    # Container foo exists. You can now use it.\n\nelse:\n    # Container foo does not exist. You can now create it.\n    container.create_container()\n"], [], ["newdf = pd.DataFrame(columns=df.columns)\ncols = list('QRST')\naggCol = 'P'\ndef aggregation(cols, origcols, aggCol, df, count=1):\n    global newdf\n    cols = origcols[:count]\n    count += 1\n    newdf = newdf.append(df.groupby(cols)[aggCol].agg('mean').round(2).reset_index().T.reindex(origcols + [aggCol]).T, ignore_index=True)\n    if cols != origcols:\n        aggregation(cols, origcols, aggCol, df, count)\n\naggregation(cols, cols, aggCol, df)\nnewdf['agg'] = newdf.pop(aggCol)\nprint(newdf)\n", "     Q    R     S    T   agg\n0   NR  NaN   NaN  NaN  4.33\n1    R  NaN   NaN  NaN  0.33\n2   NR    F   NaN  NaN  4.22\n3   NR    M   NaN  NaN  4.67\n4    R    F   NaN  NaN   0.5\n5    R    M   NaN  NaN     0\n6   NR    F   HOL  NaN  6.25\n7   NR    F  NHOL  NaN   2.6\n8   NR    M  NHOL  NaN  4.67\n9    R    F   HOL  NaN     1\n10   R    F  NHOL  NaN     0\n11   R    M  NHOL  NaN     0\n12  NR    F   HOL    F  6.25\n13  NR    F  NHOL    F   2.6\n14  NR    M  NHOL    M  4.67\n15   R    F   HOL    F     1\n16   R    F  NHOL    F     0\n17   R    M  NHOL    M     0\n", "import time\n\nu11time1 = time.time()\n\nfor i in range(5000):\n    df = pd.read_clipboard()\n    newdf = pd.DataFrame(columns=df.columns)\n    cols = list('QRST')\n    aggCol = 'P'\n    def aggregation(cols, origcols, aggCol, df, count=1):\n        global newdf\n        cols = origcols[:count]\n        count += 1\n        newdf = newdf.append(df.groupby(cols)[aggCol].agg('mean').round(2).reset_index().T.reindex(origcols + [aggCol]).T, ignore_index=True)\n        if cols != origcols:\n            aggregation(cols, origcols, aggCol, df, count)\n\n    aggregation(cols, cols, aggCol, df)\n    newdf['agg'] = newdf.pop(aggCol)\n\nu11time2 = time.time()\n\nprint('u11 time:', u11time2 - u11time1)\n\nthepyguytime1 = time.time()\n\nfor i in range(5000):\n    df = pd.read_clipboard()\n    cols = list('QRST')\n    aggCol = 'P'\n    groupCols = []\n    result = []\n    for col in cols:\n        groupCols.append(col)\n        result.append(df.groupby(groupCols)[aggCol].agg(count='count').reset_index())\n    result = pd.concat(result)[groupCols+['count']]\n\nthepyguytime2 = time.time()\n\nprint('ThePyGuy time:', thepyguytime2 - thepyguytime1)\n", "u11 time: 120.2678394317627\nThePyGuy time: 153.01533579826355\n"], ["import platform\nfrom functools import wraps\nfrom typing import Callable, Optional\n\n\ndef implement_for_os(os_name: str):\n    \"\"\"\n    Produce a decorator that defines a function only if the\n    platform returned by `platform.system` matches the given `os_name`.\n    Otherwise, replace the function with one that raises `NotImplementedError`.\n    \"\"\"\n    def decorator(previous_definition: Optional[Callable]):\n        def _decorator(func: Callable):\n            if previous_definition and hasattr(previous_definition, '_implemented_for_os'):\n                # This function was already implemented for this platform. Leave it unchanged.\n                return previous_definition\n            elif platform.system() == os_name:\n                # The current function is the correct impementation for this platform.\n                # Mark it as such, and return it unchanged.\n                func._implemented_for_os = True\n                return func\n            else:\n                # This function has not yet been implemented for the current platform\n                @wraps(func)\n                def _not_implemented(*args, **kwargs):\n                    raise NotImplementedError(\n                        f\"The function {func.__name__} is not defined\"\n                        f\" for the platform {platform.system()}\"\n                    )\n\n                return _not_implemented\n        return _decorator\n\n    return decorator\n\n\nimplement_linux = implement_for_os('Linux')\n\nimplement_windows = implement_for_os('Windows')\n", "@implement_linux(None)\ndef some_function():\n    print('Linux')\n\n@implement_windows(some_function)\ndef some_function():\n   print('Windows')\n\nimplement_other_platform = implement_for_os('OtherPlatform')\n\n@implement_other_platform(some_function)\ndef some_function():\n   print('Other platform')\n\n"], ["l = list('QRST')\ndf1 = df1.set_index(l)\nresult = [\n    df1.groupby(level=l[:i+1])['P'].agg(np.mean, engine='numba').round(2).reset_index()\n    for i in range(4)\n]\npd.concat(result)\n"], [], [], ["df=pd.DataFrame.from_records(\n[['PLAC','NR','F','HOL','F'],\n['PLAC','NR',  'F',  'NHOL',  'F'],\n['TRTB','NR',  'M',  'NHOL',  'M'],\n['PLAC','NR',  'M',  'NHOL',  'M'],\n['PLAC','NR',  'F',  'NHOL',  'F'],\n['PLAC','R', 'M', 'NHOL',  'M'],\n['TRTA','R',  'F',   'HOL',  'F'],\n['TRTA','NR',  'F',   'HOL',  'F'],\n['TRTB','NR',  'F',  'NHOL',  'F'],\n['PLAC','NR',  'F',  'NHOL',  'F'],\n['TRTB','NR',  'F',  'NHOL',  'F'],\n['TRTB','NR',  'M',  'NHOL',  'M'],\n['TRTA','NR',  'F',   'HOL',  'F'],\n['PLAC','NR',  'F',   'HOL',  'F'],\n['PLAC','R',  'F',  'NHOL',  'F']],\ncolumns = ['P','Q','R','S','T'])\n", "grdf = df.groupby(['Q','R','S','T'])['P'].apply(lambda x:len(x)).to_frame()\n", "df2 = df.unstack()\nresult2 = df2.sum(axis=1).rename(str(df2.index.names)).to_frame()\n", "def combine_aggregates(df):\n    #if type(grdf) == pd.core.frame.DataFrame:\n    df1 = df\n    result1 = df.sum(axis=1).rename(str(df1.index.names)).to_frame()\n    df2 = df1.unstack()\n    result2 = df2.sum(axis=1).rename(str(df2.index.names)).to_frame()\n    df3 = df2.unstack()\n    result3 = df3.sum(axis=1).rename(str(df3.index.names)).to_frame()\n    df4 = df3.unstack()\n    result4 = df4.sum(axis=1).rename(str(df4.index.names)).to_frame()\n\n    return result1.append(result2).append(result3).append(result4)\n\n\n\ncombine_aggregates(grdf)     \n"], [], [], ["import random\n\na = int(0)\nb = int(0)\nc = int(0)\nd = int(0)\ne = int(0)\nf = int(0)\nlimit = 101\ncount = 0\n\nwhile True:\n    g = (random.randint(1, 6))\n    print(g)\n    count += 1\n    if g == 1:\n        a += 1\n    elif g == 2:\n        b += 1\n    elif g == 3:\n        c += 1\n    elif g == 4:\n        d += 1\n    elif g == 5:\n        e += 1\n    elif g == 6:\n        f += 1\n    if count > limit:\n        break\n\nprint(f\"There are {a} 1's\")\nprint(f\"There are {b} 2's\")\nprint(f\"There are {c} 3's\")\nprint(f\"There are {d} 4's\")\nprint(f\"There are {e} 5's\")\nprint(f\"There are {f} 6's\")\n"], ["import numpy as np\n\n\na = np.array([[-1,1,-1],[-1,1,1]])\n\n# approach 1\na[a == 1] = 0\na[a == -1] = 1\n\n# approach 2\nmask = a == 1\na[mask] = 0\na[~mask] = 1\n\nprint(a)\n", "[[1 0 1]\n [1 0 0]]\n"], ["import numpy as np\n\nsample_arr = np.ones((5,2))\nprint('Converting an array with ones to zero')\nnp.where(sample_arr==1, 0, sample_arr)\n"], ["a = np.array([[-1,1,-1],[-1,1,1]])\nmask = a == 1\na[mask] = 0\na[~mask] = 1\n\n#array([[1, 0, 1],\n#       [1, 0, 0]])\n"], ["import numpy as np\n\n\nif __name__ == \"__main__\":\n    a = np.array([[-1,1,-1],[-1,1,1]])\n    a[a == 1] = 0\n    a[a == -1] = 1\n    print(a)\n\n"], [], ["import string\n\nalphabet = string.ascii_lowercase # \"set\" 1\ndigits = string.digits            # \"set\" 2\n", "N = 10 # size of a \"word\"\n# constraints on the amounts of type of characters in a word\nlower_bound1, lower_bound2 = 3, 3\n\ndef word_partitioner(list1, min1, list2, min2, word_length=10):\n    # return pairs which entries represent the amount of characters to be used to form a word of length word_length\n    # min1, min2 represents the lower bounds of the type of characters to be contained in the word\n    return [(i, j) for i, j in product(range(min1, len(list1)), range(min2, len(list2))) if i + j == word_length]\n\npartitions = word_partitioner(list1, lower_bound1, list2, lower_bound2, N)\nprint(partitions)\n", "[(3, 7), (4, 6), (5, 5), (6, 4), (7, 3)]\n", "     (3, 7)--> 78936000 --> 286442956800000\n", "import math\ndef words_stats(list1, n1, list2, n2):\n    # return total amount of words per partition + print to screen info\n    bin_prod = math.comb(len(list1), n1) * math.comb(len(list2), n2)\n    fact = math.factorial(n1 + n2)\n    tot = bin_prod * fact\n    print(f'{len(list1)}C{n1} x {len(list2)}C{n2}  x ({n1+n2})! = {bin_prod} x {fact} = {tot}')\n    return tot\n\nprint('Info:')\namount_of_possibilities = 0\nfor k1, k2 in partitions:\n    amount_of_possibilities += words_stats(list1, k1, list2, k2)\nprint(f'Total possible words: {amount_of_possibilities}')\n", "Preliminar info:\n10C3 x 26C7  x (10)! = 78936000 x 3628800 = 286442956800000\n10C4 x 26C6  x (10)! = 48348300 x 3628800 = 175446311040000\n10C5 x 26C5  x (10)! = 16576560 x 3628800 = 60153020928000\n10C6 x 26C4  x (10)! = 3139500 x 3628800 = 11392617600000\n10C7 x 26C3  x (10)! = 312000 x 3628800 = 1132185600000\nTotal possible words: 534567091968000\n", "from itertools import permutations, combinations, product, \n\ndef combinatorial_problem_formatter(list1, n1, list2, n2):\n    # return table-like string of the possible words per partition\n    print(f'Rows x Cols:  {len(list1)}C{n1} x {len(list2)}C{n2} x ({n1+n2})!')\n    s = ''\n    for pair1, pair2 in product(combinations(list1, n1), combinations(list2, n2)):\n        for p in permutations(pair1 + pair2):\n            s += ''.join(p) + ' '\n        s += '\\n'\n    return s\n", "N = 3 # size of a \"word\"\n\nlower_bound1, lower_bound2 = 1, 1   # constraints on the amounts of type of characters in a word\nlist1, list2 = digits[:4], alphabet[:4]     # restrictions (for pedagogical reasons)\n#\n\npartitions = word_partitioner(list1, lower_bound1, list2, lower_bound2, N)\nprint('Partitions', partitions)\n\nprint('Preliminar info:')\namount_of_possibilities = 0\nfor k1, k2 in partitions:\n    amount_of_possibilities += words_stats(list1, k1, list2, k2)\nprint(f'Total possible words: {amount_of_possibilities}')\n\nprint('='*40, '\\n')\n\nfor k1, k2 in partitions:\n    print(combinatorial_problem_formatter(list1, k1, list2, k2))\n    print()\n", "Partitions [(1, 2), (2, 1)]\nPreliminar info:\n4C1 x 4C2  x (3)! = 24 x 6 = 144\n4C2 x 4C1  x (3)! = 24 x 6 = 144\nTotal possible words: 288\n======================================== \n\nRows x Cols:  4C1 x 4C2 x (3)!\n0ab 0ba a0b ab0 b0a ba0 \n0ac 0ca a0c ac0 c0a ca0 \n0ad 0da a0d ad0 d0a da0 \n0bc 0cb b0c bc0 c0b cb0 \n0bd 0db b0d bd0 d0b db0 \n0cd 0dc c0d cd0 d0c dc0 \n1ab 1ba a1b ab1 b1a ba1 \n1ac 1ca a1c ac1 c1a ca1 \n1ad 1da a1d ad1 d1a da1 \n1bc 1cb b1c bc1 c1b cb1 \n1bd 1db b1d bd1 d1b db1 \n1cd 1dc c1d cd1 d1c dc1 \n2ab 2ba a2b ab2 b2a ba2 \n2ac 2ca a2c ac2 c2a ca2 \n2ad 2da a2d ad2 d2a da2 \n2bc 2cb b2c bc2 c2b cb2 \n2bd 2db b2d bd2 d2b db2 \n2cd 2dc c2d cd2 d2c dc2 \n3ab 3ba a3b ab3 b3a ba3 \n3ac 3ca a3c ac3 c3a ca3 \n3ad 3da a3d ad3 d3a da3 \n3bc 3cb b3c bc3 c3b cb3 \n3bd 3db b3d bd3 d3b db3 \n3cd 3dc c3d cd3 d3c dc3 \n\n\nRows x Cols:  4C2 x 4C1 x (3)!\n01a 0a1 10a 1a0 a01 a10 \n01b 0b1 10b 1b0 b01 b10 \n01c 0c1 10c 1c0 c01 c10 \n01d 0d1 10d 1d0 d01 d10 \n02a 0a2 20a 2a0 a02 a20 \n02b 0b2 20b 2b0 b02 b20 \n02c 0c2 20c 2c0 c02 c20 \n02d 0d2 20d 2d0 d02 d20 \n03a 0a3 30a 3a0 a03 a30 \n03b 0b3 30b 3b0 b03 b30 \n03c 0c3 30c 3c0 c03 c30 \n03d 0d3 30d 3d0 d03 d30 \n12a 1a2 21a 2a1 a12 a21 \n12b 1b2 21b 2b1 b12 b21 \n12c 1c2 21c 2c1 c12 c21 \n12d 1d2 21d 2d1 d12 d21 \n13a 1a3 31a 3a1 a13 a31 \n13b 1b3 31b 3b1 b13 b31 \n13c 1c3 31c 3c1 c13 c31 \n13d 1d3 31d 3d1 d13 d31 \n23a 2a3 32a 3a2 a23 a32 \n23b 2b3 32b 3b2 b23 b32 \n23c 2c3 32c 3c2 c23 c32 \n23d 2d3 32d 3d2 d23 d32 \n"], ["# if required (terminal commands):\n# pip install qgrid\n# pip install jupyter_contrib_nbextensions && jupyter contrib nbextension install \n# jupyter nbextension enable --py --sys-prefix qgrid\n\nimport pandas as pd\nimport qgrid\n\ndf = pd.DataFrame([('A', 'buy', 'sub'),\n                  ('B', 'sell', 'prior'),\n                  ('C', 'hold', 'sub'),\n                  ('D', 'loan', 'none'),\n                  ('A', 'hold', 'sub'),\n                  ('A', 'buy', 'none')], columns=['name', 'action', 'class'])\n\nqgrid_widget = qgrid.show_grid(df, show_toolbar=True)\nqgrid_widget\n"], ["from keras.models import Sequential\nfrom keras.layers import Dense\nimport keras\nimport numpy as np\n\n\n# Initial model\n\nmodel = Sequential()\nmodel.add(Dense(1, input_shape=(10,)))\n\noptimizer = keras.optimizers.Adam(lr=0.01)\nmodel.compile(loss='mse', optimizer=optimizer)\n\nmodel.fit(np.random.randn(50,10), np.random.randn(50), epochs=50)\n\n\n# Change learning rate to 0.001 and train for 50 more epochs\n\nnew_model = Sequential()\nnew_model.add(Dense(1, input_shape=(10,)))\n\noptimizer = keras.optimizers.Adam(lr=0.001)\nnew_model.compile(loss='mse', optimizer=optimizer)\n\nnew_model.set_weights(model.get_weights())\nmodel = new_model\n\nmodel.fit(np.random.randn(50,10), np.random.randn(50), initial_epoch=50, epochs=50)\n"], ["def select_and(df, cols, vals):\n    res = pd.Series(True, dtype='bool', index=df.index)\n    for col, val in zip(cols, vals):\n        res = (res & (df[col] == val))\n    return df[res]\n", ">>> select_and(df, cols=('name', 'action'), vals=('A', 'buy'))\n  name action class\n0    A    buy   sub\n5    A    buy  none\n", "df[select_and(df, ('name', 'action'), ('A', 'buy'))]\n"], ["df1 = df[(df['name']=='A') & (df['action']=='buy')]\n", "df1 = df[(df.name =='A') & (df.action =='buy')]\n", "df2 = df[(df.name =='A') | (df.class == 'sub')]\n", "    df2 = df[(df.name =='A') | (df.class == 'sub')]\n                                       ^\nSyntaxError: invalid syntax\n"], ["df[df[['name', 'action']].apply(tuple, axis=1) == ('A', 'buy')]\n", "  name action class\n0    A    buy   sub\n5    A    buy  none\n"], ["df.query('name == \"A\" and action==\"buy\"')\n\n name action class\n0    A    buy   sub\n5    A    buy  none\n", "df = df.set_index(['name', 'action'])\ndf = df.sort_index() # avoid performance issues \n\ndf.loc(axis=0)['A', 'buy']\n \n            class\nname action      \nA    buy      sub\n     buy     none\n"], [">>> A = [1,2,3,4]\n", ">>> list(itertools.combinations(A, 3))\n[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]\n", ">>> list(itertools.permutations(A, 3)) # 24 results omitted for brevity\n", ">>> list(itertools.combinations_with_replacement(A, 3)) # 20 results omitted for brevity\n", ">>> list(itertools.product(A, repeat=3)) # 64 results omitted for brevity\n", ">>> x = object()\n>>> candidates = [x, x, x, x]\n>>> results = list(itertools.combinations(candidates, 3))\n>>> len(results)\n4\n>>> results[0] == results[1] == results[2] == results[3]\nTrue\n", "def OP_problem():\n    for result in itertools.product(a+b, repeat=10):\n        a_count = len(x for x in result if x in a)\n        # the trick is that every element was either from a or b;\n        # so \"at least 3 a's and at least 3 b's\" is equivalent to\n        # \"at least 3 a's and at most 7 a's\".\n        if 3 <= a_count <= 7:\n            yield result\n", "(\n    # maybe you don't think this is simple enough :)\n    result for result in itertools.product(a+b, repeat=10)\n    if 3 <= len(x for x in result if x in a) <= 7\n)\n", "def make_combination(letter_positions, chosen_letters, chosen_digits):\n    result = [None] * 10\n    for letter, position in zip(chosen_letters, letter_positions):\n        result[position] = letter\n    # Figure out where the digits go, using set arithmetic to find the\n    # remaining positions, then putting them back in ascending order.\n    digit_positions = sorted(set(range(10)) - set(chosen_letters))\n    for digit, position in zip(chosen_digits, digit_positions):\n        result[position] = digit\n    assert None not in result\n    return tuple(result)\n\n\ndef five_letters_and_five_digits():\n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    digits = '0123456789'\n    # It's not *easy*, but it's fairly elegant.\n    # We separately generate the letter positions, letter selection\n    # and digit selection, using `product` to get the cartesian product\n    # of *those* possibilities; then for each of those, we translate\n    # into a desired output - using `starmap` to iterate.\n    return itertools.starmap(\n        make_combination, \n        itertools.product(\n            itertools.combinations(range(10), 5),\n            itertools.product(letters, repeat=5),\n            itertools.product(digits, repeat=5)\n        )\n    )\n                \n"], [], ["from itertools import product, permutations\n\ndef permutem(l1, l2, min_num=3, length=10):\n    for n in range(min_num, length - min_num + 1):\n        a, b = n, length - n\n        for result in permutations(product(l1, r=a), product(l2, r=b)):\n            yield result\n"], ["# Step 1 - collect all rows that are *not* duplicates (based on ID)\nnon_duplicates_to_keep = df.drop_duplicates(subset='Id', keep=False)\n\n# Step 2a - identify *all* rows that have duplicates (based on ID, keep all)\nsub_df = df[df.duplicated('Id', keep=False)]\n\n# Step 2b - of those duplicates, discard all that have \"0\" in any of the numeric columns\nduplicates_to_keep = sub_df[(sub_df[sub_df._get_numeric_data().columns[1:]] != 0).sum(axis=1) > 0]\n\n# join the 2 sets\npd.concat([non_duplicates_to_keep, duplicates_to_keep])\n"], ["cond = df[['rent', 'sale', 'Rate']].ne(0).any(axis=1)   # rows to keep for sure\n\npd.concat([df[cond],\n           (df.assign(Name=df['Name'].where(~cond, float('nan')))   # flag keep-for-sure\n              .loc[cond.sort_values().index]   # sort so that keep-for-sure are last\n              .drop_duplicates(subset='id', keep='last')   # keep 0s row only if no keep-for-sure in group \n              .dropna(subset=['Name'])\n            )\n          ])\n", "     id Name  rent  sale\n0  2340    A   180   -10\n4  4467    F   180     5\n5  2467    C    20    45\n7  4567    w    12    76\n1  1002    B     0     0\n"], [], [">>> df[df[['Sales', 'Rent', 'Rate']].eq(0).all(axis=1)].drop_duplicates('Id')\n\n       Id  Name  Sales  Rent  Rate\n2   17486     D      0     0     0\n3   27977    AM      0     0     0\n5   80210   O-9      0     0     0\n8   15545   O-8      0     0     0\n9   53549  A-M7      0     0     0\n11  40808     A      0     0     0\n"], ["df = pd.DataFrame([[1,2,3]], columns=[\"a\",\"b\",\"c\"])\ndef foobar(a,b):\n  return a,b\ndf[[\"c\",\"d\"]] = df.apply(lambda row: foobar(row[\"a\"], row[\"b\"]), axis=1)\n", "df[[\"c\",\"d\"]] = df.apply(lambda row: foobar(row[\"a\"], row[\"b\"]), axis=1, result_type=\"expand\")\n"], [], ["arr = pandas.Series([1, 1, 1, 2, 2, 2, 3, 3])\n$ for index, value in arr.items():\n   print(f\"Index : {index}, Value : {value}\")\n\nIndex : 0, Value : 1\nIndex : 1, Value : 1\nIndex : 2, Value : 1\nIndex : 3, Value : 2\nIndex : 4, Value : 2\nIndex : 5, Value : 2\nIndex : 6, Value : 3\nIndex : 7, Value : 3\n\n$ for index, value in arr.iteritems():\n   print(f\"Index : {index}, Value : {value}\")\n   \nIndex : 0, Value : 1\nIndex : 1, Value : 1\nIndex : 2, Value : 1\nIndex : 3, Value : 2\nIndex : 4, Value : 2\nIndex : 5, Value : 2\nIndex : 6, Value : 3\nIndex : 7, Value : 3\n\n$ for _, value in arr.iteritems():\n   print(f\"Index : {index}, Value : {value}\")\n\nIndex : 7, Value : 1\nIndex : 7, Value : 1\nIndex : 7, Value : 1\nIndex : 7, Value : 2\nIndex : 7, Value : 2\nIndex : 7, Value : 2\nIndex : 7, Value : 3\nIndex : 7, Value : 3\n\n$ for i, v in enumerate(arr):\n   print(f\"Index : {i}, Value : {v}\")\nIndex : 0, Value : 1\nIndex : 1, Value : 1\nIndex : 2, Value : 1\nIndex : 3, Value : 2\nIndex : 4, Value : 2\nIndex : 5, Value : 2\nIndex : 6, Value : 3\nIndex : 7, Value : 3\n\n$ for value in arr:\n   print(value)\n\n1\n1\n1\n2\n2\n2\n3\n3\n\n\n\n$ for value in arr.tolist():\n   print(value)\n\n1\n1\n1\n2\n2\n2\n3\n3\n"], ["import pandas\narr = pandas.Series([1, 1, 1, 2, 2, 2, 3, 3])\n\narr.apply(print)\n"], ["import sys\n\ndef printerd():\n    print('please enter a string:', end='')\n    sys.stdout.flush()\n    data = sys.stdin.readline()[:-1]\n    print(list(data))\n\n printerd()\n"], ["    user_input = input(\"Please enter a string: \")\n    list_of_letters = list(user_input)\n    print(list_of_letters)\n"], [], ["str1 = input(\"please enter a string:\")\nstr_set = set(str1)\nprint(str_set)\n", "str_set = set(input(\"please enter a string: \"))\n"], [], [], ["df['zero']=df.select_dtypes(['int','float']).eq(0).sum(axis=1)\ndf=df.sort_values(['zero','id']).drop_duplicates(subset=['id']).drop(columns='zero')\n"], [], ["arr = pandas.Series([1, 1, 1, 2, 2, 2, 3, 3])\n\n#Using Python range() method\nfor i in range(len(arr)):\n    print(arr[i])\n", "#List Comprehension\nprint([arr[i] for i in range(len(arr))])\n", "#Using Python enumerate() method\nfor el,j in enumerate(arr):\n    print(j)\n#Using Python NumPy module\nimport numpy as np\nprint(np.arange(len(arr)))\nfor i,j in np.ndenumerate(arr):\n    print(j)\n", "x=0\nwhile x<len(arr):\n    print(arr[x])\n    x +=1\n    \n#Using lambda function\nlist(map(lambda x:x, arr))\n"], [], ["loop: 1.80301690102 \niterrows: 0.724927186966 \napply: 0.645957946777\npandas series: 0.333024024963 \nnumpy array: 0.260366916656\n"], [], [], ["pdf['combined'] = [x for x in pdf[['a', 'b', 'c']].to_numpy()]\n# pdf['combined'] = pdf[['a', 'b', 'c']].to_numpy().tolist()\n"], ["cols = ['a', 'b', 'c']\ndf['combined'] = df[cols].apply(lambda row: list(row.values), axis=1)\n", "    a   b   c   combined\n0   1   4   7   [1, 4, 7]\n1   2   5   8   [2, 5, 8]\n2   3   6   9   [3, 6, 9]\n", "from pandarallel import pandarallel\npandarallel.initialize()\n\ncols = ['a', 'b', 'c']\ndf['combined'] = df[cols].parallel_apply(lambda row: list(row.values), axis=1)\n"], [], ["import pandas as pd\ndf = pd.DataFrame({\"a\":[1,2,3],\"b\":[4,5,6],\"c\":[7,8,9]})\ndf['combined'] = [list(row) for _, row in df.iterrows()]\n", "   a  b  c   combined\n0  1  4  7  [1, 4, 7]\n1  2  5  8  [2, 5, 8]\n2  3  6  9  [3, 6, 9]\n"], ["import pandas as pd\ndf = pd.DataFrame({\"a\":[1,2,3],\"b\":[4,5,6],\"c\":[7,8,9]})\ndf[\"combined\"] = df.apply(pd.Series.tolist,axis=1)\nprint(df)\n", "   a  b  c   combined\n0  1  4  7  [1, 4, 7]\n1  2  5  8  [2, 5, 8]\n2  3  6  9  [3, 6, 9]\n"], ["lems = ['scaena', 'persona', 'improbus']\nfor i in lems:\n    print(f\"{i:<10} whatever\")\n", "scaena     whatever\npersona    whatever\nimprobus   whatever\n"], ["python -m pip install pygame --pre --user\n"], ["import plotly.io as pio\npio.renderers.default = 'iframe' # or 'notebook' or 'colab' or 'jupyterlab'\n", "import pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\nfor text in pio.renderers:\n    print(text)\n    pio.renderers.default = text\n\n    df = px.data.iris()\n    fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\")\n    fig.show()\n"], ["import plotly.express as px\nfrom IPython.display import HTML\n\ndf = px.data.tips()\nfig = px.scatter(df, x='total_bill', y='tip', opacity=0.65,\n                 trendline='ols', trendline_color_override='darkblue')\nHTML(fig.to_html())\n"], ["A = np.array([100,200,300,200,400,500,600,400,700,200,500,800])\nB = [100, 200, 200, 500, 600, 200, 500]\n\nidx = np.arange(len(A))\nindices = {i: idx[A == i].tolist() for i in set(B)}\n[indices[i].pop(0) for i in B]\n"], ["for i, v in arr.items():\n    print(f'index: {i} and value: {v}')\n", "index: 0 and value: 1\nindex: 1 and value: 1\nindex: 2 and value: 1\nindex: 3 and value: 2\nindex: 4 and value: 2\nindex: 5 and value: 2\nindex: 6 and value: 3\nindex: 7 and value: 3\n"], [], ["A = [100,200,300,200,400,500,600,400,700,200,500,800]\nB = [100,200,200,500,600,200,500]\n\ndef get_indices(A, B):\n    a_it = enumerate(A)\n    for n in B:\n        for i, an in a_it:\n            if n == an:\n                yield i\n                break\n            \nlist(get_indices(A, B))\n# [0, 1, 3, 5, 6, 9, 10]\n"], [" re.search(r\"^[A-Z][a-z\\s].*[.?!]$\", text)\n"], [], [], ["A = [100,200,300,200,400,500,600,400,700,200,500,800]\n\nB = [100,200,200,500,600,200,500]\n\ni, j = 0, 0\nlist_index = []\nwhile j < len(B):\n    if B[j] == A[i]:\n        list_index.append(i)\n        j += 1\n    i += 1\nprint(list_index)\n"], ["A = [100, 200, 300, 200, 400, 500, 600, 400, 700, 200, 500, 800]\nB = [100, 200, 200, 500, 600, 200, 500]\n\nres = []\nfor i in B:\n    res.append(A.index(i))\n    A[A.index(i)] = None\n\nprint(res)\n", "[0, 1, 3, 5, 6, 9, 10]\n"], ["result = re.sub(r\"[#]+\",\"//\",line_of_code)\n"], [], ["pip install yfinance --upgrade --no-cache-dir \n"], [" \"*.ipynb\": \"jupyter-notebook\"\n", " \"viewType\": \"jupyter-notebook\",\n \"filenamePattern\": \"*.ipynb\"\n"], [], [], [], [], ["conda install tensorflow\n"], [], [], [], [], [], [], ["mallet_path = 'C:/mallet/mallet-2.0.8/bin/mallet.bat'\n", "@echo off\n\nrem This batch file serves as a wrapper for several\nrem  MALLET command line tools.\n\nif not \"%MALLET_HOME%\" == \"\" goto gotMalletHome\n\necho MALLET requires an environment variable MALLET_HOME.\ngoto :eof\n\n:gotMalletHome\n\nset MALLET_CLASSPATH=C:\\mallet\\mallet-2.0.8\\class;C:\\mallet\\mallet-2.0.8\\lib\\mallet-deps.jar\nset MALLET_MEMORY=1G\nset MALLET_ENCODING=UTF-8\n\nset CMD=%1\nshift\n\nset CLASS=\nif \"%CMD%\"==\"import-dir\" set CLASS=cc.mallet.classify.tui.Text2Vectors\nif \"%CMD%\"==\"import-file\" set CLASS=cc.mallet.classify.tui.Csv2Vectors\nif \"%CMD%\"==\"import-svmlight\" set CLASS=cc.mallet.classify.tui.SvmLight2Vectors\nif \"%CMD%\"==\"info\" set CLASS=cc.mallet.classify.tui.Vectors2Info\nif \"%CMD%\"==\"train-classifier\" set CLASS=cc.mallet.classify.tui.Vectors2Classify\nif \"%CMD%\"==\"classify-dir\" set CLASS=cc.mallet.classify.tui.Text2Classify\nif \"%CMD%\"==\"classify-file\" set CLASS=cc.mallet.classify.tui.Csv2Classify\nif \"%CMD%\"==\"classify-svmlight\" set CLASS=cc.mallet.classify.tui.SvmLight2Classify\nif \"%CMD%\"==\"train-topics\" set CLASS=cc.mallet.topics.tui.TopicTrainer\nif \"%CMD%\"==\"infer-topics\" set CLASS=cc.mallet.topics.tui.InferTopics\nif \"%CMD%\"==\"evaluate-topics\" set CLASS=cc.mallet.topics.tui.EvaluateTopics\nif \"%CMD%\"==\"prune\" set CLASS=cc.mallet.classify.tui.Vectors2Vectors\nif \"%CMD%\"==\"split\" set CLASS=cc.mallet.classify.tui.Vectors2Vectors\nif \"%CMD%\"==\"bulk-load\" set CLASS=cc.mallet.util.BulkLoader\nif \"%CMD%\"==\"run\" set CLASS=%1 & shift\n\nif not \"%CLASS%\" == \"\" goto gotClass\n\necho Mallet 2.0 commands: \necho   import-dir        load the contents of a directory into mallet instances (one per file)\necho   import-file       load a single file into mallet instances (one per line)\necho   import-svmlight   load a single SVMLight format data file into mallet instances (one per line)\necho   info              get information about Mallet instances\necho   train-classifier  train a classifier from Mallet data files\necho   classify-dir      classify data from a single file with a saved classifier\necho   classify-file     classify the contents of a directory with a saved classifier\necho   classify-svmlight classify data from a single file in SVMLight format\necho   train-topics      train a topic model from Mallet data files\necho   infer-topics      use a trained topic model to infer topics for new documents\necho   evaluate-topics   estimate the probability of new documents given a trained model\necho   prune             remove features based on frequency or information gain\necho   split             divide data into testing, training, and validation portions\necho   bulk-load         for big input files, efficiently prune vocabulary and import docs\necho Include --help with any option for more information\n\n\ngoto :eof\n\n:gotClass\n\nset MALLET_ARGS=\n\n:getArg\n\nif \"%1\"==\"\" goto run\nset MALLET_ARGS=%MALLET_ARGS% %1\nshift\ngoto getArg\n\n:run\n\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java\" -ea -Dfile.encoding=%MALLET_ENCODING% -classpath %MALLET_CLASSPATH% %CLASS% %MALLET_ARGS%\n\n:eof\n", "notepad mallet.bat\njava\nC:\\Program Files\\Java\\jdk-12\\bin\\java\ndir /OD\ncd %userdir%\ncd %userpath%\ncd\\\ncd users\ncd your_username\ncd appdata\\local\\temp\\2\ndir /OD\n"], [], [], ["# Using Ubuntu\ncurl -fsSL https://deb.nodesource.com/setup_15.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# Using Debian, as root\ncurl -fsSL https://deb.nodesource.com/setup_15.x | bash -\napt-get install -y nodejs\n", "conda install -c conda-forge nodejs\njupyter labextension install @jupyter-widgets/jupyterlab-manager\nconda install -c conda-forge ipywidgets\n", "jupyter labextension install js\n", "import ipywidgets as widgets\nwidgets.IntSlider()\n"], [], ["def make_bricks(small, big, goal):\n  \n  if (big is not 0 and (big*5+small*1 > goal) and goal % 5 <= small) or goal < small or big*5+small*1 == goal:\n    return True\n  else:\n    return False\n"], ["def circularArrayRotation(a, k, queries):\n    #rotation\n    for _ in range(k):\n        a.insert(0, a.pop())\n    #putting answer according to query\n    ans = []\n    for i in queries:\n        ans.append(a[i])    \n    return ans\n"], [], ["from flask_script import Manager\n\nfrom <your app name> import app,db\n\nimport os\n\nfrom config import Config\n\nfrom flask_migrate import Migrate,MigrateCommand\n\nfrom flask import Flask\n\nfrom flask_sqlalchemy import SQLAlchemy\n\n\napp.config.from_object(Config)\n\nmigrate = Migrate(app, db)\n\nmanager = Manager(app)\n\nmanager.add_command('db', MigrateCommand)\n\nif __name__ == '__main__':\n\n    manager.run()\n", "python manage.py db init \npython manage.py db migrate\npython manage.py db upgrade\n"], [], [], [], ["python3 -m venv .venv\n", "source .venv/bin/activate\n", "hash -r\n"], ["a=[1,2,3,4,5]\ns=2\ndef rotateList(arr,d,n):\n  arr[:]=arr[d-1:n]+arr[0:d-1]\n  return arr\n\nprint(rotateList(a,5,len(a)))\n"], [], ["pip install --upgrade webdrivermanager\n", "# webdrivermanager <browser>:<version> -l <location of the webdrivers in your path>, e.g.:\nwebdrivermanager chromedriver:87.0 -l C:/path/to/your/drivers\n"], [], ["pd.to_datetime('1970-01-01').value\n", "df['time'] = df['time'].apply(lambda x: x.value)\n"], ["keys = [\"NAME\",\"DATE OF BIRTH\",\"BIO\",\"HOBBIES\"]\n\nf = open(\"data.txt\", \"r\")\nresult = {}\nfor line in f:\n    line = line.strip('\\n')\n    if any(v in line for v in keys):\n        last_key = line\n    else:\n        result[last_key] = result.get(last_key, \"\") + line\n\nprint(result)\n", "{'NAME': 'John Doe', 'DATE OF BIRTH': '1992-02-16', 'BIO ': 'THIS is a PRETTY long sentence without ANY structure ', 'HOBBIES ': '//..etc..'}\n"], ["Categories = [\"NAME\", \"DOB\", \"BIO\"] // in the order they appear in text\nOutput = {}\nText = str(f)\nfor i in range(1,len(Categories)):\n    SplitText = Text.split(Categories[i])\n    Output.update({Categories[i-1] : SplitText[0] })\n    Text = SplitText[1]\nOutput.update({Categories[-1] : Text}) \n"], ["string = str(f)\nimportant_words = ['NAME', 'DATE OF BIRTH']\nlast_phrase = None\nfor phrase in important_words:\n   phrase_start = string.index(phrase)\n   phrase_end = phrase_start + len(phrase)\n   if last_phrase is not None:\n      get_data(string, last_phrase, phrase_start)\n   last_phrase = phrase_end\n\ndef get_data(string, previous_end_index, current_start_index):\n   usable_data = string[previous_end_index: current_start_index]\n   return usable_data\n"], ["import re\n\nkeys = \"\"\"\nNAME\nDATE OF BIRTH\nBIO \nHOBBIES \n\"\"\".strip().splitlines()\n\nkey_pattern = '|'.join(f'{key.strip()}' for key in keys)\npattern = re.compile(fr'^({key_pattern})', re.M)\n\n# uncomment to see the pattern\n# print(pattern)\n\nwith open(filename) as f:\n    text = f.read()\n    parts = pattern.split(text)\n\n... process parts ...\n"], [], [], [], [], [], ["jupyter nbextension enable --py widgetsnbextension\n", "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"], [], [], [], [], [], [], [], ["Name: numpy\nVersion: 1.14.5\nSummary: NumPy: array processing for numbers, strings, records, and objects.\nHome-page: http://www.numpy.org\nAuthor: Travis E. Oliphant et al.\nAuthor-email: None\nLicense: BSD\nLocation: /home/kusal/.local/lib/python3.8/site-packages\nRequires: \nRequired-by: scipy, quantipy3, pyarrow, pandas, azureml-dataset-runtime\nMetadata-Version: 2.1\nInstaller: pip\nClassifiers:\n  Development Status :: 5 - Production/Stable\n  Intended Audience :: Science/Research\n  Intended Audience :: Developers\n  License :: OSI Approved\n  Programming Language :: C\n  Programming Language :: Python\n  Programming Language :: Python :: 2\n  Programming Language :: Python :: 2.7\n  Programming Language :: Python :: 3\n  Programming Language :: Python :: 3.4\n  Programming Language :: Python :: 3.5\n  Programming Language :: Python :: 3.6\n  Programming Language :: Python :: Implementation :: CPython\n  Topic :: Software Development\n  Topic :: Scientific/Engineering\n  Operating System :: Microsoft :: Windows\n  Operating System :: POSIX\n  Operating System :: Unix\n  Operating System :: MacOS\n"], ["hypercorn app:app\n"], [], ["pip install webdriver-manager\n", "from selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ndriver = webdriver.Chrome(ChromeDriverManager(version=\"87.0.4280.88\").install())\ndriver.get(\"https://www.google.com\")\n"], [], [], [], [], ["re.search(r\"^[A-Z]+[a-z\\s]*[\\.!\\?]$\", text)\n"], ["from keras.models import Sequential\nimport numpy as np\nfrom keras.layers import Dense\nfrom keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nmodel = Sequential()\nmodel.add(Dense(1000,input_dim=(784),activation='relu') )  #imnput layer\nmodel.add(Dense(222,activation='relu'))                     #hidden layer\nmodel.add(Dense(100,activation='relu'))   \nmodel.add(Dense(50,activation='relu'))   \nmodel.add(Dense(10,activation='sigmoid'))   \nmodel.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=[\"accuracy\"])\nx_train = np.reshape(x_train,(60000,784))/255\nx_test = np.reshape(x_test,(10000,784))/255\nfrom keras.utils import np_utils\ny_train = np_utils.to_categorical(y_train) \ny_test = np_utils.to_categorical(y_test)\nmodel.fit(x_train[:1000],y_train[:1000],epochs=1,batch_size=32)\n"], ["physical_devices = tf.config.experimental.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n"], ["(base) myname-MacBook-Air:mallet-2.0.8 myname$ ./bin/mallet\n-bash: ./bin/mallet: /bin/bash: bad interpreter: Operation not permitted\n"], [], [], ["from gensim.test.utils import common_corpus, common_dictionary\nfrom gensim.models.wrappers import LdaMallet\n\nmallet_path = \"/path/Mallet/bin/mallet\"\nmodel = LdaMallet(mallet_path=mallet_path, corpus=common_corpus, num_topics=2, id2word=common_dictionary)\n"], ["                            if self.save_weights_only:\n                                self.model.save_weights(filepath, overwrite=True)\n                            else:\n                                model_json = self.model.to_json()\n                                with open(filepath+'.json','w') as fb:\n                                    fb.write(model_json)\n                                    fb.close()\n                                self.model.save_weights(filepath+'.h5', overwrite=True)\n                                with open(filepath+'-hist.pickle','wb') as fb:\n                                    trainhistory = {\"history\": self.model.history.history,\"params\": self.model.history.params}\n                                    pickle.dump(trainhistory,fb)\n                                    fb.close()\n                                # self.model.save(filepath, overwrite=True)\n"], ["py -m pip install -U pygame --user\n", "py -m pygame.examples.aliens\n"], [">>> import itertools\n>>> l = [False, True]\n>>> list(itertools.product(l, repeat=3))\n[(False, False, False), (False, False, True), (False, True, False), (False, True, True), (True, False, False), (True, False, True), (True, True, False), (True, True, True)]\n>>> \n", ">>> import itertools\n>>> l = [False, True]\n>>> [list(i) for i in itertools.product(l, repeat=3)]\n[[False, False, False], [False, False, True], [False, True, False], [False, True, True], [True, False, False], [True, False, True], [True, True, False], [True, True, True]]\n>>> \n"], [], ["import tensorflow as tf\nfrom tensorflow.compat.v1.keras.backend import set_session\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\nsess = tf.compat.v1.Session(config=config)\nset_session(sess)\n"], ["addition_str = \"2+5+10+20\"\n\naddition_str = addition_str.split('+')\n\nsum_val = 0\nfor n in addition_str:\nsum_val = sum_val + int(n)\n\nprint(sum_val)\n"], ["    import re\n    def check_sentence(text):\n      result = re.search(r\"^[A-Z][A-Za-z\\s]*[\\.\\?!]$\", text)\n      return result != None\n\n    print(check_sentence(\"Is this is a sentence?\")) # True\n    print(check_sentence(\"is this is a sentence?\")) # False\n    print(check_sentence(\"Hello\")) # False\n    print(check_sentence(\"1-2-3-GO!\")) # False\n    print(check_sentence(\"A star is born.\")) # True\n"], ["text = get(url).content\nemails = re.findall(r'[a-z0-9]+@\\S+.com', str(text))\n"], ["def make_bricks(small, big, goal):\n  total_bricks = (1 * small) + (5 * big)\n  if total_bricks >= goal:\n    if goal%5 == 0:\n      if goal/5 <= goal:\n        x = True\n    elif goal%5 <= small:\n        x = True\n    else:\n      x = False\n  else:\n    x = False\n  return x\n"], ["from scipy.spatial import distance_matrix\ndistances = distance_matrix(list_a, list_b)\n"], [], [], ["import tensorflow as tf\nphysical_devices = tf.config.list_physical_devices('GPU') \ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n", "2020-12-23 21:54:14.971709: I tensorflow/stream_executor/stream.cc:1404] [stream=000001E69C1DA210,impl=000001E6A9F88E20] did not wait for [stream=000001E69C1DA180,impl=000001E6A9F88730]\n2020-12-23 21:54:15.211338: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed\n[I 21:54:16.071 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports\nkernel 8b907ea5-33f1-4b2a-96cc-4a7a4c885d74 restarted\nkernel 8b907ea5-33f1-4b2a-96cc-4a7a4c885d74 restarted\n", "UnpicklingError: invalid load key, 'H'.\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-2-f049ceaad66a> in <module>\n", "\nInternalError: Blas GEMM launch failed : a.shape=(15, 768), b.shape=(768, 768), m=15, n=768, k=768 [Op:MatMul]\n\nDuring handling of the above exception, another exception occurred:\n", "failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.534375: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.534683: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.534923: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.539327: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.539523: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\n2020-12-23 21:31:04.539665: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\n"], ["rD <- rsDriver(browser = \"chrome\", chromever = \"87.0.4280.88\")\n"], [], ["pip install numpy \n"], ["pip install versioned-hdf5\n"], ["result = re.sub(r\"#+\",r\"//\",line_of_code)\n"], [], [], [], ["df=pd.DataFrame(['product a','product b','product c', 'product d'],index=['a','b','c','d'])\n", "df.loc[df.index.intersection(row_indices),:]  \n"], ["node -v\n", "nvm ls\n", "->      v9.11.2\n        v10.4.0\n        v12.5.0\n", "nvm use 12.5.0\n", "jupyter nbextension enable --py widgetsnbextension\njupyter labextension install @jupyter-widgets/jupyterlab-manager\n"], [], ["import re\ndef transform_comments(line_of_code):\n  result = re.sub(r'(#*#) ',r'// ',line_of_code)\n  return result\n"], ["curl -sL https://deb.nodesource.com/setup_15.x | bash -\napt-get install -y nodejsapt-get install -y nodejs\n", "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n ./Miniconda3-latest-Linux-x86_64.sh\n", "conda install -c conda-forge nodejs\njupyter labextension install @jupyter-widgets/jupyterlab-manager\n\n"], ["pip install --upgrade pip setuptools wheel\n", "sudo apt-get install libhdf5-dev\n"], [], [], ["df['time'] = df['time'].apply(lambda x: x.value)\n", "df['time'] = df['time'].apply(pd.Timestamp)\n"], ["def transform_comments(line_of_code):\n  result = re.sub(r'##*',r'//', line_of_code)\n  return result\n"], ["import re\ndef transform_comments(line_of_code):\n    result = re.sub(r\"#{1,}\",r\"//\", line_of_code)\n    return result\n"], [">>> from datetime import datetime\n>>> now = datetime.utcnow()\n>>> year_month_day_format = '%Y-%m-%d'\n>>> now.strftime(year_month_day_format)\n'2020-11-06'\n>>> hour_minute_format = '%H:%M'\n>>> now.strftime(hour_minute_format)\n'22:54'\n"], ["import os\nfrom gensim.models.wrappers import LdaMallet\n\nos.environ.update({'MALLET_HOME':r'C:/mallet/mallet-2.0.8/'})\nmallet_path = r'C:/mallet/mallet-2.0.8/bin/mallet.bat'\n\nlda_mallet = LdaMallet(\n        mallet_path,\n        corpus = corpus_bow,\n        num_topics = n_topics,\n        id2word = dct,\n    )\n"], ["re.search(r\"^[A-Z][a-z ]*[?.!]$\", text)\n"], [], [], [], [], [], ["python3 -m pip install pygame --pre --user\n"], [], ["def list_exponential(n,set1=[]):\nif n == 0:\n    print(set1)\nelse:\n    n-=1\n    list_exponential(n, [False]+set1)\n    list_exponential(n, [True]+set1)\n\nlist_exponential(5)\n", "$ python3 exponential.py 5\n[False, False, False, False, False]\n[True, False, False, False, False]\n[False, True, False, False, False]\n[True, True, False, False, False]\n[False, False, True, False, False]\n[True, False, True, False, False]\n[False, True, True, False, False]\n[True, True, True, False, False]\n[False, False, False, True, False]\n[True, False, False, True, False]\n[False, True, False, True, False]\n[True, True, False, True, False]\n[False, False, True, True, False]\n[True, False, True, True, False]\n[False, True, True, True, False]\n[True, True, True, True, False]\n[False, False, False, False, True]\n[True, False, False, False, True]\n[False, True, False, False, True]\n[True, True, False, False, True]\n[False, False, True, False, True]\n[True, False, True, False, True]\n[False, True, True, False, True]\n[True, True, True, False, True]\n[False, False, False, True, True]\n[True, False, False, True, True]\n[False, True, False, True, True]\n[True, True, False, True, True]\n[False, False, True, True, True]\n[True, False, True, True, True]\n[False, True, True, True, True]\n[True, True, True, True, True]\n"], [], ["stop=stopwords.words('english')\n\ntemp = [[genre, word.lower()] for genre in cfdconditions for word in brown.words(categories=genre) if word.lower() not in stop]\n\ncdev_cfd=nltk.ConditionalFreqDist(temp)\ncdev_cfd.tabulate(conditions=cfdconditions,samples=cfdevents)\n\nlst=[]\nfor i in temp:\n    if i[1].endswith('ing'):\n        lst.append((i[0],'ing'))\n\n    elif i[1].endswith('ed'):\n        lst.append((i[0],'ed'))\n\ninged_cfd=nltk.ConditionalFreqDist(lst)      \ninged_cfd.tabulate(conditions=cfdconditions,samples=['ed','ing'])\n"], [], [], [], ["/^[A-Z][a-z]*(\\s[a-z]+)*[\\.!\\?]$/\n", "This is a valid sentence.\n", "This is not valid !\nThis  is not valid either!\n"], ["from nltk.corpus import brown,stopwords\ndef calculateCFD(cfdconditions, cfdevents):\n\n\n# Write your code here\nstopword = set(stopwords.words('english'))\ncdev_cfd = nltk.ConditionalFreqDist([(genre, word.lower()) for genre in brown.categories() for word in brown.words(categories=genre) if not word.lower()  in stopword])\ncdev_cfd.tabulate(conditions = cfdconditions, samples = cfdevents)\ninged_cfd = [ (genre, word.lower()) for genre in brown.categories() for word in brown.words(categories=genre) if (word.lower().endswith('ing') or word.lower().endswith('ed')) ]\ninged_cfd = [list(x) for x in inged_cfd]\nfor wd in inged_cfd:\n    if wd[1].endswith('ing') and wd[1] not in stopword:\n        wd[1] = 'ing'\n    elif wd[1].endswith('ed') and wd[1] not in stopword:\n        wd[1] = 'ed'\n#print(inged_cfd)\ninged_cfd = nltk.ConditionalFreqDist(inged_cfd)\n#print(inged_cfd.conditions())    \ninged_cfd.tabulate(conditions=cfdconditions, samples = ['ed','ing'])\n"], ["pip install matplotlib==3.2.2\n"], [], ["$ jupyter labextension install jupyterlab-plotly@4.10.0\n\nAn error occured.\nValueError: Please install nodejs >=10.0.0 before continuing. nodejs may be \ninstalled using conda or directly from the nodejs website.\n", "conda install -c conda-forge nodejs\nconda install -c conda-forge/label/gcc7 nodejs\nconda install -c conda-forge/label/cf201901 nodejs\nconda install -c conda-forge/label/cf202003 nodejs '\n"], ["tab = '\\t'\nfile.write(f'''Name{tab}sn{tab}port{tab}sw_sn{tab}sw_port''')\n"], ["connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n\n# Create the BlobServiceClient object which will be used to create a container client\nblob_service_client = BlobServiceClient.from_connection_string(connect_str)\n\n# Create a unique name for the container\ncontainer_name = \"foo\"\n\n# Create the container\ntry:\n    container_client = blob_service_client.get_container_client(container_name)\n    # Container foo exists. You can now use it.\nexcept Exception as e:\n    # Container foo does not exist. You can now create it.\n    print(e)\n    container_client = blob_service_client.create_container(container_name)\n\n", "Name: azure-storage-blob\nVersion: 12.5.0\n"], ["conda search python\n"], ["nvidia-smi\n"], [], [], ["df1 = df.stack().reset_index().drop(columns='level_1').drop_duplicates()\n\ndf1['col'] = df1.groupby('level_0').cumcount()\ndf1 = (df1.pivot(index='level_0', columns='col', values=0)\n          .rename_axis(index=None, columns=None))\n\n   0  1    2    3\n0  A  B    C    D\n1  A  D    C  NaN\n2  C  B  NaN  NaN\n3  B  A  NaN  NaN\n", "import perfplot\nimport pandas as pd\nimport numpy as np\n\ndef stack(df):\n    df1 = df.stack().reset_index().drop(columns='level_1').drop_duplicates()\n\n    df1['col'] = df1.groupby('level_0').cumcount()\n    df1 = (df1.pivot(index='level_0', columns='col', values=0)\n              .rename_axis(index=None, columns=None))\n    return df1\n\ndef apply_drop_dup(df):\n    return pd.DataFrame.from_dict(df.apply(lambda x: x.drop_duplicates().tolist(),\n                                           axis=1).to_dict(), orient='index')\n\ndef apply_unique(df):\n    return pd.DataFrame(df.apply(pd.Series.unique, axis=1).tolist())\n\n\ndef list_map(df):\n    return pd.DataFrame(list(map(pd.unique, df.values)))\n\n\nperfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(list('ABCD'), (n, 4)),\n                                 columns=list('abcd')), \n    kernels=[\n        lambda df: stack(df),\n        lambda df: apply_drop_dup(df),\n        lambda df: apply_unique(df),\n        lambda df: list_map(df),\n    ],\n    labels=['stack', 'apply_drop_dup', 'apply_unique', 'list_map'],\n    n_range=[2 ** k for k in range(18)],\n    equality_check=lambda x,y: x.compare(y).empty,  \n    xlabel='~len(df)'\n)\n", "def with_numpy(df):\n    arr = np.sort(df.to_numpy(), axis=1)\n    r = np.roll(arr, 1, axis=1)\n    r[:, 0] = np.NaN\n    \n    arr = np.where((arr != r), arr, np.NaN)\n    \n    # Move all NaN to the right. Credit @Divakar\n    mask = pd.notnull(arr)\n    justified_mask = np.flip(np.sort(mask, axis=1), 1)\n    out = np.full(arr.shape, np.NaN, dtype=object) \n    out[justified_mask] = arr[mask]\n    \n    return pd.DataFrame(out, index=df.index).dropna(how='all', axis='columns')\n\nwith_numpy(df)\n#   0  1    2    3\n#0  A  B    C    D\n#1  A  C    D  NaN\n#2  B  C  NaN  NaN     # B/c this method sorts, B before C\n#3  A  B  NaN  NaN\n", "perfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(list('ABCD'), (n, 4)),\n                                 columns=list('abcd')), \n    kernels=[\n        lambda df: stack(df),\n        lambda df: with_numpy(df),\n    ],\n    labels=['stack', 'with_numpy'],\n    n_range=[2 ** k for k in range(3, 22)],\n    # Lazy check to deal with string/NaN and irrespective of sort order. \n    equality_check=lambda x, y: (np.sort(x.fillna('ZZ').to_numpy(), 1) \n                                 == np.sort(y.fillna('ZZ').to_numpy(), 1)).all(),\n    xlabel='len(df)'\n)\n"], ["pip install matplotlib==3.1.1\npip install pyinstaller==3.6\n"], ["tips_filtered = tips_df.reindex(columns=filtered_columns)\n"], ["print (pd.DataFrame(df.apply(pd.Series.unique, axis=1).tolist()))\n\n   0  1     2     3\n0  A  B     C     D\n1  A  D     C  None\n2  C  B  None  None\n3  B  A  None  None\n"], ["df_final = pd.DataFrame.from_dict(df.apply(lambda x: x.drop_duplicates().tolist(),\n                                               axis=1).to_dict(), orient='index')\n\nOut[268]:\n   0  1     2     3\n0  A  B     C     D\n1  A  D     C  None\n2  C  B  None  None\n3  B  A  None  None\n"], ["df = pd.DataFrame(list(map(pd.unique, df.values)))\nOut[447]: \n   0  1     2     3\n0  A  B     C     D\n1  A  D     C  None\n2  C  B  None  None\n3  B  A  None  None\n"], ["duplicates = df.apply(pd.Series.duplicated, axis=1)\ndf.where(~duplicates, np.nan).apply(lambda x: pd.Series(sorted(x, key=pd.isnull)), axis=1)\n", "| 0   | 1   | 2   | 3   |\n|:----|:----|:----|:----|\n| A   | B   | C   | D   |\n| A   | D   | C   | NaN |\n| C   | B   | NaN | NaN |\n| B   | A   | NaN | NaN |\n"], ["from tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\n"], [], ["cdev_cfd = nltk.ConditionalFreqDist([(genre, word.lower()) for genre in cfdconditions for word in brown.words(categories=genre) if word.lower() not in stopword])\n"], ["def circularArrayRotation(a, k, queries):\n    reverse_a = list(reversed(a))\n    reverse_a_copy = reverse_a.copy()\n    for x in range(k):\n        item = reverse_a[x]\n        reverse_a_copy.remove(item)\n        reverse_a_copy.append(item)\n    line = []\n    for x in queries:\n        line.append(list(reversed(reverse_a_copy))[x])\n    return line\n"], [], [], [], [], [], ["datas = [   datas = [\n    (mpl_data_dir, \"mpl-data\"),\n]\n"], [" NameError: name 'defaultParams' is not defined\n", "cd [download dir]  #change directory to donwloaded and unziped dir\npip install .\n"], ["jupyter labextension install jupyterlab-plotly\n"], ["static int[] circularArrayRotation(int[] a, int k, int[] queries) {\n\n    LinkedList<Integer> list = Arrays.stream(a).boxed()\n            .collect(Collectors.toCollection(LinkedList::new));\n    \n    for (int i = 0; i < k; i++) {\n        list.push(list.pollLast());\n    }\n    \n    for (int i = 0; i < queries.length; i++) {\n        if (queries[i] < list.size()) {\n            queries[i] = list.get(queries[i]);\n        }\n    }\n    \n    return queries;\n}\n"], [], ["def Alollz(df):\n    idx = pd.MultiIndex.from_product([np.unique(df['ID']), \n                                      np.arange(df['year'].min(), df['year'].max()+1)],\n                                     names=['ID', 'year'])\n   \n    df_b = pd.DataFrame({'number': 0}, index=idx)\n    df_b.update(df.set_index(['ID', 'year']))\n    \n    m = (df_b.groupby(level=0)['number'].cummax().eq(1) \n         & df_b[::-1].groupby(level=0)['number'].cummax().eq(1))\n    \n    return df_b.loc[m].reset_index()\n", "Alollz(df)\n\n  ID  year  number\n0  A  2017     1.0\n1  A  2018     0.0\n2  A  2019     1.0\n3  B  2017     1.0\n4  B  2018     1.0\n5  C  2016     1.0\n6  C  2017     0.0\n7  C  2018     0.0\n8  C  2019     1.0\n"], ["[int(len(data)*0.8),int(len(data)*0.2)]\n", "[int(len(data)*0.8)+int(len(data)*0.2)]=2297\n", "[int(np.floor(len(data)*0.8)),int(np.ceil(len(data)*0.2))])\n"], [], ["g = df.groupby(\"ID\")[\"year\"].agg({\"min\":\"min\",\"max\":\"max\"}).reset_index()\nid_years = pd.DataFrame(list(g.apply(lambda row: list(row[\"ID\"]) + \n                    list(pd.date_range(start=f\"01/01/{row['min']}\", \\\n                    end=f\"01/01/{row['max']+1}\",freq='12M').year), axis=1))).melt(0).dropna()[[0,\"value\"]]\n\nid_years.loc[:,\"value\"] = id_years[\"value\"].astype(int)\nid_years = id_years.rename(columns = {0:\"ID\",\"value\":'year'})\nid_years = id_years.sort_values([\"ID\",\"year\"]).reset_index(drop=True)\n\n## Merge two dataframe\noutput_df = pd.merge(id_years, df, on=[\"ID\",\"year\"], how=\"left\").fillna(0)\noutput_df.loc[:,\"number\"] = output_df[\"number\"].astype(int)\noutput_df\n", "    ID  year    number\n0   A   2017    1\n1   A   2018    0\n2   A   2019    1\n3   B   2017    1\n4   B   2018    1\n5   C   2016    1\n6   C   2017    0\n7   C   2018    0\n8   C   2019    1\n"], ["idx = df.groupby('ID')['year'].apply(lambda x: pd.Series(np.arange(x.iloc[0], x.iloc[-1]+1))).reset_index()\ndf.set_index(['ID','year']).reindex(pd.MultiIndex.from_arrays([idx['ID'], idx['year']]), fill_value=0).reset_index()\n", "  ID  year  number\n0  A  2017       1\n1  A  2018       0\n2  A  2019       1\n3  B  2017       1\n4  B  2018       1\n5  C  2016       1\n6  C  2017       0\n7  C  2018       0\n8  C  2019       1\n"], ["u = df.groupby('ID')['year'].apply(lambda x: range(x.min(),x.max()+1)).explode()\n\nout = (df.set_index(['ID','year']).reindex(u.reset_index().to_numpy(),fill_value=0)\n         .reset_index())\n", "  ID  year  number\n0  A  2017       1\n1  A  2018       0\n2  A  2019       1\n3  B  2017       1\n4  B  2018       1\n5  C  2016       1\n6  C  2017       0\n7  C  2018       0\n8  C  2019       1\n"], ["t = df.groupby('ID')['year'].agg(['min','max']).reset_index()\nt['missing'] = t.transform(lambda x: [y for y in range(x['min'], x['max']+1) if y not in x.values], axis=1)\nt = t[['ID','missing']].explode('missing').dropna()\nt['number'] = 0\nt.columns = ['ID','year','number']\npd.concat([df,t]).sort_values(by=['ID','year'])\n", "    ID  year    number\n0   A   2017    1\n0   A   2018    0\n1   A   2019    1\n2   B   2017    1\n3   B   2018    1\n4   C   2016    1\n2   C   2017    0\n2   C   2018    0\n5   C   2019    1\n"], ["letter_keys = df.ID.unique()\ndata = df.values\nmissing_records = []\nfor letter in letter_keys:\n    print(letter)\n    years = [x[1] for x in data if x[0] == letter]\n    min_year = min(years)\n    max_year = max(years)\n    current_year = min_year\n    while current_year<max_year:\n        if current_year not in years:\n            missing_records.append([letter, current_year,0])\n            print('missing', current_year)\n        current_year +=1\n\nnew_df = df.append(pd.DataFrame(missing_records, columns = df.columns)).sort_values(['ID','year'])\n", "| ID   |   year |   number |\n|:-----|-------:|---------:|\n| A    |   2017 |        1 |\n| A    |   2018 |        0 |\n| A    |   2019 |        1 |\n| B    |   2017 |        1 |\n| B    |   2018 |        1 |\n| C    |   2016 |        1 |\n| C    |   2017 |        0 |\n| C    |   2018 |        0 |\n| C    |   2019 |        1 |\n"], ["df.pivot(index='ID', columns='year', values='number').fillna(0).stack().to_frame('number')\n", "    number\nID  year    \nA   2016    0.0\n2017    1.0\n2018    0.0\n2019    1.0\nB   2016    0.0\n2017    1.0\n2018    1.0\n2019    0.0\nC   2016    1.0\n2017    0.0\n2018    0.0\n2019    1.0\n\n"], [], ["vector<int> Solution::spiralOrder(const vector<vector<int>> &arr) {\n\nint r = arr.size();\nint c = arr[0].size();\n\nint left = 0;\nint right = c-1;\n\nint top = 0;\nint bottom = r-1;\nint dir = 0;\nvector<int> ans;\n\nwhile(left <= right && top <= bottom){\n      \n    if(dir == 0){\n        for(int i = left; i<=right; i++){\n            ans.push_back(arr[top][i]);\n        }\n        top++;\n        dir = 1;\n    }\n    else if(dir == 1){\n        for(int i = top; i<=bottom; i++){\n            ans.push_back(arr[i][right]);\n        }\n        right--;\n        dir = 2;\n    }\n    else if(dir == 2){\n        for(int i = right; i>=left; i--){\n            ans.push_back(arr[bottom][i]);\n        }\n        bottom--;\n        dir = 3;\n    }\n    else if(dir == 3){\n        for(int i = bottom; i>=top; i--){\n            ans.push_back(arr[i][left]);\n        }\n        left++;;\n        dir = 0;\n    }\n}\n\nreturn ans;\n"], ["cd 'My Drive'/\n"], ["import tensorflow as tf\n\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(physical_devices))\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n"], ["In [194]: A = np.arange(1, 5, 0.5) \n     ...: B = np.arange(11, 15, 0.5)                                                                            \n", "In [196]: C = A.tolist()                                                                                        \nIn [197]: for i,v in enumerate(B): \n     ...:     C.insert(2*i+1,v) \n     ...:                                                                                                       \nIn [198]: A                                                                                                     \nOut[198]: array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\nIn [199]: B                                                                                                     \nOut[199]: array([11. , 11.5, 12. , 12.5, 13. , 13.5, 14. , 14.5])\nIn [200]: C                                                                                                     \nOut[200]: \n[1.0,\n 11.0,\n 1.5,\n 11.5,\n 2.0,\n 12.0,\n 2.5,\n 12.5,\n 3.0,\n 13.0,\n 3.5,\n 13.5,\n 4.0,\n 14.0,\n 4.5,\n 14.5]\n", "In [201]: np.vstack((A,B))                                                                                      \nOut[201]: \narray([[ 1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5],\n       [11. , 11.5, 12. , 12.5, 13. , 13.5, 14. , 14.5]])\nIn [202]: np.vstack((A,B)).ravel(order='F')                                                                     \nOut[202]: \narray([ 1. , 11. ,  1.5, 11.5,  2. , 12. ,  2.5, 12.5,  3. , 13. ,  3.5,\n       13.5,  4. , 14. ,  4.5, 14.5])\n", "In [203]: import itertools                                                                                      \nIn [204]: [(i,j) for i,j in zip(A,B)]                                                                           \nOut[204]: \n[(1.0, 11.0),\n (1.5, 11.5),\n (2.0, 12.0),\n (2.5, 12.5),\n (3.0, 13.0),\n (3.5, 13.5),\n (4.0, 14.0),\n (4.5, 14.5)]\nIn [205]: list(itertools.chain(*[(i,j) for i,j in zip(A,B)]))                                                   \nOut[205]: \n[1.0,\n 11.0,\n 1.5,\n 11.5,\n 2.0,\n 12.0,\n 2.5,\n 12.5,\n 3.0,\n 13.0,\n 3.5,\n 13.5,\n 4.0,\n 14.0,\n 4.5,\n 14.5]\n"], ["C = np.dstack((A,B)).flatten()\n", "C = [None]*(len(A)+len(B))\nC[::2] = A\nC[1::2] = B\n", "[1.0, 11.0, 1.5, 11.5, 2.0, 12.0, 2.5, 12.5, 3.0, 13.0, 3.5, 13.5, 4.0, 14.0, 4.5, 14.5]\n"], ["C = [item for sublist in zip(A, B) for item in sublist]\n", "C = []\nfor sublist in zip(A, B):\n    for item in sublist:\n         C.append(item)\n", "C = []\nfor sublist in zip(A, B):\n    C.extend(sublist)\n"], ["A = [1,2,3,41,10,5,3,100]\nB = [10,21,22,4,18,1,2,9]\n\nodd_lst = []\neven_lst = []\n\n\ndef is_even_odd(item):\n    if int(item) % 2 == 0:\n        even_lst.append(item)\n    else:\n        odd_lst.append(item)\n\nfor a_item, b_item in zip(A,B):\n    is_even_odd(a_item)\n    is_even_odd(b_item)\n\n\nprint([item for tup in zip(odd_lst, even_lst) for item in tup])\n", "[1, 10, 21, 2, 3, 22, 41, 4, 5, 10, 1, 18, 3, 2, 9, 100]  # new list with odd,even elements sequence\n"], ["  import numpy as np\n  def distance(x,y):\n      x=np.array(x)\n      y=np.array(y)\n      p=np.sum((x-y)**2)\n      d=np.sqrt(p)\n      return d\n"], ["from azure.core.exceptions import ResourceExistsError\n\nblob_service_client = BlobServiceClient.from_connection_string(connection_string)\n\ntry:\n    # Attempt to create container\n    blob_service_client.create_container(container_name)\n\n# Catch exception and print error\nexcept ResourceExistsError as error:\n    print(error)\n\n# Do something with container\n", "The specified container already exists.\nRequestId:5b70c1d8-701e-0028-3397-3c77d8000000\nTime:2020-06-07T06:46:15.1526773Z\nErrorCode:ContainerAlreadyExists\nError:Non\n", "try:\n    # Attempt to create container\n    blob_service_client.create_container(container_name)\n\n# Catch exception and ignore it completely\nexcept ResourceExistsError:\n    pass\n"], ["from keras import backend as K\nK.set_value(model.optimizer.learning_rate, 0.001)\n", "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\nimport keras\nimport numpy as np\n\nmodel = Sequential()\n\nmodel.add(Dense(1, input_shape=(10,)))\n\noptimizer = keras.optimizers.Adam(lr=0.01)\nmodel.compile(loss='mse', optimizer=optimizer)\n\nprint(\"Learning rate before first fit:\", model.optimizer.learning_rate.numpy())\n\nmodel.fit(np.random.randn(50,10), np.random.randn(50), epochs=50, verbose=0)\n\n# Change learning rate to 0.001 and train for 50 more epochs\nK.set_value(model.optimizer.learning_rate, 0.001)\nprint(\"Learning rate before second fit:\", model.optimizer.learning_rate.numpy())\n\nmodel.fit(np.random.randn(50,10), \n          np.random.randn(50), \n          initial_epoch=50, \n          epochs=50,\n          verbose=0)\n"], [], ["import random\n\ntotal = 0\ndiceRollList = []\n\n# Define dice rolling function\ndef rollDice():\n    rollResult = random.randint(1, 6)\n    if rollResult == 6:\n        # If 6 Rolled run two more rolls and sum the results\n        print(\"Rolled a 6 Rolling 2 more\")\n        return sum([rollDice() for _ in range(2)])\n    # If 6 not rolled return the result\n    print(f\"Rolled a {rollResult}\")\n    return rollResult\n\nwhile True:\n\n    numberOfDice = int(input(\"How many Die to throw: \"))\n\n    if numberOfDice not in range(1, 6):\n        print(\"Number of dice should be between 1 and 5\")\n        break\n\n    for dice in range(numberOfDice):\n        print(f\"Rolling Dice {dice}\")\n        # Add result to the total\n        total += rollDice()\n        print(f\"Running Total: {total}\")\n"], ["for i in range(numDices):\n", "i = 0\nwhile i <= numDices:\n    ...\n    ...\n    if ...:\n        ...\n    elif ...:\n    ...\n    i += 1\n", "dicesArray = list(range(numDices))`\n...\ndicesArray[i]\n...\n", "...\ndicesArray = []\ni = 0\nwhile i <= numDices:\n    dicesArray.append(random.randint(1, 6))\n    total += dicesArray[-1]\n    if dicesArray[-1] == 1:\n        ...\n    ...\n"], ["import random\nremaining_dices=0\ntotal = 0\nprint(\"------------\")\nprint(\"DICE ROLLING\")\nprint(\"------------\")\nprint()\n\nwhile True:\n    remaining_dices=int(input(\"How many dices to throw? (1-5) \"))\n    if remaining_dices<1 or remaining_dices>5:\n        print(\"Wrong input, try again\")\n        break\n    dicesArray = list()\n    while remaining_dices>0:\n        dice_value = random.randint(1, 6)\n        dicesArray.append(dice_value)\n        print(dice_value)\n        total += dice_value\n        remaining_dices -= 1\n        if(dice_value == 6):\n            total-=6\n            remaining_dices +=2\n            print(\"You rolled a 6, rolling two new dices\")\n        else:\n            print(\"You rolled a \" + str(dice_value) + \", the total is : \" +str(total))\n\n    restart=(input(\"Do you want to restart press Enter, to quit press 9. \"))\n    if restart==\"9\":\n        exit()\n    else:\n        print()\n"], ["import random\nnumDices=0\ntotal = 0\nprint(\"------------\")\nprint(\"DICE ROLLING\")\nprint(\"------------\")\nprint()\nstart = True\nwhile start:\n    numDices=int(input(\"How many dices to throw? (1-5) \"))\n    if numDices<1 or numDices>5:\n        print(\"Wrong input, try again\")\n        break\n    total = 0\n    dices_counter = 0\n    while numDices > 0 :\n        eyes = random.randint(1, 6)\n        dices_counter+=1 \n        total += eyes\n        if eyes == 1:\n            print(\"You rolled a one, the total is: \",str(total))\n            numDices-=1\n        elif eyes == 2:\n            print(\"You rolled a two, the total is: \",str(total))\n            numDices-=1\n        elif eyes == 3:\n            print(\"You rolled a three, the total is: \",str(total))\n            numDices-=1\n        elif eyes == 4:\n            print(\"You rolled a four, the total is: \",str(total))\n            numDices-=1\n        elif eyes == 5:\n            print(\"You rolled a five, the total is: \",str(total))\n            numDices-=1\n        elif eyes == 6:\n            total-=6\n            numDices+=2\n            print(\"You rolled a six, rolling two new dices\")\n    print(\"The total sum is\",str(total),\"with\",dices_counter,\"number of rolls.\")\n    print()\n    start=(input(\"Do you want to restart press Enter, to quit press 9. \"))\n    if start==\"9\":\n        break\n    else:\n        print()\n        start = True\n"], [], [], [], ["df['SUM']=df.sum(min_count=1,axis=1)\n#df.sum(min_count=1,axis=1)\nOut[199]: \n0     0.0\n1     8.0\n2    23.0\n3     NaN\n4    30.0\n5    22.0\ndtype: float64\n"], [], ["addition_str = \"2+5+10+20\"\n\nsum_val=0\nfor a in addition_str:\n    a.split(addition_str)\n    sum_val = sum(map(int,addition_str.split(\"+\")))\nprint(sum_val)\n"], [], ["Out[132]:\n      last_year  next_year\nfoo           1          4\nbar           2          5\nstar          3          6\nbar          33         66\n\npd.pivot_table(df, index=df.index, margins=True, aggfunc=sum)\n\nOut[134]:\n      last_year  next_year\nbar          35         71\nfoo           1          4\nstar          3          6\nAll          39         81\n", "pd.pivot_table(df, index=df.index, margins=True, aggfunc={'last_year': sum})\n\n.....\n    220                     grand_margin[k] = getattr(v, aggfunc)()\n    221                 elif isinstance(aggfunc, dict):\n--> 222                     if isinstance(aggfunc[k], compat.string_types):\n    223                         grand_margin[k] = getattr(v, aggfunc[k])()\n    224                     else:\n\nKeyError: 'next_year'\n"], ["df = pd.DataFrame(\n    {'last_year': [1, 2, 3], 'next_year': [4, 5, 6]}, \n    index=['foo', 'bar', 'star']\n)\n\ndf.append(df.sum().rename('Total')).assign(Total=lambda d: d.sum(1))\n", "     last_year   next_year   Total\nfoo      1           4         5\nbar      2           5         7\nstar     3           6         9\nTotal    6          15        21\n"], ["df43 = pd.DataFrame(\n{'last_year': [1, 2, 3], 'next_year': [4, 5, 6]}, \nindex=['foo', 'bar', 'star'])\ndf43 = df43.T #.T is transpose\ndf43['total'] = df43.sum(axis=1)\ndf43\n"], ["df['Sum'] = df.dropna(how='all').sum(1)\n", "   Surf1  Surf2   Sum\n0   10.0   22.0  32.0\n1    NaN    8.0   8.0\n2    8.0   15.0  23.0\n3    NaN    NaN   NaN\n4   16.0   14.0  30.0\n5   15.0    7.0  22.0\n"], ["data.loc[:,'Sum'] = data.loc[:,['Surf1','Surf2']].sum(axis=1, min_count=1)\n", "   Surf1  Surf2\n0   10.0   22.0\n1    NaN    8.0\n2    8.0   15.0\n3    NaN    NaN\n4   16.0   14.0\n5   15.0    7.0\n   Surf1  Surf2   Sum\n0   10.0   22.0  32.0\n1    NaN    8.0   8.0\n2    8.0   15.0  23.0\n3    NaN    NaN   NaN\n4   16.0   14.0  30.0\n5   15.0    7.0  22.0\n"], ["df.sum(1).mask(df.isna().all(1))\n\n0     0.0\n1     8.0\n2    23.0\n3     NaN\n4    30.0\n5    22.0\ndtype: float64\n"], ["# The normal method. Read from folder / Drive\nI = io.imread('%s/images/%s/%s'%(dataDir,dataType,img['file_name']))\n\n# Instead, use this! Url to load image\nI = io.imread(img['coco_url'])\n"], ["(df.stack().reset_index(name='value')\n   .pivot_table(index='level_0', columns='level_1', values='value', margins=True,\n               aggfunc='sum')\n) \n", "level_1  last_year  next_year  All\nlevel_0                           \nbar              2          5    7\nfoo              1          4    5\nstar             3          6    9\nAll              6         15   21\n"], [">>> def _ifdef_decorator_impl(plat, func, frame):\n...     if platform.system() == plat:\n...         return func\n...     elif func.__name__ in frame.f_locals:\n...         return frame.f_locals[func.__name__]\n...     else:\n...         def _not_implemented(*args, **kwargs):\n...             raise NotImplementedError(\n...                 f\"Function {func.__name__} is not defined \"\n...                 f\"for platform {platform.system()}.\")\n...         return _not_implemented\n...             \n...\n>>> def windows(func):\n...     return _ifdef_decorator_impl('Windows', func, sys._getframe().f_back)\n...     \n>>> def macos(func):\n...     return _ifdef_decorator_impl('Darwin', func, sys._getframe().f_back)\n", ">>> @macos\n... def zulu():\n...     print(\"world\")\n...     \n>>> @windows\n... def zulu():\n...     print(\"hello\")\n...     \n>>> zulu()\nworld\n>>> \n", "@mydecorator\ndef foo():\n    pass\n", "foo = mydecorator(foo)\n", ">>> def ifdef(plat):\n...     frame = sys._getframe().f_back\n...     def _ifdef(func):\n...         return _ifdef_decorator_impl(plat, func, frame)\n...     return _ifdef\n...     \n>>> @ifdef('Darwin')\n... def ice9():\n...     print(\"nonsense\")\n", ">>> @macos\n... class CallableClass:\n...     \n...     @macos\n...     def __call__(self):\n...         print(\"CallableClass.__call__() invoked.\")\n...     \n...     @macos\n...     def func_with_inner(self):\n...         print(\"Defining inner function.\")\n...         \n...         @macos\n...         def inner():\n...             print(\"Inner function defined for Darwin called.\")\n...             \n...         @windows\n...         def inner():\n...             print(\"Inner function for Windows called.\")\n...         \n...         inner()\n...         \n...     @macos\n...     class InnerClass:\n...         \n...         @macos\n...         def inner_class_function(self):\n...             print(\"Called inner_class_function() Mac.\")\n...             \n...         @windows\n...         def inner_class_function(self):\n...             print(\"Called inner_class_function() for windows.\")\n", ">>> class IfDefDecoratorPlaceholder:\n...     def __init__(self, func):\n...         self.__name__ = func.__name__\n...         self._func    = func\n...         \n...     def __call__(self, *args, **kwargs):\n...         raise NotImplementedError(\n...             f\"Function {self._func.__name__} is not defined for \"\n...             f\"platform {platform.system()}.\")\n...\n>>> def _ifdef_decorator_impl(plat, func, frame):\n...     if platform.system() == plat:\n...         if type(func) == IfDefDecoratorPlaceholder:\n...             func = func._func\n...         frame.f_locals[func.__name__] = func\n...         return func\n...     elif func.__name__ in frame.f_locals:\n...         return frame.f_locals[func.__name__]\n...     elif type(func) == IfDefDecoratorPlaceholder:\n...         return func\n...     else:\n...         return IfDefDecoratorPlaceholder(func)\n...\n>>> def linux(func):\n...     return _ifdef_decorator_impl('Linux', func, sys._getframe().f_back)\n", ">>> @macos\n... @linux\n... def foo():\n...     print(\"works!\")\n...     \n>>> foo()\nworks!\n"], ["pip install textract-trp\n"], [], ["def move_zeros(array):\n    zero_count = array.count(0)\n    array1 = list(filter(lambda a: a != 0 or isinstance(a, bool), array)) + [0 for i in range(zero_count)]\n    return array1\n"], [], [], ["# Import package\nimport numpy as np\n\n# Define unequal matrices\nxy1 = np.array([[0,1], [2,2], [5,4], [3,6], [4,2]])\nxy2 = np.array([[0,1],[5,4]])\n\nP = np.add.outer(np.sum(xy1**2, axis=1), np.sum(xy2**2, axis=1))\nN = np.dot(xy1, xy2.T)\ndists = np.sqrt(P - 2*N)\nprint(dists)\n"], ["{% load static %}\n<img src=\"{% static \"images/hi.jpg\" %}\" alt=\"Hi!\">  \n", "Here is a complete guide for serving images on Heroku: \n", "1) http://whitenoise.evans.io/en/stable/django.html\n\n2) https://docs.djangoproject.com/en/3.0/howto/static-files/\n\n3) https://docs.djangoproject.com/en/3.0/ref/templates/builtins/#std:templatetag-static\n"], ["from functools import wraps\n\nclass ServiceInjector:\n    deps = {}\n\n    def register(self, name=None):\n        name = name\n\n        def decorator(thing):\n            \"\"\"\n            thing here can be class or function or anything really\n            \"\"\"\n\n            if not name:\n                if not hasattr(thing, '__name__'):\n                    raise Exception('no name')\n                thing_name = thing.__name__\n            else:\n                thing_name = name\n            self.__class__.deps[thing_name] = thing\n            return thing\n\n        return decorator\n\n    class inject:\n        def __init__(self, *args):\n            self.selected_deps = args\n\n        def __call__(self, func):\n            @wraps(func)\n            def decorated(*args, **kwargs):\n                selected_deps = {k: v for k, v in ServiceInjector.deps.items() if k in self.selected_deps}\n                new_kwargs = {**kwargs, **selected_deps}\n                return func(*args, **new_kwargs)\n\n            return decorated\n", "si = ServiceInjector()\n\n# use func.__name__, registering func\n@si.register()\ndef foo(*args):\n    return sum(args)\n", "@si.register(name='uppercase')\nclass UpperCaseRepresentation:\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return self.value.upper()\n", "si.register(name=\"PI\")(3.141592653)\n", "@si.inject('foo', 'PI', 'uppercase')\ndef bar(a, b, c, uppercase: UpperCaseRepresentation, **kwargs):\n    \"\"\"\n    You can specify dependencies as keyword arguments and add typehint annotation.\n    \"\"\"\n    UpperCase, foo = kwargs['UpperCase'], kwargs['foo']\n    print(uppercase('abc')) # ABC\n    print(PI) # 3.141592653\n    print(foo(a, b, c, 4, 5)) # = 15\n\nbar(1, 2, 3)\n", "class Bar:\n    @si.inject('foo')\n    def my_method(self, a, b, foo, kwarg1=30):\n        return foo(a, b, kwarg1)\n\nprint(Bar().my_method(1, 2, kwarg1=50)) # = 53\n"], [], ["pip3 --version\n\n", "# Create a virtualenv \nvirtualenv -p /usr/bin/python3.8 venv38\n\n# acticate the enviroment\nsource venv38/bin/activate\n\n# now you can see somethi like\n\n(venv38) user@mycomputer:~/Desktop/\n\n# install pygame\npip install pygame\n\n"], [], ["$ python -m venv my_venv\n$ source my_venv/bin/activate\n(venv)$ pip install pygame\n"], [], [], ["conda uninstall gensim\nconda install gensim\n"], ["import tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*4))])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n"], ["item[\"x\"][\"y\"].split(' ')[-1]\n"], [], ["    import os\n    os.environ.update({'MALLET_HOME': r'C:/new_mallet/mallet-2.0.8/'})\n    mallet_path = 'C:/new_mallet/mallet-2.0.8/bin/mallet'  # update this path\n    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n"], ["import functools\nimport sys\nimport types\n\n\ndef os_dispatch(func):\n    registry = {}\n\n    def dispatch(platform):\n        try:\n            return registry[platform]\n        except KeyError:\n            return registry[None]\n\n    def register(platform, func=None):\n        if func is None:\n            if isinstance(platform, str):\n                return lambda f: register(platform, f)\n            platform, func = platform.__name__, platform  # it is a function\n        registry[platform] = func\n        return func\n\n    def wrapper(*args, **kw):\n        return dispatch(sys.platform)(*args, **kw)\n\n    registry[None] = func\n    wrapper.register = register\n    wrapper.dispatch = dispatch\n    wrapper.registry = types.MappingProxyType(registry)\n    functools.update_wrapper(wrapper, func)\n    return wrapper\n", "@os_dispatch  # fallback in case OS is not supported\ndef my_callback():\n    print('OS not supported')\n\n@my_callback.register('linux')\ndef _():\n    print('Doing something @ Linux')\n\n@my_callback.register('windows')\ndef _():\n    print('Doing something @ Windows')\n\nmy_callback()  # dispatches on sys.platform\n", "@os_dispatch\ndef my_callback():\n    print('OS not supported')\n\n@my_callback.register\ndef linux():\n    print('Doing something @ Linux')\n\n@my_callback.register\ndef windows():\n    print('Doing something @ Windows')\n"], [], [], [], ["optimizer = tf.keras.optimizers.Adam(0.001)\noptimizer.learning_rate.assign(0.01)\nprint(optimizer.learning_rate)\n", "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>\n"], ["np.array(\n[np.sqrt((list_a[:,1]-list_b[i,1])**2+(list_a[:,0]-list_b[i,0])**2) for i in range(len(list_b))]\n).T\n", "array([[0.        , 5.83095189],\n       [2.23606798, 3.60555128],\n       [5.83095189, 0.        ],\n       [5.83095189, 2.82842712],\n       [4.12310563, 2.23606798]])\n"], ["import numpy as np\n\nlist_a = np.array([[0,1], [2,2], [5,4], [3,6], [4,2]])\nlist_b = np.array([[0,1],[5,4]])\n\ndef run_euc(list_a,list_b):\n    return np.array([[ np.linalg.norm(i-j) for j in list_b] for i in list_a])\n\nprint(run_euc(list_a, list_b))\n", "[[0.         5.83095189]\n [2.23606798 3.60555128]\n [5.83095189 0.        ]\n [5.83095189 2.82842712]\n [4.12310563 2.23606798]]\n"], ["os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n", "physical_devices = tf.config.experimental.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"], [], ["total = sum([int(i) for i in original_string.split('+')])\n", "accumulator = 0\nfor i in original_string.split('+'):\n    accumulator += int(i)\n\ntotal = accumulator\n"], ["linux = platform.system() == \"Linux\"\nwindows = platform.system() == \"Windows\"\nmacos = platform.system() == \"Darwin\"\n\nif linux:\n    def my_callback(*args, **kwargs):\n        print(\"Doing something @ Linux\")\n        return\n\nif windows:\n    def my_callback(*args, **kwargs):\n        print(\"Doing something @ Windows\")\n        return\n", "if linux:\n    def my_callback(*args, **kwargs):\n        print(\"Doing something @ Linux\")\n        return\n\nelif windows:\n    def my_callback(*args, **kwargs):\n        print(\"Doing something @ Windows\")\n        return\n\nelse:\n     raise NotImplementedError(\"This platform is not supported\")\n"], ["from collections import defaultdict\nimport inspect\nimport os\n\n\nclass PlatformFunction(object):\n    mod_funcs = defaultdict(dict)\n\n    @classmethod\n    def get_function(cls, mod, func_name):\n        return cls.mod_funcs[mod][func_name]\n\n    @classmethod\n    def set_function(cls, mod, func_name, func):\n        cls.mod_funcs[mod][func_name] = func\n\n\ndef linux(func):\n    frame_info = inspect.stack()[1]\n    mod = inspect.getmodule(frame_info.frame)\n    if os.environ['OS'] == 'linux':\n        PlatformFunction.set_function(mod, func.__name__, func)\n\n    def call(*args, **kwargs):\n        return PlatformFunction.get_function(mod, func.__name__)(*args,\n                                                                 **kwargs)\n\n    return call\n\n\ndef windows(func):\n    frame_info = inspect.stack()[1]\n    mod = inspect.getmodule(frame_info.frame)\n    if os.environ['OS'] == 'windows':\n        PlatformFunction.set_function(mod, func.__name__, func)\n\n    def call(*args, **kwargs):\n        return PlatformFunction.get_function(mod, func.__name__)(*args,\n                                                                 **kwargs)\n\n    return call\n\n\n@linux\ndef myfunc(a, b):\n    print('linux', a, b)\n\n\n@windows\ndef myfunc(a, b):\n    print('windows', a, b)\n\n\nif __name__ == '__main__':\n    myfunc(1, 2)\n"], ["import numpy as np\nnp.unique([1,6,6,2,2,3,4,5,5,5], return_index=True)\n\n>>> (array([1, 2, 3, 4, 5, 6]), array([0, 3, 5, 6, 7, 1], dtype=int64))\n"], ["for i in lems:\n    i = i.strip()\n    print('{}{}{}'.format(i, '\\t', 'text'))\n"], ["import pandas as pd\ndf = pd.DataFrame([r.split() for r in '''Index Type    Class   Area    Decision\n0   A       1       North   Yes\n1   B       1       North   Yes\n2   C       2       South   No\n3   A       3       South   No\n4   B       3       South   No\n5   C       1       South   No\n6   A       2       North   Yes\n7   B       3       South   Yes\n8   B       1       North   No\n9   C       1       East    No\n10  C       2       West    Yes'''.split('\\n')])\ndf.columns = df.iloc[0]\ndf = df.iloc[1:]\n\ntable = pd.pivot_table(df, values='Class', index=['Type'], columns=['Area'], aggfunc='count').fillna(0)\ntable = table.div(table.sum(axis=1), axis=0)\n", "Area  East     North     South  West\nType                                \nA     0.00  0.666667  0.333333  0.00\nB     0.00  0.500000  0.500000  0.00\nC     0.25  0.000000  0.500000  0.25 \n"], ["(\n    df.groupby('Type')\n    .apply(lambda x: x.groupby('Area').Class.count()).unstack(fill_value=0)\n    .transform(lambda x: x/x.sum(), axis=1)\n)\n"], [], [">>> df.groupby('Type')['Area'].value_counts(normalize = True).unstack().fillna(0)\nArea  East     North     South  West\nType                                \nA     0.00  0.666667  0.333333  0.00\nB     0.00  0.500000  0.500000  0.00\nC     0.25  0.000000  0.500000  0.25\n"], [], ["from google.colab import drive\ndrive.mount('/content/drive')\n", "cd 'drive/My Drive'\n"], ["myadam = keras.optimizers.Adam(learning_rate=0.1)\n"], [], ["from keras.callbacks import LearningRateScheduler\n\n# This is a sample of a scheduler I used in the past\ndef lr_scheduler(epoch, lr):\n    decay_rate = 0.85\n    decay_step = 1\n    if epoch % decay_step == 0 and epoch:\n        return lr * pow(decay_rate, np.floor(epoch / decay_step))\n    return lr\n", "callbacks = [LearningRateScheduler(lr_scheduler, verbose=1)]\n\nmodel = build_model(pretrained_model=ka.InceptionV3, input_shape=(224, 224, 3))\nhistory = model.fit(train, callbacks=callbacks, epochs=EPOCHS, verbose=1)\n"], ["df = df[(~df).all(axis=1)]\n#if want seelct only boolean columns\ndf = df[(~df.select_dtypes(bool)).all(axis=1)]\nprint (df.head())\n     Meta_Description_contains_kw  Title_1_contains_kw  H1-1_contains_kw  \\\n50                          False                False             False   \n97                          False                False             False   \n99                          False                False             False   \n100                         False                False             False   \n101                         False                False             False   \n\n     H2-1_contains_kw  H2-2_contains_kw  \n50              False             False  \n97              False             False  \n99              False             False  \n100             False             False  \n101             False             False  \n"], [], ["for column in df.columns:\n    if df[column].dtype == bool and not any(df[column]):\n            print('The column `%s` has all false values'%column)\n"], [], ["example_data = example_data.loc[!(column1 & column2 &.....)]\n"], [], [], ["ImportError: cannot import name 'Document'\n", "from trp.trp import Document\n"], [], [], [], ["from sklearn.metrics import classification_report\n\ny_pred = model.predict(x_test, batch_size=64, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\n\nprint(classification_report(y_test, y_pred_bool))\n", "model.compile(loss='categorical_crossentropy'\n              , metrics=['acc'], optimizer='adam')\n", "hist = model.fit(x_train, y_train, batch_size=24, epochs=1000, verbose=2,\n                 callbacks=[checkpoint],\n                 validation_data=(x_valid, y_valid)\n\n                 )\n# Plot training & validation accuracy values\nplt.plot(hist.history['acc'])\nplt.plot(hist.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n"], ["class Student:\n    def study():\n        print(\"I'm studying\")\n\n\nStudent.study()\n"], ["student = Student()\nstudent.study()\n"], ["class Student:\n    @staticmethod\n    def study():\n        print(\"I'm studying\")\n"], [], [], [], [], ["#training a model correctly initializes cuDNN\nmodel=Sequential()\nmodel.add(Conv2D(32,...))\nmodel.add(Dense(num_classes,...))\nmodel.compile(...)\nmodel.fit() #this all works fine\n", "#this script relies on cuDNN already being initialized by the script above\nfrom keras.models import load_model\nmodel = load_model(modelPath) #works\nmodel = Model(inputs=model.inputs, outputs=model.layers[1].output) #works\nfeature_maps = model.predict(img) #produces the error only if the first piece of code is not run\n"], ["MIDDLEWARE_CLASSES = (\n    # Simplified static file serving.\n    # https://warehouse.python.org/project/whitenoise/\n    'whitenoise.middleware.WhiteNoiseMiddleware',\nSTATIC_URL = '/static/'\n\nSTATICFILES_DIRS = (\n    os.path.join(BASE_DIR, \"static\"),\n)\n\nSTATIC_ROOT = os.path.join(BASE_DIR, \"your_static_folder\")\n\nSTATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'\n\nMEDIA_URL = \"/media/\"\n\nMEDIA_ROOT = os.path.join(BASE_DIR, \"your_media_folder\")\n"], ["STATIC_URL = '/static/'\n", "{% load static %}\n<img src=\"{% static 'my_app/example.jpg' %}\" alt=\"My image\">\n"], ["websockets.connect(uri, ping_interval=None)\n"], ["import pandas as pd\ndf = pd.DataFrame({'time': [pd.to_datetime('2019-01-15 13:25:43')]})\n\n# pd.to_timedelta(df.time).dt.total_seconds() # Is deprecated\n(df.time - pd.to_datetime('1970-01-01')).dt.total_seconds()\n", "0    1.547559e+09\nName: time, dtype: float64\n"], ["pip install pygame==2.0.0.dev6\n"], [], [], ["   outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n"], [], [], ["from google.colab import drive\ndrive.mount('/content/drive')\n"], ["from bs4 import BeautifulSoup\nfrom requests import get\nimport re\n\npage = \"https://www.eurocham-cambodia.org/member/476/2-LEau-Protection\"\n\ncontent = get(page).content\nsoup = BeautifulSoup(content, \"lxml\")\n\nexp = re.compile(r\"(?:.*?='(.*?)')\")\n# Find any element with the mail icon\nfor icon in soup.findAll(\"i\", {\"class\": \"icon-mail\"}):\n    # the 'a' element doesn't exist, there is a script tag instead\n    script = icon.next_sibling\n    # the script tag builds a long array of single characters- lets gra\n    chars = exp.findall(script.text)\n    output = []\n    # the javascript array is iterated backwards\n    for char in reversed(list(chars)):\n        # many characters use their ascii representation instead of simple text\n        if char.startswith(\"|\"):\n            output.append(chr(int(char[1:])))\n        else:\n            output.append(char)\n    # putting the array back together gets us an `a` element\n    link = BeautifulSoup(\"\".join(output))\n    email = link.findAll(\"a\")[0][\"href\"][8:]\n    # the email is the part of the href after `mailto: `\n    print(email)\n"], ["from selenium import webdriver\n\nurl = \"https://www.eurocham-cambodia.org/member/476/2-LEau-Protection\"\n\n\noption = webdriver.ChromeOptions()\noption.add_argument(\"--headless\")\nbrowser = webdriver.Chrome(executable_path=\"./chromedriver\", options=option)\n\nbrowser.get(url)\n\nprint(browser.find_element_by_css_selector(\".icon-mail~ a\").text)\n", "information@2leau-protection.com\n"], ["    import lxml\n    import requests\n\n    page = requests.get(https://www.eurocham-cambodia.org/member/476/2-LEau- Protection).text\n    tree = html.fromstring(page)\n    print(lxml.html.tostring(tree, pretty_print=True).decode())\n\n", "    <div class=\"col-sm-12 col-md-6\">\n       <ul class=\"iconlist\">\n          <li>\n             <i class=\"icon-phone\"> </i>(+33) 02 98 19 43 86</li>\n\n          <li>\n              <i class=\"icon-mail\"> </i><script type=\"text/javascript\">\n                //<![CDATA[\n                var l=new Array();\n    l[0]='>';l[1]='a';l[2]='/';l[3]='<';l[4]='|109';l[5]='|111';l[6]='|99';l[7]='|46';l[8]='|110';l[9]='|111';l[10]='|105';l[11]='|116';l[12]='|99';l[13]='|101';l[14]='|116';l[15]='|111';l[16]='|114';l[17]='|112';l[18]='|45';l[19]='|117';l[20]='|97';l[21]='|101';l[22]='|108';l[23]='|50';l[24]='|64';l[25]='|110';l[26]='|111';l[27]='|105';l[28]='|116';l[29]='|97';l[30]='|109';l[31]='|114';l[32]='|111';l[33]='|102';l[34]='|110';l[35]='|105';l[36]='|32';l[37]='>';l[38]='\"';l[39]='|109';l[40]='|111';l[41]='|99';l[42]='|46';l[43]='|110';l[44]='|111';l[45]='|105';l[46]='|116';l[47]='|99';l[48]='|101';l[49]='|116';l[50]='|111';l[51]='|114';l[52]='|112';l[53]='|45';l[54]='|117';l[55]='|97';l[56]='|101';l[57]='|108';l[58]='|50';l[59]='|64';l[60]='|110';l[61]='|111';l[62]='|105';l[63]='|116';l[64]='|97';l[65]='|109';l[66]='|114';l[67]='|111';l[68]='|102';l[69]='|110';l[70]='|105';l[71]='|32';l[72]=':';l[73]='o';l[74]='t';l[75]='l';l[76]='i';l[77]='a';l[78]='m';l[79]='\"';l[80]='=';l[81]='f';l[82]='e';l[83]='r';l[84]='h';l[85]=' ';l[86]='a';l[87]='<';\n    for (var i = l.length-1; i >= 0; i=i-1){\n    if (l[i].substring(0, 1) == '|') document.write(\"&#\"+unescape(l[i].substring(1))+\";\");\n    else document.write(unescape(l[i]));}\n    //]]>\n              </script>\n           </li>\n           <li>\n            <i class=\"icon-globe\"></i> <a href=\"http://www.2leau-protection.com/\" target=\"_blank\"><i style=\"background-color:#2C3E50\"></i>http://www.2leau-protection.com/</a>\n          </li>\n        </ul>\n     </div>\n"], ["<script type=\"text/javascript\">\n    //<![CDATA[\n    var l=new Array();\n    l[0]='>';l[1]='a';l[2]='/';l[3]='<';l[4]='|109';l[5]='|111';l[6]='|99';l[7]='|46';l[8]='|110';l[9]='|111';l[10]='|105';l[11]='|116';l[12]='|99';l[13]='|101';l[14]='|116';l[15]='|111';l[16]='|114';l[17]='|112';l[18]='|45';l[19]='|117';l[20]='|97';l[21]='|101';l[22]='|108';l[23]='|50';l[24]='|64';l[25]='|110';l[26]='|111';l[27]='|105';l[28]='|116';l[29]='|97';l[30]='|109';l[31]='|114';l[32]='|111';l[33]='|102';l[34]='|110';l[35]='|105';l[36]='|32';l[37]='>';l[38]='\"';l[39]='|109';l[40]='|111';l[41]='|99';l[42]='|46';l[43]='|110';l[44]='|111';l[45]='|105';l[46]='|116';l[47]='|99';l[48]='|101';l[49]='|116';l[50]='|111';l[51]='|114';l[52]='|112';l[53]='|45';l[54]='|117';l[55]='|97';l[56]='|101';l[57]='|108';l[58]='|50';l[59]='|64';l[60]='|110';l[61]='|111';l[62]='|105';l[63]='|116';l[64]='|97';l[65]='|109';l[66]='|114';l[67]='|111';l[68]='|102';l[69]='|110';l[70]='|105';l[71]='|32';l[72]=':';l[73]='o';l[74]='t';l[75]='l';l[76]='i';l[77]='a';l[78]='m';l[79]='\"';l[80]='=';l[81]='f';l[82]='e';l[83]='r';l[84]='h';l[85]=' ';l[86]='a';l[87]='<';\n    for (var i = l.length-1; i >= 0; i=i-1){\n    if (l[i].substring(0, 1) == '|') document.write(\"&#\"+unescape(l[i].substring(1))+\";\");\n    else document.write(unescape(l[i]));}\n    //]]>\n</script>\n"], [], ["from trp import Document\n"], [], ["pip install tensorflow-gpu==1.13.1\n"], [". my_env_name/bin/activate\n", "my_env_name\\Scripts\\activate\n"], ["l=[1,2,2,3,4,5,5,5] # Your list\nindexes=[] # Your output list\nfor elem in set(l):\n       indexes.append(l.index(elem))\n"], [], ["def uniqueIndexes(l):\n    seen = set()\n    res = []\n    for i, n in enumerate(l):\n        if n not in seen:\n            res.append(i)\n            seen.add(n)\n    return res\n\nl=[1,2,2,3,4,5,5,5,2]\n\nuniqueIndexes(l)\n", "[0, 1, 3, 4, 5]\n"], ["indexes = [l.index(x) for x in sorted(set(l))]\n"], ["from collections import OrderedDict\ndef get_unique_indexes(l):\n    # OrdedDict is used to preserve the order of the indexes\n    result = OrderedDict()\n    for i in range(0, len(l)):\n        val = l[i]\n        if not val in result:\n            result[val] = i\n\n    return result.values()\n"], [], ["conda install django-rest-auth==0.9.3\n"], ["from collections import deque\n    def circularArrayRotation(a, k, queries):\n        result=[]\n        a = deque(a) \n        a.rotate(k) \n        a = list(a) \n\n        for v in queries:\n            result.append(a[v])\n\n        return(result)\n"], [], ["from functools import wraps\n\nclass ServiceInjector:\n\n    def __init__(self):\n        self.deps = {}\n\n    def register(self, name=None):\n\n        name = name\n        def decorator(thing):\n            \"\"\"\n            thing here can be class or function or anything really\n            \"\"\"\n\n            if not name:\n                if not hasattr(thing, \"__name__\"):\n                    raise Exception(\"no name\")\n                thing_name = thing.__name__\n            else:\n                thing_name = name\n            self.deps[thing_name] = thing\n            return thing\n\n        return decorator\n\n    def inject(self, func):\n\n        @wraps(func)\n        def decorated(*args, **kwargs):\n            new_args = args + (self.deps, )\n            return func(*new_args, **kwargs)\n\n        return decorated\n\n# usage:\n\n\nsi = ServiceInjector()\n\n# use func.__name__, registering func\n@si.register()\ndef foo(*args):\n    return sum(args)\n\n\n# we can rename what it's been registered as, here, the class is registered \n# with name `UpperCase` instead of the class name `UpperCaseRepresentation`\n@si.register(name=\"UpperCase\")\nclass UpperCaseRepresentation:\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return self.value.upper()\n\n#register float\nsi.register(name=\"PI\")(3.141592653)\n\n\n# inject into functions\n@si.inject \ndef bar(a, b, c, _deps): # the last one in *args would be receiving the dependencies\n    UpperCase, PI, foo = _deps['UpperCase'], _deps['PI'], _deps['foo']\n    print(UpperCase('abc')) # ABC\n    print(PI) # 3.141592653\n    print(foo(a, b, c, 4, 5)) # = 15\n\nbar(1, 2, 3)\n\n# inject into class methods\nclass Foo:\n\n    @si.inject\n    def my_method(self, a, b, _deps, kwarg1=30):\n        return _deps['foo'](a, b, kwarg1)\n\nprint(Foo().my_method(1, 2, kwarg1=50)) # = 53\n"], [" sys.setrecursionlimit(1500)\n"], [], ["def mult(m, n):\n    result = sum(m for _ in range(abs(n)))\n    if n < 0:\n        return -result\n    else:\n        return result\n\n\nm = int(input())\nn = int(input())\nprint(mult(m, n))\n\n", "3\n-12\n-36\n"], ["import sys\nsys.setrecursionlimit(1000)\n"], [], ["[global]\ntimeout = 30\n"], [], ["import math\n\nprint(math.factorial(4))\n", "def factorial(n):\n    return 1 if ( n == 1 or n == 0 ) else n * factorial(n - 1)\n"], ["def FirstFactorial(num):\n    x = 1\n    if num == 1:\n        return 1\n    else:\n        for i in range(1,num+1):\n            x = x*(i)   \n    return x\n\nprint (FirstFactorial(4)) \n>>24\n", "def FirstFactorial(num):\n    x = 1\n    for i in range(1,num+1):\n        x = x*(i)   \n    return x\n\nprint (FirstFactorial(1))\n>> 1\n"], ["import math\nprint(math.factorial(4))  \n"], [], [], [], ["# even number of teams required\nteams = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nn = int(len(teams) / 2)\n\nstages = []\nfor i in range(len(teams) - 1):\n    t = teams[:1] + teams[-i:] + teams[1:-i] if i else teams\n    stages.append(list(zip(t[:n], reversed(t[n:]))))\n\nprint(stages)\n# [\n#     [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6)],\n#     [(1, 9), (10, 8), (2, 7), (3, 6), (4, 5)],\n#     [(1, 8), (9, 7), (10, 6), (2, 5), (3, 4)],\n#     [(1, 7), (8, 6), (9, 5), (10, 4), (2, 3)],\n#     [(1, 6), (7, 5), (8, 4), (9, 3), (10, 2)],\n#     [(1, 5), (6, 4), (7, 3), (8, 2), (9, 10)],\n#     [(1, 4), (5, 3), (6, 2), (7, 10), (8, 9)],\n#     [(1, 3), (4, 2), (5, 10), (6, 9), (7, 8)],\n#     [(1, 2), (3, 10), (4, 9), (5, 8), (6, 7)]\n# ]\n"], ["[(4, 10), (2, 8), (1, 4), (4, 6), (2, 3)]\n[(3, 9), (2, 6), (4, 5), (7, 9), (7, 8)]\n[(4, 9), (1, 10), (2, 9), (5, 9), (3, 4)]\n[(6, 9), (1, 7), (7, 10), (8, 10), (2, 4)]\n[(1, 8), (5, 6), (3, 5), (1, 6), (5, 8)]\n[(1, 3), (4, 7), (3, 7), (6, 7), (3, 6)]\n[(3, 8), (1, 5), (6, 8), (5, 10), (2, 7)]\n[(5, 7), (1, 9), (2, 10), (9, 10), (1, 2)]\n[(8, 9), (6, 10), (2, 5), (3, 10), (4, 8)]\n"], ["import itertools\nteams = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ncombo = list(itertools.combinations(teams, 2))\ndef group(c = [], s = []):\n   _c = {i for b in c for i in b}\n   if all(i in _c for i in teams):\n     yield c\n     _s = [i for i in combo if i not in s]\n     if _s:\n       yield from group(c=[_s[0]], s=s+[_s[0]])\n   else:\n     _c = {i for b in c for i in b}\n     _s = [i for i in combo if i not in s and all(j not in _c for j in i)]\n     for i in _s:\n       yield from group(c=c+[i], s = s+[i])\n\n\nresults, combos = [], group()\n_start = next(combos)\nwhile all(all(j not in i for i in results) for j in _start):\n   results.append(_start)\n   _start = next(combos)\n", "[[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)], \n [(1, 3), (2, 4), (5, 7), (6, 9), (8, 10)], \n [(1, 4), (2, 3), (5, 8), (6, 10), (7, 9)], \n [(1, 5), (2, 6), (3, 7), (4, 10), (8, 9)], \n [(1, 6), (2, 5), (3, 8), (4, 9), (7, 10)], \n [(1, 7), (2, 8), (3, 9), (4, 6), (5, 10)], \n [(1, 8), (2, 9), (3, 10), (4, 5), (6, 7)], \n [(1, 9), (2, 10), (3, 5), (4, 7), (6, 8)], \n [(1, 10), (2, 7), (3, 6), (4, 8), (5, 9)]]\n"], ["from itertools import combinations\n\nteams = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ncombo = list(combinations(teams, 2))\n\nsets = []\n\ndef is_combo_value_in_set(c, s):\n    for val in c:\n        for val_s in s:\n            for v in val_s:\n                if val == v:\n                    return True\n    return False\n\nfor c in combo:\n    should_add_set = True\n    for current_set in sets:\n        if is_combo_value_in_set(c, current_set) is False:\n            should_add_set = False\n            current_set.add(c)\n            break\n    if should_add_set:\n        sets.append(set())\n        sets[-1].add(c)\n\nfor v in sets:\n    print(sorted(v))\n", "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]\n[(1, 3), (2, 4), (5, 7), (6, 8)]\n[(1, 4), (2, 3), (5, 8), (6, 7)]\n[(1, 5), (2, 6), (3, 7), (4, 8)]\n[(1, 6), (2, 5), (3, 8), (4, 7)]\n[(1, 7), (2, 8), (3, 5), (4, 6)]\n[(1, 8), (2, 7), (3, 6), (4, 5)]\n[(1, 9), (2, 10)]\n[(1, 10), (2, 9)]\n[(3, 9), (4, 10)]\n[(3, 10), (4, 9)]\n[(5, 9), (6, 10)]\n[(5, 10), (6, 9)]\n[(7, 9), (8, 10)]\n[(7, 10), (8, 9)]\n", "from itertools import combinations, chain\nfrom random import choice\n\nteams = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ncombo = list(combinations(teams, 2))\n\navailable = combo.copy()\nrv = []\n\ndef random_pop(l):\n    ch = choice(l)\n    l.remove(ch)\n    return ch\n\nnum_tries = 0\n\nwhile True:\n    num_tries += 1\n    if num_tries > 99999:\n        available = combo.copy()\n        rv = []\n        num_tries = 0\n\n    l = [random_pop(available), random_pop(available), random_pop(available), random_pop(available), random_pop(available)]\n    flat = list(chain.from_iterable(l))\n    if len(set(flat)) == len(flat):\n        #is unique\n        rv.append(l)\n    else:\n        for i in l:\n            available.append(i)\n    if len(available) == 0:\n        break\n\nfor l in rv:\n    print(sorted(l))\n", "[(1, 8), (2, 4), (3, 5), (6, 10), (7, 9)]\n[(1, 5), (2, 7), (3, 6), (4, 9), (8, 10)]\n[(1, 10), (2, 6), (3, 8), (4, 7), (5, 9)]\n[(1, 3), (2, 9), (4, 8), (5, 6), (7, 10)]\n[(1, 9), (2, 3), (4, 6), (5, 10), (7, 8)]\n[(1, 4), (2, 5), (3, 7), (6, 8), (9, 10)]\n[(1, 7), (2, 10), (3, 4), (5, 8), (6, 9)]\n[(1, 6), (2, 8), (3, 9), (4, 10), (5, 7)]\n[(1, 2), (3, 10), (4, 5), (6, 7), (8, 9)]\n"], ["import copy\n\n\ndef extract_one_list(xdata):\n    \"\"\"\n    `xdata` .......... `external data`\n    \"\"\"\n    old_type = type(xdata[0])\n    # we are going to be testing for whether\n    # two tuples have any elements in common.\n    # For example, do (4, 5) and (7, 8) have any elements common?\n    # the answer is `no`.\n    prohibited_elements = set(xdata.pop(0))\n    iout = [copy.copy(prohibited_elements)]\n    # `iout`.......... `internal output`\n    candi = 0\n    while True:\n        # `candi`......... candidate index\n        # `candy`......... candidate\n        if candi >= len(xdata):\n            break\n        candy = set(xdata[candi])\n        if len(prohibited_elements.intersection(candy)) == 0:\n            iout.append(candy)\n            prohibited_elements.update(xdata.pop(candi))\n        candi = candi + 1\n\n    # Next, convert sets into the type of container\n    # which was originally used (tuples, lists,\n    # or some other type of container\n    # Let external iout (xout) be:\n    # the old_type of the internal element (ielem)\n    # for each internal element in the internal iout (iout)\n    xout = [old_type(ielem) for ielem in iout]\n    return xout\n\ndef extract_all_lists(xdata):\n    lol = list()\n    # `lol`...... `list of lists`\n    while len(xdata) > 0:\n        lyst = extract_one_list(unsorted_data)\n        lol.append(lyst)\n    return lol\n\nunsorted_data = [(1, 2), (1, 3), (1, 4), (1, 5), (1, 6),\n                 (1, 7), (1, 8), (1, 9), (1, 10), (2, 3),\n                 (2, 4), (2, 5), (2, 6), (2, 7), (2, 8),\n                 (2, 9), (2, 10), (3, 4), (3, 5), (3, 6),\n                 (3, 7), (3, 8), (3, 9), (3, 10), (4, 5),\n                 (4, 6), (4, 7), (4, 8), (4, 9), (4, 10),\n                 (5, 6), (5, 7), (5, 8), (5, 9), (5, 10),\n                 (6, 7), (6, 8), (6, 9), (6, 10), (7, 8),\n                 (7, 9), (7, 10), (8, 9), (8, 10), (9, 10)]\n\nlol = extract_all_lists(unsorted_data)\nprint('\\n'.join([str(x) for x in lol]))\n", "[(1, 2), (3, 4), (5, 6), (8, 7), (9, 10)]\n[(1, 3), (2, 4), (5, 7), (8, 6)]\n[(1, 4), (2, 3), (8, 5), (6, 7)]\n[(1, 5), (2, 6), (3, 7), (8, 4)]\n[(1, 6), (2, 5), (8, 3), (4, 7)]\n[(1, 7), (8, 2), (3, 5), (4, 6)]\n[(8, 1), (2, 7), (3, 6), (4, 5)]\n[(1, 9), (2, 10)]\n[(1, 10), (9, 2)]\n[(9, 3), (10, 4)]\n[(10, 3), (9, 4)]\n[(9, 5), (10, 6)]\n[(10, 5), (9, 6)]\n[(9, 7), (8, 10)]\n[(10, 7), (8, 9)]\n"], ["d = {}\nfor i in combo:\n    s = set(teams) - set(i)\n    d[i] = [list(s)[k:k+2] for k in range(0, len(s), 2)]\n", "{(5, 9): [[1, 2], [3, 4], [6, 7], [8, 10]], (4, 7): [[1, 2], [3, 5], [6, 8], [9, 10]], (1, 3): [[2, 4], [5, 6], [7, 8], [9, 10]], (4, 8): [[1, 2], [3, 5], [6, 7], [9, 10]], (5, 6): [[1, 2], [3, 4], [7, 8], [9, 10]], (2, 8): [[1, 3], [4, 5], [6, 7], [9, 10]], (6, 9): [[1, 2], [3, 4], [5, 7], [8, 10]], (8, 9): [[1, 2], [3, 4], [5, 6], [7, 10]], (1, 6): [[2, 3], [4, 5], [7, 8], [9, 10]], (3, 7): [[1, 2], [4, 5], [6, 8], [9, 10]], (2, 5): [[1, 3], [4, 6], [7, 8], [9, 10]], (5, 8): [[1, 2], [3, 4], [6, 7], [9, 10]], (1, 2): [[3, 4], [5, 6], [7, 8], [9, 10]], (4, 9): [[1, 2], [3, 5], [6, 7], [8, 10]], (2, 9): [[1, 3], [4, 5], [6, 7], [8, 10]], (3, 10): [[1, 2], [4, 5], [6, 7], [8, 9]], (6, 10): [[1, 2], [3, 4], [5, 7], [8, 9]], (8, 10): [[1, 2], [3, 4], [5, 6], [7, 9]], (1, 5): [[2, 3], [4, 6], [7, 8], [9, 10]], (3, 6): [[1, 2], [4, 5], [7, 8], [9, 10]], (1, 10): [[2, 3], [4, 5], [6, 7], [8, 9]], (7, 9): [[1, 2], [3, 4], [5, 6], [8, 10]], (4, 10): [[1, 2], [3, 5], [6, 7], [8, 9]], (2, 6): [[1, 3], [4, 5], [7, 8], [9, 10]], (7, 10): [[1, 2], [3, 4], [5, 6], [8, 9]], (4, 5): [[1, 2], [3, 6], [7, 8], [9, 10]], (1, 4): [[2, 3], [5, 6], [7, 8], [9, 10]], (2, 10): [[1, 3], [4, 5], [6, 7], [8, 9]], (9, 10): [[1, 2], [3, 4], [5, 6], [7, 8]], (3, 9): [[1, 2], [4, 5], [6, 7], [8, 10]], (2, 3): [[1, 4], [5, 6], [7, 8], [9, 10]], (1, 9): [[2, 3], [4, 5], [6, 7], [8, 10]], (6, 8): [[1, 2], [3, 4], [5, 7], [9, 10]], (6, 7): [[1, 2], [3, 4], [5, 8], [9, 10]], (3, 5): [[1, 2], [4, 6], [7, 8], [9, 10]], (2, 7): [[1, 3], [4, 5], [6, 8], [9, 10]], (5, 10): [[1, 2], [3, 4], [6, 7], [8, 9]], (4, 6): [[1, 2], [3, 5], [7, 8], [9, 10]], (7, 8): [[1, 2], [3, 4], [5, 6], [9, 10]], (5, 7): [[1, 2], [3, 4], [6, 8], [9, 10]], (3, 8): [[1, 2], [4, 5], [6, 7], [9, 10]], (1, 8): [[2, 3], [4, 5], [6, 7], [9, 10]], (1, 7): [[2, 3], [4, 5], [6, 8], [9, 10]], (3, 4): [[1, 2], [5, 6], [7, 8], [9, 10]], (2, 4): [[1, 3], [5, 6], [7, 8], [9, 10]]}\n"], ["RIGHT = 0\nDOWN = 1\nLEFT = 2\nUP = 3\nNB_DIRECTIONS = 4\n\n\ndef next_direction(direction):\n    return (direction + 1) % NB_DIRECTIONS\n\n\ndef update_position(position, direction):\n    x, y = position\n    if direction == RIGHT:\n        return x + 1, y\n    elif direction == DOWN:\n        return x, y + 1\n    elif direction == LEFT:\n        return x - 1, y\n    elif direction == UP:\n        return x, y - 1\n\n", "def get_value(array, position):\n    x, y = position\n    return array[y][x]\n\n\ndef set_as_visited(array, position):\n    x, y = position\n    array[y][x] = '*'\n\n\ndef is_visited(array, position):\n    return get_value(array, position) == '*'\n", "def snail_arr(array):\n    # compute the array size\n    array_size = len(array) * len(array[0])\n\n    # surround the array of '*'\n    array = [['*' for _ in range(len(array[0]) + 2)]] + [\n        ['*'] + array[i] + ['*']\n        for i in range(len(array))\n    ] + [['*' for _ in range(len(array[0]) + 2)]]\n\n    # initialize position and direction\n    position = 1, 1\n    direction = RIGHT\n\n    result = [get_value(array, position)]\n    set_as_visited(array, position)\n    nb_visited = 1\n\n    while nb_visited < array_size:\n        new_position = update_position(position, direction)\n        if not is_visited(array, new_position):\n            result += [get_value(array, new_position)]\n            set_as_visited(array, new_position)\n            position = new_position\n            nb_visited += 1\n        else:\n            direction = next_direction(direction)\n    return result\n", "array = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\nprint(snail_arr(array))  \n# [1, 2, 3, 4, 8, 12, 16, 15, 14, 13, 9, 5, 6, 7, 11, 10]\n", "def is_in_bounds(array, position):  # valid only for square array\n    x, y = position\n    array_size = len(array)\n    return (0 <= x < array_size) and (0 <= y < array_size)\n"], ["\ndirections = [\n    lambda i, j: (i, j + 1),\n    lambda i, j: (i + 1, j),\n    lambda i, j: (i, j - 1),\n    lambda i, j: (i - 1, j),\n]\n\narray = [[1,2,3,4],\n         [4,5,6,7],\n         [8,9,10,11],\n         [12,13,14,15]]\n\ndef in_matrix(i, j):\n    return 0 <= i < len(array) and 0 <= j < len(array)\n\ndef is_visited(i, j):\n    return array[i][j] == 0\n\n\ndef snail(array):\n    direction_cnt = 0\n    i, j = 0, 0\n    ret = []\n    ret.append(array[i][j])\n    array[i][j] = 0  # mark as visited\n    while True:\n        direction_func = directions[direction_cnt % 4]  # turning directions in circle\n        tmp_i, tmp_j = direction_func(i, j)  # attempt to head one step\n        if (not in_matrix(tmp_i, tmp_j)) or is_visited(tmp_i, tmp_j):  # over border or visted\n            direction_cnt += 1  # change direction\n        else:\n            i, j = tmp_i, tmp_j  # confirm this step\n            ret.append(array[i][j])\n            array[i][j] = 0  # mark as visited\n            if len(ret) == len(array)**2:  # simple terminal criteria\n                return ret\n\n\nif __name__ == '__main__':\n    print snail(array)\n\n"], ["        else:\n\n            while m > 0:\n                new_snail.append(array[n][m])\n                m -= 1\n\n            while n > 0:\n                new_snail.append(array[n][m])\n                n -= 1\n            m += 1\n            n += 1\n        j+=1\n"], [], ["python --version\n", "python\npython              python2.7           python2-config      python2-pbr            python3.6           python3.6m          python3-config      python3m            python-config       \npython2             python2.7-config    python2-jsonschema  python3             python3.6-config    python3.6m-config   python3-jsonschema  python3m-config     \n"], ["python3\n"], [], ["import datetime\n\ncurrent_date = datetime.date.today()\nnew_date = current_date.replace(\n  month = current_date.month - 1,\n  day = 20\n)\n\nprint(new_date)\n#2019-03-20\n", "import datetime\n\ncurrent_date = datetime.date(2019, 2, 17)\nmonth = current_date.month - 1\nyear = current_date.year\nif not month:\n  month, year = 12, year - 1\n\nnew_date = datetime.date(year=year, month=month, day=20)\n"], ["# Last month\nt = time.gmtime()\nprint(f\"{t.tm_year}-{t.tm_mon-1}-20\")\n", "print(\"{0}-{1}-{2}\".format(t.tm_year, t.tm_mon -1, 20))\n"], ["import datetime\ntime = datetime.datetime.today()\nprint(time)\ntimestr = time.strftime(\"%Y-%m-%d\")\nyear, month, day = timestr.split(\"-\")\nprint(\"{}-{}-{}\".format(year, int(month)-1, day))\n", "import datetime\ntime = datetime.datetime.today()\nprint(time)\ntimestr = time.strftime(\"%Y-%m-%d\")\nyear, month, day = timestr.split(\"-\")\nif month in [1, \"01\", \"1\"]: # I don't remember how January is represented\n    print(\"{}-{}-{}\".format(int(year) - 1, 12, day)) # use December of last year\nelse:\n    print(\"{}-{}-{}\".format(year, int(month)-1, day))\n"], ["from datetime import date, timedelta\n\ndt = date.today() - timedelta(30)// timedelta(days No.)\nprint('Current Date :',date.today())\nprint(dt)\n"], ["from datetime import date, timedelta\ntoday = date.today()\nlast_day_prev_month = today - timedelta(days=today.day)\ntwenty_prev_month = last_day_prev_month.replace(day=20)\nprint(twenty_prev_month)  # 2019-03-20\n"], ["from google.colab import drive\ndrive.mount('/content/drive')\n", "import os\nos.chdir(\"drive/My Drive/cocodataset\")\n"], ["!wget http://images.cocodataset.org/zips/train2017.zip\n"], ["for row in df.head(5).itertuples():\n    # do something\n"], [], [], [], [], ["pool = set(map(frozenset, L))\ngroups = []\nwhile pool:\n    group = set()\n    groups.append([])\n    while True:\n        for candidate in pool:\n            if not group or group & candidate:\n                group |= candidate\n                groups[-1].append(tuple(candidate))\n                pool.remove(candidate)\n                break\n        else:\n            break\n", "[[('A', 'B'), ('C', 'B'), ('C', 'D')],\n [('G', 'H'), ('H', 'I'), ('G', 'J'), ('G', 'I')],\n [('E', 'F')]]\n"], ["import itertools, functools\n\ndef partition(pred, iterable):\n    t1, t2 = itertools.tee(iterable)\n    return itertools.filterfalse(pred, t1), filter(pred, t2)\n\ngroups = []\nfor a, b in L:\n    unrelated, related = partition(lambda group: any(aa == a or bb == b or aa == b or bb == a for aa, bb in group), groups)\n    groups = [*unrelated, sum(related, [(a, b)])]\n"], ["l = [(\"A\",\"B\"), (\"B\",\"C\"), (\"C\",\"D\"), (\"E\",\"F\"), (\"G\",\"H\"), (\"H\",\"I\"), (\"G\",\"I\"), (\"G\",\"J\")]\n\nresult = []\nif len(l) > 1:\n  tmp = [l[0]]\n  for i in range(1,len(l)):\n    if l[i][0] == l[i-1][1] or l[i][1] == l[i-1][0] or l[i][1] == l[i-1][1] or l[i][0] == l[i-1][0]:\n      tmp.append(l[i])\n    else:\n      result.append(tmp)\n      tmp = [l[i]]\n  result.append(tmp)\nelse:\n  result = l\n\nfor elem in result:\n  print(elem)\n", "[('A', 'B'), ('B', 'C'), ('C', 'D')]\n[('E', 'F')]\n[('G', 'H'), ('H', 'I'), ('G', 'I'), ('G', 'J')]\n"], [], [], [], ["\\venv\\Scripts\\activate.bat \n"], ["import datetime\n\ncurrent_datetime=datetime.datetime.now()\nprint(\"current_year:{}\".format(current_datetime.year))\nprint(\"current_month:{}\".format(current_datetime.month))\nprint(\"current_day:{}\".format(current_datetime.day))\n"], ["from datetime import datetime, timedelta\n\nyesterday = datetime.today() - timedelta(1)\n\nprint(yesterday)\n\nyear = yesterday.year\nmonth = yesterday.month\nday = yesterday.day\n\nprint(year)\nprint(month)\nprint(day)\n", "2019-03-10 21:19:36.695577\n2019\n3\n10\n"], ["import datetime\nfrom datetime import date, timedelta\nyesterday = date.today() - timedelta(1)\nprint (yesterday)\nyear = yesterday.year\nmonth = yesterday.month\nday=yesterday.day\nprint (year)\nprint (month)\nprint (day)\n"], ["lems = ['scaena', 'persona', 'improbus']\nfor i in lems:\n    print(f\"{i}\\t{'whatever'}\")\n", "from reprlib import repr\n\nlems = ['scaena', 'persona', 'improbus']\nfor i in lems:\n    print(repr(f\"{i}\\t{'whatever'}\"))\n", "'scaena\\twhatever'\n'persona\\twhatever'\n'improbus\\twhatever'\n"], ["scaena    whatever\npersona   whatever\nimprobus  whatever\n"], ["url = [\"www.annauniv.edu\", \"www.google.com\", \"www.ndtv.com\", \"www.website.org\", \"www.bis.org.in\", \"www.rbi.org.in\"]\ndef myFn(s):\n    return s.split('.')[-1]\n\nprint sorted(url,key=myFn) \n", "def myFunc(s, order=('edu','com','in','org')):\n    try: return order.index(s.split('.')[-1])\n    except ValueError: return len(order)\n"], ["print(sorted(url,key=lambda x: x.split('.')[-1]))\n", "def func(x):\n    d = {'edu':'e','com':'m','org':'o'}\n    return d.get(x.split('.')[-1],'z')\n\nprint(sorted(urls, key=func))\n"], ["return s.split('.')[-1]\n", "def myFn(s):\n    preferred_order = [\"edu\", \"com\", \"org\", \"in\"]\n    tld=s.split('.')[-1]\n    return preferred_order.index(tld)\n", "def myFn(s):\n    preferred_order = [\"edu\", \"com\", \"org\", \"in\"]\n    tld=s.split('.')[-1]\n    try:\n        ranking = preferred_order.index(tld)\n    except ValueError:\n        ranking = 99999 # to sort unknowns at end; use -1 to sort at beginning \n    return ranking\n"], ["urls = [\"www.annauniv.edu\", \"www.google.com\", \"www.ndtv.com\", \"www.website.org\", \"www.bis.org.in\", \"www.rbi.org.in\"]\n\ndef func(x):\n  x = x.split('.')[-1]\n  print(x)\n  if x == 'edu':\n    return 'e'\n  elif x == 'com':\n    return 'm'\n  elif x == 'org':\n    return 'o'\n  else:\n    return 'z'\n\nprint(sorted(urls, key=func))\n", "['www.annauniv.edu', 'www.google.com', 'www.ndtv.com', 'www.website.org', 'www.bis.org.in', 'www.rbi.org.in']\n"], ["www.bis.org.in\n> ['www', 'bis', 'org', 'in']\n", "> 'in'\n", "url = [\"www.annauniv.edu\", \"www.abc.co.uk\", \"x.dev\", \"x.mom\", \"www.google.com\", \"www.ndtv.com\", \"www.website.org\", \"www.bis.org.in\", \"www.rbi.org.in\"]\n\ndef myFn(x):\n    order = {'edu': 0, 'com': 1, 'org': 2, 'in': 3}\n    tld = x.split('.')[-1]\n    return order[tld] if tld in order.keys() else tld\n\nprint(sorted(url, key=myFn))\n", "['www.annauniv.edu',\n 'www.google.com',\n 'www.ndtv.com',\n 'www.website.org',\n 'www.bis.org.in',\n 'www.rbi.org.in',\n 'x.dev',\n 'x.mom',\n 'www.abc.co.uk']\n"], ["urls = [\"www.annauniv.edu\", \"www.google.com\", \"www.ndtv.com\", \"www.website.org\", \"www.bis.org.in\", \"www.rbi.org.in\"]\n\ndef key_function(s):\n    # Turn \"www.google.com\" into [\"www\", \"google\", \"com\"], then\n    # reverse it to [\"com\", \"google\", \"www\"].\n    return list(reversed(s.split('.')))\n\n# Now this will sort \".com\" before \".edu\", \"google.com\" before \"ndtv.com\",\n# and so on.\nprint(sorted(urls, key=key_function))\n"], ["url = ['www.annauniv.edu', 'www.google.com', 'www.ndtv.com', 'www.website.org', 'www.bis.org.in', 'www.rbi.org.in']\n\ndef topLevelDomain(domain: str):\n    # Split from the right, max of one split.\n    # This only takes the right hand side after the last period in the string.\n    return domain.rsplit('.', 1)[-1]\n\nprint(sorted(url, key=topLevelDomain))\n"], ["from collections import ChainMap\n\nfoo = { 'a':1, 'b':2, 'c':3 }\nbar = { 'd':4, 'f':5, 'g':6 }\n\nChainMap(foo, bar)['h']\n"], ["class MyDict(dict):\n    def get(self, key, default=None, error=None):\n        res = super().get(key,default)\n        if res is None:\n            if error == 'raise':\n                raise SyntaxError()\n            elif error == 'Exception':\n                return SyntaxError()\n        return res\n", "foo = MyDict({ 'a':1, 'b':2, 'c':3 })\nbar = MyDict({ 'd':4, 'f':5, 'g':6 })\nfoo.get('h', bar.get('h', error=\"Exception\")) #  returns a syntaxerror object\nfoo.get('h', bar.get('h', error=\"raise\"))  # raises a syntax error\n"], ["class GetAndRaise:\n    def __init__(self):\n        self.dict = dict()\n    def __getitem__(self, key):\n        try:\n            return self.dict[key]\n        except ValueError:\n            raise MyException\n    def __setitem__(self, key, value):\n        self.dict[key] = value\n    def get(self, key):\n        return self[key]\n"], ["d={}\nd['unknown key'] --> Raises a KeyError\n", "try:\n    d['unknown key']\nexcept KeyError:\n    raise CustomException('Custom message')\n", "try:\n    d['unknown key']\nexcept KeyError as e:\n    raise CustomException('Custom message') from e\n"], ["class MyException(Exception):\n    pass\n\n\ntry:\n    value = dict['h']\nexcept KeyError:\n    raise MyException('my message')\n"], ["python flasky.py\n"], ["resp = await websocket.recv()\n", "print('Reconnecting')\nwebsocket = await websockets.connect(ws_url)\n"], ["sum_val = sum(map(int,addition_str.split(\"+\")))\n"], [], ["accumulation = []\nmylist = []\n\naddition_str = \"2+5+10+20\"\nstr_nums = (addition_str.split(\"+\"))\nfor i in str_nums:\n    mylist.append(int(i))\n    accumulation.append(sum(mylist))\n"], ["import pandas as pd\ndf = pd.DataFrame({'time': [pd.to_datetime('2019-01-15 13:25:43')]})\ndf_unix_sec = pd.to_datetime(df['time']).astype(int)/ 10**9\nprint(df_unix_sec)\n"], ["def circularArrayRotation(a, k, queries):\n\n    new_arr = a[-k%len(a):] + a[:-k%len(a)]\n    # list slicing is done here.  it will get the right rotated array \n\n    result = []\n    for i in queries:\n        result.append(new_arr[i])\n        # running queries on rotated array\n    return result\n"], ["D = {}\nfor i, e in enumerate(L1):\n  D[e] = L2[i%len(L2)]\n\nD #=> {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}\n", "{ e: L2[i%len(L2)] for i, e in enumerate(L1) }\n#=> {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}\n"], ["from itertools import cycle\ndict(zip(L1, cycle(L2)))\n# {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}\n", "# dict(zip(L1, L2 * 2))\ndict(zip(L1, L2 + L2))\n# {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}\n"], ["from collections import deque\n\nL1 = ['A', 'B', 'C', 'D', 'E']    \nL2 = deque(['1', '2', '3'])\n\nresult = {}\nfor letter in L1:\n    number = L2.popleft()\n    result[letter] = number\n    L2.append(number)\n\nprint(result)\n# {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}\n"], ["from itertools import cycle\n\nL1 = ['A', 'B', 'C', 'D', 'E']\nL2 = ['1', '2', '3']\n\nresult = dict(zip(L1, cycle(L2)))\n\nprint(result)\n", "{'E': '2', 'B': '2', 'A': '1', 'D': '1', 'C': '3'}\n", "result = {v: L2[i % len(L2)] for i, v in enumerate(L1)}\nprint(result)\n"], [], [">>> import itertools  # library of magic\n>>> length = 3        # length of your wanted permutations\n>>> result = itertools.combinations(    # combinations based on position\n...     [*[True, False] * length],      # generates the items needed\n...     length                          # length of the wanted results\n... )\n>>> print([list(r) for in result])\n[[False, False, False], [False, False, True], [False, True, False], [False, True, True], [True, False, False], [True, False, True], [True, True, False], [True, True, True]]\n"], ["import itertools\nl=[False,True]\nll=list(itertools.product(l,repeat=3))\n", "lll=[]\nfor each in ll:\n    lll.append([EACH for EACH in each])\n", "[list(elem) for elem in lll]\n"], ["In [1]: from itertools import product\n\nIn [2]: product([True, False], repeat=2)\nOut[2]: <itertools.product at 0x1c7eff51b40>\n", "In [3]: list(product([True, False], repeat=2))\nOut[3]: [(True, True), (True, False), (False, True), (False, False)]\n\nIn [4]: list(product([True, False], repeat=3))\nOut[4]:\n[(True, True, True),\n (True, True, False),\n (True, False, True),\n (True, False, False),\n (False, True, True),\n (False, True, False),\n (False, False, True),\n (False, False, False)]\n\nIn [5]: list(product([True, False], repeat=5))\nOut[5]:\n[(True, True, True, True, True),\n (True, True, True, True, False),\n (True, True, True, False, True),\n (True, True, True, False, False),\n (True, True, False, True, True),\n...\n", "[list(tup) for tup in mylist]\n"], [], ["def permuteBool(n, l):\n...      if n==0:\n...         return l\n...      return [permuteBool(n-1, l+[True])] + [permuteBool(n-1, l+[False])]\n... \n>>> permuteBool(3, [])\n[[[[True, True, True], [True, True, False]], [[True, False, True], [True, False, False]]], [[[False, True, True], [False, True, False]], [[False, False, True], [False, False, False]]]]\n"], ["def __getitem__(self, index):\n\n    if type(index) == torch.Tensor:\n        index = index.item()\n\n    x = torch.tensor(self.x_data.iloc[index].values, dtype=torch.float)\n    y = torch.tensor(self.y_data.iloc[index], dtype=torch.float)\n    return (x, y)\n"], [], ["self.len = len(self.x_data)\n"], ["def random_split(dataset, lengths):\n    \"\"\"\n    Randomly split a dataset into non-overlapping new datasets of given lengths.\n    Arguments:\n        dataset (Dataset): Dataset to be split\n        lengths (sequence): lengths of splits to be produced\n    \"\"\"\n    if sum(lengths) != len(dataset):\n        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n\n    indices = randperm(sum(lengths)).tolist()\n    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n"], ["def make_bricks(small_bricks, big_bricks, goal):\n\n  if ( (5*big_bricks + small_bricks) < goal ):  #we can't reach the length\n\n    return False\n\n  elif  (small_bricks < (goal%5)) :   #we can surpass the length but we don't have \n                                      #enough 1s to fill goal's residual w.r.t. 5 div\n    return False\n\n  else:      #We can reach the length and have enough 1s to fill residual\n\n    return True\n"], ["def make_bricks(small, big, goal):\n    max_big = goal // 5  # max number of big we can use\n    nb_big = min(big,  max_big)  # big ones we really use\n    return small >= goal - 5 * nb_big  # True if we have enough small ones to complete\n"], ["limit = 2\ndf = pd.DataFrame({\"col1\": [1,2,3], \"col2\": [4,5,6], \"col3\": [7,8,9]})\ndf[:limit].loc[df[\"col3\"] == 7]\n"], ["import itertools\nlimit = 5\nfor index, row in itertools.islice(df.iterrows(), limit):\n    ...\n"], [], ["for index, row in df.iterrows():\n    ...\n"], [], ["pd.DataFrame(df.text.str.split(' ').tolist()).iloc[:,0]\nOut[15]: \n0     FirstName\n1    FirstName2\nName: 0, dtype: object\n"], ["import pandas as pd\n\ndata = ['FirstName LastName StudentID',\n'FirstName2 LastName2 StudentID2']\n\ndf = pd.DataFrame(data=data, columns=['text'])\n\ndf['id'] = df.text.apply(lambda x: x.split()[-1])\n\nprint(df)\n", "text          id\n0     FirstName LastName StudentID   StudentID\n1  FirstName2 LastName2 StudentID2  StudentID2\n", "df['id'] = [x.split()[-1] for x in df.text]\nprint(df)\n", "text          id\n0     FirstName LastName StudentID   StudentID\n1  FirstName2 LastName2 StudentID2  StudentID2\n"], ["students = [\n    [\"FirstName\", \"LastName\", \"StudentID\"],\n    [\"FirstName2\", \"LastName2\", \"StudentID2\"]\n]\n\nprint([student[2] for student in students])\n", "['StudentID', 'StudentID2']\n"], ["ids = [val[-1] for val in your_string.split()]\n"]]