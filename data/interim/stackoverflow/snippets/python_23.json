[[], [], [], ["python -m pip install numpy== --dry-run --python-version 2.4 --no-deps --target foo\n", "ERROR: Could not find a version that satisfies the requirement numpy== (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1)\nERROR: No matching distribution found for numpy==\n", "python -m pip install numpy --dry-run --python-version 3.11 --no-deps --target foo | grep Would\n", "Would install numpy-1.25.0\n"], [], ["# Check if an element is present using JS path.\n# Particularly useful for shadow DOM elements\ndef is_element_present(driver, js_path):\n    try:\n        element = driver.execute_script(f\"return {js_path}\")\n        return element is not None\n    except Exception:\n        pass\n    try:\n        outer_html = driver.execute_script(f\"return {js_path}.outerHTML;\")\n        if outer_html:\n            return True;    \n    except Exception:\n        return False\n \n# used to click shadow elements w/ js path.\n# uses javascript to allow for explicit waits \ndef click_shadow_element(driver, errorName, js_path, wait_time = 10):\n    try:\n        WebDriverWait(driver, wait_time).until(lambda x: self.is_element_present(js_path))\n        driver.execute_script(f\"return {js_path}.click()\")\n    except (TimeoutException, ElementNotVisibleException, ElementNotInteractableException) as e:\n        raise Exception(f\"The {errorName} button was not found. Additional info: {str(e)}\")\n"], [], ["  \"jupyter.debugJustMyCode\": false \n"], [], ["override: Whether to override the system environment variables with the variables\n            from the `.env` file.\n"], [], ["a: int = int(input())\n\nif a > 0:\n    a_str = f'{a:b}'\n    print(a_str[::-1])\n", "x: int = int(input())\n\nwhile x > 0:\n   print(x % 2, end='')\n   x = x // 2\nprint()\n"], [], ["from pathlib import Path\n\nmy_path = Path('/shopping_list.txt')\n\nmy_str = '\\nFresh Spinach\\nFrozen Strawberries'\n\nmy_path.write_text(my_path.read_text() + my_str)\n"], [], [], ["from textwrap import fill\n\ntext = \"blah \" * 20\nprint(fill(text, 16))\n\n# Output\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah\n"], ["env HDF5_DIR=/opt/homebrew/opt/hdf5 python3 -m pip install tables\n"], ["def process_tweet(tweet):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n               word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n            return tweet \n"], [], [], ["pip3 install awsebcli --upgrade\npip3 install --upgrade awscli\n", " File \"/usr/lib/python3/dist-packages/OpenSSL/crypto.py\", line 1573, in X509StoreFlags\n    CB_ISSUER_CHECK = _lib.X509_V_FLAG_CB_ISSUER_CHECK\nAttributeError: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'\n", "pip install pip --upgrade\npip install pyopenssl --upgrade\n"], [], [], ["class StratifiedGroupShuffleSplit(StratifiedShuffleSplit):\n    \"\"\"\n    Note there is an assumption that the samples in a same group have a same label.\n    \"\"\"\n    def __init__(\n        self, n_splits = 10, *, test_size = None, \n        train_size = None, random_state = None\n    ):\n        super().__init__(\n            n_splits = n_splits,\n            test_size = test_size,\n            train_size = train_size,\n            random_state = random_state,\n        )\n        self._default_test_size = 0.1\n\n    def _iter_indices(self, X, y, groups = None):\n        if groups is None:\n            raise ValueError(\"The 'groups' parameter should not be None.\")\n        groups = check_array(groups, input_name = \"groups\", ensure_2d = False, dtype = None)\n        classes, group_indices = np.unique(groups, return_inverse = True)\n        stratify = np.array([y[indices[0]] for indices in group_indices])\n\n        for group_train, group_test in super()._iter_indices(X = classes, y = stratify):\n            # these are the indices of classes in the partition\n            # invert them into data indices\n\n            train = np.flatnonzero(np.in1d(group_indices, group_train))\n            test = np.flatnonzero(np.in1d(group_indices, group_test))\n\n            yield train, test\n"], ["username = input('Enter name')\ndownload_folder = r'C:\\Users\\Alex\\Downloads'\ndownload_folder = download_folder.replace(\"Alex\", username)\nprint(download_folder)\n"], ["import attridict\na = attridict({'foo': 'bar'})\nprint(a.foo) #'bar'\n"], [], [], ["(labelimg) C:\\Users\\user>pip install pyqt5-tools\n", "(labelimg) C:\\Users\\user>cd C:\\Users\\user\\labelimg\n", "(labelimg) C:\\Users\\user\\labelImg>pyrcc5 -o resources.py resources.qrc\n\n(labelimg) C:\\Users\\user\\labelImg>pyrcc5 -o libs/resources.py resources.qrc\n"], [], ["pip uninstall flexget thinc spacy\npip install langchain -U\n"], ["python3 -m uvicorn main:app --reload\n"], [], [], [], [], ["from datetime import datetime, timedelta\nimport numpy as np\n\nstart_date = datetime.strptime(\"2023-06-10\", \"%Y-%m-%d\")\nend_date = start_date + timedelta(13)\n\nfut_dates = np.arange(fut_start_date, fut_end_date, dtype=\"datetime64[D]\")\n", "array(['2023-06-10', '2023-06-11', '2023-06-12', '2023-06-13',\n       '2023-06-14', '2023-06-15', '2023-06-16', '2023-06-17',\n       '2023-06-18', '2023-06-19', '2023-06-20', '2023-06-21',\n       '2023-06-22'], dtype='datetime64[D]')\n", "fut_dates.astype(datetime)\n\narray([datetime.date(2023, 6, 10), datetime.date(2023, 6, 11),\n       datetime.date(2023, 6, 12), datetime.date(2023, 6, 13),\n       datetime.date(2023, 6, 14), datetime.date(2023, 6, 15),\n       datetime.date(2023, 6, 16), datetime.date(2023, 6, 17),\n       datetime.date(2023, 6, 18), datetime.date(2023, 6, 19),\n       datetime.date(2023, 6, 20), datetime.date(2023, 6, 21),\n       datetime.date(2023, 6, 22)], dtype=object)\n", "[dt.strftime(\"%Y-%m-%d\") for dt in pd.to_datetime(fut_dates)]\n", "['2023-06-10',\n '2023-06-11',\n '2023-06-12',\n '2023-06-13',\n '2023-06-14',\n '2023-06-15',\n '2023-06-16',\n '2023-06-17',\n '2023-06-18',\n '2023-06-19',\n '2023-06-20',\n '2023-06-21',\n '2023-06-22']\n"], ["# Creating a sample model\npattern = Sequential()\nmodel.add( Dense(64, activation='relu', input_shape=(784,))))\nmodel.add( Dense(32, activation='relu'))\nmodel.add( Dense(10, activation='softmax'))\n"], ["curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n", "sudo python3 get-pip.py\n", "pip3 --version\n"], [], ["dataset_full.targets = [dataset_full.targets[index] for index in idx]\n"], [], ["git -C path/to/submodule clone --depth 1 --no-checkout --filter=blob:none https://gitlab-ci-token:$GITLAB_PROJECT_TOKEN@gitlab.example.com/your/repo.git\ngit -C path/to/submodule checkout COMMIT_HASH/BRANCH_NAME -- FILE_NAME\n"], [], ["[tool.bandit]\nexclude_dirs = [\".venv\", \"tests\"]\nskips = [\"B307\"]\n", "\"python.linting.banditArgs\": [\n    \"--configfile\",\n    \"pyproject.toml\"\n]\n"], [], [], ["N = 100\nfor i in range(N):\n    with my_path.open('a') as fp\n        fp.write_text(f\"{N}\\n\")\n"], ["pip install scikit-learn==0.22.2\n"], ["File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\chatterbot\\tagging.py\", line 13, in __init__\n\n> `self.nlp = spacy.load(self.language.ISO_639_1.lower())`\n\n", "self.nlp = spacy.load(self.language.ISO_639_1.lower())\n"], ["import typing\nimport types\n\ndef field_is_optional(cls: type, field_name: str):\n    \"\"\"A field is optional when it has Union type with a NoneType alternative.\n    Note that Optional[] is a special form which is converted to a Union with a NoneType option\n    \"\"\"\n    field_type = typing.get_type_hints(cls).get(field_name, None)\n    origin = typing.get_origin(field_type)\n    #print(field_name, \":\", field_type, origin)\n    if origin is typing.Union:\n        return type(None) in typing.get_args(field_type)\n    if origin is types.UnionType:\n        return type(None) in typing.get_args(field_type)\n    return False\n", "from dataclasses import dataclass\nfrom typing import Optional, Union\n\n@dataclass\nclass A:\n    foo : Optional[int] = None\n    bar : int|None = None\n    baz : Union[int, float, None] = None\n    x : int = 1\n\na=A()\nassert field_is_optional(type(a), \"foo\")\nassert field_is_optional(type(a), \"bar\")\nassert field_is_optional(type(a), \"baz\")\nassert field_is_optional(type(a), \"x\") == False\n"], ["ImportError: cannot import name 'plot_roc_curve' from 'sklearn.metrics'\n"], [], ["foundStringList = [i for i in ListVar if stringORsubstring in i.casefold()] # returns string as list [\"string\"]\n\nListVar.index([i for i in ListVar if stringORsubstring in i.casefold()][0]) # returns index of first found string. \n"], ["gcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin europe-west3-docker.pkg.dev\n"], ["import os\nif 'PYTHONHOME' in os.environ:\n   del os.environ['PYTHONHOME']\n", "Python path configuration:\n  PYTHONHOME = '/tmp/.mount_buskilParXTg/opt/python3.7'\n  PYTHONPATH = (not set)\n  program name = '/usr/bin/python3'\n  isolated = 0\n  environment = 1\n  user site = 1\n  import site = 1\n  sys._base_executable = '/usr/bin/python3'\n  sys.base_prefix = '/tmp/.mount_buskilParXTg/opt/python3.7'\n  sys.base_exec_prefix = '/tmp/.mount_buskilParXTg/opt/python3.7'\n  sys.executable = '/usr/bin/python3'\n  sys.prefix = '/tmp/.mount_buskilParXTg/opt/python3.7'\n  sys.exec_prefix = '/tmp/.mount_buskilParXTg/opt/python3.7'\n  sys.path = [\n    '/tmp/.mount_buskilParXTg/opt/python3.7/lib/python38.zip',\n    '/tmp/.mount_buskilParXTg/opt/python3.7/lib/python3.8',\n    '/tmp/.mount_buskilParXTg/opt/python3.7/lib/python3.8/lib-dynload',\n  ]\nFatal Python error: init_fs_encoding: failed to get the Python codec of the filesystem encoding\nPython runtime state: core initialized\n\nModuleNotFoundError: No module named 'encodings'\n"], [], [], ["pip install opencv-python\n", "pip show opencv-python\n", "(Screen_Rec-leAIY5iD) C:\\Users\\Administrator\\Desktop\\Screen_Rec>pip show opencv- \npython\nName: opencv-python\nVersion: 4.7.0.72\nSummary: Wrapper package for OpenCV python bindings.\nHome-page: https://github.com/opencv/opencv-python\nAuthor:\nAuthor-email:\nLicense: Apache 2.0\nLocation: C:\\Users\\Administrator\\.virtualenvs\\Screen_Rec \nleAIY5iD\\Lib\\site- \npackages\nRequires: numpy, numpy, numpy, numpy, numpy\nRequired-by:\n", "pip install opencv-python\n", "Import \"cv2\" could not be resolved Pylance (reportMissingImports)\n"], ["selenium.common.exceptions.WebDriverException: Message: unknown error: ChromeDriver only supports characters in the BMP\n"], ["def Pric_Precision(price, symbol):\n\n    return str(round(float(price),[x['pricePrecision'] for x in client.futures_exchange_info()['symbols'] if x['symbol'] == symbol][0]))\n\n\ndef QUN_Precision(quantity, symbol):\n    return str(round(float(quantity),[x['quantityPrecision'] for x in client.futures_exchange_info()['symbols'] if x['symbol'] == symbol][0]))\n"], ["poetry source add -p explicit pytorch https://download.pytorch.org/whl/cpu\npoetry add --source pytorch torch torchvision\n"], [], [], ["request_body = request.body\n\nrequest_data = request.data\n\n...\n\nevent = stripe.Webhook.construct_event(payload=request_body, sig_header=signature, secret=webhook_secret)\n"], [], [], ["python3 -m pip install -U requests-ntlm\n"], [], ["from functools import reduce\n\n\ndata_dictionary = {\n    'foo': {\n        'bar': {\n            'buzz': 'lightyear'\n        },\n        'baz': {\n            'asd': 2023,\n            'zxc': [\n                {'patrick': 'star'},\n                {'spongebob': 'squarepants'}\n            ],\n            'qwe': ['john', 'sarah']\n        }\n    },\n    'hello': {\n        'world': 'hello world',\n    },\n}\n\n\ndef optional_chaining_v1(dictionary={}, *property_list):\n    def reduce_callback(current_result, current_dictionary):\n        if current_result is None:\n            return dictionary.get(current_dictionary)\n        if type(current_result) != dict:\n            return None\n        return current_result.get(current_dictionary)\n    return reduce(reduce_callback, property_list, None)\n\n\n# or in one line\noptional_chaining_v1 = lambda dictionary={}, *property_list: reduce(lambda current_result, current_dictionary: dictionary.get(current_dictionary) if current_result is None else None if type(current_result) != dict else current_result.get(current_dictionary), property_list, None)\n\n# usage\noptional_chaining_v1_result1 = optional_chaining_v1(data_dictionary, 'foo', 'bar', 'baz')\nprint('optional_chaining_v1_result1:', optional_chaining_v1_result1)\noptional_chaining_v1_result2 = optional_chaining_v1(data_dictionary, 'foo', 'bar', 'buzz')\nprint('optional_chaining_v1_result2:', optional_chaining_v1_result2)\n\n# optional_chaining_v1_result1: None\n# optional_chaining_v1_result2: lightyear\n\n\ndef optional_chaining_v2(dictionary={}, list_of_property_string_separated_by_dot=''):\n    property_list = list_of_property_string_separated_by_dot.split('.')\n\n    def reduce_callback(current_result, current_dictionary):\n        if current_result is None:\n            return dictionary.get(current_dictionary)\n        if type(current_result) != dict:\n            return None\n        return current_result.get(current_dictionary)\n    return reduce(reduce_callback, property_list, None)\n\n\n# or in one line\noptional_chaining_v2 = lambda dictionary={}, list_of_property_string_separated_by_dot='': reduce(lambda current_result, current_dictionary: dictionary.get(current_dictionary) if current_result is None else None if type(current_result) != dict else current_result.get(current_dictionary), list_of_property_string_separated_by_dot.split('.'), None)\n\n# usage\noptional_chaining_v2_result1 = optional_chaining_v2(data_dictionary, 'foo.bar.baz')\nprint('optional_chaining_v2_result1:', optional_chaining_v2_result1)\noptional_chaining_v2_result2 = optional_chaining_v2(data_dictionary, 'foo.bar.buzz')\nprint('optional_chaining_v2_result2:', optional_chaining_v2_result2)\n\n# optional_chaining_v2_result1: None\n# optional_chaining_v2_result2: lightyear\n\n"], [], ["def path2wav(path):\n    audio_raw = tf.io.read_file(path)\n    return tf.audio.decode_wav(audio_raw)\n\ndataset = tf.data.Dataset.list_files(PATH + \"*.wav\")\ndataset = dataset.map(path2wav)\n"], ["sudo pip3 install awscli==1.27.140 boto3==1.26.140 botocore==1.29.140 s3transfer==0.6.1\n"], [], ["jupytext --to notebook <name_of_script_file>.py\n"], [], ["FROM ubuntu:16.04\n \n# tzdata for timzone\nRUN apt-get update -y\nRUN apt-get install -y tzdata\n \n# timezone env with default\nENV TZ=America/Bogota\n"], [], [], [], ["user_text = input()\ncount = 0\n\nfor char in user_text:\n    if char != ' ' and char != '.' and char != ',':\n        count += 1\n\nprint(count)\n"], [], ["iterator = iter(preprocessed_train_dataset)\nmax_iterations = len(preprocessed_train_dataset)\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    # Iterate over the batches of the dataset.\n    i = 0\n    while i < max_iterations:\n        print(\"Currently running {} batch\".format(i))\n        try:\n            i = i + 1\n            x_batch_train, y_batch_train = next(iterator)\n            with tf.GradientTape() as tape:\n                logits = model(x_batch_train, training=True)\n                loss_value = loss_fn(y_batch_train, logits)\n\n            grads = tape.gradient(loss_value, model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n            # Log every 200 batches.\n            if i % 200 == 0:\n                print(\n                    \"Training loss (for one batch) at step %d: %.4f\"\n                    % (i, float(loss_value))\n                )\n                print(\"Seen so far: %s samples\" % ((i + 1) * batch_size))\n\n            train_acc = train_acc_metric.result()\n            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n\n            # Reset training metrics at the end of each epoch\n            train_acc_metric.reset_states()\n            for x_batch_val, y_batch_val in preprocessed_val_dataset:\n                val_logits = model(x_batch_val, training=False)\n                # Update val metrics\n                val_acc_metric.update_state(y_batch_val, val_logits)\n            val_acc = val_acc_metric.result()\n            val_acc_metric.reset_states()\n            print(\"Validation acc: %.4f\" % (float(val_acc),))\n        except Exception as e:\n            continue\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(preprocessed_test_dataset)\n"], ["pamac search -a code-features\npamac build code-features\n"], [], [], ["p1 = Person(\"Sandeep\",49)\nprint(\"%s is %d years old.\" % (p1._name, p1._age))\n# prints \"Sandeep is 49 years old.\"\np1._age = 50\nprint(\"Now %s is %d years old.\" % (p1._name, p1._age))\n# prints \"Now Sandeep is 50 years old.\"\n"], ["         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n", "     col\ntwo     \nt      0\nu      1\nv      2\nw      3\n", "         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n", "         col\none two     \nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n", "df.loc[isin(one=[\"b\", \"d\"])]\n", "         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n", "df.loc[isin(two=[\"t\", \"w\"])] \n", "         col\none two     \nc   u      9\n", "df.loc[isin(one=\"c\", two=\"u\")]\n", "         col\none two     \nc   u      9\na   w      3\n", "df.loc[isin(one=[\"c\", \"a\"], two=[\"u\", \"w\"])]\n", "df.loc[isin(one=\"c\", two=\"u\") | isin(one=\"a\", two=\"w\")]\n", "from pandas_indexing import semijoin\nsemijoin(df, pd.MultiIndex.from_tuples([(\"c\", \"u\"), (\"a\", \"w\")], names=[\"one\", \"two\"]))\n", "         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    t      8\nd   t     12\n", "df.loc[isin(one=\"a\") | isin(two=\"t\")]\n", "         col\none two     \na   u      1\n    v      2\nb   u      5\n    v      6\nd   w     11\n    w     15\n", "df.loc[isin(one=[\"a\", \"b\"], two=[\"u\", \"v\"]) | isin(one=\"d\", two=\"w\")]\n", "         col\none two     \nb   7      4\n    9      5\nc   7     10\nd   6     11\n    8     12\n    8     13\n    6     15\n", "df2.loc[isin(two=lambda s: s > 5)]\n"], ["sudo apt-get install libpq5=12.12-0ubuntu0.20.04.1 && sudo apt-get install libpq-dev\n", "sudo apt-get install libpq5=14.5-0ubuntu0.22.04.1 && sudo apt-get install libpq-dev\n"], [], [], ["pip3 install numpy==1.24.0  --compile --force-reinstall\n"], [], ["brew install cairo\nbrew install pkg-config\npip install pycairo\n"], ["!pip install torch==1.11.0+cu102 torchvision==0.12.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n"], ["from PIL import Image\nimport numpy as np\n\ntemp = np.arange(8)\ntemp = temp.reshape(1, -1) + temp.reshape(-1, 1)\ntemp = temp % 2 == 0\npixels = (255 * temp).astype(np.uint8)\n\nimage = Image.fromarray(pixels, mode=\"L\")\nlarger = image.resize((256, 256), resample=Image.Resampling.NEAREST)\nlarger.show()\n", "[[ 0  1  2  3  4  5  6  7]\n [ 1  2  3  4  5  6  7  8]\n [ 2  3  4  5  6  7  8  9]\n [ 3  4  5  6  7  8  9 10]\n [ 4  5  6  7  8  9 10 11]\n [ 5  6  7  8  9 10 11 12]\n [ 6  7  8  9 10 11 12 13]\n [ 7  8  9 10 11 12 13 14]]\n"], [], [" \"emmet.includeLanguages\": {\"jinja-html\": \"html\"},\n"], ["import os\n\nfile_path = '/content/drive/MyDrive/models/dataset2/train/images/.ipynb_checkpoints.csv'\n\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(\"File deleted successfully.\")\nelse:\n    print(\"File not found.\")\n\n", "import shutil\nimport os\n\nfolder_path = '/content/drive/MyDrive/models/dataset2/train/images/.ipynb_checkpoints'\n\nif os.path.exists(folder_path):\n    shutil.rmtree(folder_path)\n    print(\"Folder deleted successfully.\")\nelse:\n    print(\"Folder not found.\")\n"], [], ["A.size() ; B.size()\n# this works\ntorch.matmul(A[:450, ...], B).size\n# this doesn't\ntorch.matmul(A, B)\n", "torch.Size([512, 256, 3, 3, 4])\ntorch.Size([4])\ntorch.Size([450, 256, 3, 3])\n\nRuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling \n`cublasSgemv(handle, op, m, n, &alpha, a, lda, x, incx, &beta, y, incy)`\n"], [], ["from collections import UserDict\n\nclass AttrDict(UserDict):\n    def __getattr__(self, key):\n        return self.__getitem__(self, key)\n    def __setattr__(self, key, value):\n        if key == \"data\":\n            return super().__setattr__(key, value)\n        return self.__setitem__(key, value)\n"], [], [], [">netstat -ano | findstr \"888\"\n  TCP    127.0.0.1:8888         0.0.0.0:0              LISTENING       16072\n"], [], ["sudo apt install libcairo2-dev pkg-config python3-dev\n", "pip3 install pycairo\n"], ["DeprecationError: PdfFileReader is deprecated and was removed in PyPDF2 3.0.0. Use PdfReader instead.\n"], [], ["# From\nfrom PyPDF2 import PdfFileMerger\nfrom PyPDF2 import PdfFileReader\n\n# To\nfrom PyPDF2 import PdfMerger\nfrom PyPDF2 import PdfReader\n"], ["FROM public.ecr.aws/lambda/python:3.9\n\nRUN cd /tmp && \\\n    curl -L -O https://www.sqlite.org/2023/sqlite-autoconf-3410200.tar.gz && \\\n    tar -xzf sqlite-autoconf-3410200.tar.gz && \\\n    cd sqlite-autoconf-3410200 && \\\n    ./configure && \\\n    make -j$(nproc) && \\\n    make install && \\\n    cp /usr/local/lib/libsqlite3.so /var/lang/lib/libsqlite3.so && \\\n    cp /usr/local/lib/libsqlite3.so.0 /var/lang/lib/libsqlite3.so.0 && \\\n    cp /usr/local/lib/libsqlite3.so.0.8.6 /var/lang/lib/libsqlite3.so.0.8.6 && \\\n    cp /usr/local/lib/libsqlite3.so.0 /usr/lib64/libsqlite3.so.0 && \\\n    cp /usr/local/lib/libsqlite3.so.0.8.6 /usr/lib64/libsqlite3.so.0.8.6\n"], ["import streamlit.web.bootstrap\nstreamlit.web.bootstrap.run(\"APP_NAME.py\", '', [], [])\n"], ["         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n", "         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n"], [" \"python.analysis.extraPaths\": [\"/Users/lilly/micromamba/lib/python3.11\"],\n", "\"python.defaultInterpreterPath\": \"/Users/lilly/micromamba/bin/python\",\n"], ["$ gunicorn -k uvicorn.workers.UvicornWorker www.asgi:application\n"], ["{\n    \"asset\": \"USDT\",\n    \"fiat\": \"NGN\",\n    \"merchantCheck\": true,\n    \"page\": 1,\n    \"payTypes\": [\"BANK\"],\n    \"publisherType\": null,\n    \"rows\": 20,\n    \"tradeType\": \"SELL\",\n    \"transAmount\":  \"5000\"\n}\n"], [], ["sudo apt-get update\n\napt-get install libjs-mathjax fonts-mathjax\n\nsudo apt-get intall python3-pip\n"], [], ["***\n\n2023-04-10 02:53:10,945 ERROR :: Error while making GET request to /fapi/v1/account/: {'code': -1021, 'msg': \"Timestamp for this request was 1000ms ahead of the server's time.\"} (error code 400) 2023-04-10 02:53:10,949 INFO :: Binance Futures Client successfully initialized\n\n***\n"], ["apt-get -y install git\n"], [], ["xcode-select --install\n\nbrew upgrade\n\nbrew install libxml2 libxmlsec1\n\npip install xmlsec\n"], [], ["[provider_sect]\ndefault = default_sect\nlegacy = legacy_sect\n\n[default_sect]\nactivate = 1\n\n[legacy_sect]\nactivate = 1\n"], [], [], [], [], [], [], ["pip install \"camelot-py[base]\"\n"], ["     pyrcc icons.qrc -o icon_qrc.py\n"], ["pip install --upgrade pip setuptools wheel\n"], ["def __get_trimmed_quantity(self, quantity):\n    trimmed_quantity = round(quantity / self.step_size) * self.step_size\n    return trimmed_quantity\n\ndef __get_trimmed_price(self, price):\n    trimmed_price = round(price / self.tick_size) * self.tick_size\n    return trimmed_price\n"], ["import textwrap\n\ntext = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed vehicula lorem vitae quam commodo, at interdum lectus maximus.\"\n\nwrapped_text = textwrap.fill(text, width=16)\n\nprint(wrapped_text)\n"], [], [], [], [], ["gcloud config configurations list\n", "gcloud config set project <project_id>\n"], ["import jupytext\nnotebook = jupytext.read('example.py')\njupytext.write(notebook, 'example.ipynb', fmt='.ipynb')\n"], ["from datetime import timedelta\n\ndef date_range_list(start_date, end_date):\n    # Return list of datetime.date objects (inclusive) between start_date and end_date (inclusive).\n    date_list = []\n    curr_date = start_date\n    while curr_date <= end_date:\n        date_list.append(curr_date)\n        curr_date += timedelta(days=1)\n    return date_list\n", "from datetime import date, timedelta\n\ndef date_range_list(start_date, end_date):\n    # Return list of datetime.date objects (inclusive) between start_date and end_date (inclusive).\n    date_list = []\n    curr_date = start_date\n    while curr_date <= end_date:\n        date_list.append(curr_date)\n        curr_date += timedelta(days=1)\n    return date_list\n\nstart_date = date(year=2021, month=12, day=20)\nstop_date = date(year=2021, month=12, day=25)\ndate_list = date_range_list(start_date, stop_date)\n\ndate_list\n", "[datetime.date(2021, 12, 20),\n datetime.date(2021, 12, 21),\n datetime.date(2021, 12, 22),\n datetime.date(2021, 12, 23),\n datetime.date(2021, 12, 24),\n datetime.date(2021, 12, 25)]\n", "from datetime import timedelta\n\ndef date_range_list(start_date, end_date):\n    # Return generator for a list datetime.date objects (inclusive) between start_date and end_date (inclusive).\n    curr_date = start_date\n    while curr_date <= end_date:\n        yield curr_date \n        curr_date += timedelta(days=1)\n", "from datetime import date, timedelta\n\ndef date_range_list(start_date, end_date):\n    # Return generator for a list datetime.date objects (inclusive) between start_date and end_date (inclusive).\n    curr_date = start_date\n    while curr_date <= end_date:\n        yield curr_date \n        curr_date += timedelta(days=1)\n\nstart_date = date(year=2021, month=12, day=20)\nstop_date = date(year=2021, month=12, day=25)\ndate_list = date_range_list(start_date, stop_date)\n\nfor date in date_list:\n   print(date)\n", "2021-12-20\n2021-12-21\n2021-12-22\n2021-12-23\n2021-12-24\n2021-12-25\n"], ["pip install 'PyPDF2<3.0'\n"], [], [], [], [], ["from toolz.dicttoolz import get_in\n\ndictionary1 = {\"required\": {\"value1\": \"one\", \"value2\": \"two\"}, \"optional\": {\"value1\": \"one\"}}\ndictionary2 = {\"required\": {\"value1\": \"one\", \"value2\": \"two\"}}\n\nget_in((\"optional\", \"value1\"), dictionary1)\n# 'one'\n\nget_in((\"optional\", \"value1\"), dictionary2)\n# None\n\n", "import operator\nfrom functools import reduce\n\ndef get_in(keys, coll, default=None, no_default=False):\n    try:\n        return reduce(operator.getitem, keys, coll)\n    except (KeyError, IndexError, TypeError):\n        if no_default:\n            raise\n        return default\n"], ["my_index_cols = ['Name'] # this can also be a list of multiple columns\ndf.set_index(my_index_cols).to_excel('filename.xlsx', index=True, header=None)\n"], ["....\nspec:\n  containers:\n  - name: python\n    env:\n    -name: TZ\n     value: \"Asia/Colombo\"\n....\n\n"], [], ["venv\\Scripts\\activate\n", "set-executionpolicy remotesigned\n"], [], ["self.client.post(webhook_url, data=payload, content_type='application/json', **headers)\n"], [], ["$ dpkg -l libpq*\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name           Version            Architecture Description\n+++-==============-==================-============-============================================\nii  libpq-dev      15.1-1.pgdg20.04+1 amd64        header files for libpq5 (PostgreSQL library)\nii  libpq5:amd64   15.1-1.pgdg20.04+1 amd64        PostgreSQL C client library\n"], ["   \"python.sortImports.args\": [\"--profile\", \"black\"],\n", "   \"isort.args\": [\"--profile\", \"black\"],\n"], [], [], [], [], [], ["import re\nimport string\nimport numpy as np\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs\n"], [], [], ["df = pd.DataFrame({'Name': ['Tesla','Tesla','Toyota','Ford','Ford','Ford'],\n                   'Type': ['Model X','Model Y','Corolla','Bronco','Fiesta','Mustang']})\n\n# Use the groupby() function to group the rows by 'Name'\ngrouped = df.groupby('Name')\n\n# Use the first() function to find the first row of each group\nfirst_rows = grouped.first()\n\n# Create a new column 'start_row' that contains the index of the first row of each group\nfirst_rows['start_row'] = first_rows.index.map(lambda x: (df['Name'] == x).idxmax())\n\n# Create a new column 'end_row' that contains the index of the last row of each group\nfirst_rows['end_row'] = grouped.last().index.map(lambda x: (df['Name'] == x).idxmax())\n\n# Create an empty list to store the merge ranges\nmerge_ranges = []\n\n# Iterate over the first_rows dataframe and add the merge ranges to the list\nfor index, row in first_rows.iterrows():\n    merge_ranges.append((row['start_row'], 0, row['end_row'], 0))\n\n# Write the dataframe to an excel file and apply the merge ranges\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', index=False)\nworksheet = writer.sheets['Sheet1']\nfor merge_range in merge_ranges:\n    worksheet.merge_range(*merge_range, \"\", worksheet.get_default_format())\nwriter.save()\n"], ["rm -rf `find -type d -name .ipynb_checkpoints`\n"], ["C:\\Users\\lenovo\\miniconda3\\envs\\labelme\\Scripts\\pyrcc5.exe -o\nF:\\labelmg_master\\labelImg\\venv\\Lib\\site-packages\\pip\\_vendor\\distlib\\resources.py\nF:\\labelmg_master\\labelImg\\resources.qrc\n"], [], [], ["    \"configurations\": [\n    {\n        \"name\": \"Python: Current File\",\n        \"type\": \"python\",\n        \"request\": \"test\",\n        \"program\": \"${file}\",\n        \"console\": \"integratedTerminal\",\n        \"justMyCode\": false,\n        \"purpose\": [\"debug-in-terminal\"]\n    }\n]\n"], ["from sklearn.metrics import plot_roc_curve\nsvc_disp = plot_roc_curve(svc, X_test, y_test)\nrfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\n", "from sklearn.metrics import RocCurveDisplay\nsvc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\nrfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=svc_disp.ax_)\n"], ["RUN apk add --update linux-headers;\n"], [], ["ARCHFLAGS=\"-arch arm64\" pip install numpy  --compile --no-cache-dir\n"], ["curl_setopt_array($curl, array(\n...\nCURLOPT_CUSTOMREQUEST => 'PATCH',\n...\n"], [], ["public static void ResyncWindowsTime()\n{\n    var commands = new List<string>() \n    { \n        \"net stop w32time\",\n        \"w32tm /unregister\",\n        \"w32tm /register\",\n        \"net start w32time\",\n        \"w32tm /resync\"\n    };\n\n    var process = new Process();\n    var startInfo = new ProcessStartInfo();\n    startInfo.WindowStyle = ProcessWindowStyle.Hidden;\n    startInfo.FileName = \"cmd.exe\";\n    process.StartInfo = startInfo;\n\n    foreach (var command in commands)\n    {\n        startInfo.Arguments = \"/C \" + command;\n        process.Start();\n        process.WaitForExit();\n    }\n}\n"], [], [], [], ["for /r %%v in (*.py) do p2j \"%%v\" \n"], ["dataframe=pd.pivot_table(df,index=[column name...])\ndf.to_excel(dataframe)\n"], [], [], ["from pathlib import Path\nimport imghdr\n\nfrom pathlib import Path\nimport imghdr\n\nimg_link=list(Path(\"/home/user/datasets/samples/\").glob(r'**/*.jpg'))\n\ncount_num=0\nfor lnk in img_link:\n    binary_img=open(lnk,'rb')\n    find_img=tf.compat.as_bytes('JFIF') in binary_img.peek(10)#The JFIF is a JPEG File Interchange Format (JFIF). It is a standard which we gauge if an image is corrupt or substandard\n    if not find_img:\n        count_num+=1\n        os.remove(str(lnk))\nprint('Total %d pcs image delete from Dataset' % count_num)\n#this should help you delete the bad encoded\n"], [], [], [], ["socket.c -o build/temp.linux-armv8l-cpython-311/aiohttp/_websocket.o\naiohttp/_websocket.c:198:12: fatal error: 'longintrepr.h' file not found\n#include \"longintrepr.h\"                                   \n          ^~~~~~~                        1 error generated.\nerror: command '/data/data/com.termux/files/usr/bin/arm-linux-androideabi-clang' \nfailed with exit code 1\n[end of output]\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed building wheel for aiohttp\nFailed to build aiohttp\nERROR: Could not build wheels for aiohttp, which is required to install\npyproject.toml-based projects\n", "aiohttp==3.8.1\nyarl==1.4.2\nfrozenlist==1.3.0\n", "aiohttp==3.8.2\nyarl==1.8.1\nfrozenlist==1.3.1\n"], [], [], ["\"python.testing.pytestArgs\": [\n    \"${workspaceFolder}/tests\"\n],\n\"python.testing.unittestEnabled\": false,\n\"python.testing.nosetestsEnabled\": false,\n\"python.testing.pytestEnabled\": true,\n"], [], ["image = tf.io.read_file(\"im.png\")\nimage = tf.image.decode_png(image, channels=3)\n"], [], [], ["pip install jupyter notebook jupyterlab pyzmq --upgrade --no-cache-dir\n"], ["         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n", "         col\none two     \nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n", "         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n", "         col\none two     \nc   u      9\n", "         col\none two     \nc   u      9\na   w      3\n", "         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    t      8\nd   t     12\n", "         col\none two     \na   u      1\n    v      2\nb   u      5\n    v      6\nd   w     11\n    w     15\n", "         col\none two     \nb   7      4\n    9      5\nc   7     10\nd   6     11\n    8     12\n    8     13\n    6     15\n"], [], [], ["docker buildx uninstall\n"], [], [], [], [], ["plt.title('Blind Surveys 01-FEB-2022\\n'\n         f'Recording{n}; Mean dose rate: {m.get_mean_doserate()*1000:.4f}'\n         r' $\\mathrm{\\mu}$Sv/hr', fontsize='x-large')\n"], ["!pip install torchvision \n"], ["\"code-runner.executorMap\": {\n  \"python\": \"$pythonPath -u $fullFileName\"\n}\n"], ["pip3 install requests\n"], [], ["python -m notebook\n", "pip install notebook\n"], ["not_escaped = \"\\not_escaped\"\nhalf_escaped = rf\"escaped_{not_escaped}\"\n\nprint(half_escaped)\n### outputs:\n# escaped_\n# ot_escaped\n", "escaped = r\"\\n_escaped\"\nfull_escaped = rf\"escaped_too_{escaped}\"\n\nprint(full_escaped)\n# escaped_too_\\n_escaped\n"], [], ["import torch\nimport numpy as np\n\nx = torch.randn(size=(100, 200, 300)) \n\nindex = np.array(range(x.size(0)))  # get dim 0 index\n\ndel_index = np.array([29, 31, 49])  # the raw you want to delete\n\nnew_index = np.delete(index,del_index,axis=0) # get new index\n    \nnew_x=x[new_index,:,:]              # get new x which del row\n\n# code as a function\ndef delete_tensor_row(x,delete_raw_list,dim)\n\n    index = np.array(range(x.size(dim)))\n\n    del_index = np.array(delete_raw_list)\n\n    new_index = np.delete(index,del_index,axis=dim)\n    \n    if dim==0:\n        new_x=x[new_index,:,:]\n    if dim==1:\n        new_x=x[:,new_index,:]\n    if dim==2:\n        new_x=x[:,:,new_index]\n\n    return new_x\n\n"], ["[MAIN]\ninit-hook=\"import sys; import os; from pylint.config import find_pylintrc; sys.path.append(os.path.dirname(find_pylintrc()))\"\nload-plugins=pylint_django\ndjango-settings-module=<project>.settings\n"], [], ["binance = Binance(public_key = 'my_pub_key', secret_key = 'my_secret_key', sync=True)\nbinance.synced('order_market_buy',  symbol='BNBBTC', quantity=10)\n"], [], [], ["pip install --upgrade tables\n"], ["from tensorflow.keras.models import Sequential\n\nfrom tensorflow.python.keras.models import Sequential\n", "from keras.models import Sequential\n"], [], [], ["def octal_to_string(octal):\n result = \"\"\n value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n # Iterate over each of the digits in octal\n for i in [int(n) for n in str(octal)]:\n    # Check for each of the permissions values\n    for value, letter in value_letters:\n        if i >= value:\n            result += letter\n            i -= value\n        else:\n            result += '-'\n return result\n", "  # Iterate over each of the digits in octal\n  for i in [int(n) for n in str(octal)]:\n", "value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\nfor value, letter in value_letters:     # 1st loop | 2nd .. | ...\n if i >= value:                         # i=7 , 7>4  | i=3 , 3>2   | i=1 , 1=1 \n            result += letter            # result = r | result = rw | result = rwx\n            i -= value                  # 7-4=3      | 3-2=1       | 1-1=0\n        else:\n            result += '-'                                         # | i = 0 , 0<1\n return result                                                    # | result = rwx-\n"], ["sudo apt-get install libpq5=14.5-0ubuntu0.22.04.1\n\nsudo apt-get install libpq-dev\n", "pip install psycopg2\n"], ["binance.setTimeOffset(-1000); // -1 sec\n"], ["assert_used:\n    skips: [\"*/test_*.py\"]\n", "\"python.linting.banditArgs\": [\n    \"-r\",\n    \"--configfile\",\n    \"${workspaceFolder}/bandit.yaml\"\n],\n"], [], ["xcode-select --install\nbrew upgrade\nbrew install libxml2 libxmlsec1\npip install xmlsec\n", "python3.10 -m venv .venv\nsource .venv/bin/activate\npip install xmlsec\n"], ["def octal_to_string(octal):\n    result = \"\"\n    value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n    \n    for num in [int(n) for n in str(octal)]:\n        \n        for value, letter in value_letters:\n            if num >= value:       #checks if num not exceeds value of value_letters\n                result += letter   #adds coherent letter to result \n                num -= value       #substracts for new value to check next permission\n            else:\n                result += \"-\"\n    return result\n    \nprint(octal_to_string(755)) # Should be rwxr-xr-x\nprint(octal_to_string(644)) # Should be rw-r--r--\nprint(octal_to_string(750)) # Should be rwxr-x---\nprint(octal_to_string(600)) # Should be rw-------\n"], [], [], [], ["import mysql.connector\n\nmydb =  mysql.connector.MySQLConnection(\n    host=\"localhost\",\n    user=\"veensew\",\n    password=\"%T5687j5IiYe\",\n    charset=\"utf8mb3\"\n)\n\nprint(mydb)\n"], ["\"C:\\Users\\Saran\\anaconda3\\Library\\bin\\pyrcc5.bat\" -o libs/resources.py resources.qrc\n"], ["from datetime import date, timedelta\n\n#-- the actual method --#\ndef get_start_to_end(start_date, end_date):\n    date_list = []\n    for i in range(0, (end_date - start_date).days + 1):\n        date_list.append(  str(start_date + timedelta(days=i))  ) #<-- here\n    return date_list\n#-- end of the actual method --#\n\n# -- demonstrating it --#\nsd = date(2022,8,12)\ned = date(2022,11,17)\ndates = get_start_to_end(sd, ed)\n\nfor d in dates:\n    print(d)\n\n#-- You can just append the date object, the default string (iso)\n#-- or use strftime for a different format\n#-- (start_date + timedelta(days=i)) <-- date object\n#-- str(start_date + timedelta(days=i))  <-- default string\n#-- (start_date + timedelta(days=i)).strftime(\"%b %d, %Y\") <-- other string format\n"], [], [], [], [], [], [], ["def preprocess_tweet(tweet):\n\n\n# cleaning\ntweet = re.sub(r'^RT[\\s]+','',tweet)\n\ntweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n\ntweet = re.sub(r'#', '',tweet)\ntweet= re.sub(r'@', '',tweet)\n\n# tokenization\n\ntoken = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n\ntweet_tokenized = token.tokenize(tweet)\n\n# STOP WORDS\n\nstopwords_english = stopwords.words('english')\ntweet_processed = []\n\nfor word in tweet_tokenized:\n    if (word not in stopwords_english and\n       word not in string.punctuation):\n        \n        tweet_processed.append(word)\n        \n# stemming \ntweet_stem = []\n\nstem = PorterStemmer()\n\nfor word in tweet_processed:\n    stem_word = stem.stem(word)\n    tweet_stem.append(stem_word)\n    \n    \n    \nreturn tweet_stem\n"], [], ["user_text = input()\n\n''' Type your code here. '''\nuser_text = user_text.replace(' ','')\nuser_text = user_text.replace('.','')\nuser_text = user_text.replace('!','')\nuser_text = user_text.replace(',','')\n\nprint(len(user_text))\n"], [], ["user_input = input()\n\nchar_count = 0\n\nfor value in user_input:\n    if value not in (' ', '.', ','):\n        char_count += 1\nprint(char_count)\n"], ["self.nlp = spacy.load(self.language.ISO_639_1.lower()) \n", "if self.language.ISO_639_1.lower() == 'en':\n   self.nlp = spacy.load('en_core_web_sm')\nelse:\n  self.nlp = spacy.load(self.language.ISO_639_1.lower()) \n"], [], [], [], [], [], ["sudo apt-get install sox ffmpeg libcairo2 libcairo2-dev\n\nsudo apt install libgirepository1.0-dev\n"], ["actions = ActionChains(driver)\n\ndef triple_click(element_x):\n    actions.click(element_x).click(element_x).click(element_x).perform()\n\ntriple_click(your_element)\n"], ["pip3 install mysql-connector-python==8.0.29\n"], [], ["def odd_numbers(n):\n    return [x for x in range(0, n+1) if x % 2 == 1]\n"], [], ["sudo apt-get install pkg-config libxml2-dev libxmlsec1-dev libxmlsec1-openssl\n"], ["def odd_numbers(n):\n    return [x for x in range(1,n+1) if x % 2 ==1]\n\nprint(odd_numbers(5))  # Should print [1, 3, 5]\nprint(odd_numbers(10)) # Should print [1, 3, 5, 7, 9]\nprint(odd_numbers(11)) # Should print [1, 3, 5, 7, 9, 11]\nprint(odd_numbers(1))  # Should print [1]\nprint(odd_numbers(-1)) # Should print []\n"], [], ["hashlib.new('md4', \"test\".encode()).hexdigest()\n"], ["pip3 uninstall numpy\n", "pip3 install numpy\n"], ["pip install pymongo==3.12.3\n"], [], ["sudo apt-get install libpq5=14.3-0ubuntu0.22.04.1\n"], [], ["pip install jax==0.3.13 https://whls.blob.core.windows.net/unstable/cuda111/jaxlib-0.3.7+cuda11.cudnn82-cp38-none-win_amd64.whl\n"], [], [], [], [], ["from datetime import date, timedelta\n\ndef timer():\n    global datelist\n    sdate = date(2022, 5, 1)\n    edate = date(2022, 6, 1)\n\n    delta = edate - sdate       \n    datetimes = []\n    for i in range(delta.days + 1):\n        day = sdate + timedelta(days=i)\n        datetimes.append(day)\n\n\n    def formatting():\n        global converted\n        converted = pd.to_datetime(datetimes)\n        return converted\n\n    datelist = converted.strftime(\"%Y-%m-%d\").tolist()\n\n    formatting()\n"], ["# Setting `Playing ` status\nawait bot.change_presence(activity=discord.Game(name=\"a game\"))\n\n# Setting `Streaming ` status\nawait bot.change_presence(activity=discord.Streaming(name=\"My Stream\", url=my_twitch_url))\n\n# Setting `Listening ` status\nawait bot.change_presence(activity=discord.Activity(type=discord.ActivityType.listening, name=\"a song\"))\n\n# Setting `Watching ` status\nawait bot.change_presence(activity=discord.Activity(type=discord.ActivityType.watching, name=\"a movie\"))\n"], ["python3 -m pip install --upgrade pip\npython3 -m pip uninstall awscli\npython3 -m pip install awscli\n"], [], ["python -m pip install jupyter notebook -U\n", "Failed to start the Kernel. \n\nThe 'python3912jvsc74a57bd0f40663dfee16db09be6af2ff2550db36bd5354ec153147c40353828514dedb99' kernel is not available. Please pick another suitable kernel instead, or install that kernel. \n\nView Jupyter log for further details.\n"], ["gcloud auth configure-docker us-east1-docker.pkg.dev\n"], [], [], ["user_text = input()\noutput = 0  #set variable\n\nfor char in user_text:  #similar to ZyBooks lesson 4.9\n    if not(char in \" .,\"):  #not similar to the lesson\n        output += 1 \nprint(output)\n"], ["user_text = input()\ncount = 0 \nfor char in user_text:\nif not(char in \" .,\"):\n    count += 1 \nprint(count)\n"], ["num = int(input())\n\nwhile num > 0:\n    y =(num % 2)\n    print(y, end='')\n    num = (num//2)\nprint()\n", "num = int(input(\"Enter a number\"))\nstring = \"\"\n\nwhile num > 0:\n    y =str(num % 2)\n    string+=y\n    num = (num//2)\n\nreverse=string[::-1]\nprint(reverse)\n"], [], ["from pydantic import BaseModel, create_model\nfrom typing import Optional\n\ndef make_optional(baseclass):\n    # Extracts the fields and validators from the baseclass and make fields optional\n    fields = baseclass.__fields__\n    validators = {'__validators__': baseclass.__validators__}\n    optional_fields = {key: (Optional[item.type_], None) for key, item in fields.items()}\n    return create_model(f'{baseclass.__name__}Optional', **optional_fields, __validators__=validators)\n\nclass Parent(BaseModel):\n    id: int\n    name: str\n    email: str\n\nParentUpdate = make_optional(Parent)\n", "Parent.__fields__\n\n{'id': ModelField(name='id', type=int, required=True),\n 'name': ModelField(name='name', type=str, required=True),\n 'email': ModelField(name='email', type=str, required=True)}\n\nParentUpdate.__fields__\n\n{'id': ModelField(name='id', type=Optional[int], required=False, default=None),\n 'name': ModelField(name='name', type=Optional[str], required=False, default=None),\n 'email': ModelField(name='email', type=Optional[str], required=False, default=None)}\n"], [], [], [], ["apt install libpython3.9-dev\n"], ["driver.executescript(\"return document.querySelector('dps-app').shadowRoot.querySelector('dps-navigation-header').shadowRoot.querySelector('header.dps-navigation-header dps-login').shadowRoot.querySelector('dps-button')\").Click()\n"], [], [], ["    {\n        \"version\": \"0.2.0\",\n        \"configurations\": [\n            {\n                \"name\": \"Debug test\",\n                \"type\": \"python\",\n                \"request\": \"attach\",\n                \"console\": \"externalTerminal\",\n                \"justMyCode\": false,\n                \"stopOnEntry\": true,\n                \"envFile\": \"${workspaceFolder}/.env.test\",\n                \"purpose\": [\"debug-test\"]\n            }\n        ]\n    }\n"], ["sudo apt-get install pkg-config libxml2-dev libxmlsec1-dev libxmlsec1-openssl\n"], [], [], ["class Parent(BaseModel):\n    id: int\n    name: str\n    email: str\n\nclass ParentUpdate(Parent): ## Note that this inherits 'Parent' class (not BaseModel)\n    id: Optional[int]  # this will convert id from required to optional\n"], ["payload = request.body.decode('utf-8')\n"], ["sudo apt update\nsudo apt install libpg-dev\n"], ["Raises:\n    ValueError: if `summary()` is called before the model is built.**\n"], ["jupyter labextension disable my-extension\n"], [], ["integer_input = int(input())                     # input integert number\nbinary_of_integer_input = bin(integer_input)     # convert integer to binary\nprint(binary_of_integer_input[2:])               # Print binary of integer input\nprint(binary_of_integer_input[:1:-1]) # Print reverse binary of integer input\n", "integer_input = int(input())                     # input integert number\nbinary_of_integer_input = bin(integer_input)     # convert integer to binary\nx = binary_of_integer_input[2:]\nJ = binary_of_integer_input[:1:-1]\nprint(x)                                         # Print binary of integer input\nprint(J)                                         # Print reverse binary of integer input\n"], ["step1 = input(\"what number? \")#gets your input\nstep2 = int(step1) #makes sure it's an int not float\nstep3 = bin(step2) #converts it to binairy (you method doesn't work for e.g. 7)\nstep4 = step3.replace(\"0b\", \"\") #removes 0b from the binairy number\nstep5 = step4[::-1] #reveses the string\nprint (step5)\n", "print(bin(int(input(\"what number? \"))).replace(\"0b\", \"\")[::-1])\n"], [], [">>> x = 100\n>>> res = []\n>>> while x > 0:\n...   x = x//2\n...   J = x%2\n...   res.append(J)\n...\n>>> res\n[0, 1, 0, 0, 1, 1, 0]\n>>> \"\".join(str(i) for i in res[::-1])\n'0110010'\n"], [], ["x = int(input())\n\nwhile x > 0:\n    print( x % 2, end = '')\n    x = x//2\n"], [], ["import pandas as pd\n\ndef slice_df_by(df_, slice_by=[\"Oman\", \"Nairobi\",], slice_idx='country'):\n    idxn = df_.index.names.index(slice_idx)\n    return df_.loc[tuple([slice(None)]*idxn +[slice_by] ), :]\n\ngender = tuple([\"male\", \"female\"]*6)\nthrown = tuple([\"rock\", \"scissors\", \"paper\"]*4) \ncountry = tuple([\"Nairobi\", \"Oman\", \"Djibouti\", \"Belize\"]*3) \nnames = tuple([\"Chris\", \"Pat\", \"Michele\", \"Thomy\", \"Musa\", \"Casey\"]*2)\n\ntuples = list(zip(gender, thrown, country, names))\n\nidx = pd.MultiIndex.from_tuples(tuples, \n                                names=[\"gender\", \"thrown\", \"country\", \"name\"])\n\ndf = pd.DataFrame({'Count A': [12., 70., 30., 20.]*3, \n                   'Count B': [12., 70., 30., 20.]*3}, index=idx)\n", "print(slice_df_by(df))\n\n                                 Count A  Count B\ngender thrown   country name                     \nfemale scissors Oman    Pat         70.0     70.0\n       paper    Oman    Casey       70.0     70.0\n       rock     Oman    Thomy       70.0     70.0\nmale   rock     Nairobi Chris       12.0     12.0\n       scissors Nairobi Musa        12.0     12.0\n       paper    Nairobi Michele     12.0     12.0\n", "idxz = lambda ixln=4: [chr(i) for i in np.arange(ixln)+65]\ndf.index.names = idxz(len(df.index.names))\nprint(idxz())\nOut[132]: ['A', 'B', 'C', 'D']\n"], [], [], [], [], ["load-plugins=pylint_django\ndjango-settings-module=myproject.settings\n"], ["pip3 install Django==2.1.*\n"], ["pip install tables==3.6.1\n"], [], ["import pip\npackage= ['nbconvert'] # install any package you need without any error forever\n\nfor i in package:\n    pip.main(['install', i])\n"], ["from pathlib import Path\nimport imghdr\n\ndata_dir = \"/home/user/datasets/samples/\"\nimage_extensions = [\".png\", \".jpg\"]  # add there all your images file extensions\n\nimg_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\nfor filepath in Path(data_dir).rglob(\"*\"):\n    if filepath.suffix.lower() in image_extensions:\n        img_type = imghdr.what(filepath)\n        if img_type is None:\n            print(f\"{filepath} is not an image\")\n        elif img_type not in img_type_accepted_by_tf:\n            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n"], ["powershell -ExecutionPolicy ByPass -NoExit -Command \"& 'C:\\users\\<username>\\Anaconda3\\condabin\\conda_hook.bat' ; conda activate <yourcondaEnvironment>\"\n"], [], ["@dataclass_json\n@dataclass\nclass Command:\n  param1: str\n  param2: int\n\n\n@dataclass_json\n@dataclass\nclass Result:\n  val1: str\n  val2: int\n", "def main(input: DownloadFileRequest) -> DownloadFileResponse:\n  # function code\n  result: DownloadFileResponse = DownloadFileResponse(\"some\", 123)\n  return result\n"], [], [], [], [], ["curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n"], [], [], ["class Person\n    def __init__(self, name, age):\n        self._name = name\n        self._age = age\n\n    def get_age(self):\n        return self._age\n\n    def set_age(self, new_age):\n        if isinstance(new_age, int) & new_age>0 & new_age<120:\n            self._age = new_age\n\n    def get_name(self):\n        return self._name\n    name = property(get_name)\n    age = property(get_age, set_age)\n\n    def __str__(self):\n        return 'Person[' + self.name + '] is ' + str(self.age)\n    \np1 = Person(\"Sandeep\", 49)\n"], [], ["python3.10 -m pip install ipykernel\n", "sudo apt-get install python3.10-distutils\n", "  curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10\n", " /bin/python3.10 ~/.vscode/extensions/ms-python.python-2022.0.1786462952/pythonFiles/shell_exec.py /bin/python3.10 -m pip install -U notebook /tmp/tmp-5290PWIe78U4HgLu.log\n"], [], [], ["if \"optional\" in dictionary2:\n    value1 = dictionary2[\"optional\"].get(\"value1\")\nelse:\n    value1 = \"\"\n", "from collections import defaultdict\n\ndictionary2 = {\"required\": {\"value1\": \"one\", \"value2\": \"two\"}}\nddic2 = defaultdict(dict,dictionary2)\nvalue1 = ddic2[\"optional\"].get(\"value1\")\n"], ["dictionary2 = {\"required\": {\"value1\": \"one\", \"value2\": \"two\"}}\ntry:\n    value1 = dictionary2[\"optional\"][\"value1\"]\nexcept (KeyError, AttributeError) as e:\n    value1 = \"\"\n", "def get_val(data, keys):\n    try:\n        for k in keys:\n            data = data[k]\n        return data\n    except (KeyError, AttributeError) as e:\n        return \"\"\n\ndictionary2 = {\"required\": {\"value1\": \"one\", \"value2\": \"two\"}}\nprint(get_val(dictionary2, (\"required\", \"value2\")))\nprint(get_val(dictionary2, (\"optional\", \"value1\")))\n", "two\n\n"], [], [], ["def octal_to_string(octal):\n    result = \"\"\n    value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n    # Iterate over each of the digits in octal\n    for i in [int(n) for n in str(octal)]:\n        # Check for each of the permissions values\n        for value, letter in value_letters:\n            if i >= value:\n                result += letter\n                i -= value\n            else:\n                result += \"-\"\n    return result\n    \nprint(octal_to_string(755)) # Should be rwxr-xr-x\nprint(octal_to_string(644)) # Should be rw-r--r--\nprint(octal_to_string(750)) # Should be rwxr-x---\nprint(octal_to_string(600)) # Should be rw-------\n"], [], ["user_text = input()\n\ncomma_count = 0\nperiod_count = 0\nspace_count = 0\nfor char in user_text:\n    initial_len = len(user_text)\n    if char == ',':\n        comma_count += 1\n    elif char == '.':\n        period_count += 1\n    elif char == ' ':\n        space_count += 1\nnot_allowed = (comma_count + period_count + space_count)\nprint(initial_len - not_allowed)\n"], [], [], [], [], ["options = Options()\noptions.add_argument(\"start-maximized\")\noptions.add_argument(\"disable-infobars\")\noptions.add_argument(\"--disable-extensions\")\noptions.add_argument(\"--disable-dev-shm-usage\")\noptions.add_argument(\"--no-sandbox\")\nif headless:\noptions.add_argument('--headless')\noptions.binary_location = \"/usr/bin/chromium-browser\"\nself.driver = webdriver.Chrome(options=options)\n"], ["driver = webdriver.Chrome(options=options)\n"], [], [], ["from binance.client import Client\nclient = Client()\ninfo = client.futures_exchange_info()\n\nrequestedFutures = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'DYDXUSDT']\nprint(\n    {si['symbol']:si['quantityPrecision'] for si in info['symbols'] if si['symbol'] in requestedFutures}\n)\n"], [], ["RUN sudo apt-get update; \\\n    sudo apt-get -y upgrade; \\\n    sudo apt-get install -y gnupg2 wget lsb_release \n", "RUN sudo apt-get update; \\\n    sudo apt-get -y upgrade; \\\n    sudo apt-get install -y gnupg2 wget lsb-release \n"], ["base = /home/env3/educ\nprojectname = educ\nvirtualenv = /home/env3/%(projectname)\n", "virtualenv = /home/env3/%(projectname)/env\n"], [], ["for epoch in range(2):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        if (labels==4)|(labels==3):\n\n        # zero the parameter gradients\n            optimizer.zero_grad()\n\n        # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:    # print every 2000 mini-batches\n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss / 2000))\n                running_loss = 0.0\n\nprint('Finished Training')\n"], [], [], [], [], [], ["options.setBinary(System.getProperty(\"user.home\") + \"\\\\AppData\\\\Local\\\\Chromium\\\\Application\\\\chrome.exe\");\n"], [], ["from datetime import datetime, timedelta\nfrom datetime import date\n\n\ndef date_bwn_two_dates(start_date, end_date):\n    date_list = [] # The list where we want to store\n    for i in range(int((end_date-start_date).days)+1): # Iterate between the range of dates\n        year = (start_date+timedelta(i)).strftime(\"%Y\") # Get the Year\n        month = (start_date+timedelta(i)).strftime(\"%m\") # Get the month\n        date_a = (start_date+timedelta(i)).strftime(\"%d\") # Get the day\n        date_list.append([year, month, date_a]) # Append the Objects accquired\n    return date_list # return the list\n\n\nfor i in date_bwn_two_dates(date(2020, 12, 1), date(2021, 12, 1)):\n    print(i)\n"], ["for i in range(1, len(list)):\n    if list[i-1] == list[i]:\n        print('Repeated')\n"], ["import numpy as np\nnp_arr = np.array(lst)  # don't use 'list' for your object name. \ndiffs = np.diff(np_arr)\ndiffs_indices = np.where(diffs != 0)[0]\n", ">>> diffs_indexes\narray([0, 1, 3, 4, 5])\n"], ["list = [1, 2, 1, 1, 5, 6, 1,1]\n\nfor i in range(len(list)):\n    if list[i] in list[i+1:i+2]:\n        print('repeated')\n"], ["if any(item == successor for item,successor in zip(lst,lst[1:])):\n    print('repeated')\n", "if any(duplicate for _,(_,*duplicate) in itertools.groupby(lst)):\n    print('repeated')\n", "prev = object() # non-matching initial value\nfor x in lst:\n    if prev==x:              # compare to previous\n       print('repeated')\n       break\n    prev = x                 # track previous for next iteration\n", "predecessor = iter(lst)         # iterate over items from first\nfor x in lst[1:]:               # iterate from 2nd item\n    if x == next(predecessor):  # compare to corresponding predecessor\n        print('repeated')\n        break\n"], ["for i in range(len(list) - 1):\n    if list[i] == list[i + 1]:\n        print('Repeated')\n"], [], ["list = [1, 2, 1, 1, 5, 6, 1, 1]\n\nfor i in range(len(list)):    \n    if i+1 < len(list) and list[i] == list[i+1]:\n        print('Repeated')\n"], ["for i in range(1, len(list)): \n    if list[i-1] == list[i]:\n        print('Repeated')\n"], [], ["from pathlib import Path\n\nPath('file.txt').write_text('my text')\nPath('file1.txt').write_bytes(b'my text')\n", "f = Path('file.txt')\nf.open(\"a\")\nf.write_text('my text')\n# or\nf.write_bytes(b'my text')\n", "f = Path('file1.txt').open('a')\nf.write('my text')\nf.close()\n", "fp = Path('test.txt').open('a')\n<_io.TextIOWrapper name='test.txt' mode='a' encoding='UTF-8'>\nfp.write('my text')\n\nfq = Path('test1.txt').open('ab', encoding='iso8859-1')\n<_io.TextIOWrapper name='test1.txt' mode='a' encoding='iso8859-1'>\nfq.write(b'my text')\n"], [], ["$ poetry --version\nPoetry version 1.1.11\n", "# pyproject.toml\n[tool.poetry.dependencies]\npython = \"~3.9\"\ntorch = [\n  {url = \"https://download.pytorch.org/whl/cpu/torch-1.10.0%2Bcpu-cp39-cp39-linux_x86_64.whl\", markers = \"sys_platform == 'linux'\"},\n  {url = \"https://download.pytorch.org/whl/cpu/torch-1.10.0%2Bcpu-cp39-cp39-win_amd64.whl\", markers = \"sys_platform == 'win32'\", }\n]\nnumpy = \"^1.21.4\"\n\n[build-system]\nrequires = [\"poetry-core>=1.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n", "$ python\nPython 3.9.9 (main, Nov 23 2021, 00:34:08) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n/home/redqueen/machine_learning/.venv/lib/python3.9/site-packages/torch/package/_directory_reader.py:17: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:68.)\n  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n>>> quit()\n", "$ python\nPython 3.9.9 (main, Nov 23 2021, 00:34:08) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> torch.cuda.is_available()\nFalse\n>>> quit()\n"], [], [], [], ["import hmac\nimport time\nfrom hashlib import sha256\n\nimport requests\nimport stripe\n\nwebhook_url = 'https://your-site/api/stripe/webhook/'\nstripe_secret = 'YOUR_STRIPE_WEBHOOK_SIGN_SECRET'  # whsec_..\nwebhook_json_file = 'PATH_TO_JSON_FILE_WITH_WH_DATA/webhook_example.json'\n\n\ndef generate_stripe_signature_header(payload: str):\n    timestamp_utc = int(time.time())\n    signed_payload = \"%d.%s\" % (timestamp_utc, payload)\n    v1_sig = compute_signature(signed_payload, secret=stripe_secret)\n    return f't={timestamp_utc},v1={v1_sig}'\n\n\ndef compute_signature(payload: str, secret):\n    \"\"\"Take from stripe\"\"\"\n    mac = hmac.new(\n        secret.encode(\"utf-8\"),\n        msg=payload.encode(\"utf-8\"),\n        digestmod=sha256,\n    )\n    return mac.hexdigest()\n\n\nwith open(webhook_json_file, 'r') as f:\n    payload = f.read()\n\nsignature = generate_stripe_signature_header(payload=payload)\n\nheaders = {\n    'STRIPE-SIGNATURE': signature,  \n    'Content-Type': 'application/json',\n}\n\nsession = requests.Session()\nsession.headers.update(**headers)\nresponse = session.post(webhook_url, data=payload)\n\nprint(response.content)\nprint(response.status_code)\nresponse.raise_for_status()\n", "signature = generate_stripe_signature_header(payload=payload)\nheaders = {\n    'HTTP_STRIPE-SIGNATURE': signature,  \n    'Content-Type': 'application/json',\n}\nself.client.post(webhook_url, data=payload, **headers)\n"], ["wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\nsudo apt-get update\n", "sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\n    libnvinfer-dev=6.0.1-1+cuda10.1 \\\n    libnvinfer-plugin6=6.0.1-1+cuda10.1\n"], ["from pathlib import Path\nusername = input('Enter name')\ndownload_folder = Path('C:/Users', username, 'Downloads')\n"], ["assert_used:\n  skips: [\"*/test_*.py\", \"*/test_*.py\"]\n"], [], ["objIE.ExecuteScript \"arguments[0].value = arguments[1]\", Array(objIE.FindElementById(\"sqlvalue1\"), Sheets(\"SheetName\").Range(\"A1\").Value)\n"], [], [], ["$locate uvicorn\n", "/home/username/.local/bin/uvicorn \n", "$ls -a          \n", "$nano .bashrc\n\nexport PATH:$PATH:/home/username/.local/bin\n"], [], ["UnsatisfiableError: The following specifications were found\nto be incompatible with the existing python installation in your environment:\n\nSpecifications:\n\n  - ipykernel -> python[version='>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.8,<3.9.0a0|>=3.5,<3.6.0a0|>=3.9,<3.10.0a0']\n\nYour python: python=3.10\n", "# Create virtual environment\n# Use a version of Python that is less than 3.10\nconda create --name your_env_name python<3.10\n\n# Activate new environment\nconda activate your_env_name\n\n# Install ipykernel\nconda install -c anaconda ipykernel\n\n# Add this new environment to your Jupyter Notebook kernel list\nipython kernel install --name your_env_name --user\n\n# Windows only: When trying to launch `jupyter notebook`, you may receive a win32api error.\n# The command below fixes that issue.\nconda install -c anaconda pywin32\n"], [], ["def odd_numbers(n):\n    return [x for x in range(0, n+1) if x % 2 != 0]\n", "def odd_numbers(n):\n    return [x for x in range(1, n+1) if x % 2 != 0]\n"], [" DOMAIN=example.org\n ADMIN_EMAIL=admin@${DOMAIN}\n ROOT_URL=${DOMAIN}/app\n", " from dotenv import load_dotenv, find_dotenv\n\n load_dotenv(find_dotenv())\n", " import sys\n from dotenv import load_dotenv\n load_dotenv(sys.path[1]) #try .path[0] if 1 doesn't work\n", "import sys\nfrom dotenv import load_dotenv\npath = sys.path[1]+'/config/.env'  #try .path[0] if 1 doesn't work\nload_dotenv(path)\n"], [], ["client = commands.Bot (command_prefix = \"!\" , activity = discord.Game(name=\"your help command here\")) \n"], ["ORIGINAL POSITIVE RATIO: 0.5\nFold : 0\nTRAIN POSITIVE RATIO: 0.4375\nTEST POSITIVE RATIO : 0.5714285714285714\nTRAIN GROUPS        : {1, 3, 4, 5, 6, 7, 10, 11}\nTEST GROUPS         : {2, 8, 9, 12, 13}\nFold : 1\nTRAIN POSITIVE RATIO: 0.5\nTEST POSITIVE RATIO : 0.5\nTRAIN GROUPS        : {2, 4, 5, 7, 8, 9, 11, 12, 13}\nTEST GROUPS         : {1, 10, 3, 6}\nFold : 2\nTRAIN POSITIVE RATIO: 0.5454545454545454\nTEST POSITIVE RATIO : 0.375\nTRAIN GROUPS        : {1, 2, 3, 6, 8, 9, 10, 12, 13}\nTEST GROUPS         : {11, 4, 5, 7}\n"], ["bot = commands.Bot(command_prefix=\"!\", activity=activity, status=discord.Status.idle)\n"], ["yum install libxml2-devel xmlsec1-devel xmlsec1-openssl-devel libtool-ltdl-devel\n"], [], [], ["#!/usr/bin/python3\nimport gitlab\nimport sys\n\n\ndef download_file(host, token, project_name, branch_name, file_path, output):\n    try:\n        gl = gitlab.Gitlab(host, private_token=token)\n        pl = gl.projects.list(search=project_name)\n        for p in pl:\n            if p.name == project_name:\n                project = p\n                break\n        with open(output, 'wb') as f:\n            project.files.raw(file_path=file_path, ref=branch_name, streamed=True, action=f.write)\n    except Exception as e:\n        print(\"Error:\", e)\n\n\nnum_arguments = len(sys.argv)\nif num_arguments < 6:\n    print('Usage: ./download-gitlab-file.py host token project_name branch_name file_path output')\nelse:\n    download_file(\n        sys.argv[1],\n        sys.argv[2],\n        sys.argv[3],\n        sys.argv[4],\n        sys.argv[5],\n        sys.argv[6]\n    )\n"], ["$ pip install django-crispy-forms\n\n$ pip install crispy-bootstrap5\n", "INSTALLED_APPS = [\n ...,\n'crispy_forms',\n'crispy_bootstrap5',  # Forgetting this was probably your error\n ]\n", "CRISPY_ALLOWED_TEMPLATE_PACKS = \"bootstrap5\"\nCRISPY_TEMPLATE_PACK = \"bootstrap5\"\n"], ["async def main(req: func.HttpRequest, starter: str) -> func.HttpResponse:\n    client = df.DurableOrchestrationClient(starter)\n\n    req_data = req.get_json()\n    img_url = req_data['img_url']\n    payload = {\"img_url\": img_url}\n    \n    instance_id = await client.start_new(req.route_params[\"functionName\"], None, payload)\n\n    logging.info(f\"Started orchestration with ID = '{instance_id}'.\")\n\n    return client.create_check_status_response(req, instance_id)\n", "def orchestrator_function(context: df.DurableOrchestrationContext):\n    input_context = context.get_input()\n    img_url = input_context.get('img_url')\n\n    some_response= yield context.call_activity('MyActivity', img_url)\n    \n    return [some_response]\n", "def main(imgUrl: str) -> str:\n    print(f'.... Image URL = {imgUrl}')\n\n    return imgUrl\n"], [], ["pip uninstall Pyzmq \npip install Pyzmq==19.0.2\n"], [], ["sudo apt-get remove libpg5\n\nsudo apt-get install libpg5 libpg-dev\n"], ["text = text[:15]\nprint(text)\n"], ["cd ~ && wget https://www.sqlite.org/2020/sqlite-autoconf-3320100.tar.gz && tar xvfz sqlite-autoconf-3320100.tar.gz && cd sqlite-autoconf-3320100 && ./configure && make && make install\n", "option_settings:\n  aws:elasticbeanstalk:application:environment:\n    LD_LIBRARY_PATH: \"/usr/local/lib\"\ncommands:\n  01_upgrade_sqlite:\n    command: \"cd ~ && wget https://www.sqlite.org/2020/sqlite-autoconf-3320100.tar.gz && tar xvfz sqlite-autoconf-3320100.tar.gz && cd sqlite-autoconf-3320100 && ./configure && make && make install\"\n"], [], ["def delete_row_tensor(a, del_row, device):\n    n = a.cpu().detach().numpy()\n    n = np.delete(n, del_row, 0)\n    n = torch.from_numpy(n).to(device)\n    return n\n"], [], [], [], [], [], ["st = datetime.datetime.fromtimestamp(timestamp).date()\nreturn (\"{}\".format(st));\n", "st = datetime.datetime.fromtimestamp(timestamp)  \nreturn (\"{}\".format(st.strftime(\"%Y-%m-%d\")))\n"], ["\n# pyproject.toml\n\n[tool.poetry.dependencies]\npython = \"^3.8\"\ntorch = { version = \"=1.90+cu111\", source = \"pytorch\" }\n\n[[tool.poetry.source]]\nname = \"pytorch\"\nurl = \"https://download.pytorch.org/whl/cu111/\"\nsecondary = true\n\n"], [], [], ["DatetimeIndex(['2019-03-22', '2019-03-23', '2019-03-24', '2019-03-25',\n           '2019-03-26', '2019-03-27', '2019-03-28', '2019-03-29',\n           '2019-03-30', '2019-03-31', '2019-04-01', '2019-04-02',\n           '2019-04-03', '2019-04-04', '2019-04-05', '2019-04-06',\n           '2019-04-07', '2019-04-08'],\n          dtype='datetime64[ns]', freq='D')\n"], ["echo $LD_LIBRARY_PATH #path\nsudo find /usr/ -name 'libcuda.so.*' #version\n", "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.0/compat\n"], [], ["{\n    ...,\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n", "{\n    ...,\n    \"configurations\": [\n        {\n            \"name\": \"Python: Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false\n        }\n    ]\n}\n"], ["sudo apt-get install libxmlsec1-dev pkg-config\n"], ["docker build --no-cache .\n"], [], ["pip install pysqlite3-binary\n", "__import__('pysqlite3')\nimport sys\nsys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n"], ["conda install -c anaconda networkx\n"], ["import tensorflow as tf\ntf.config.list_physical_devices()\n", "with tf.device('/device:GPU:0'):\n    model.fit(x_train, y_train)\n"], [], ["{\n  \"registry-mirrors\": [],\n  \"insecure-registries\": [],\n  \"debug\": false,\n  \"experimental\": false,\n  \"features\": {\n    \"buildkit\": true\n  },\n  \"builder\": {\n    \"gc\": {\n      \"enabled\": true,\n      \"defaultKeepStorage\": \"20GB\"\n    }\n  }\n}\n", "\"features\": {\n    \"buildkit\": true\n  },\n"], [], [], [], [], ["conda install -y networkx\">=2.5\"\n"], ["tradingPairs = ['BTCUSDT','ETHUSDT','BNBUSDT']\n\n#Loop though cryptos\n\nfor i in range(0,len(tradingPairs)):\n\ninfo = client.futures_exchange_info()\n\nif info['symbols'][0]['pair'] == tradingPairs[i]:\n\nprint(\"Price Pre \",info['symbols'][0]['pricePrecision'])\n\npricePrecision = info['symbols'][0]['pricePrecision']\nquantityS = 5.2\nquantityB = \"{:0.0{}f}\".format(quantityS, pricePrecision)\n"], ["\nawait bot.change_presence(activity=discord.Streaming(name=\"My Stream\", url=my_twitch_url))\n\nawait bot.change_presence(activity=discord.Activity(type=discord.ActivityType.listening, name=\"a song\"))\n\nawait bot.change_presence(activity=discord.Activity(type=discord.ActivityType.watching, name=\"a movie\"))```\n\n#but if you want to make the bot change status every 5 minutes try this :\n\nasync def ch_pr():\n await client.wait_until_ready()\n\n statuses = [\"Vodka Or beer? || bb:help\",f\"listening on {len(client.guilds)} server's\",\"Still need help? do bb:guide for more help!\"]\n\n while not client.is_closed():\n\n   status = random.choice(statuses)\n\n   await client.change_presence(activity=discord.Game(name=status))\n\n   await asyncio.sleep(5)\n\nclient.loop.create_task(ch_pr())\n"], [], ["[MASTER]\nload-plugins=pylint_django\ndjango-settings-module=myproject.settings\n", "[MASTER]\nload-plugins=pylint_django\ndjango-settings-module=myproject.settings\n\n[FORMAT]\nmax-line-length=120\n\n[MESSAGES CONTROL]\ndisable=missing-docstring,invalid-name\n\n[DESIGN]\nmax-parents=13\n\n[TYPECHECK]\ngenerated-members=REQUEST,acl_users,aq_parent,\"[a-zA-Z]+_set{1,2}\",save,delete\n\n"], [], [], [], [], ["def odd_numbers(n):\n    return [x for x in range(0,n+1) if x%2 > 0 ]\n"], ["conda install -c apple tensorflow-deps\npython -m pip install tensorflow-macos\npython -m pip install tensorflow-metal\n"], ["pip install opencv-python-headless\n", "cv2.error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n", "pip install opencv-contrib-python\n"], ["from scipy.optimize import linear_sum_assignment as linear_assignment\n"], [], [], [], ["import inspect   \nfrom pydantic import BaseModel   \n\n\ndef optional(*fields):\n    \"\"\"Decorator function used to modify a pydantic model's fields to all be optional.\n    Alternatively, you can  also pass the field names that should be made optional as arguments\n    to the decorator.\n    Taken from https://github.com/samuelcolvin/pydantic/issues/1223#issuecomment-775363074\n    \"\"\"   \n    def dec(_cls):\n        for field in fields:\n            _cls.__fields__[field].required = False\n        return _cls\n\n    if fields and inspect.isclass(fields[0]) and issubclass(fields[0], BaseModel):\n        cls = fields[0]\n        fields = cls.__fields__\n        return dec(cls)\n\n    return dec\n\n   \n", "@optional\nclass ParentUpdate(Parent):\n    pass\n"], ["#import os\n#os.environ[\"TF_DISABLE_MLC\"] = \"1\"\n#os.environ[\"TF_MLC_LOGGING\"] = \"1\"\nimport tensorflow as tf\nfrom tensorflow.python.compiler.mlcompute import mlcompute\n\ntf.compat.v1.disable_eager_execution()\nmlcompute.set_mlc_device(device_name='gpu')\nprint(\"is_apple_mlc_enabled %s\" % mlcompute.is_apple_mlc_enabled())\nprint(\"is_tf_compiled_with_apple_mlc %s\" % mlcompute.is_tf_compiled_with_apple_mlc())\nprint(f\"eagerly? {tf.executing_eagerly()}\")\nprint(tf.config.list_logical_devices())\n\nfrom tensorflow.keras import datasets, layers, models\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10))\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels, epochs=10,\n                    validation_data=(test_images, test_labels))\n"], [], [], ["from tensorflow import keras\nfrom keras.models import Sequential\nimport tensorflow as tf\n", "from tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\n"], ["unset PYTHONPATH\nunset PYTHONHOME\n"], ["conda install -c conda-forge networkx=2.5\n"], [], ["import pandas as pd\n\n# Create a test df\ndf = pd.DataFrame({'Name': ['Tesla','Tesla','Toyota','Ford','Ford','Ford'],\n                   'Type': ['Model X','Model Y','Corolla','Bronco','Fiesta','Mustang']})\n\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', index=False)\nworkbook = writer.book\nworksheet = writer.sheets['Sheet1']\nmerge_format = workbook.add_format({'align': 'center', 'valign': 'vcenter', 'border': 2})\n\nfor car in df['Name'].unique():\n    # find indices and add one to account for header\n    u=df.loc[df['Name']==car].index.values + 1\n\n    if len(u) <2: \n        pass # do not merge cells if there is only one car name\n    else:\n        # merge cells using the first and last indices\n        worksheet.merge_range(u[0], 0, u[-1], 0, df.loc[u[0],'Name'], merge_format)\nwriter.save()\n"], ["    \"files.associations\": {\n     \"*.html\": \"jinja-html\"\n    },\n"], [], ["print [int(s) for s in l.split(',')]\n", "print([int(s) for s in l.split(',')])\n"], ["$ pip install opencv-python opencv-python-headless\n"], [], ["def octal_to_string(octal):\n    result = \"\"\n    value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n    # Iterate over each of the digits in octal\n    for x in [int(n) for n in str(octal)]:\n        # Check for each of the permissions values\n        for value, letter in value_letters:\n            if x >= value:\n                result += letter\n                x -= value\n            else:\n                result +=\"-\"\n    return result\n    \nprint(octal_to_string(755)) # Should be rwxr-xr-x\nprint(octal_to_string(644)) # Should be rw-r--r--\nprint(octal_to_string(750)) # Should be rwxr-x---\nprint(octal_to_string(600)) # Should be rw-------\n"], ["conda install snappy\n"], ["nano /etc/apt/sources.list\n", "deb http://http.kali.org/kali kali-rolling main non-free contrib \ndeb-src http://http.kali.org/kali kali-rolling main non-free contrib\n", "apt-get update\napt-get upgrade\n", "apt-get install python-pip   #For Python2\napt-get install python3-pip  #For Python3\n"], ["df.sql(<SQL select statement>)\n"], [], ["ds.map(lambda x: tf.strings.split(x, sep='$')[1])\n"], ["user = 'Alex'\ndirToSee = fr'C:\\Users\\{user}\\Downloads'\nprint (dirToSee) # prints C:\\Users\\Alex\\Downloads\n"], ["\"python.sortImports.args\": [\"-rc\", \"--atomic\"],\n"], [], [], ["def odd_numbers(n):\n    return [x for x in range(1, n+1) if x % 2 ==1]\n"], [], [], ["bandit --configfile bandit.yaml\n", "assert_used:\n  skips: ['*_test.py', 'test_*.py']\n"], ["curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py && python get-pip.py\n"], [">>> text = \"blah \" * 20\n>>> print(text[:15])\nblah blah blah \n>>> print(text[15:])\nblah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah \n", ">>> text = \"blah \" * 20\n>>> while len(text) > 16:\n...     index = text[:16].rindex(' ') # finds the last space in the first 16 chars\n...     print(text[:index].strip()) # strip() removes trailing spaces\n...     text = text[index:] # Updates text to remove what we just printed\n... \nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\nblah blah blah\n\n>>> print(text.strip()) # Prints any remaining text after the loop\nblah blah\n"], ["text = input('Enter your text: ')\nlist_ = text.split(' ')\n\nif len(text) >= 16:\n   for i, j in zip(range(len(list_)),list_):\n      if i == len(list_)-1:\n         break\n      print(j, end=' ')\n   print('\\n'+ j)\n"], ["blah blah blah\nblah\n"], ["text = input('Enter your text: ')\nif len(text) >= 16:\n     last_space_before_16 = text[:16].rindex(' ')\n     text = text[:last_space_before_16] + '\\n' + text[last_space_before_16+1:]\n     print(f'Your new text is:\\n{text}')\n"], ["self.nlp = spacy.load(self.language.ISO_639_1.lower()) \n", "  if self.language.ISO_639_1.lower() == 'en':\n     self.nlp = spacy.load('en_core_web_sm')\n  else:\n    self.nlp = spacy.load(self.language.ISO_639_1.lower()) \n"], [], ["[tool.poetry.dev-dependencies]\npoethepoet = \"^0.10.0\"\n\n[tool.poe.tasks]\nforce-cuda11 = \"python -m pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\"\n", "poetry install\n", "poe force-cuda11  # relies on pip and use PyTorch wheels repo\n"], [], [], [], ["pip install torch==1.7.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n", "!nvcc --version\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2019 NVIDIA Corporation\n# Built on Sun_Jul_28_19:07:16_PDT_2019\n# Cuda compilation tools, release 10.1, V10.1.243\n"], ["[sdate+timedelta(days=x) for x in range((edate-sdate).days)]\n", "[datetime.date(2019, 3, 22),\n datetime.date(2019, 3, 23),\n datetime.date(2019, 3, 24),\n          :\n datetime.date(2019, 4, 7),\n datetime.date(2019, 4, 8)]\n"], ["import spacy\nfrom spacy.cli.download import download\ndownload(model=\"en_core_web_sm\")\n"], [], [], ["curl https://bootstrap.pypa.io/2.7/get-pip.py -o get-pip.py\npython get-pip.py\n", "$ pip --version    \npip 20.3.4 from /home/kali/.local/lib/python2.7/site-packages/pip (python 2.7)\n\n$ pip3 --version\npip 21.0.1 from /home/kali/.local/lib/python3.9/site-packages/pip (python 3.9)\n"], [], ["def odd_numbers(n):\n    return [x for x in range(0, n+1) if x%2 != 0]\n"], [], [], ["pip install opencv-python\n", "pip install opencv-python-headless\n"], ["def process_tweet(tweet):\n\"\"\"Process tweet function.\nInput:\n    tweet: a string containing a tweet\nOutput:\n    tweets_clean: a list of words containing the processed tweet\n\n\"\"\"\nstemmer = PorterStemmer()\nstopwords_english = stopwords.words('english')\n# remove stock market tickers like $GE\ntweet = re.sub(r'\\$\\w*', '', tweet)\n# remove old style retweet text \"RT\"\ntweet = re.sub(r'^RT[\\s]+', '', tweet)\n# remove hyperlinks\ntweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n# remove hashtags\n# only removing the hash # sign from the word\ntweet = re.sub(r'#', '', tweet)\n# tokenize tweets\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ntweet_tokens = tokenizer.tokenize(tweet)\n\ntweets_clean = []\nfor word in tweet_tokens:\n    if (word not in stopwords_english and  # remove stopwords\n            word not in string.punctuation):  # remove punctuation\n        # tweets_clean.append(word)\n        stem_word = stemmer.stem(word)  # stemming word\n        tweets_clean.append(stem_word)\n"], ["import moment\n\ndef dates_bwn_twodates(start_date, end_date):\n    diff = abs(start_date.diff(end_date).days)\n    \n    for n in range(0,diff+1):\n        yield start_date.strftime(\"%Y-%m-%d\")\n        start_date = (start_date).add(days=1)\n\nsdate = moment.date('2019-03-22')   #start date\nedate = moment.date('2019-04-09')   #end date  \n", "dates = list(dates_bwn_twodates(sdate,edate)) #dates as a list\n", "for date in dates_bwn_twodates(sdate,edate):\n    #do something with each date\n"], [], [" curl --header \"PRIVATE-TOKEN: <your_access_token>\" \"https://gitlab.example.com/api/v4/projects/13083/repository/files/path%2Fto%2Ffile%2Efoo/raw?ref=master\"\n"], ["import sys\nfrom streamlit import cli as stcli\nimport streamlit\n    \ndef main():\n# Your streamlit code\n\nif __name__ == '__main__':\n    if streamlit._is_running_with_streamlit:\n        main()\n    else:\n        sys.argv = [\"streamlit\", \"run\", sys.argv[0]]\n        sys.exit(stcli.main())\n"], [], [" python -m spacy download en_core_web_sm\n", "nlp = spacy.load(\"en_core_web_sm\")\n", "python -m spacy link en_core_web_sm en\n"], [], ["install:\n    commands:\n      - pip3 install awsebcli --upgrade\n      - eb --version\n      - pip3 install --upgrade awscli\n\n    pre_build:\n      commands:\n      - AWS_REGION=${AWS_DEFAULT_REGION}\n      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)\n      - IMAGE_VERSION=${COMMIT_HASH}\n      ...\n"], ["from pathlib import Path as p\nt1 = \"The quick brown fox\" \nt2 = \"just jumped over the fence\"\nt3 = \"to greet the lazy poodle.\"\nmypath = p(\"D:\\Try_this\")\nmyfile = p(\"fox.txt\")\nif not(mypath.is_dir()):\n    mypath.mkdir()\nwholepath = p(mypath / myfile)\nwholepath.write_text(\"\\n\".join([t1,t2,t3]))\n"], ["File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\chatterbot\\tagging.py\", line 13, in __init__\n    self.nlp = spacy.load(self.language.ISO_639_1.lower())\n"], [], ["def octal_to_string(octal):\n    result = \"\"\n    value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n    # Iterate over each of the digits in octal\n    for x in [int(n) for n in str(octal)]:\n        # Check for each of the permissions values\n        for value, letter in value_letters:\n            if x >= value:\n                result += letter\n                x -= value\n            else:\n                result += \"-\"\n    return result\n"], [], [], [], [], ["class ParentBase(BaseModel):\n    \"\"\"Shared properties.\"\"\"\n    name: str\n    email: str\n\nclass ParentCreate(ParentBase):\n    \"\"\"Properties to receive on item creation.\"\"\"\n    # dont need id here if your db autocreates it\n    pass\n\nclass ParentUpdate(ParentBase):\n    \"\"\"Properties to receive on item update.\"\"\"\n    # dont need id as you are likely PUTing to /parents/{id}\n    # other fields should not be optional in a PUT\n    # maybe what you are wanting is a PATCH schema?\n    pass\n\nclass ParentInDBBase(ParentBase):\n    \"\"\"Properties shared by models stored in DB - !exposed in create/update.\"\"\"\n    # primary key exists in db, but not in base/create/update\n    id: int                             \n\nclass Parent(ParentInDBBase):\n    \"\"\"Properties to return to client.\"\"\"\n    # optionally include things like relationships returned to consumer\n    # related_things: List[Thing]\n    pass\n\nclass ParentInDB(ParentInDBBase):\n    \"\"\"Additional properties stored in DB.\"\"\"\n    # could be secure things like passwords?\n    pass\n", "def clean_article_url(cls, v):\n    return clean_context_url(v.strip())\n\nclass MyModel(BaseModel):\n    article_url: str\n\n    _clean_url = pydantic.validator(\"article_url\", allow_reuse=True)(clean_article_url)\n"], ["from selenium import webdriver\n\noptions = webdriver.ChromeOptions() \noptions.add_argument(\"start-maximized\")\noptions.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\noptions.add_experimental_option('useAutomationExtension', False)\ndriver = webdriver.Chrome(options=options, executable_path=r'C:\\WebDrivers\\chromedriver.exe')\ndriver.get('https://developer.servicenow.com/dev.do')\nSignInButton = driver.execute_script(\"return document.querySelector('dps-app').shadowRoot.querySelector('dps-navigation-header').shadowRoot.querySelector('header.dps-navigation-header dps-login').shadowRoot.querySelector('dps-button')\")\nSignInButton.click()\n        \n"], ["$ pyenv rehash\n"], ["poetry add torch --platform linux --python \"^3.7\"\n"], [], ["@client.event\nasync def on_ready():\n    await client.change_presence(activity=discord.Streaming(name='Fortnite', url='https://www.twitch.tv/UR_TWITCH_GOES_HERE You cant do YT only Twitch.'))\n    print(\"Bot is connected to all of the available servers in the bots mainframe.\")\n"], [], ["export LD_LIBRARY_PATH=/path/of/libcuda.so.1:$LD_LIBRARY_PATH\n"], ["import boto3\nfrom botocore.config import Config\n\nconfig = Config(retries=dict(max_attempts=10))\nregion = \"us-east-1\"\n\ntranslate = boto3.client(\n    service_name=\"translate\",\n    region_name=region,\n    use_ssl=True,\n    config=config,\n)\n"], [], ["$ python3 --version\nPython 3.7.9\n$ python3\n>>> import sqlite3\n>>> sqlite3.sqlite_version\n'3.7.17'\n>>> exit()\n", "$ python3\n>>> import sqlite3\n>>> sqlite3.sqlite_version\n'3.8.11'\n>>> exit()\n"], [], ["nano /etc/apt/source.list\n\ndeb http://http.kali.org/kali kali-last-snapshot main non-free contrib\ndeb http://http.kali.org/kali kali-rolling main non-free contrib deb-src\nhttp://http.kali.org/kali kali-rolling main non-free contrib    \n", "sudo apt-get update && upgrade\n", "apt-get install python-pip or apt-get install python3-pip\n"], ["    \"emmet.includeLanguages\": { \"jinja-html\": \"html\" },\n    \"editor.defaultFormatter\": \"vscode.emmet\"\n"], [], ["$ python\nPython 3.7.3 (default, Apr 12 2019, 16:23:13) \n>>> import sqlite3\n>>> sqlite3.sqlite_version\n'3.27.2'\n", "sudo LD_RUN_PATH=/usr/local/lib ./configure --enable-optimizations\nsudo LD_RUN_PATH=/usr/local/lib make altinstall\n"], [], ["sudo apt-get install software-properties-common\nsudo apt-add-repository universe\nsudo apt-get update\nsudo apt-get install python-pip\n"], ["def process_tweet(tweet):\nstemmer = PorterStemmer()\nstopwords_english = stopwords.words('english')\ntweet = re.sub(r'\\$\\w*', '', tweet)\ntweet = re.sub(r'^RT[\\s]+', '', tweet)\ntweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\ntweet = re.sub(r'#', '', tweet)\ntokenizer = TweetTokenizer(preserve_case=False,        strip_handles=True,reduce_len=True)\ntweet_tokens = tokenizer.tokenize(tweet)\n\ntweets_clean = []\nfor word in tweet_tokens:\n    if (word not in stopwords_english and  \n            word not in string.punctuation): \n        stem_word = stemmer.stem(word)  # stemming word\n        tweets_clean.append(stem_word)\n\nreturn tweets_clean\n"], ["python -m uvicorn \n"], [], [], [], ["python -m ensurepip\n"], [], [], [], [], ["sudo apt-get install libpq5=12.5-0ubuntu0.20.04.1\n", "sudo apt-get install libpq-dev\n"], [], [], ["-->pip install p2j\n", "-->p2j myscript.py\n"], [], ["user_text = input()\nnumOfChars = 0 #number of charachters\ncount = 0 #to count index of user_text\nfor i in user_text:\n    if user_text[count] != ' ' and user_text[count] != '.' and user_text[count] != ',':\n        numOfChars += 1\n    count += 1\nprint(numOfChars)\n"], [], ["import discord\nfrom discord.ext import commands\nimport datetime\n\nfrom urllib import parse, request\nimport re\n\nbot = commands.Bot(command_prefix='prefix here', description=\"desc here\")\n\n@bot.event\nasync def on_ready():\n    await bot.change_presence(activity=discord.Streaming(name=\"to keep it a secret\", url=\"http://www.twitch.tv/dheeran2010\"))\n    print('Im Ready')\n\n\nbot.run('Token here')\n"], ["xyz@xyz-pc-ubuntu:~$ pip3 check\nqtconsole 4.7.6 requires pygments, which is not installed.\nnbconvert 5.6.1 requires pygments, which is not installed.\njupyter-console 6.2.0 requires pygments, which is not installed.\nipython 7.18.1 requires pygments, which is not installed.\n\nxyz@xyz-pc-ubuntu:~$ pip3 install pygments\nSuccessfully installed pygments-2.7.2\n"], [], [], ["pip install Django==2.1.*\n"], ["         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n", "df.loc[['a']]\n", "TypeError: Expected tuple, got str\n", "df.xs('a', level=0, axis=0, drop_level=False)\n# df.xs('a', drop_level=False)\n", "df.query(\"one == 'a'\")\n", "df[df.index.get_level_values('one') == 'a']\n# If your levels are unnamed, or if you need to select by position (not label),\n# df[df.index.get_level_values(0) == 'a']\n", "     col\ntwo     \nt      0\nu      1\nv      2\nw      3\n", "df.loc['a'] # Notice the single string argument instead the list.\n", "df.xs('a', level=0, axis=0, drop_level=True)\n# df.xs('a')\n", "v = df.loc[['a']]\nprint(v)\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n\nprint(v.index)\nMultiIndex(levels=[['a', 'b', 'c', 'd'], ['t', 'u', 'v', 'w']],\n           labels=[[0, 0, 0, 0], [0, 1, 2, 3]],\n           names=['one', 'two'])\n", "v.index = v.index.remove_unused_levels()\n", "print(v.index)\nMultiIndex(levels=[['a'], ['t', 'u', 'v', 'w']],\n           labels=[[0, 0, 0, 0], [0, 1, 2, 3]],\n           names=['one', 'two'])\n", "         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n", "df.loc[(slice(None), 't'), :]\n", "idx = pd.IndexSlice\ndf.loc[idx[:, 't'], :]\n", "df.loc(axis=0)[pd.IndexSlice[:, 't']]\n", "df.xs('t', axis=0, level=1, drop_level=False)\n", "df.query(\"two == 't'\")\n# Or, if the first level has no name, \n# df.query(\"ilevel_1 == 't'\") \n", "df[df.index.get_level_values('two') == 't']\n# Or, to perform selection by position/integer,\n# df[df.index.get_level_values(1) == 't']\n", "         col\none two     \nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n", "df.loc[['b', 'd']]\n", "items = ['b', 'd']\ndf.query(\"one in @items\")\n# df.query(\"one == @items\", parser='pandas')\n# df.query(\"one in ['b', 'd']\")\n# df.query(\"one == ['b', 'd']\", parser='pandas')\n", "df[df.index.get_level_values(\"one\").isin(['b', 'd'])]\n", "         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n", "df.loc[pd.IndexSlice[:, ['t', 'w']], :] \n", "items = ['t', 'w']\ndf.query(\"two in @items\")\n# df.query(\"two == @items\", parser='pandas') \n# df.query(\"two in ['t', 'w']\")\n# df.query(\"two == ['t', 'w']\", parser='pandas')\n", "df[df.index.get_level_values('two').isin(['t', 'w'])]\n", "         col\none two     \nc   u      9\n", "df.loc[('c', 'u'), :]\n", "df.loc[pd.IndexSlice[('c', 'u')]]\n", "PerformanceWarning: indexing past lexsort depth may impact performance.\n", "df_sort = df.sort_index()\ndf_sort.loc[('c', 'u')]\n", "df.xs(('c', 'u'))\n", "df.query(\"one == 'c' and two == 'u'\")\n", "m1 = (df.index.get_level_values('one') == 'c')\nm2 = (df.index.get_level_values('two') == 'u')\ndf[m1 & m2]\n", "         col\none two     \nc   u      9\na   w      3\n", "df.loc[[('c', 'u'), ('a', 'w')]]\n# df.loc[pd.IndexSlice[[('c', 'u'), ('a', 'w')]]]\n", "cses = [('c', 'u'), ('a', 'w')]\nlevels = ['one', 'two']\n# This is a useful check to make in advance.\nassert all(len(levels) == len(cs) for cs in cses) \n\nquery = '(' + ') or ('.join([\n    ' and '.join([f\"({l} == {repr(c)})\" for l, c in zip(levels, cs)]) \n    for cs in cses\n]) + ')'\n\nprint(query)\n# ((one == 'c') and (two == 'u')) or ((one == 'a') and (two == 'w'))\n\ndf.query(query)\n", "df[df.index.droplevel(unused_level).isin([('c', 'u'), ('a', 'w')])]\n", "         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    t      8\nd   t     12\n", "pd.concat([\n    df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]\n])\n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n    t      0   # Does this look right to you? No, it isn't!\nb   t      4\n    t      8\nd   t     12\n", "v = pd.concat([\n        df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]\n])\nv[~v.index.duplicated()]\n", "df.query(\"one == 'a' or two == 't'\")\n", "m1 = (df.index.get_level_values('one') == 'a')\nm2 = (df.index.get_level_values('two') == 't')\ndf[m1 | m2] \n", "         col\none two     \na   u      1\n    v      2\nb   u      5\n    v      6\nd   w     11\n    w     15\n", "keys = [('a', 'u'), ('a', 'v'), ('b', 'u'), ('b', 'v'), ('d', 'w')]\ndf.loc[keys, :]\n", "pd.concat([\n     df.loc[(('a', 'b'), ('u', 'v')), :], \n     df.loc[('d', 'w'), :]\n   ], axis=0)\n", "         col\none two     \nb   7      4\n    9      5\nc   7     10\nd   6     11\n    8     12\n    8     13\n    6     15\n", "df2.query(\"two > 5\")\n", "df2[df2.index.get_level_values('two') > 5]\n", "np.random.seed(0)\nmux3 = pd.MultiIndex.from_product([\n        list('ABCD'), list('efgh')\n], names=['one','two'])\n\ndf3 = pd.DataFrame(np.random.choice(10, (3, len(mux))), columns=mux3)\nprint(df3)\n\none  A           B           C           D         \ntwo  e  f  g  h  e  f  g  h  e  f  g  h  e  f  g  h\n0    5  0  3  3  7  9  3  5  2  4  7  6  8  8  1  6\n1    7  7  8  1  5  9  8  9  4  3  0  3  5  0  2  3\n2    8  1  3  3  3  7  0  1  9  9  0  4  7  3  2  7\n", " df3.loc[:, ....] # Notice how we slice across the index with `:`. \n", " df3.loc[:, pd.IndexSlice[...]]\n", " df.loc[:, {condition}] \n", " df3.T.query(...).T\n"], ["sudo apt update       \nsudo apt install python3   \nsudo apt install python3-pip  \npython3 -m pip install -r \n"], ["if torch.cuda.is_available:\n  print('GPU available')\nelse:\n  print('Please set GPU via Edit -> Notebook Settings.')\n"], [], [], [], [], [], ["dataset_full = torchvision.datasets.FashionMNIST(data_folder, train = True, download = True, transform = transforms.ToTensor())\n# Selecting classes 7, 2, 5 and 6\nidx = (dataset_full.targets==7) | (dataset_full.targets==2) | (dataset_full.targets==5) | (dataset_full.targets==6)\ndataset_full.targets = dataset_full.targets[idx]\ndataset_full.data = dataset_full.data[idx]\n"], ["#include <iostream>\n#include <string>\nusing namespace std;\n\nint main() {\n   string userText;\n   int i = 0;\n   string str1;\n   string str2=\"\";\n   \n   getline(cin, userText);  // Gets entire line, including spaces. \n\n   while(i<userText.size()){\n      if(userText.at(i) != ' ' && userText.at(i) !=',' && userText.at(i) !='.'){\n         str1 = userText.at(i);\n         str2 = str2 + str1;\n      }\n   ++i;\n   }\n   cout << str2.size()<<endl;\n      \n\n   return 0;\n}\n"], ["idx = dataset.train_labels == 1\ndataset.train_labels = dataset.train_labels[idx]\n"], ["def octal_to_string(octal):\n    result = \"\"\n    value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n    # Iterate over each of the digits in octal\n    for y in [int(n) for n in str(octal)]:\n        # Check for each of the permissions values\n        for value, letter in value_letters:\n            if y >= value:\n                result += letter\n                y-= value\n            else:\n                result+=\"-\"\n    return result\n"], ["pip install --upgrade pip setuptools wheel\n"], ["model = Model(some-input, some-output, \"model-name\")\n", "model = Model(some-input, some-output, name=\"model-name\")\n"], [" apt-get install libpq==12.4-0ubuntu0.20.04.1\n"], ["fig.set_constrained_layout_pads(w_pad=4./72., h_pad=4./72.,\n            hspace=0./72., wspace=0./72.)\n", "''' Here is the modified code '''\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport matplotlib.gridspec as gridspec\nimport numpy as np\n\n\nfig = plt.figure(constrained_layout=True, figsize=(11, 8.5))\nfig.set_constrained_layout_pads(w_pad=2./12., h_pad=4./12.,\n            hspace=0., wspace=0.)\npage_grid = gridspec.GridSpec(nrows=2, ncols=1, figure=fig)\n\nfig.suptitle(\"My BIG Label\", fontweight=\"bold\", fontsize=\"x-large\", y=0.98)\n\n\n# this doesn't appear to do anything with constrained_layout=True\npage_grid.update(left=0.2, right=0.8, bottom=0.2, top=0.8)\n\ntop_row_grid = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=page_grid[0])\nfor i in range(3):\n    ax = fig.add_subplot(top_row_grid[:, i], aspect=\"equal\")\n\nn_bottom_row_plots = 10\nqc_grid = gridspec.GridSpecFromSubplotSpec(1, n_bottom_row_plots, subplot_spec=page_grid[1])\nfor i, metric in enumerate(range(n_bottom_row_plots)):\n    ax = fig.add_subplot(qc_grid[:, i])\n    plt.plot(np.arange(5), np.arange(5))\n\n\n# this ruins the constrained layout\n# plt.subplots_adjust(left=0.2,right=0.8, bottom=0.2, top=0.8)\n\nfig.savefig(\"temp.png\", facecolor=\"coral\")\n"], ["wget https://kojipkgs.fedoraproject.org//packages/sqlite/3.8.11/1.fc21/x86_64/sqlite-3.8.11-1.fc21.x86_64.rpm\n\nsudo yum install sqlite-3.8.11-1.fc21.x86_64.rpm\n"], ["from sklearn.model_selection import GroupShuffleSplit\ngroups = df.groupby('label')\nall_train = []\nall_test = []\nfor group_id, group in groups:\n    # if a group is already taken in test or train it must stay there\n    group = group[~group['groups'].isin(all_train+all_test)]\n    # if group is empty \n    if group.shape[0] == 0:\n        continue\n    train_inds, test_inds = next(GroupShuffleSplit(\n        test_size=valid_size, n_splits=2, random_state=7).split(group, groups=group['groups']))\n\n    all_train += group.iloc[train_inds]['groups'].tolist()\n    all_test += group.iloc[test_inds]['groups'].tolist()\n\n\n\ntrain= df[df['groups'].isin(all_train)]\ntest= df[df['groups'].isin(all_test)]\n\nform_train = set(train['groups'].tolist())\nform_test = set(test['groups'].tolist())\ninter = form_train.intersection(form_test)\n\nprint(df.groupby('label').count())\nprint(train.groupby('label').count())\nprint(test.groupby('label').count())\nprint(inter) # this should be empty\n"], ["fig = plt.figure(figsize=(11, 8.5), facecolor='coral')\n# you code already has this\nleft, right, bottom, top = [0.1, 0.95, 0.1, 0.5]\n# You can specify wspace and hspace to hold axes labels and some other texts.\nwspace = 0.25\nhspace = 0.1\n\nnrows=1\nncols=3\ngs1 = fig.add_gridspec(nrows=1, ncols=3, left=left, right=right, bottom=0.6, \ntop=0.9, wspace=wspace, hspace=hspace)\naxes1 = [fig.add_subplot(gs1[row, col]) for row in range(nrows) for col in \nrange(ncols)]\n\nnrows=1\nncols=10\n# this grid have larger wspace than gs1\ngs2 = fig.add_gridspec(nrows=1, ncols=ncols, left=left, right=right, \nbottom=0.1, top=top, wspace=0.6, hspace=hspace)\naxes2 = [fig.add_subplot(gs2[row, col]) for row in range(nrows) for col in \nrange(ncols)]\n"], [], ["import os\nimport datetime\n\ndef file_date(filename):\n  # Create the file in the current directory\n  with open(filename, \"w+\") as file:\n    pass\n  timestamp = os.path.getmtime(filename)\n  tm = datetime.datetime.fromtimestamp(timestamp).date()\n  # Convert the timestamp into a readable format, then into a string\n  \n  # Return just the date portion \n  return (\"{}\".format(tm))\n\nprint(file_date(\"newfile.txt\")) \n# Should be today's date in the format of yyyy-mm-dd\n"], [], [], [], [], [], ["Description Limit\nCharacter encoding  UTF-8\nMaximum document size (UTF-8 characters)    5,000 bytes\n", "Description Limit\nCharacter encoding  UTF-8\nMaximum number of characters per document   1,000,000\nMaximum size per document   20 MB\nMaximum number of documents in batch    1,000,000\nMaximum size of total documents in batch    5 GB\nMaximum number of parallel batch translation jobs   10\n", "response = client.start_text_translation_job(\n    JobName='string',\n    InputDataConfig={\n        'S3Uri': 'string',\n        'ContentType': 'string'\n    },\n    OutputDataConfig={\n        'S3Uri': 'string'\n    },\n    DataAccessRoleArn='string',\n    SourceLanguageCode='string',\n    TargetLanguageCodes=[\n        'string',\n    ],\n    TerminologyNames=[\n        'string',\n    ],\n    ClientToken='string'\n)\n"], ["from selenium import webdriver\nfrom selenium.webdriver.common.action_chains import ActionChains\n\ndriver = webdriver.Chrome()\nactions = ActionChains(driver)\nfor i in range(3):\n    actions.click()\n    actions.perform()\n    print('click')\n", "from selenium import webdriver\nfrom selenium.webdriver.common.action_chains import ActionChains\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://en.wikipedia.org/wiki/Home\")\nactions = ActionChains(driver)\n# first element and last element in the paragraph\nstart = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div/div[1]')\nend = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div/div[4]')\n\nactions.drag_and_drop(start, end).perform()\n"], ["def  js_triple_click(element, deltaY = 60, offsetX = 0, offsetY = 0):\n    driver.execute_script(\"\"\"\n      \"var target = arguments[0];                                 \" +\n      \"var offsetX = arguments[1];                                \" +\n      \"var offsetY = arguments[2];                                \" + \n      \"var rect = target.getBoundingClientRect();                 \" +\n      \"var cx = rect.left + (offsetX || (rect.width / 2));        \" +        \n      \"var cy = rect.top + (offsetY || (rect.height / 2));        \" +\n      \"                                                           \" +\n      \"emit('mousedown', {clientX: cx, clientY: cy, buttons: 1}); \" +\n      \"emit('mouseup',   {clientX: cx, clientY: cy});             \" +\n      \"emit('mousedown', {clientX: cx, clientY: cy, buttons: 1}); \" +\n      \"emit('mouseup',   {clientX: cx, clientY: cy});             \" +\n      \"emit('mousedown', {clientX: cx, clientY: cy, buttons: 1}); \" +\n      \"emit('mouseup',   {clientX: cx, clientY: cy});             \" +\n      \"emit('click',     {clientX: cx, clientY: cy, detail: 3});  \" +\n      \"                                                           \" +\n      \"function emit(name, init) {                                \" +\n    \"target.dispatchEvent(new MouseEvent(name, init));        \" +\n      \"}                                                          \" ;\n    \"\"\")\n\nelement = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.TAG_NAME, \"p\"))) # replace the locator as per your usecase\nActionChains(driver).move_to_element(element).perform()\njs_triple_click(element)\nprint(\"Tripple click performed\")\n", "Tripple click performed\n"], ["from selenium.webdriver.common.action_chains import ActionChains\n", "times = 3\nwhile(times >0):\n            ActionChains(driver).click().perform()\n            times -= 1;\n"], ["from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pyautogui\n\n\ndriver = webdriver.Firefox(executable_path=r'C:\\\\Path\\\\To\\\\Your\\\\geckodriver.exe')\ndriver.get('https://stackoverflow.com/questions/63253535/how-to-triple-click-on-python-to-select-a-paragraph')\ndriver.maximize_window()\n\ntest_paragraph = WebDriverWait(driver, 10).until(\n    EC.element_to_be_clickable((By.XPATH, \"//p[contains(text(), 'Someone please tell me a way to triple-click ')]\")))\n\n# import time\n# time.sleep(3)\npanel_height = driver.execute_script('return window.outerHeight - window.innerHeight;')\nabs_x = test_paragraph.location['x']\ny = test_paragraph.location['y']\nabs_y = y + panel_height\nprint(\"Absolute x : \" + str(abs_x))\nprint(\"Absolute y : \" + str(abs_y))\n\npyautogui.moveTo(abs_x + 10, abs_y)\npyautogui.click(clicks=3)\n"], ["nano /etc/apt/sources.list\n", "deb http://http.kali.org/kali kali-rolling main non-free contrib\ndeb-src http://http.kali.org/kali kali-rolling main non-free contrib\n", "apt-get update\napt-get upgrade \napt-get install python-pip #or python3-pip\n", "pip --version #or pip3\n"], [], [], ["assert get_origin(Dict[str, int]) is dict\nassert get_args(Dict[int, str]) == (int, str)\n\nassert get_origin(Union[int, str]) is Union\nassert get_args(Union[int, str]) == (int, str)\n", "def is_optional(field):\n    return typing.get_origin(field) is Union and \\\n           type(None) in typing.get_args(field)\n"], [], [], ["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nclass ThreeLayerMLP(keras.Model):\n\n  def __init__(self, name=None):\n    super(ThreeLayerMLP, self).__init__(name=name)\n    self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')\n    self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')\n    self.pred_layer = layers.Dense(10, name='predictions')\n\n  def call(self, inputs):\n    x = self.dense_1(inputs)\n    x = self.dense_2(x)\n    return self.pred_layer(x)\n\ndef get_model():\n  return ThreeLayerMLP(name='3_layer_mlp')\n\nmodel = get_model()\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\n\nmodel.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=keras.optimizers.RMSprop())\n\nmodel.summary() # This will throw an error as follows\n# ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.\n\n# Need to run with real data to infer shape of different layers\nhistory = model.fit(x_train, y_train,\n                    batch_size=64,\n                    epochs=1)\n\nmodel.summary()\n"], ["def octal_to_string(octal):\n result = \"\"\n value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n # Iterate over each of the digits in octal\n for i in [int(n) for n in str(octal)]:\n    # Check for each of the permissions values\n    for value, letter in value_letters:\n        if i >= value:\n            result += letter\n            i -= value\n        else:\n            result += '-'\n return result\n"], [], ["driver.set_page_load_timeout(120)\n"], ["import re\nimport sys\nfrom streamlit.cli import main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(main())\n", "import sys\nfrom streamlit import cli as stcli\n\nif __name__ == '__main__':\n    sys.argv = [\"streamlit\", \"run\", \"APP_NAME.py\"]\n    sys.exit(stcli.main())\n"], [], ["cd /\ncd etc/apt/\nsudo nano sources.list\n", "deb http://http.debian.net/debian jessie-backports main\ndeb http://http.kali.org/kali kali-rolling main contrib non-free\n", "sudo apt upgrade\nsudo apt install python-pip\n"], [], ["mv <file>.py <file>.ipynb\n"], [], [], [], [], [], ["pytest --collect-only\n", "addopts= --cov <path> -ra\n"], [], [], ["def octal_to_string(octal):\n    permission = [\"---\", \"--x\", \"-w-\", \"-wx\", \"r--\", \"r-x\", \"rw-\", \"rwx\"]\n    result = \"\"\n    # Iterate over each of the digits in octal\n    for ___ in [int(n) for n in str(octal)]:\n        result += permission[___]\n    return result\n\nprint(octal_to_string(755)) \nprint(octal_to_string(644)) \nprint(octal_to_string(750)) \nprint(octal_to_string(600)) \n", "def octal_to_string(octal):\n   result = \"\"\n   value_letters = [(4,\"r\"),(2,\"w\"),(1,\"x\")]\n   # Iterate over each of the digits in octal\n   for ___ in [int(n) for n in str(octal)]:\n      # Check for each of the permissions values\n      for value, letter in value_letters:\n          if ___ >= value:\n               result += letter\n               ___ -= value\n          else:\n               result += \"-\"\n   return result\n\nprint(octal_to_string(755))\nprint(octal_to_string(644))\nprint(octal_to_string(750))\nprint(octal_to_string(600))\n"], ["{\n    \"python.pythonPath\": \"${workspaceFolder}/code/venv/bin/python\"\n}\n"], ["Type: AWS::Serverless::Function\n    Properties:\n       Events:\n         ProxyResource:\n           Type: Api\n           Properties:\n             RestApiId: ...\n             Path: /{proxy+}\n             Method: ANY\n         RootResource:\n           Type: Api\n           Properties:\n             RestApiId: ...\n             Path: /\n             Method: ANY\n"], ["from datetime import datetime\n\ntimestamp = 1545730073\ndatetimeobj = datetime.fromtimestamp(timestamp).date()\n", "datetime.date(2018, 12, 25)\n", "datetimeobj.strftime(\"%Y-%m-%d\")\n//'2018-12-25'\n"], ["from scikitplot.metrics import plot_roc_curve\n"], [], [], [], [], ["sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\n    libnvinfer-dev=6.0.1-1+cuda10.1 \\\n    libnvinfer-plugin6=6.0.1-1+cuda10.1\n"], ["import pandas as pd\n\n# Create a test df\ndf = pd.DataFrame({'Name': ['Tesla','Tesla','Toyota','Ford','Ford','Ford'],\n                   'Type': ['Model X','Model Y','Corolla','Bronco','Fiesta','Mustang']})\n\n# Create the list where we 'll capture the cells that appear for 1st time,\n# add the 1st row and we start checking from 2nd row until end of df\nstartCells = [1]\nfor row in range(2,len(df)+1):\n    if (df.loc[row-1,'Name'] != df.loc[row-2,'Name']):\n        startCells.append(row)\n\n\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', index=False)\nworkbook = writer.book\nworksheet = writer.sheets['Sheet1']\nmerge_format = workbook.add_format({'align': 'center', 'valign': 'vcenter', 'border': 2})\n\n\nlastRow = len(df)\n\nfor row in startCells:\n    try:\n        endRow = startCells[startCells.index(row)+1]-1\n        if row == endRow:\n            worksheet.write(row, 0, df.loc[row-1,'Name'], merge_format)\n        else:\n            worksheet.merge_range(row, 0, endRow, 0, df.loc[row-1,'Name'], merge_format)\n    except IndexError:\n        if row == lastRow:\n            worksheet.write(row, 0, df.loc[row-1,'Name'], merge_format)\n        else:\n            worksheet.merge_range(row, 0, lastRow, 0, df.loc[row-1,'Name'], merge_format)\n\n\nwriter.save()\n"], [], ["root_dir = '/content/drive/My Drive/DeepCID/model_cnn'\ndatafile3 = 'Xscale.npy'\n\ni=0\nfor (root, dirs, files) in os.walk(root_dir):\n    for d in dirs:\n        if not d.startswith('.'):\n            dir_path = os.path.join(root, d)\n            file_path = os.path.join(dir_path, datafile3)\n            Xscale = np.load(file_path)\n", "from pathlib import Path\n\nroot_dir = '/content/drive/My Drive/DeepCID/model_cnn'\ndatafile3 = 'Xscale.npy'\n\ni=0\nfor (root, dirs, files) in os.walk(root_dir):\n    for d in dirs:\n        if not d.startswith('.'):\n            fp = Path(root) / d / datafile3\n            Xscale = np.load(str(fp))\n"], ["import tensorflow as tf\n\ntf.enable_eager_execution()\n\ndef load_audio_file(file_path):\n    # you should decode bytes type to string type\n    print(\"file_path: \",bytes.decode(file_path),type(bytes.decode(file_path)))\n    return file_path\n\ntrain_dataset = tf.data.Dataset.list_files('clean_4s_val/*.wav')\ntrain_dataset = train_dataset.map(lambda x: tf.py_func(load_audio_file, [x], [tf.string]))\n\nfor one_element in train_dataset:\n    print(one_element)\n\nfile_path:  clean_4s_val/1.wav <class 'str'>\n(<tf.Tensor: id=32, shape=(), dtype=string, numpy=b'clean_4s_val/1.wav'>,)\nfile_path:  clean_4s_val/3.wav <class 'str'>\n(<tf.Tensor: id=34, shape=(), dtype=string, numpy=b'clean_4s_val/3.wav'>,)\nfile_path:  clean_4s_val/2.wav <class 'str'>\n(<tf.Tensor: id=36, shape=(), dtype=string, numpy=b'clean_4s_val/2.wav'>,)\n", "InvalidArgumentError: TypeError: descriptor 'decode' requires a 'bytes' object but received a 'tensorflow.python.framework.ops.EagerTensor'\n"], ["export TF_CPP_MIN_LOG_LEVEL=\"2\"\n", "2020-04-10 10:04:13.365696: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n"], [], [], ["# X is a train dataset with features excluding a target variable\n\ninput_shape = X.shape  \nmodel.build(input_shape) \nmodel.summary()\n"], [], ["conda install pytables\n"], [], ["switch = True\nepoch = 0\nwait = 0\nprevious = 10E+10\nwhile switch:\n    history = model.fit( X, y, batch_size=1, epochs=step, verbose=False )\n    epoch += step\n    current = history.history[\"loss\"][-1]\n    if current >= previous:\n        wait += 1\n        if wait >= tolerance:\n            switch = False\n    else:\n        wait = 0\n    if epoch >= max_epochs:\n        switch = False\n    previous = current\n"], [], [], [], [], ["wget -O - http://updates.atomicorp.com/installers/atomic |sh\nyum install  atomic-sqlite\nLD_LIBRARY_PATH='/opt/atomicorp/atomic/root/usr/lib64/' python3\n>>> import sqlite3\n>>> sqlite3.sqlite_version\n'3.8.5'\n"], ["    async sendKeysWithEmojis(element, text) {\n        const script = `var elm = arguments[0],\n        txt = arguments[1];elm.value += txt;\n        elm.dispatchEvent(new Event('keydown', {bubbles: true}));\n        elm.dispatchEvent(new Event('keypress', {bubbles: true}));\n        elm.dispatchEvent(new Event('input', {bubbles: true}));\n        elm.dispatchEvent(new Event('keyup', {bubbles: true}));`;\n        await this.driver.executeScript(script, element, text);\n    }\n"], [], [], [], [], ["#!/usr/bin/env python3\n\nfrom PIL import Image\n\n# Create new black image of entire board\nw, h = 12, 6\nimg = Image.new(\"RGB\", (w,h))\npixels = img.load()\n\n# Make pixels white where (row+col) is odd\nfor i in range(w):\n    for j in range(h):\n        if (i+j)%2:\n            pixels[i,j] = (255,255,255)\n\nimg.save('result.png')\n", "img = img.resize((15*w,15*h), Image.NEAREST)\n"], [], ["poetry add https://download.pytorch.org/whl/torch_stable.html/cpu/torch-1.3.1%2Bcpu-cp36-cp36m-win_amd64.whl\n", "[tool.poetry.dependencies]\ntorch = { url = \"https://download.pytorch.org/whl/torch_stable.html/cpu/torch-1.3.1%2Bcpu-cp36-cp36m-win_amd64.whl\" }\n"], ["from datetime import date, timedelta\n\nsdate = date(2019,3,22)   # start date\nedate = date(2019,4,9)   # end date\ndate_modified=sdate\nlist=[sdate] \n\n\nwhile date_modified<edate:\n    date_modified+=timedelta(days=nbDaysbtw2dates) \n    list.append(date_modified)\n\nprint(list) \n"], ["print([str(d) for d in dates_bwn_twodates(sdate,edate)])\n"], ["python --version\n"], [], [], [], [], ["conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n"], [], ["activity = discord.Game(name=\"Just\")\nawait client.change_presence(status=discord.Status.idle, activity=activity)\n"], ["D:\\Programme\\Python\\Python37\nD:\\Programme\\Python\\Python37\\Scripts\n"], ["/home/vhawk19/.local/bin/unvicorn\n", "export PATH=$PATH:$HOME/.local/bin\n"], ["    index = pd.MultiIndex.from_product([['a','b'],\n                               ['stock1','stock2','stock3'],\n                               ['price','volume','velocity']])\n\n    df = pd.DataFrame([1,2,3,4,5,6,7,8,9,\n                      10,11,12,13,14,15,16,17,18], \n                       index)\n\n                        0\n    a stock1 price      1\n             volume     2\n             velocity   3\n      stock2 price      4\n             volume     5\n             velocity   6\n      stock3 price      7\n             volume     8\n             velocity   9\n    b stock1 price     10\n             volume    11\n             velocity  12\n      stock2 price     13\n             volume    14\n             velocity  15\n      stock3 price     16\n             volume    17\n             velocity  18\n", "    df.xs(('stock1', 'velocity'), level=(1,2))\n\n        0\n    a   3\n    b  12\n", "   df.iloc[df.index.isin(['stock1'], level=1) & \n           df.index.isin(['velocity'], level=2)] \n\n                        0\n    a stock1 velocity   3\n    b stock1 velocity  12\n", "    df.iloc[df.index.isin(['stock1','stock3'], level=1) & \n            df.index.isin(['velocity'], level=2)] \n\n                        0\n    a stock1 velocity   3\n      stock3 velocity   9\n    b stock1 velocity  12\n      stock3 velocity  18\n"], ["pip install --user tables\n"], ["sqlite3 --version \n", "$python\n>>> import sqlite3\n>>> sqlite3.sqlite_version\n"], ["name = input(\"What is your name? \")\nprint(r\"C:\\Users\\{name}\\Downloads\".format(name=name))\n"], [], ["pip3.7 install requests\n"], [], ["C:\\Python35\\Scripts\\pyrcc5 -o C:\\Desktop\\labelImg\\libs\\resources.py C:\\Desktop\\labelImg\\resources.qrc\n"], ["with my_path.open(\"a\") as f:\n    f.write(\"...\")\n", "f = my_path.open(\"a\")\nf.write(\"...\")\n"], ["volumes:\n    - /etc/localtime:/etc/localtime\n"], ["import detetime\nimport pytz\n\ndatetime.now(tz=pytz.timezone('Europe/Amsterdam'))\n\n# or datetime.now(tz=pytz.timezone(os.environ.get('TZ'))\n"], [], ["import re\nstring = \"Hello, Mr. John. Have a good day.\"\nprint(len(\"\".join(re.findall(r'[A-Z0-9a-z]', string))) ) \n"], ["import re\nstring = \"Hello, Mr. John. Have a good day.\"\nprint(len(re.sub(r'[,.\\s]+', '', string)))\n"], ["In [8]: import re                                                               \n\nIn [9]: string = \"Hello, Mr. John. Have a good day.\"                                     \n\nIn [10]: new_str = re.sub('[ .,]', '', string)                                  \n\nIn [11]: len(new_str)                                                           \nOut[11]: 23\n"], ["print(len(string.replace(\",\", \"\").replace(\".\", \"\").replace(\" \",\"\")))\n"], ["model = Sequential()\nmodel.add(Bidirectional(LSTM(n_hidden,return_sequences=False, dropout=0.25, \nrecurrent_dropout=0.1),input_shape=(n_steps,dim_input)))\n"], ["cd ~\nwget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tar.xz\ntar xJf Python-3.7.3.tar.xz\ncd Python-3.7.3\n./configure\nmake && make install\nexport PATH=$HOME/opt/python-3.7.3/bin:$PATH\n", "python3 --version\nPython 3.7.3 \n", "$ cd ~\n$ wget https://www.sqlite.org/2019/sqlite-autoconf-3290000.tar.gz\n$ tar zxvf sqlite-autoconf-3290000.tar.gz\ncd sqlite-autoconf-3290000\n\n$./configure --prefix=$HOME/opt/sqlite\n$ make && make install\n", "export PATH=$HOME/opt/sqlite/bin:$PATH\nexport LD_LIBRARY_PATH=$HOME/opt/sqlite/lib\nexport LD_RUN_PATH=$HOME/opt/sqlite/lib\n", "sqlite3 --version \n3.29.0 2019-07-10 17:32:03\n", "python3.7 -m pip3 install virtualenv\n\n(myvenv37)[me@test my_project]$ python3.7 -m pip3 install django\nSuccessfully installed django-2.2.3 pytz-2019.1 sqlparse-0.3.0\n", "(myvenv37)[me@test my_project]$ python3.7 manage.py runserver \n", "(venv)[me@test my_project]$ python3.7 manage.py migrate\n"], ["curl https://gitlab.com/api/v4/projects/:id/repository/files/:filename\\?ref\\=:ref\n", "curl https://gitlab.com/api/v4/projects/12949323/repository/files/.gitignore\\?ref\\=master\n"], ["def StratifiedGroupShuffleSplit(df_main):\n\n    df_main = df_main.reindex(np.random.permutation(df_main.index)) # shuffle dataset\n\n    # create empty train, val and test datasets\n    df_train = pd.DataFrame()\n    df_val = pd.DataFrame()\n    df_test = pd.DataFrame()\n\n    hparam_mse_wgt = 0.1 # must be between 0 and 1\n    assert(0 <= hparam_mse_wgt <= 1)\n    train_proportion = 0.6 # must be between 0 and 1\n    assert(0 <= train_proportion <= 1)\n    val_test_proportion = (1-train_proportion)/2\n\n    subject_grouped_df_main = df_main.groupby(['subject_id'], sort=False, as_index=False)\n    category_grouped_df_main = df_main.groupby('category').count()[['subject_id']]/len(df_main)*100\n\n    def calc_mse_loss(df):\n        grouped_df = df.groupby('category').count()[['subject_id']]/len(df)*100\n        df_temp = category_grouped_df_main.join(grouped_df, on = 'category', how = 'left', lsuffix = '_main')\n        df_temp.fillna(0, inplace=True)\n        df_temp['diff'] = (df_temp['subject_id_main'] - df_temp['subject_id'])**2\n        mse_loss = np.mean(df_temp['diff'])\n        return mse_loss\n\n    i = 0\n    for _, group in subject_grouped_df_main:\n\n        if (i < 3):\n            if (i == 0):\n                df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n            elif (i == 1):\n                df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n            else:\n                df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n\n        mse_loss_diff_train = calc_mse_loss(df_train) - calc_mse_loss(df_train.append(pd.DataFrame(group), ignore_index=True))\n        mse_loss_diff_val = calc_mse_loss(df_val) - calc_mse_loss(df_val.append(pd.DataFrame(group), ignore_index=True))\n        mse_loss_diff_test = calc_mse_loss(df_test) - calc_mse_loss(df_test.append(pd.DataFrame(group), ignore_index=True))\n\n        total_records = len(df_train) + len(df_val) + len(df_test)\n\n        len_diff_train = (train_proportion - (len(df_train)/total_records))\n        len_diff_val = (val_test_proportion - (len(df_val)/total_records))\n        len_diff_test = (val_test_proportion - (len(df_test)/total_records)) \n\n        len_loss_diff_train = len_diff_train * abs(len_diff_train)\n        len_loss_diff_val = len_diff_val * abs(len_diff_val)\n        len_loss_diff_test = len_diff_test * abs(len_diff_test)\n\n        loss_train = (hparam_mse_wgt * mse_loss_diff_train) + ((1-hparam_mse_wgt) * len_loss_diff_train)\n        loss_val = (hparam_mse_wgt * mse_loss_diff_val) + ((1-hparam_mse_wgt) * len_loss_diff_val)\n        loss_test = (hparam_mse_wgt * mse_loss_diff_test) + ((1-hparam_mse_wgt) * len_loss_diff_test)\n\n        if (max(loss_train,loss_val,loss_test) == loss_train):\n            df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n        elif (max(loss_train,loss_val,loss_test) == loss_val):\n            df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n        else:\n            df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n\n        print (\"Group \" + str(i) + \". loss_train: \" + str(loss_train) + \" | \" + \"loss_val: \" + str(loss_val) + \" | \" + \"loss_test: \" + str(loss_test) + \" | \")\n        i += 1\n\n    return df_train, df_val, df_test\n\ndf_train, df_val, df_test = StratifiedGroupShuffleSplit(df_main)\n"], ["def split(df, based_on='subject_id', cv=5):\n    splits = []\n    based_on_uniq = df[based_on]#set(df[based_on].tolist())\n    based_on_uniq = np.array_split(based_on_uniq, cv)\n    for fold in based_on_uniq:\n        splits.append(df[df[based_on] == fold.tolist()[0]])\n    return splits\n\n\nif __name__ == '__main__':\n    df = pd.DataFrame([{'note_id': 1, 'subject_id': 1, 'category': 'test1', 'note': 'test1'},\n                       {'note_id': 2, 'subject_id': 1, 'category': 'test2', 'note': 'test2'},\n                       {'note_id': 3, 'subject_id': 2, 'category': 'test3', 'note': 'test3'},\n                       {'note_id': 4, 'subject_id': 2, 'category': 'test4', 'note': 'test4'},\n                       {'note_id': 5, 'subject_id': 3, 'category': 'test5', 'note': 'test5'},\n                       {'note_id': 6, 'subject_id': 3, 'category': 'test6', 'note': 'test6'},\n                       {'note_id': 7, 'subject_id': 4, 'category': 'test7', 'note': 'test7'},\n                       {'note_id': 8, 'subject_id': 4, 'category': 'test8', 'note': 'test8'},\n                       {'note_id': 9, 'subject_id': 5, 'category': 'test9', 'note': 'test9'},\n                       {'note_id': 10, 'subject_id': 5, 'category': 'test10', 'note': 'test10'},\n                       ])\n    print(split(df))\n"], ["    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n\n", "filepath = self.filepath.format(epoch=epoch + 1, **logs)\n"], ["from typing import *\na = Optional[int]\n\nfrom typedload import typechecks\ntypechecks.is_union(a) and type(None) in typechecks.uniontypes(a)\n"], [], ["import re\nfrom typing import Optional\n\nfrom dataclasses import dataclass, fields\n\n\n@dataclass(frozen=True)\nclass TestClass:\n    required_field_1: str\n    required_field_2: int\n    optional_field: Optional[str]\n\n\ndef get_optional_fields(klass):\n    class_fields = fields(klass)\n    for field in class_fields:\n        if (\n            hasattr(field.type, \"__args__\")\n            and len(field.type.__args__) == 2\n            and field.type.__args__[-1] is type(None)\n        ):\n            # Check if exactly two arguments exists and one of them are None type\n            yield field.name\n\n\nprint(list(get_optional_fields(TestClass)))\n"], [], ["const { exec } = require('child_process');\n\nif(typeof isNewContainer === 'undefined'){\n     const isNewContainer = true \n\n    // run a shell script, in javascript we use shell exec and \n    // then have a callback for when it exits so the execution is non blocking and allows \n    // the code below to execute.\n    exec('./script.sh & sleep 1 && kill -- -$(pgrep script.sh)', (err, stdout, stderr) => {\n    // close db connections\n   }\n\n\n}\n\n// handle the request\n\n", "#!/bin/bash\nexitCallback() {\n    trap - SIGTERM # clear the trap\n    kill -- -$$ # Sends SIGTERM to child/sub processes\n}\n\ntrap exitCallback SIGTERM\n\nsleep infinity\n"], [], [], ["\"justMyCode\": false\n"], [], [], [], ["# see current envs\nconda info -e\n\n# make new environment, feel free to add your version of python with python=3.7 handle\nconda create -n test\n\nactivate test\n\nconda list   #This should appear empty\nconda install jupyter  #y to install everything.\n\njupyter notebook  #launch jupyter notebook\n"], [], ["data= [['SiteUrl','Url','Title'],['SiteUrl','Url','Title']]\ndef recursive_apply(x, f=lambda v: v.lower()):\n    if type(x) is list: \n        return [recursive_apply(el) for el in x]\n    return f(x)\nrecursive_apply(data)   \n"], ["data= [['SiteUrl','Url','Title'],['SiteUrl','Url','Title']]\n\ndata = list(map(lambda x: list(map(lambda y: y.lower(), x)), data))\nprint(data)\n", "[['siteurl', 'url', 'title'], ['siteurl', 'url', 'title']]\n"], ["data = [[x.casefold() for x in sublst] for sublst in data]\n", "data = [list(map(str.casefold, x)) for x in data]\n"], ["data = [[string.lower() for string in sublist] for sublist in data]\n"], ["import numpy as np\nfrom PIL import Image\nn = 50 # size of one element, row = 8*n, chessboard = 8*n x 8*n\n\nsegment_black = np.zeros(shape = [n,n])\nsegment_white = np.ones(shape = [n,n])*255\nchessboard = np.hstack((segment_black,segment_white))\nfor i in range(4):\n    chessboard = np.hstack((chessboard,segment_black))\n    chessboard = np.hstack((chessboard,segment_white))\ntemp = chessboard\nfor i in range(7):\n    chessboard = np.concatenate((np.fliplr(chessboard),temp))\nimg = Image.fromarray(chessboard.astype(np.uint8))\nimg.save('chess.jpg')\nimg.show()\n"], ["from PIL import Image\n\nsize = 16\nimg = Image.new(\"RGB\", (size,size), \"white\") # create a new 15x15 image\npixels = img.load() # create the pixel map\n\nblack_2 = []\nfor i in range(img.size[0]):\n    if i % 2 == 0:\n        black_2.append(i)\n\nblack_1 = [i-1 for i in black_2 if i > 0]\nif img.size[0] % 2 == 0: # 'that' if statement\n    black_1.append(img.size[0]-1)\n\n\nfor i in black_1:\n    for j in range(0, size, 2):\n        pixels[i,j] = (0,0,0)\n\nfor k in black_2:\n    for l in range(1, size+1, 2):\n        pixels[k,l] = (0,0,0)\n\nimg.show()\n"], ["for n in range(15/2):\n    img.paste(img, (0, n*2))\n"], ["{\n    \"python.linting.enabled\": false,\n    \"python.unitTest.unittestEnabled\": false,\n    \"python.unitTest.nosetestsEnabled\": false,\n    \"python.unitTest.pyTestEnabled\": true,\n    \"python.pythonPath\": \"venv3/bin/python\"\n}\n"], ["map(int, l.split(','))\n", "l='2,3,4,5,6'\nprint(map(int, l.split(',')))\n", "[2, 3, 4, 5, 6]\n"], [], ["list(map(int,[i for i in l if i.isdigit()]))\n"], ["list_of_numbers = [int(i) for i in l.split(\",\")]\n", "import ast\ns = \"[\" + l + \"]'\nlist_of_numbers = ast.literal_eval(s)\n"], ["l='2,3,4,5,6'\n\nresult = [int(i) for i in l.split(',')]\nprint(result)\n", "[2, 3, 4, 5, 6]\n", "result = []\nfor i in l.split(','):\n    result.append(i)\n", "l = '2,3,4,5,6'\nresult = list(map(int, l.split(',')))\nprint(result)\n", "[2, 3, 4, 5, 6]\n"], ["\"debugOptions\": [\"DebugStdLib\"]\n"], [], []]