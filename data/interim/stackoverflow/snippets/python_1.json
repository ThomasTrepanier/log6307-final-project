[["wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.20_amd64.deb\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.20_amd64.deb\n"], ["def is_hardlink(path):\n    try:\n        os.readlink(path)\n        return True\n    except:\n        return False\n"], [], [], [], ["pip install \"Flask==1.1.4\"\n", "pip install \"werkzeug==1.0.1\"\n"], ["python3 -m pip uninstall flask\npython3 -m pip install flask==1.1.4\n"], ["from ._compat import text_type on original flask-script file\n", "from flask_script._compat import text_type\n", "pip install flask==1.1.4\n", "pip install markupsafe==2.1.0\n", "pip install flask_script\npip install flask_bootstrap\n"], ["from flask_script import Manager\n", "manager = Manager(app)\n"], [], ["from ._compat import text_type` \n", "from flask_script._compat import text_type\n"], ["@dataclass\nclass Foo:\n    a: Dict = field(default_factory=dict)\n"], ["pip install \"setuptools<58.0.0\" wheel\n"], ["from django.db.models import Func, FloatField\n\nquery = MapPoint.objects.annotate(\n    lat=Func(\"p\", function=\"ST_Y\", output_field=FloatField()),\n    lon=Func(\"p\", function=\"ST_X\", output_field=FloatField()),\n).values()\n\nfor place in query:\n    print(place)\n# {'id': 1, 'lat': 52.018948, 'lon': 8.819528}\n# {'id': 2, 'lat': 53.692242, 'lon': 9.373505}\n# ... \n"], ["SignalName = Signal(providing_args=[\"request\", \"user\"])\n", "SignalName = Signal([\"request\", \"user\"])\n"], [], ["pip install \"setuptools<58.0.0\" wheel --no-cache-dir\n", "pip install django-celery\n"], ["# transfer model layer\nlstm_layer = modelTL.layers[0]  \n\n# kernel non-trainable\nlstm_layer.cell.kernel.trainable = False\n"], [], ["from werkzeug.urls import url_encode\nImportError: cannot import name 'url_encode' from 'werkzeug.urls'\n", "pip install Werkzeug --upgrade\n", "pip install werkzeug==0.16.0\npip install Werkzeug --upgrade\npip install -U Flask-WTF\n"], ["def computeInt(nNum: int, nTimes: int) -> int:\n    result = 0\n    # how many times to multiple\n    for i in range(0, nTimes):\n        # result adding in each loop the iterator = 5 + n * str(5)\n        # first is 5, second is 55, third is 555 the result is adding all of that as integer\n        result += int(str(nNum) + i * str(nNum))\n    return result\n\n\nprint(computeInt(5, 3))\n"], [" row_exists = (s == dfObj).all(axis=1).any()\n"], [], [], [], [".python-version\n", ".python-version filter=smudgeScript\n"], [], [], ["class EditForm(forms.Form):\n\n    body = forms.CharField(widget=forms.Textarea())\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.fields['body'].widget.attrs['rows'] = 3\n", "class EditForm(forms.Form):\n\n    # your fields defined here followed by Meta\n\n    class Meta:\n        fields = ['title', 'body' ]\n        widgets = {\n            'body': forms.Textarea(attrs={'rows': 3}),\n        }\n"], [], [], [], ["file = open('../files/temp.txt')\n", "    \"configurations\": [\n        \n        {\n            \"name\": \"Python: File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${workspaceFolder}\"\n        },\n\n    ]\n}\n", "{\n    \"configurations\": [\n        \n        {\n            \"name\": \"Python: File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${workspaceFolder}/src\"\n        },\n\n    ]\n}\n"], ["import re, logging\nlogger = logging.getLogger(__name__)\ndef singularize(plural:str) -> str:\n    '''\n    If I can make a reaonable guess at a singular form, return it\n\n    :param plural: Candidate plural like 'Philadelphia 76ers'\n    :return: Singular form if calculated, or argument\n    '''\n    match plural:\n        case ies if form := re.fullmatch(r'(?i)(\\w+)ies', ies):\n            # gravies\n            singular = f\"{form.group(1)}y\"\n        case oes if form := re.fullmatch(r'(?i)(\\w+o)es', oes):\n            # potatoes\n            singular = form.group(1)\n        case xim if form := re.fullmatch(r'(?i)(\\w+)im', xim):\n            # chaverim\n            singular = form.group(1)\n        case xes if form := re.fullmatch(r'(?i)(\\w+)es', xes):\n            #glasses\n            singular = form.group(1)\n        case xxs if form := re.fullmatch(r'(?i)(\\w*[A-RT-Z]s)', xxs):\n            # books\n            singular = form.group(1)\n        case ata if form := re.fullmatch(r'(?i)(\\w+at)a', ata):\n            # data\n            singular = f\"{form.group(1)}um\"\n        case xxi if form := re.fullmatch(r'(?i)(\\w+)i', xxi):\n            # illuminati\n            singular = f\"{form.group(1)}us\"\n        case xae if form := re.fullmatch(r'(?i)(\\w+a)e', xae):\n            # alumnae\n            singular = form.group(1)\n        case default:\n            # all sorts of plural forms not covered, but I'm really just trying to convert labels to label\n            singular = default\n    logger.debug(f\"{singular} singularized from {plural}\")\n    return singular\n"], [], ["import torch\n\ndef main()\n    # do everything here\n\nif __name__ == '__main__':\n    main()\n"], [], ["wget https://www.openssl.org/source/openssl-1.1.1o.tar.gz\ntar -zxvf openssl-1.1.1o.tar.gz\ncd openssl-1.1.1o\n./config\nmake\nmake test\nsudo make install\nfind / -name libssl.so.1.1\nln -s /usr/local/lib64/libssl.so.1.1  /usr/lib64/libssl.so.1.1\nln -s /usr/local/lib64/libssl.so.1.1  /usr/lib/libssl.so.1.1\nfind / -name libcrypto.so.1.1\nln -s /home/ubuntu/openssl-1.1.1o/libcrypto.so.1.1    /usr/lib64/libcrypto.so.1.1\nln -s /home/ubuntu/openssl-1.1.1o/libcrypto.so.1.1     /usr/lib/libcrypto.so.1.1\n"], [], [], ["def first_bad_pair(s):\nlens=len(s)\nfor i in range (0,lens-1):\n    if s[i] >= s[i+1]:\n        return i\nreturn -1\n\n\ndef solution(sequence):\n    if len(sequence) == 2:\n        return True\n    seqe=sequence\n    j = first_bad_pair(seqe)\n    if j == -1:\n        return True\n    if j == len(seqe) - 1:\n        return True\n    if j == len (seqe) - 2:\n            return True\n    if first_bad_pair(seqe[j-1:j]+seqe[j+1:]) == -1:\n        return True\n    if first_bad_pair(seqe[j:j+1]+seqe[j+2:]) == -1:\n        return True\n    return False \n"], [], [], ["brew install python-certifi\n"], ["Access Key: minioadmin\nSecret Key: minioadmin\n"], [], [], [], [], ["HKEY_LOCAL_MACHINE\\SOFTWARE\\Policies\\Microsoft\\Edge\\URLAllowlist\\1=ms-word:*\n"], [], [], [], [], [], [], ["!pip install langchain\n"], [], [], [], [], [], ["ext install ms-python.python\n"], [], ["[info] Formatting requested before server has started.\n", "BlackFormatter: Restart Server\n"], [], ["from typing_extensions import Annotated\n\nfrom pydantic import BaseModel, ConfigDict\nfrom pydantic.functional_validators import AfterValidator\n\nfrom bson.objectid import ObjectId\n\n\ndef object_id_validate(v: ObjectId | str) -> ObjectId | str:\n    assert ObjectId.is_valid(v), f'{v} is not a valid ObjectId'\n    if isinstance(v, str):\n        return ObjectId(v)\n    return str(v)\n\n\nPyObjectId = Annotated[ObjectId | str, AfterValidator(object_id_validate)]\n\n\nclass MyModel(BaseModel):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    user_id: PyObjectId\n\n\nprint(MyModel(user_id=str(ObjectId()))) # user_id=ObjectId('653087c8c8640ef5700a1bb5')\nprint(MyModel(user_id=ObjectId())) # user_id='653087c8c8640ef5700a1bb6'\n"], ["eval `ssh-agent -s`\nssh-add - <<< '${{ secrets.PRIVATE_SSH_KEY }}'\n", "- uses: webfactory/ssh-agent@v0.7.0\n   with:\n     ssh-private-key: ${{ secrets.PRIVATE_SSH_KEY }}\n", "-e git+git@github.com:ORG/B.git#egg=B\n", "-run: pip install -r requirements.txt\n", "- run: git clone git@github.com:ORG/B.git\n- run: pip install -e B\n"], [], [], [], ["$ pip uninstall werkzeug\n$ pip install werkzeug==2.3.0\n"], [], ["apps -|\n      |- reports -|\n                  |- tests -|\n                            |- test_file.py\n", "apps -|\n      |- reports -|\n                  |- tests -|\n                            |- reports -|\n                                        |- test_file.py\n"], ["ln -s /opt/homebrew/Cellar/mysql/8.1.0/lib/libmysqlclient.22.dylib /opt/homebrew/Cellar/mysql/8.1.0/lib/libmysqlclient.21.dylib\n", "export DYLD_LIBRARY_PATH=\"/opt/homebrew/Cellar/mysql/8.1.0/lib:$DYLD_LIBRARY_PATH\"\n"], [], ["import openpyxl\nimport pandas as pd\n\nwith open(filePath,'rb') as fid:\n    DataFrame = pd.read_excel(fid,\"sheetName\")\ndataWorkbook = openpyxl.load_workbook(filePath)\ndataSheet = dataWorkbook[\"sheetName\"]\n\n--> Logic for editing data here\n\n#Iterate over dataframe to write to the format in openpyxl\nfor col, header in enumerate(DataFrame):\n    for row in range(len(DataFrame)):\n        cellRef = dataSheet.cell(row=row+2,column=col+1) #2: OpenPyXl does not track headers internally 1:Indexing starts at 1 in excel\n        cellRef.value = DataFrame.loc[row,header]\ndataWorkbook.save(filePath)\n"], [], ["pip install qiskit qiskit-aer\n", "from qiskit_aer.aerprovider import AerSimulator\n"], [], [], [], ["openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n", "You've reached your usage limit. See your usage dashboard and billing settings for more details. If you have further questions, please contact us through our help center at help.openai.com.\n"], [], [], [], [], ["pip install aes-pkcs5\n"], ["async def get_graphql_schema(endpoint, api_key):\n    headers = {\"X-API-KEY\": api_key}\n    transport = AIOHTTPTransport(url=endpoint, headers=headers)\n    async with Client(transport=transport, fetch_schema_from_transport=True) as session:\n        query_intros = get_introspection_query(descriptions=True)\n        query = gql(query_intros)\n        intros_result = await session.execute(query)\n        schema = build_client_schema(intros_result)\n        return schema\n\ndef save_schema_to_json(schema):\n    schema_dict = introspection_from_schema(schema)\n    output_file = 'schema.json'\n    with open(output_file, 'w') as json_file:\n        dump(schema_dict, json_file, indent=2)\n\nschema = asyncio.run(get_graphql_schema(env_dev['url'], env_dev['key']))\nsave_schema_to_json(schema)\n"], [], ["display(HTML('<section><h1>Hello</h1><p>Text</p></section>'))\n", "display(HTML('<table><tr><td>Hello</td></tr></table>'))\n"], ["\"\n/*textarea {\nheight: 60vh;\nwidth: 90%;\nresize: none;\nmargin-top: 10px;\n}*/\n\"\n", "class NewPageForm(forms.Form):\ntitle = forms.CharField(label=\"New Title\",widget=forms.TextInput(attrs={'name':'title'}))\ncontent = forms.CharField(label=\"Content:\",widget=forms.Textarea(attrs={'name':'content','style':'width: 90%; height: 60vh; resize: none; margin-top: 10px;'}))   \n"], [], ["integer = [1,3,2,4]\nfor i in range(len(integer[1:])):\n    print(integer[i])\n", "l = [1,2,3,4]\nfor i, e in enumerate(l[1:]):\n    # i is the index, e is the actual element\n    print(e)\n"], ["integer = [1,3,2,4]\nfor i in range(1,len(integer)):\n    print (integer[i])\n", "3\n2\n4\n"], ["true_dist = torch.zeros_like(pred)\ntrue_dist.scatter_(1, target.data.unsqueeze(1), 1 - self.smoothing)\ntrue_dist += self.smoothing / num_classes\n", "true_dist = torch.zeros_like(pred)\ntrue_dist.fill_(self.smoothing / (num_classes - 1))\ntrue_dist.scatter_(1, target.data.unsqueeze(1), 1 - self.smoothing)\n"], ["# pip install yolosplitter\nfrom yolosplitter import YoloSplitter\n\nys = YoloSplitter(imgFormat=['.jpg', '.jpeg', '.png'], labelFormat=['.txt'] )\n\n# If you have yolo-format dataset already on the system\n# ratio=(train,val,test) \ndf = ys.from_yolo_dir(input_dir=\"yolo_dataset\",ratio=(0.7,0.2,0.1))\n \n# If you have mixed Images and Labels in the same directory\n# ratio=(train,val,test) \ndf = ys.from_mixed_dir(input_dir=\"mydataset\",ratio=(0.7,0.2,0.1))\n\n# To save dataset with  'data.yaml' file\nys.save_split(output_dir=\"potholes\")    \n\n# show dataframe\nys.show_dataframe\n\n# show files which have error\nys.show_errors\n"], [], ["print(instance.model_dump(mode=\"json\"))\n", "def model_dump_json(self, ...) -> str:\n   \"\"\"\n   previously `json()`, arguments as above\n   effectively equivalent to `json.dump(self.model_dump(..., mode='json'))`,\n   but more performant\n   \"\"\"\n"], [], [], [], ["class Model(BaseModel):\n    the_id: UUID = Field(default_factory=uuid4)\n\nprint(json.loads(Model().json()))\n", "{'the_id': '4c94e7bc-78fe-48ea-8c3b-83c180437774'}\n", "orjson.loads(Model().json())\n", "mode: Literal['json', 'python'] | str = 'python'\n", "class Model(BaseModel):\n    the_id: UUID = Field(default_factory=uuid4)\n\nprint(Model().model_dump(mode='json'))\n# {'the_id': '4c94e7bc-78fe-48ea-8c3b-83c180437774'}\n"], [], [], ["            if flask_version.startswith(\"2.\"):\n                if python == \"3.7\":\n                    session.install(\"werkzeug==2.2.3\")\n                else:\n                    session.install(\"werkzeug==2.3.7\")\n"], [], ["bcrypt==4.0.1\nblinker==1.6.2\nclick==8.1.7\ndnspython==2.4.2\nemail-validator==2.0.0.post2\nFlask==3.0.0\nFlask-Bcrypt==1.0.1\nFlask-Login==0.6.2\nFlask-SQLAlchemy==3.1.1\nFlask-WTF==1.2.0\ngreenlet==2.0.2\nidna==3.4\nitsdangerous==2.1.2\nJinja2==3.1.2\nMarkupSafe==2.1.3\nSQLAlchemy==2.0.21\ntyping_extensions==4.8.0\nWerkzeug==3.0.0\nWTForms==3.0.1\n", "bcrypt==4.0.1\nblinker==1.6.2\nclick==8.1.7\ndnspython==2.4.2\nemail-validator==2.0.0.post2\nFlask==2.3.0\nFlask-Bcrypt==1.0.1\nFlask-Login==0.6.2\nFlask-SQLAlchemy==3.1.1\nFlask-WTF==1.2.1\ngreenlet==2.0.2\nidna==3.4\nitsdangerous==2.1.2\nJinja2==3.1.2\nMarkupSafe==2.1.3\nSQLAlchemy==2.0.21\ntyping_extensions==4.8.0\nWerkzeug==2.3.0\nWTForms==3.0.1\n"], [], [], [], [], [], ["import gdown\nurl='0B1lRQVLFjBRNR3Jqam1menVtZnc'\noutput='letter.pdf'\ngdown.download(url, output, quiet=False)\n"], [], ["\"python.formatting.provider\": \"black\",\n\"editor.formatOnSave\": true,\n", "$ python\nPython 3.7.3 (default, Mar 27 2019, 22:11:17) \n[GCC 7.3.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import black\n>>> \n"], [">>> import math\n>>> math.sqrt(9)\n3.0\n", ">>> 9 ** (1/2)\n3.0\n>>> 9 ** .5  # Same thing\n3.0\n>>> 2 ** .5\n1.4142135623730951\n", ">>> 8 ** (1/3)\n2.0\n>>> 125 ** (1/3)\n4.999999999999999\n", ">>> (-25) ** .5  # Should be 5j\n(3.061616997868383e-16+5j)\n>>> 8j ** .5  # Should be 2+2j\n(2.0000000000000004+2j)\n", ">>> import cmath\n>>> cmath.sqrt(-25)\n5j\n>>> cmath.sqrt(8j)\n(2+2j)\n", ">>> n = 10**30\n>>> x = n**2\n>>> root = x**.5\n>>> root == n\nFalse\n>>> root - n  # how far off are they?\n0.0\n>>> int(root) - n  # how far off is the float from the int?\n19884624838656\n", ">>> decimal.Decimal('9') ** .5\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for ** or pow(): 'decimal.Decimal' and 'float'\n>>> decimal.Decimal('9') ** decimal.Decimal('.5')\nDecimal('3.000000000000000000000000000')\n"], [], [], [], ["git+https://YOUR_TOKEN_HERE@github.com/ORG/REPO_NAME.git@master#egg=REPO_NAME\n"], ["from typing import Annotated\nfrom fastapi import Depends\nfrom pydantic import BaseModel, Field\n\nclass Model(BaseModel):\n   query_param1: str = Field(...)\n   query_param2: int | None = Field(None)\n\n\n@app.get(\"\")\nasync def _(query_params: Model = Depends()):\n        ...\n"], ["conda install -c conda-forge mysqlclient\n"], ["[requires]\npython_version = \"3.7\"\n"], ["app = Flask(__name__)\napp.config['JSON_SORT_KEYS'] = False\n", "app.json.sort_keys = False\n"], [], ["rm -rf $(pyenv root)\n"], [], ["import pandas as pd\npd.DataFrame.iteritems = pd.DataFrame.items\n"], [], ["wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n"], ["pip install selenium --upgrade\n", "browser_version = driver.capabilities['browserVersion']\ndriver_version = driver.capabilities['chrome']['chromedriverVersion'].split(' ')[0]\n\nprint (\"Selenium version:\", selenium.__version__)\nprint(\"browser version\", browser_version)\nprint(\"driver version\", driver_version)\n"], [], ["from msedge.selenium_tools import Edge\nfrom msedge.selenium_tools import EdgeOptions\n\n# EDIT ME : CONFIGURE YOUR msedgedriver.exe location, download from : [https://msedgewebdriverstorage.z22.web.core.windows.net/?prefix=113.0.1774.57/][1]\nedge_driver = r'C:\\\\Users\\<<LOCAL_FOLDER_PATH>>\\\\msedgedriver'\n\nedge_options = EdgeOptions()\nedge_options.use_chromium = True\n\n# EDIT ME : Change USER_NAME, or point it to Edge User Data folder ( This is to re-use existing saved options )\nedge_options.add_argument(\"user-data-dir=C:\\\\Users\\\\<<USER_NAME>>\\\\AppData\\\\Local\\\\Microsoft\\\\Edge\\\\User Data\")\nedge_options.add_argument(\"profile-directory=Default\")\nedge_options.add_argument(\"--no-sandbox\")\nedge_options.add_argument(\"--disable-dev-shm-usage\")\n\nedge_capabilities = edge_options.capabilities\nedge_capabilities[\"platform\"] = \"ANY\"\nedge_capabilities.update(edge_options.to_capabilities())\n\ndriver = Edge(executable_path=edge_driver, options=edge_options)\n"], [], ["Name: langchain\nVersion: 0.0.220\nSummary: Building applications with LLMs through composability\nHome-page: https://www.github.com/hwchase17/langchain\nAuthor: \nAuthor-email: \nLicense: MIT\nLocation: /home/anaconda3/lib/python3.9/site-packages\nRequires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\nRequired-by: jupyter_ai, jupyter_ai_magics\n", "python3.10 -m pip install langchain   \n", "Name: langchain\nVersion: 0.0.264\nSummary: Building applications with LLMs through composability\nHome-page: https://www.github.com/hwchase17/langchain\nAuthor: \nAuthor-email: \nLicense: MIT\nLocation: /home/.local/lib/python3.10/site-packages\nRequires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\nRequired-by: \n"], [], [], [], [], ["import urllib.request\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\nfrom webdriver_manager.chrome import ChromeDriverManager\n\ntry:\n    service = Service(ChromeDriverManager().install())\nexcept ValueError:\n    latest_chromedriver_version_url = \"https://chromedriver.storage.googleapis.com/LATEST_RELEASE\"\n    latest_chromedriver_version = urllib.request.urlopen(latest_chromedriver_version_url).read().decode('utf-8')\n    service = Service(ChromeDriverManager(version=latest_chromedriver_version).install())\n\noptions = Options()\noptions.add_argument('--headless') #optional.\ndriver = webdriver.Chrome(service=service, options=options)\ndriver.get(url)\n"], [], ["pip install --upgrade webdriver-manager\n"], ["from pydantic import TypeAdapter\n\nusers = [\n    {\"name\": \"user1\", \"age\": 15}, \n    {\"name\": \"user2\", \"age\": 28}\n]\n\nta = TypeAdapter(List[User])\nm = ta.validate_python(users)\n"], [], ["pip install --upgrade webdriver-manager\n"], ["from pydantic import BaseModel\nfrom bson.objectid import ObjectId as BsonObjectId\n\n\nclass PydanticObjectId(BsonObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        if not isinstance(v, BsonObjectId):\n            raise TypeError('ObjectId required')\n        return str(v)\n\n\nclass User(BaseModel):\n    who: PydanticObjectId\n\n\nprint(User(who=BsonObjectId('123456781234567812345678')))\n\n", "who='123456781234567812345678'\n"], ["from time import sleep\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\noptions = webdriver.ChromeOptions()\ndriver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), \noptions=options)\n\nurl = 'https://www.google.com/'\ndriver.get(url)\ndriver.maximize_window()\nsleep(10)\n"], [], ["mypy --ignore-missing-imports your_script.py\n"], [], ["from enum import StrEnum\n\nclass Directions(StrEnum):\n    NORTH = 'north'\n    SOUTH = 'south'\n\nprint(Directions.NORTH)\n>>> north\n", "class Directions(StrEnum):\n    NORTH = 'north',    # notice the trailing comma\n    SOUTH = 'south'\n", "from strenum import StrEnum\n\nclass URLs(StrEnum):\n    GOOGLE = 'www.google.com'\n    STACKOVERFLOW = 'www.stackoverflow.com'\n\nprint(URLs.STACKOVERFLOW)\n\n>>> www.stackoverflow.com\n"], [], [], [], [], [], ["from selenium import webdriver\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://www.crawler-test.com\")\nprint(driver.title)\n", "Crawler Test Site\n\nProcess finished with exit code 0\n"], [], [], [], ["[tool.poetry.dependencies]\npython = \"^3.11.4\"\n", "exit\n", "# rm -rf <path_to_virtual_env> \n# in my case:\nrm -rf .venv\n", "pyenv local 3.11.4\n", "poetry env use 3.11.4\n", "poetry shell\n", "python -V\n"], ["wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.19_amd64.deb\n"], ["python -m unittest discover  .\\utest\\ \n"], ["python3 -m sgqlc.introspection --exclude-deprecated --include-description ****-H \"Authorization: Bearer {TOKEN}\" http://yourgrapqlservice.com schema.json\n", "sgqlc-codegen schema schema1.json schema.py\n"], ["import enum\nfrom pydantic import BaseModel\n\n\nclass Group(enum.Enum):\n    user = 0\n    manager = 1\n    admin = 2\n\n\nclass User(BaseModel):\n    id: int\n    username: str\n    group: Group\n\n    class Config:\n        json_encoders = {Group: lambda g: g.name}\n\n\nuser = User(id=5, username=\"admin\", group=2)\nprint(user)  # id=5 username='admin' group=<Group.admin: 2>\nprint(user.json())  # {\"id\": 5, \"username\": \"admin\", \"group\": \"admin\"}\n"], [], [], [], [], [], [], ["from flask import Flask\n\napp = Flask(__name__)\napp.json.sort_keys = False\n"], ["$ mkdir $HOME/opt && cd $HOME/opt\n# Download a supported openssl version. e.g., openssl-1.1.1o.tar.gz or openssl-1.1.1t.tar.gz\n$ wget https://www.openssl.org/source/openssl-1.1.1o.tar.gz\n$ tar -zxvf openssl-1.1.1o.tar.gz\n$ cd openssl-1.1.1o\n$ ./config && make && make test\n$ mkdir $HOME/opt/lib\n$ mv $HOME/opt/openssl-1.1.1o/libcrypto.so.1.1 $HOME/opt/lib/\n$ mv $HOME/opt/openssl-1.1.1o/libssl.so.1.1 $HOME/opt/lib/\n", "export LD_LIBRARY_PATH=$HOME/opt/lib:$LD_LIBRARY_PATH\n"], ["brew install mysql\n"], ["import re\n\n\nclass RegexEqual(str):\n    def __eq__(self, pattern):\n        return bool(re.search(pattern, self))\n"], ["from django.db.models import Func, FloatField\n\n\nclass GeometryPointFunc(Func):\n    template = \"%(function)s(%(expressions)s::geometry)\"\n\n    def __init__(self, expression: Any) -> None:\n        super().__init__(expression, output_field=FloatField())\n\n\nclass Latitude(GeometryPointFunc):\n    function = \"ST_Y\"\n\n\nclass Longitude(GeometryPointFunc):\n    function = \"ST_X\"\n", "query = MapPoint.objects.annotate(lat=Latitude(\"p\"), lng=Longitude(\"p\")).values()\n"], [], ["from bson import ObjectId\nfrom pydantic import BaseModel\n\n\nclass ObjId(ObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v: str):\n        try:\n            return cls(v)\n        except InvalidId:\n            raise ValueError(\"Not a valid ObjectId\")\n\n\nclass Foo(BaseModel):\n    object_id_field: ObjId = None\n\n    class Config:\n        json_encoders = {\n            ObjId: lambda v: str(v),\n        }\n\n\n\nobj = Foo(object_id_field=\"60cd778664dc9f75f4aadec8\")\nprint(obj.dict())\n# {'object_id_field': ObjectId('60cd778664dc9f75f4aadec8')}\nprint(obj.json())\n# {'object_id_field': '60cd778664dc9f75f4aadec8'}\n", "from bson import ObjectId as BaseObjectId\n\nclass ObjectId(str):\n\"\"\"Creating a ObjectId class for pydantic models.\"\"\"\n\n    @classmethod\n    def validate(cls, value):\n        \"\"\"Validate given str value to check if good for being ObjectId.\"\"\"\n        try:\n            return BaseObjectId(str(value))\n        except InvalidId as e:\n            raise ValueError(\"Not a valid ObjectId\") from e\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n"], [], [], ["import re\nimport pandas as pd \nfrom io import StringIO\n\n\ndef read_markdown_table(table_str: str) -> pd.DataFrame:\n    \"\"\"Read markdown table from string and return pandas DataFrame.\"\"\"\n    # Ref: https://stackoverflow.com/a/76184953/\n    cleaned_table_str = re.sub(r'(?<=\\|)( *[\\S ]*? *)(?=\\|)', lambda match: match.group(0).strip(), table_str)\n    df = pd.read_table(StringIO(cleaned_table_str), sep=\"|\", header=0, skipinitialspace=True) \\\n           .dropna(axis=1, how='all') \\\n           .iloc[1:]\n    df.columns = df.columns.str.strip() \n    return df\n"], ["class Group(enum.Enum):\n    user = 0\n    manager = 1\n    admin = 2\n\n    def __repr__(self) -> str:\n        return self.name\n"], ["\"requireExactSource\": false\n", "    {\n        \"name\": \".NET Core Attach\",\n        \"type\": \"coreclr\",\n        \"request\": \"attach\",\n        \"requireExactSource\": false\n    }\n"], [], [], ["\ndef split_img_label_2(data_train,data_test,folder_train,folder_test):\n    \n    #os.mkdir(folder_train)\n    #os.mkdir(folder_test)\n    \n    \n    train_ind=list(data_train.index)\n    test_ind=list(data_test.index)\n    \n    \n    # Train folder\n    for i in tqdm(range(len(train_ind))):\n        \n        os.system('cp '+data_train[train_ind[i]]+' ./'+ folder_train + '/'  +data_train[train_ind[i]].split('/')[2])\n        os.system('cp '+data_train[train_ind[i]].split('.jpg')[0]+'.txt'+'  ./'+ folder_train + '/'  +data_train[train_ind[i]].split('/')[2].split('.jpg')[0]+'.txt')\n    \n    # Test folder\n    for j in tqdm(range(len(test_ind))):\n        \n        os.system('cp '+data_test[test_ind[j]]+' ./'+ folder_test + '/'  +data_test[test_ind[j]].split('/')[2])\n        os.system('cp '+data_test[test_ind[j]].split('.jpg')[0]+'.txt'+'  ./'+ folder_test + '/'  +data_test[test_ind[j]].split('/')[2].split('.jpg')[0]+'.txt')\n\n\nos.mkdir(folder_train)\nos.mkdir(folder_test)\nlist_folder = [folder1,folder2,.........folder40]\n\nfor folder_name in list_folder :\n\n    PATH = 'folder_name' # pass the right path\n\n    list_img=[img for img in os.listdir(PATH) if img.endswith('.jpg')==True]\n    list_txt=[img for img in os.listdir(PATH) if img.endswith('.txt')==True]\n\n    path_img=[]\n\n    for i in range (len(list_img)):\n       path_img.append(PATH+list_img[i])\n    \n    df=pd.DataFrame(path_img)\n\n    # split \n    data_train, data_test, labels_train, labels_test = train_test_split(df[0], \n    df.index, test_size=0.20, random_state=42)\n\n    # Function split \n  split_img_label_2(data_train,data_test,folder_train_name,folder_test_name)\n"], ["$ where poetry\n\n$ /opt/homebrew/bin/poetry\n", "pyenv shell python3.11\npython -m pip install poetry\npython -m poetry install\n"], [], [], ["user_list = [User(**user) for user in users]\n"], [], ["cd path/to/site-packages/problem-package/\nfind . -name '*.so'  # to see if the files exist\nfind . -name '*.so' -delete  # to delete the .so files\n"], ["class PydanticObjectId(BsonObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        if not isinstance(v, BsonObjectId):\n            raise TypeError('ObjectId required')\n        return str(v)\n    \nclass Bird(BaseModel):\n    id: PydanticObjectId = Field(..., alias=\"_id\")\n\n"], [], ["def split_img_label(data_train,data_test,folder_train,folder_test):\n    \n    os.mkdir(folder_train)\n    os.mkdir(folder_test)\n    \n    \n    train_ind=list(data_train.index)\n    test_ind=list(data_test.index)\n    \n    \n    # Train folder\n    for i in tqdm(range(len(train_ind))):\n        \n        os.system('cp '+data_train[train_ind[i]]+' ./'+ folder_train + '/'  +data_train[train_ind[i]].split('/')[2])\n        os.system('cp '+data_train[train_ind[i]].split('.jpg')[0]+'.txt'+'  ./'+ folder_train + '/'  +data_train[train_ind[i]].split('/')[2].split('.jpg')[0]+'.txt')\n    \n    # Test folder\n    for j in tqdm(range(len(test_ind))):\n        \n        os.system('cp '+data_test[test_ind[j]]+' ./'+ folder_test + '/'  +data_test[test_ind[j]].split('/')[2])\n        os.system('cp '+data_test[test_ind[j]].split('.jpg')[0]+'.txt'+'  ./'+ folder_test + '/'  +data_test[test_ind[j]].split('/')[2].split('.jpg')[0]+'.txt')\n\n", "\nimport pandas as pd \nimport os \n\nPATH = './TrainingsData/'\nlist_img=[img for img in os.listdir(PATH) if img.endswith('.jpg')==True]\nlist_txt=[img for img in os.listdir(PATH) if img.endswith('.txt')==True]\n\npath_img=[]\n\nfor i in range (len(list_img)):\n    path_img.append(PATH+list_img[i])\n    \ndf=pd.DataFrame(path_img)\n\n# split \ndata_train, data_test, labels_train, labels_test = train_test_split(df[0], df.index, test_size=0.20, random_state=42)\n\n# Function split \nsplit_img_label(data_train,data_test,folder_train_name,folder_test_name)\n\n"], [], ["import os\ndef are_hardlinked(f1, f2):\n    if not (os.path.isfile(f1) and os.path.isfile(f2)):\n        return False\n    return os.path.samefile(f1, f2) or (os.stat(f1).st_ino == os.stat(f2).st_ino)\n"], [], [], [], ["__pycache__/\n*/__pycache__/\n*\\__pycache__\\\n"], ["poetry env list\n", "deactivate\n", "poetry env remove project_name-QI_LjVaV-py3.9\n", "poetry env list\n", "which python3\n", "poetry env use /usr/bin/python3\n", "poetry env info \n", "poetry install\n"], ["import MetaTrader5 as mt5\n\n# account details\naccount1 = {\n    \"login\": 123456,\n    \"password\": \"password1\",\n    \"server\": \"MT5Server1\",\n}\n\naccount2 = {\n    \"login\": 654321,\n    \"password\": \"password2\",\n    \"server\": \"MT5Server2\",\n}\n\n# connect to MT5 servers\nmt5.initialize()\n\n# login to first account\naccount1_result = mt5.login(account1[\"login\"], account1[\"password\"], account1[\"server\"])\nif account1_result:\n    print(f\"Logged in to account {account1['login']}\")\nelse:\n    print(f\"Failed to login to account {account1['login']}: {mt5.last_error()}\")\n\n# login to second account\naccount2_result = mt5.login(account2[\"login\"], account2[\"password\"], account2[\"server\"])\nif account2_result:\n    print(f\"Logged in to account {account2['login']}\")\nelse:\n    print(f\"Failed to login to account {account2['login']}: {mt5.last_error()}\")\n\n# disconnect from MT5 servers\nmt5.shutdown()\n"], [], ["import pandas as pd\nfrom pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"Pandas to Spark DataFrame\") \\\n    .getOrCreate()\n\n# Disable Arrow optimization\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n\n# Create a pandas DataFrame\npdf = pd.DataFrame({'i': [1, 2, 3], 'j': [1, 2, 3]})\n\n# Convert pandas DataFrame to Spark DataFrame\nsdf = spark.createDataFrame(pdf)\n\n# Show the Spark DataFrame\nsdf.show()\n"], [], ["pyinstaller -F demo.py\n"], ["pip install \"setuptools<58.0.0\" \n"], ["- uses: webfactory/ssh-agent@v0.7.0\n    with:\n      ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY_VOW_SHARED }}\n", "docker build --ssh default=${SSH_AUTH_SOCK} .\n", "build:\n  ssh:\n    default: ${SSH_AUTH_SOCK}\n", "RUN mkdir -p -m 0600 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts\nRUN --mount=type=ssh pip install -r requirements.txt\n", "env:\n  DOCKER_BUILDKIT: 1\n"], [], ["import duckdb\nnrows = 10\nfile_path = 'path/to/data/parquet_file.parquet'\ndf = duckdb.query(f'SELECT * FROM \"{file_path}\" LIMIT {nrows};').df()\n", "import duckdb\nimport pyarrow.dataset as ds\nnrows = 10\ndataset = ds.dataset('path/to/data', \n                     format='parquet',\n                     partitioning='hive')\ncon = duckdb.connect()\ncon.register('data_table_name', dataset)\ndf = con.execute(f\"SELECT * FROM data_table_name LIMIT {nrows};\").df()\n"], [], [], [], ["pip uninstall poetry\npip install poetry\n"], [], [], ["wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\nsudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb\n\n"], [], [], [], ["choco install wget openssl\n"], ["import dask.dataframe as dd\ndf = dd.read_parquet(path= 'filepath').head(10)\n"], ["wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2.17_amd64.deb && sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2.16_amd64.deb\n"], ["NAME=\"myapp\"                                  # Name of the application\nDJANGODIR=/webapps/myapp             # Django project directory\nSOCKFILE=/webapps/myapp/run/gunicorn.sock  # we will communicte using this unix socket\nUSER=root                                        # the user to run as\nGROUP=root                                     # the group to run as\nNUM_WORKERS=3                                     # how many worker processes should Gunicorn spawn\nDJANGO_SETTINGS_MODULE=myapp.settings             # which settings file should Django use\nDJANGO_WSGI_MODULE=myapp.wsgi                     # WSGI module name\n\necho \"Starting $NAME as `whoami`\"\n\n# Activate the virtual environment\ncd $DJANGODIR\nsource ../bin/activate\nexport DJANGO_SETTINGS_MODULE=$DJANGO_SETTINGS_MODULE\nexport PYTHONPATH=$DJANGODIR:$PYTHONPATH\n\n# Create the run directory if it doesn't exist\nRUNDIR=$(dirname $SOCKFILE)\ntest -d $RUNDIR || mkdir -p $RUNDIR\n\n# Start your Django Unicorn\n# Programs meant to be run under supervisor should not daemonize themselves (do not use --daemon)\nexec ../bin/gunicorn ${DJANGO_WSGI_MODULE}:application \\\n  --name $NAME \\\n  --workers $NUM_WORKERS \\\n  --user=$USER --group=$GROUP \\\n  --bind=unix:$SOCKFILE \\\n  --log-level=debug \\\n  --log-file=- \\\n  --timeout 120 #This \n"], [], [], [], ["sudo snap install pycharm-professional --classic\n"], ["pip install pyzmq==19.0.2 \npip install jupyter\n"], ["from pyspark.sql.dataframe import DataFrame\nfrom functools import wraps\n\n# Create a decorator to add a function to a python object\ndef add_attr(cls):\n    def decorator(func):\n        @wraps(func)\n        def _wrapper(*args, **kwargs):\n            f = func(*args, **kwargs)\n            return f\n\n        setattr(cls, func.__name__, _wrapper)\n        return func\n\n    return decorator\n\n  \n# Extensions to the Spark DataFrame class go here\ndef dataframe_extension(self):\n  @add_attr(dataframe_extension)\n  def drop_records():\n    return(\n      self\n      .where(~((col('test1') == 'ABC') & (col('test2') =='XYZ')))\n      .where(~col('test1').isin(['AAA', 'BBB']))\n    )\n  return dataframe_extension\n\nDataFrame.dataframe_extension = property(dataframe_extension)\n"], [], [], [], ["\"[python]\": {\n    \"editor.defaultFormatter\": null,\n    \"editor.insertSpaces\": true,\n    \"editor.tabSize\": 4,\n    \"editor.formatOnSave\": true\n}\n", "\"editor.defaultFormatter\": null\n"], [], ["class Choices(int, Enum):\n    anne = 1\n    ben = 2\n    charlie = 3\n    dave = 4\n\n    @classmethod\n    def __get_validators__(cls):\n        cls.lookup = {v: k.value for v, k in cls.__members__.items()}\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        try:\n            return cls.lookup[v]\n        except KeyError:\n            raise ValueError('invalid value')\n\nclass Model(BaseModel):\n    choice: Choices\n\ndebug(Model(choice='charlie'))\n"], [], ["curl https://pyenv.run | bash\n"], ["# after `import torch`:    \n\nimport torch.multiprocessing as mp\n\nmp.set_start_method('fork', force=True)\n"], [], ["import os\nimport pathlib\n\nfoo_path = os.path.join(pathlib.Path(__file__).parent.absolute(), \"..\", \"foo.txt\")\n\nwith open(foo_path) as f:\n    s = f.read()\n    print(s)\n"], ["import pandas as pd\n\ndf = pd.DataFrame({'Entry1': ['Modified 1', 'Modified 2 ', 'Modified 3'],\n                   'Entry2': ['Value 1', 'Value 2','Value 2']})\n\nwith pd.ExcelWriter('Original_File.xlsx', engine='openpyxl'\n                    mode='a', if_sheet_exists='overlay') as writer:\n    \n    df.to_excel(writer, sheet_name='SheetName', startrow=1,\n                startcol=2, header=False, index=False)\n"], ["python3 -m pip install --upgrade pip setuptools wheel\nbrew install portaudio --HEAD\npython3 -m pip install pyaudio --global-option=\"build_ext\" --global-option=\"-I/opt/homebrew/include\" --global-option=\"-L/opt/homebrew/lib\"\n"], ["export C_INCLUDE_PATH=/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/Headers\n"], [], [], [], [], [], [], ["\"\ntextarea {\n    height: 90vh;\n    width: 80%;\n}\n\"\n"], [], [], [], [], [], ["print([[i, j, k] for i in range(x + 1) for j in range(y + 1)\n    for k in range(z + 1) if i + j + k != n])\n"], [], [], ["Warning: Python 3.7 was not found on your system...\nNeither 'pyenv' nor 'asdf' could be found to install Python.\nYou can specify specific versions of Python with:\n$ pipenv --python path/to/python\n"], ["function sawtooth(arr) {\n  if (arr.length < 2) return 0;\n  \n  let previousLongest = 1;\n  let result = 0;\n  for (let i = 1; i < arr.length; i++) {\n    if (i >= arr.length) break;\n    if (arr[i - 1] === arr[i]) continue;\n    if (arr[i - 1] > arr[i] && arr[i] < arr[i + 1] || arr[i - 1] < arr[i] && arr[i] > arr[i + 1]) {\n      previousLongest += 1;\n    } else {\n      previousLongest = 1;\n    }\n    result += previousLongest;\n  }\n  return result;\n}\n"], [], ["python3 --version\n", "python --version\n", "pip3 install -r requirements.txt\n"], [], ["def solution(arr: list) -> int:\n    '''\n    for every char, check if still current sawtooth\n    if still currently sawtooth, numberOfWays += length\n    else reset temp counter\n    '''\n    l, r = 0, 1\n    ways = 0\n    while r < len(arr):\n\n        # check if current char + past 2 chars are sawtooth\n        if r-l > 1 and (arr[r-2] < arr[r-1] > arr[r] or\n                        arr[r-2] > arr[r-1] < arr[r]):  \n            ways += r-l\n\n        # check if current char + past 1 chars are sawtooth\n        elif arr[r-1] != arr[r]:                \n            ways += 1\n            l = r-1\n\n        else:                                   \n            # reset left pointer\n            l = r\n\n        r += 1\n    return ways\n"], ["$ echo \"deb http://security.ubuntu.com/ubuntu focal-security main\" | sudo tee /etc/apt/sources.list.d/focal-security.list\n$ apt-get update && \\\n    apt-get install libssl1.1\n$ dpkg -L libssl1.1\n/.\n/usr\n/usr/lib\n/usr/lib/x86_64-linux-gnu\n/usr/lib/x86_64-linux-gnu/engines-1.1\n/usr/lib/x86_64-linux-gnu/engines-1.1/afalg.so\n/usr/lib/x86_64-linux-gnu/engines-1.1/capi.so\n/usr/lib/x86_64-linux-gnu/engines-1.1/padlock.so\n/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1      <---\n/usr/lib/x86_64-linux-gnu/libssl.so.1.1         <---\n/usr/share\n/usr/share/doc\n/usr/share/doc/libssl1.1\n/usr/share/doc/libssl1.1/NEWS.Debian.gz\n/usr/share/doc/libssl1.1/changelog.Debian.gz\n/usr/share/doc/libssl1.1/copyright\n\n", "# /usr/local/bin/mysql -uroot -h127.0.0.1 -p\n/usr/local/bin/mysql: error while loading shared libraries: libcrypto.so.1.1: cannot open shared object file: No such file or directory\n", "$ ldd /usr/local/bin/mysql\n    linux-vdso.so.1 (0x00007fff1e576000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7e6db3e000)\n    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f7e6db39000)\n    libcrypto.so.1.1 => not found\n    libssl.so.1.1 => not found\n    libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007f7e6db25000)\n    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f7e6db1e000)\n    libncurses.so.5 => /lib/x86_64-linux-gnu/libncurses.so.5 (0x00007f7e6daf8000)\n    libtinfo.so.5 => /lib/x86_64-linux-gnu/libtinfo.so.5 (0x00007f7e6dac9000)\n    libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f7e6d89d000)\n    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f7e6d7b6000)\n    libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f7e6d796000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7e6d56c000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f7e6db4b000)\n", "$ wget -c https://www.openssl.org/source/openssl-1.1.1s.tar.gz && \\\n    tar xf openssl-1.1.1s.tar.gz && \\\n    cd openssl-1.1.1s/ && \\\n    ./config --prefix=\"/usr/local/openssl\" && \\\n    make && \\\n    make test && \\\n    make install && \\\n    export LD_LIBRARY_PATH=/usr/local/openssl/lib:$LD_LIBRARY_PATH\" >> /etc/profile.d/startEnv.sh && \\\n    echo \"export LD_LIBRARY_PATH=/usr/local/openssl/lib:$LD_LIBRARY_PATH\" >> /etc/profile.d/startEnv.sh && \\\n    echo $LD_LIBRARY_PATH\n"], ["pip install --upgrade --force jupyter-console\n", "pip uninstall botocore\n", "pip install --upgrade --force jupyter-console\n"], [], ["print([[a, b, c] for a in range(x + 1) for b in range(y + 1) for c in range(z + 1) if a + b + c != n])\n"], ["from math import comb\ndef solution(arr):\n    \n    n=len(arr)\n    \n    if arr[1]!=arr[0]:\n        l=2\n        \n    else:\n        l=0\n    pre=arr[1]-arr[0]\n    ans=0\n    \n    for i in range(2,n):\n        cur=arr[i]-arr[i-1]\n        \n        if cur*pre<0:\n            l+=1\n        else:\n            if l==2:\n                ans+=1\n            elif l==0:\n                ans+=0\n            else:\n                ans+=comb(l,2)\n            if cur!=0:\n                l=2\n            else:\n                l=0\n        pre=cur\n    if l==2:\n        ans+=1\n    elif l==0:\n        ans+=0\n    else:\n        ans+=comb(l,2)\n    return ans\n"], ["import inspect\n\nfrom fastapi import  Query, FastAPI, Depends\nfrom pydantic import BaseModel, ValidationError\nfrom fastapi.exceptions import  RequestValidationError\n\n\nclass QueryBaseModel(BaseModel):\n    def __init_subclass__(cls, *args, **kwargs):\n        field_default = Query(...)\n        new_params = []\n        for field in cls.__fields__.values():\n            default = Query(field.default) if not field.required else field_default\n            annotation = inspect.Parameter.empty\n\n            new_params.append(\n                inspect.Parameter(\n                    field.alias,\n                    inspect.Parameter.POSITIONAL_ONLY,\n                    default=default,\n                    annotation=annotation,\n                )\n            )\n\n        async def _as_query(**data):\n            try:\n                return cls(**data)\n            except ValidationError as e:\n                raise RequestValidationError(e.raw_errors)\n\n        sig = inspect.signature(_as_query)\n        sig = sig.replace(parameters=new_params)\n        _as_query.__signature__ = sig  # type: ignore\n        setattr(cls, \"as_query\", _as_query)\n\n    @staticmethod\n    def as_query(parameters: list) -> \"QueryBaseModel\":\n        raise NotImplementedError\n\nclass ParamModel(QueryBaseModel):\n    start_datetime: datetime\n    \napp = FastAPI()\n\n@app.get(\"/api\")\ndef test(q_param: ParamModel: Depends(ParamModel.as_query))\n    start_datetime = q_param.start_datetime\n    ...\n    return {}\n\n"], ["/members?member_ids=1&member_ids=2\n", "class Model(BaseModel):\n    member_ids: List[str]\n", " class Model(BaseModel):\n     member_ids: List[str] = Field(Query([]))\n"], [], [], ["Supported runtimes\npython-3.10.8 on all supported stacks (recommended)\npython-3.9.15 on all supported stacks\npython-3.8.15 on Heroku-18 and Heroku-20 only\npython-3.7.15 on Heroku-18 and Heroku-20 only\n"], [], ["self.options.add_argument(\"--user-data-dir=.config/google-chrome\")\n\n\n   \n"], ["#include <bits/stdc++.h>\nusing namespace std;\n\nvoid solve(vector<int> arr) {\n    int n = arr.size(), ans = 0;\n    // vector<vector<int>> dp(n, vector<int>(2, 0));\n    int inc = 0, dec = 0;\n    for(int i = 1; i < n; i++) {\n        if (arr[i] > arr[i-1]) {\n            // dp[i][0] = dp[i-1][1] + 1;\n            inc = dec + 1;\n            dec = 0;\n        } else if (arr[i] < arr[i-1]) {\n            // dp[i][1] = dp[i-1][0] + 1;\n            dec = inc + 1;\n            inc = 0;\n        } else {\n            inc = 0, dec = 0;\n        }\n        // ans += dp[i][0] + dp[i][1];\n        ans += (inc + dec);\n    }\n    cout << ans << endl;\n}\n\nint main() {\n    auto inp = {-442024811,447425003,365210904,823944047,943356091,-781994958,872885721,-296856571,230380705,944396167,-636263320,-942060800,-116260950,-126531946,-838921202};\n    solve(inp);\n    return 0;\n}\n"], [], ["import awswrangler as wr\n\ndf = wr.s3.select_query(\n        sql=\"SELECT * FROM s3object s limit 5\",\n        path=\"s3://filepath\",\n        input_serialization=\"Parquet\",\n        input_serialization_params={},\n        use_threads=True,\n)\n"], ["x=int(input())\ny=int(input())\nz=int(input())\nn=int(input())\nans[]\nfor i in range(x+1):\n  for j in range(y+1):\n    for k in range(z+1):\n\n\n       if(i+j+k)!=n:\n          ans.append([i,j,k])\n\nprint(ans)\n"], [], ["MWE -|\n     |- tests -|\n               |- test_file.py\n", "MWE -|\n     |- tests_package -|\n                       |- test_file.py\n", "GitFolderOfProject -|\n                    |- MWE -|\n                            |- tests -|\n                                      |- test_file.py\n"], ["%%shell\njupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\n"], [], [], [], [], ["import math\n\ndef comb(x):\n    st = 0\n    total_comb = 0\n    if len(x) < 2: #edge case\n        return 0\n    if len(x) == 2: #edge case\n        return 2\n    \n    seq_s = 0\n    for i in range(1, len(x)-1): \n        if  (x[i]<x[i-1] and x[i]<x[i+1]) or (x[i]>x[i-1] and x[i]>x[i+1]):\n            continue\n        else:\n            print(x[seq_s:i+1])\n            if i+1-seq_s == 2 and x[i] == x[i-1]: #means we got two same nums like 10, 10\n                pass\n            else: total_comb+=math.comb(i+1-seq_s,2)\n            seq_s=i\n            i+=1\n            \n    print(x[seq_s:])\n    if i+1-seq_s == 2 and x[i] == x[i-1]: #means we got two same nums like 10, 10\n        pass\n    else: total_comb+=math.comb(len(x)-seq_s,2)\n    return total_comb\n    \n\nx= [1,2,1,3,4,-2]\nprint(comb(x))\n"], ["    content = forms.CharField(widget=forms.Textarea(attrs={\"rows\":\"5\"}))\n"], ["\"python.formatting.blackArgs\": [\"--target-version=py310\"]\n"], ["class MyEnum(Enum, str):\n    state1 = 'state1'\n    state2 = 'state2'\n", "TypeError: new enumerations should be created as `EnumName([mixin_type, ...] [data_type,] enum_type)`\n", "print('This is the state value: ' + state)\n", "msg = f'This is the state value: {state}'  # works without inheriting from str\n"], ["    def parseJsonc(text):\n        text_without_comment = re.sub(r'\\/\\*(\\*(?!\\/)|[^*])*\\*\\/|\\/\\/.*', '', text)\n        return json.loads(text_without_comment)\n"], [], ["folder A\n-- folder B\n|     main.py\n|     file.txt\n"], ["python3.9 -m venv venv\n"], ["wget https://www.openssl.org/source/openssl-1.1.1o.tar.gz\ntar -zxvf openssl-1.1.1o.tar.gz\ncd openssl-1.1.1o\n./config\nmake\nmake test      (failed 2 tests)\nsudo make install (on this moment you can't install python by pyenv)\nsudo find / -name libssl.so.1.1\nsudo ln -s /usr/local/lib/libssl.so.1.1  /usr/lib/libssl.so.1.1\nsudo find / -name libcrypto.so.1.1\nsudo ln -s /usr/local/lib/libcrypto.so.1.1 /usr/lib/libcrypto.so.1.1\n"], ["$ pyenv versions\n", "$ pyenv install 3.9.6\n\npyenv: /home/slesage/.pyenv/versions/3.9.6 already exists\ncontinue with installation? (y/N) y\nDownloading Python-3.9.6.tar.xz...\n-> https://www.python.org/ftp/python/3.9.6/Python-3.9.6.tar.xz\nInstalling Python-3.9.6...\nInstalled Python-3.9.6 to /home/slesage/.pyenv/versions/3.9.6\n"], ["wget https://www.openssl.org/source/openssl-1.1.1o.tar.gz\ntar -zxvf openssl-1.1.1o.tar.gz\ncd openssl-1.1.1o\n./config\nmake\nmake test\nsudo make install\n"], ["import json\n\nintrospection_dict = your_schema_object.introspect()\n\n# Or save the schema into some file\nwith open(\"schema.json\", \"w\") as fp:\n    json.dump(introspection_dict, fp)\n"], [], [">>> import numpy as np\n>>> np.sqrt(25)\n5.0\n>>> np.sqrt([2, 3, 4])\narray([1.41421356, 1.73205081, 2.        ])\n", ">>> a = np.array([4, -1, np.inf])\n>>> np.sqrt(a)\n<stdin>:1: RuntimeWarning: invalid value encountered in sqrt\narray([ 2., nan, inf])\n>>> np.emath.sqrt(a)\narray([ 2.+0.j,  0.+1.j, inf+0.j])\n", ">>> a = a.astype(complex)\n>>> np.sqrt(a)\narray([ 2.+0.j,  0.+1.j, inf+0.j])\n"], [], [], [], ["from dataclasses import dataclass\nfrom inspect import signature\n\n\ndef dataclass_init_kwargs(cls, *args, **kwargs):\n    cls = dataclass(cls, *args, **kwargs)\n\n    def from_kwargs(**kwargs):\n        cls_fields = {field for field in signature(cls).parameters}\n        native_arg_keys = cls_fields & set(kwargs.keys())\n        native_args = {k: kwargs[k] for k in native_arg_keys}\n        ret = cls(**native_args)\n        return ret\n\n    setattr(cls, 'from_kwargs', from_kwargs)\n    return cls\n\n"], [], [], ["export DYLD_LIBRARY_PATH=/usr/local/mysql/lib\n"], ["class MyStrEnum(str, Enum):\n\n    OK     = 'OK'\n    FAILED = 'FAILED'\n\n    def __str__(self) -> str:\n        return self.value\n"], [], ["import timeit\nimport random\n\n\ndef increasing_sequence_pos(sequence):\n    for n, (a, b) in enumerate(zip(sequence[:-1], sequence[1:])):\n        if a >= b:\n            return False, n + 1\n    return True, -1\n\n\ndef almost_increasing_sequence(sequence):\n    increasing, n = increasing_sequence_pos(sequence)\n    return (\n        # either it was already increasing\n        increasing or\n        # or the problem is with the last element\n        n == len(sequence)-1 or\n        ((\n            # or the first element was the problem \n            n == 1\n            # or the element at position n was the problem\n            (sequence[n - 1] < sequence[n + 1]) or\n            # or the element at position n-1 was the problem\n            (sequence[n - 2] < sequence[n] < sequence[n + 1])\n        ) and increasing_sequence_pos(sequence[n:])[0])\n    )\n\n\nsize = 1000000\n\n# time on simple increasing series\nnumbers = list(range(size))\nprint(timeit.timeit(lambda: almost_increasing_sequence(numbers), number=1))\nprint(f'Result: {almost_increasing_sequence(numbers)}')\n\n# time on single issue\nnumbers[random.randint(1, size)] = 0\nprint(timeit.timeit(lambda: almost_increasing_sequence(numbers), number=1))\nprint(f'Result: {almost_increasing_sequence(numbers)}')\n\n# time on two issues issue\nnumbers[random.randint(1, size)] = 0\nprint(timeit.timeit(lambda: almost_increasing_sequence(numbers), number=1))\nprint(f'Result: {almost_increasing_sequence(numbers)}')\n", "0.07000060000000001\nResult: True\n0.06909959999999998\nResult: True\n0.06430069999999999\nResult: False\n"], ["class EditForm(forms.Form):\n\n    title =  forms.CharField(widget=forms.TextInput(attrs={'name':'title'}))\n\n    body = forms.CharField(widget=forms.Textarea(attrs={'name':'body', \n                                                        'style': 'height: 3em;'))\n"], ["pip install mysqlclient\n", "import pymysql \npymysql.install_as_MySQLdb()\n", "pip install pymysql\n"], ["def solution(arr):\n    # holds the count of sawtooths at each index of our input array,\n    # for sawtooth lengths up to that index\n    saws = [0 for x in range(0, len(arr))]\n    # the resulting total sawtooth counts\n    totalSawCounts = 0\n    previousCount = 0\n\n    for currIdx in range(1, len(arr)):\n        currCount = 0\n        before = currIdx -1\n        if (arr[currIdx] > arr[before]):\n            goingUp = True\n        elif (arr[currIdx] < arr[before]):\n            goingUp = False\n        else:\n            break\n\n        # if we made it here, we have at least one sawtooth\n        currCount =  1\n\n        # see if there was a previous solution (the DP part)\n        # and if it continues our current sawtooth\n        if before >= 1:\n            if goingUp:\n                if arr[before-1] > arr[before]:\n                    currCount = previousCount + currCount\n            else:\n                if arr[before-1] < arr[before]:\n                    currCount = previousCount + currCount\n        previousCount = currCount\n        totalSawCounts = totalSawCounts + currCount\n\n    return totalSawCounts\n", "arr = [9,8,7,6,5]\nprint(solution(arr)) # 4\n\narr2 = [1,2,1,3,4,-2]\nprint(solution(arr2)) # 9\n\narr3 = [1,2,1,2,1]\nprint(solution(arr3)) # 10\n\narr4 = [10,10,10]\nprint(solution(arr4)) # 0\n\n# from medium article comments\narr5 = [-442024811,447425003,365210904,823944047,943356091,-781994958,872885721,-296856571,230380705,944396167,-636263320,-942060800,-116260950,-126531946,-838921202]\nprint(solution(arr5)) # 31\n"], ["    - name: Install requirements\n      run: |\n        git config --global url.\"https://${{ secrets.ACESS_TOKEN }}@github\".insteadOf https://github\n        pip install -r requirements.txt\n"], ["pip uninstall pyzmq        #Steven-MSFT's solution\npip install pyzmq==19.0.2\n\npip install pyzmq          #Update\n"], [], ["- uses: actions/checkout@v2\n  with:\n    ssh-key: ${{ secrets.SSH_PRIVATE_KEY }}\n    repository: organization_name/repo_name\n"], [], ["sudo apt-get install python3-venv\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3 -\n"], [], [], [], ["backports.zoneinfo==0.2.1\n", "backports.zoneinfo;python_version<\"3.9\"\n", "backports.zoneinfo==0.2.1;python_version<\"3.9\"\n"], [], ["pip install \"setuptools<58.0.0\"\n", "pip install django-celery\n"], [], ["from fractions import Fraction\n\ndef sqrt(x, n):\n    x = x if isinstance(x, Fraction) else Fraction(x)\n    upper = x + 1\n    for i in range(0, n):\n        upper = (upper + x/upper) / 2\n    lower = x / upper\n    if lower > upper:\n        raise ValueError(\"Sanity check failed\")\n    return (lower, upper)\n"], ["kubectl get secrets\nNAME                              TYPE                                  DATA   AGE\ndefault-token-hxzsv               kubernetes.io/service-account-token   3      5h34m\nminio-sa-token-nxdpt              kubernetes.io/service-account-token   3      14m\nmino-test-minio                   Opaque                                2      14m\nmy-s3-keys                        Opaque                                2      3h33m\nmypostgres-secret                 Opaque                                2      5h20m\nsh.helm.release.v1.mino-test.v1   helm.sh/release.v1                    1      14m\nalex@pop-os:~/coding/preso_hive$ kubectl get secret mino-test-minio -o yaml\napiVersion: v1\ndata:\n  rootPassword: bWluaW8xMjM=\n  rootUser: bWluaW8=\nkind: Secret\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: mino-test\n    meta.helm.sh/release-namespace: default\n  creationTimestamp: \"2022-06-14T10:15:14Z\"\n  labels:\n    app: minio\n    app.kubernetes.io/managed-by: Helm\n    chart: minio-4.0.2\n    heritage: Helm\n    release: mino-test\n  name: mino-test-minio\n  namespace: default\n  resourceVersion: \"58285\"\n  uid: c23ce2d4-657e-4feb-adea-df83bba489c5\ntype: Opaque\n", "$ echo bWluaW8= | base64 --decode\nminioalex\n\n$ echo bWluaW8xMjM= | base64 --decode\nminio123\n"], ["pip install setuptools~=57.5.0\n"], [], [], ["gcloud config set auth/disable_ssl_validation  True\n"], [], [], ["pattern = re.compile(r'(\\d+\\.\\d+)|(\\d+)|(\\w+)|(\".*)\"')\nToken = namedtuple('Token', ('kind', 'value', 'position'))\nenv = {'x': 'hello', 'y': 10}\n\nfor s in ['123', '123.45', 'x', 'y', '\"goodbye\"']:\n    mo = pattern.fullmatch(s)\n    match mo.lastindex:\n        case 1:\n            tok = Token('NUM', float(s), mo.span())\n        case 2:\n            tok = Token('NUM', int(s), mo.span())\n        case 3:\n            tok = Token('VAR', env[s], mo.span())\n        case 4:\n            tok = Token('TEXT', s[1:-1], mo.span())\n        case _:\n            raise ValueError(f'Unknown pattern for {s!r}')\n    print(tok) \n", "Token(kind='NUM', value=123, position=(0, 3))\nToken(kind='NUM', value=123.45, position=(0, 6))\nToken(kind='VAR', value='hello', position=(0, 1))\nToken(kind='VAR', value=10, position=(0, 1))\nToken(kind='TEXT', value='goodbye', position=(0, 9))\n", "pattern = re.compile(r\"\"\"(?x)\n    (?P<float>\\d+\\.\\d+) |\n    (?P<int>\\d+) |\n    (?P<variable>\\w+) |\n    (?P<string>\".*\")\n\"\"\")\n", "for s in ['123', '123.45', 'x', 'y', '\"goodbye\"']:\n    mo = pattern.fullmatch(s)\n    match mo.lastgroup:\n        case 'float':\n            tok = Token('NUM', float(s), mo.span())\n        case 'int':\n            tok = Token('NUM', int(s), mo.span())\n        case 'variable':\n            tok = Token('VAR', env[s], mo.span())\n        case 'string':\n            tok = Token('TEXT', s[1:-1], mo.span())\n        case _:\n            raise ValueError(f'Unknown pattern for {s!r}')\n    print(tok)\n", "for s in ['123', '123.45', 'x', 'y', '\"goodbye\"']:\n    if (mo := re.fullmatch('\\d+\\.\\d+', s)):\n        tok = Token('NUM', float(s), mo.span())\n    elif (mo := re.fullmatch('\\d+', s)):\n        tok = Token('NUM', int(s), mo.span())\n    elif (mo := re.fullmatch('\\w+', s)):\n        tok = Token('VAR', env[s], mo.span())\n    elif (mo := re.fullmatch('\".*\"', s)):\n        tok = Token('TEXT', s[1:-1], mo.span())\n    else:\n        raise ValueError(f'Unknown pattern for {s!r}')\n    print(tok)\n"], [], [], ["environment:\n            MINIO_ROOT_USER: ${MINIO_ROOT_USER}\n            MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}\n"], ["user_logged_in = Signal(providing_args=[\"request\", \"user\"])\n\n# Typically followed by `user_logged_in` (unless, e-mail verification kicks in)\nuser_signed_up = Signal(providing_args=[\"request\", \"user\"])\n\npassword_set = Signal(providing_args=[\"request\", \"user\"])\npassword_changed = Signal(providing_args=[\"request\", \"user\"])\npassword_reset = Signal(providing_args=[\"request\", \"user\"])\n\nemail_confirmed = Signal(providing_args=[\"request\", \"email_address\"])\nemail_confirmation_sent = Signal(\n    providing_args=[\"request\", \"confirmation\", \"signup\"])\n\nemail_changed = Signal(\n    providing_args=[\n        \"request\", \"user\",\n        \"from_email_address\", \"to_email_address\"])\nemail_added = Signal(providing_args=[\"request\", \"user\", \"email_address\"])\nemail_removed = Signal(providing_args=[\"request\", \"user\", \"email_address\"])\n"], ["|    |   a |   b |\n|---:|----:|----:|\n|  0 |   0 |   2 |\n|  1 |   1 |   3 |\n"], [], [], [], ["# The white space on both columns and values need to be removed\n|    |     a |    b |\n|---:|------:|-----:|\n|  0 |     3 | 2222 |\n|  1 | 10000 |    3 |\n", "       a     b\n0      3  2222\n1  10000     3\n"], ["[mypy]\nignore_missing_imports = True\n"], [], [], ["export DYLD_LIBRARY_PATH=/usr/local/mysql/lib\n"], ["poetry env use <path to python executable>\n"], [], [], [], [], [], ["  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\"\n  }\n", "  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true\n  }\n"], [], ["$ git status\n", "$ git push heroku master\n"], ["import MetaTrader5 as mt5\nimport Meta2 as mt2\nimport time\n\nif not mt5.initialize(path=\"C:/Program Files/Fusion Markets MetaTrader 5/terminal64.exe\",login=xxxx, server=\"FusionMarkets-Demo\",password=\"xxxxx\"):\n        print(\"initialize() failed, error code =\",mt5.last_error())\n\nif not mt2.initialize(path=\"C:/Program Files/MetaTrader 5 EXNESS/terminal64.exe\",login=xxxxx, server=\"Exness-MT5Trial\",password=\"xxxxx\"):\n        print(\"initialize() failed, error code =\",mt5.last_error())  \n       \nfusion_ticker = mt5.symbol_info_tick(\"EURUSD\")\nexness_ticker = mt2.symbol_info_tick(\"EURUSDm\")\n"], ["app.config['JSON_SORT_KEYS'] = False\n"], [], [], ["from win32com.client import Dispatch\nimport pandas as pd\n\nxlApp = Dispatch(\"Excel.Application\")\nxlApp.Visible = 1\nxlApp.Workbooks.Open(r'c:\\Chadee\\test.xlsx')\nxlApp.ActiveSheet.Cells(1,1).Select\n\nd = {'col1': [1, 2], 'col2': [3, 4]}\ndf = pd.DataFrame(data=d)\ndf.to_clipboard(index=False)\n\nxlApp.ActiveWorkbook.ActiveSheet.PasteSpecial()\n"], ["brew install portaudio\n", "pip install pyaudio \n"], [], ["import re\nimport json\n\n\nwith open(\"contents.json\", \"r\") as JSONfile:\n    objec = json.loads(\"\".join(re.split(r\"(?://|#).*(?=\\n)\", JSONfile.read())).strip())\n"], ["pip install xlrd\n", "pip install pandas\n", "import pandas as pd\n\ndf = pd.read_excel(\"filesFolder/excelFile.xls\", engine='xlrd')\n"], [], ["check_request_enabled = Signal(providing_args=[\"request\"])\n", "check_request_enabled = Signal(\"request\")\n"], ["if __name__ == '__main__':\n    x, y, z, n = (int(input().strip()) for _ in range(4))\n    print([[i,j,k] for i in range(x+1) for j in range(y+1) for k in range(z+1) if i+j+k!=n ])\n"], [], ["pip install jupyter\n"], ["from django.dispatch import Signal, receiver\nnotification=Signal()\n@receiver(notification)\ndef show_notification(sender, **kwargs):\n    print(\"sender,\", sender)\n    print(\"Kwargs\", kwargs)\n    print(\"Notification\")        \n", "notification.send(sender=None, request=request, user=['Sakib', 'Malik'])\n"], [], ["class dfc:\n  def __init__(self, df):\n    self.df = df\n    \n  def func(self, num):\n    self.df = self.df.selectExpr(f\"id * {num} AS id\")\n  \n  def func1(self, num1):\n    self.df = self.df.selectExpr(f\"id * {num1} AS id\")\n    \n  def dfdis(self):\n    self.df.show()\n", "df = spark.range(10)\n\nob = dfc(df)\n\nob.func(2)\n\nob.func(2)\n\nob.dfdis()\n"], ["pip install --upgrade --no-cache-dir gdown\n", "!pip install --upgrade --no-cache-dir gdown\n"], [], [], [], [], ["import mysql.connector\ncnx = mysql.connector.connect(user='mensfort', password='zhongcan',\n                              host='127.0.0.1', database='restaurant')\ncursor = cnx.cursor()\ncursor.execute('select dongle from personnel')\nfor dongle in cursor:\n    print(dongle)\ncursor.close()\ncnx.close()\n", "pip uninstall mysql-connector\n", "pip install mysql-connector-python\n"], ["for file in os.scandir(test_folder):\n    filename = os.fsdecode(file)\n    if '.DS_Store' not in filename:\n        execute_function(file)\n"], ["curl -sSL https://install.python-poetry.org | python3 - --uninstall\ncurl -sSL https://install.python-poetry.org | python3 -\n"], [], [], [], [], [], ["from flask_login._compat import text_type\n"], ["pip install --force-reinstall --no-binary :all: cffi\n"], ["pip install --upgrade cffi xcffib\n"], ["from math import isqrt\n\ndef str_sqrt(num, digits):\n    \"\"\" Arbitrary precision square root\n\n        num arg must be a string\n        Return a string with `digits` after\n        the decimal point\n\n        Written by PM 2Ring 2022.01.26\n    \"\"\"\n\n    int_part , _, frac_part = num.partition('.')\n    num = int_part + frac_part\n\n    # Determine the required precision\n    width = 2 * digits - len(frac_part)\n\n    # Truncate or pad with zeroes\n    num = num[:width] if width < 0 else num + '0' * width\n    s = str(isqrt(int(num)))\n\n    if digits:\n        # Pad, if necessary\n        s = '0' * (1 + digits - len(s)) + s\n        s = f\"{s[:-digits]}.{s[-digits:]}\"\n    return s\n", "print(str_sqrt(\"2.0\", 30))\n", "1.414213562373095048801688724209\n"], [], ["sudo yum groupinstall \"Development Tools\" -y\n", "sudo yum install bison byacc cscope ctags cvs diffstat doxygen flex gcc gcc-c++ gcc-gfortran gettext git indent intltool libtool patch patchutils rcs redhat-rpm-config rpm-build subversion swig systemtap\n"], ["open -t .bash_profile \n", "export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"\n", "open -t ~/.zshrc \n", "export DYLD_LIBRARY_PATH=\"/usr/local/mysql/lib:$PATH\"\n"], ["from django.dispatch import Signal\n\nmodel_delete_signal = Signal()\n", "model_delete_signal.send(sender='session_delete', instance=self)\n", "@receiver(session_delete)\ndef delete_session(sender, **kwargs):\n    instance = kwargs['instance']\n"], ["new_estimate = (estimate + num/estimate) / 2\n", "def newtons_method(num, estimate):\n    # Computing a new_estimate\n    new_estimate = (estimate + num/estimate) / 2\n    print(new_estimate)\n    # Base Case: Comparing our estimate with built-in functions value\n    if new_estimate == math.sqrt(num):\n        return True\n    else:\n        return newtons_method(num, new_estimate)\n", "newtons_method(30,5)\n", "5.5\n5.477272727272727\n5.4772255752546215\n5.477225575051661\n"], ["import sympy\nsympy.sqrt(2)\n# => sqrt(2)\n", "sympy.sqrt(8) / sympy.sqrt(27)\n# => 2*sqrt(6)/9\n", "s = sympy.sqrt(2)\ns**2\n# => 2\ntype(s**2)\n#=> <class 'sympy.core.numbers.Integer'>\n", "(2**0.5)**2\n# => 2.0000000000000004\n\nfrom decimal import Decimal\n(Decimal('2')**Decimal('0.5'))**Decimal('2')\n# => Decimal('1.999999999999999999999999999')\n", "from sympy import Symbol, integrate, pi, sqrt, exp, oo\nx = Symbol('x')\nintegrate(exp(-x**2), (x, -oo, oo))\n# => sqrt(pi)\nintegrate(exp(-x**2), (x, -oo, oo)) == sqrt(pi)\n# => True\n", "sympy.N(sympy.sqrt(2), 1_000_000)\n# => 1.4142135623730950488016...........2044193016904841204\n"], [], ["pip3 -vvv install --upgrade --force-reinstall cffi\n"], ["    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    import pandas as pd\n    # Create subplots and mention plot grid size\n    title=df.symbol.unique()[0]\n\n    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, \n               vertical_spacing=0.02, \n               row_width=[0.25, 0.75])\n\n    # Plot OHLC on 1st row\n    fig.add_trace(go.Candlestick(x=df.index,\n                    open=df['open'], high=df['high'],\n                    low=df['low'], close=df['close'],showlegend=False),row=1, col=1,)\n\n    # Bar trace for volumes on 2nd row without legend\n    # fig.add_trace(go.Bar(x=df.index, y=df['volume'], showlegend=False), row=2, col=1)\n\n    df['color']=''\n    df['color']=['red' if (x>y) else t for x,y,t in zip(df['open'],df['close'],df['color'])]\n    df['color']=['green' if (x<y) else t for x,y,t in zip(df['open'],df['close'],df['color'])]\n    colors=df.color.tolist()\n    df['prev_color']=[colors[0]]+colors[:(len(colors)-1)]\n    df.loc[((df.open==df.close) & (df.color=='')),'color']=[z for x,y,z,t in zip(df['open'],df['close'],df['prev_color'],df['color']) if (x==y and t=='')]\n    colors=df.color.tolist()\n    df['prev_color']=[colors[0]]+colors[:(len(colors)-1)]\n    df.loc[((df.open==df.close) & (df.color=='')),'color']=[z for x,y,z,t in zip(df['open'],df['close'],df['prev_color'],df['color']) if (x==y and t=='')]\n    \n    markers=['green','red']\n\n    for t in markers:\n        df_tmp=df.loc[~(df.color==t)] ## somehow the color it takes is opposite so take negation to \n        fig.add_trace(go.Bar(x=df_tmp.index, y=df_tmp['volume'], showlegend=False), row=2, col=1)\n\n    # Do not show OHLC's rangeslider plot \n    fig.update(layout_xaxis_rangeslider_visible=False)\n    fig.layout.yaxis2.showgrid=False\n    fig.update_layout(title_text=title,title_x=0.45)\n\n    fig.show()\n"], [], [], [], ["   \"python.formatting.blackArgs\": [\"--skip-numeric-underscore-normalization\"],  \n"], [], ["Installing collected packages: wcwidth, traitlets, parso, tornado, pyzmq, pygments, prompt-toolkit, pickleshare, nest-asyncio, matplotlib-inline, jupyter-core, jedi, entrypoints, decorator, backcall, jupyter-client, ipython, debugpy, argcomplete, ipykernel\nSuccessfully installed argcomplete-2.0.0 backcall-0.2.0 debugpy-1.5.1 decorator-5.1.1 entrypoints-0.3 ipykernel-6.6.1 ipython-7.31.0 jedi-0.18.1 jupyter-client-7.1.0 jupyter-core-4.9.1 matplotlib-inline-0.1.3 nest-asyncio-1.5.4 parso-0.8.3 pickleshare-0.7.5 prompt-toolkit-3.0.24 pygments-2.11.2 pyzmq-22.3.0 tornado-6.1 traitlets-5.1.1 wcwidth-0.2.5\n"], ["web: gunicorn --workers=3 app:app --timeout 200 --log-file -\n"], ["pip uninstall pyzmq\npip install pyzmq==19.0.2\n"], [], [], ["user_logged_in = Signal(providing_args=[\"request\", \"user\"])\n", "user_logged_in = Signal()\n"], [], [], [], [], [], [], [], [], ["dataset = importer.ImportYoloV5(path_to_annotations)\ndataset.splitter.StratifiedGroupShuffleSplit(train_pct=.6, val_pct=.2, test_pct=.2, batch_size=1)\ndataset.analyze.ShowClassSplits()\n"], ["brew install mysql\npip install mysqlclient\n", "sudo apt-get install python3-dev default-libmysqlclient-dev build-essential\npip install mysqlclient\n", "sudo yum install python3-devel mysql-devel\npip install mysqlclient\n"], ["import glob\nimport random\nimport os\nfilelist  = glob.glob('train/*.txt')\ntest = random.sample(filelist, int(len(filelist)*0.15))\noutput_path = 'test/'\nif not os.path.exists(output_path):\n    os.makedirs(output_path)\n\nfor file in test:\n    txtpath = file\n    impath = file[:-4] + '.jpg'\n    out_text = os.path.join(output_path, os.path.basename(txtpath))\n    out_image = os.path.join(output_path, os.path.basename(impath))\n    print(txtpath,impath,out_text,out_image)\n    os.system('powershell mv ' + txtpath + ' ' + out_text)\n    os.system('powershell mv ' + impath + ' ' + out_image)\n"], ["def solution(sequence):\n    \n    if len(sequence) < 3: # since list with only 1 element is strictly increasing and \n                           # with 2 elements we can always remove 1 to make it strictly increasing\n        return True \n    \n    original_sequence = sequence.copy()\n    # original_sequence = sequence\n    # there is one problem with copying lists in this way. If you modify\n#new_list, old_list is also modified. It is because the new list is\n#referencing or pointing to the same old_list object.\n\n    \n    position_to_pop = 0 #this will be used to remember the position where the strictly increasing does not hold\n    # for example 11,31,21,11 will give position_to_pop as index 2(21) in the for loop below...if it doesn't work then we remove the index 1 (31) to check whether it makes the list strictly increasing\n    \n    if is_sequence_increasing(sequence):\n        return True\n    else:\n        for index, num in enumerate(sequence):\n            if index != len(sequence) -1: # if statment since we want to loop until second last item\n                if sequence[index+1] <= num:\n                    position_to_pop = index + 1\n                    break #breaking since we do not want to check further in the for loop\n    \n    sequence.pop(position_to_pop) #first removing right integer(index+1) where the problem occured\n    \n    if is_sequence_increasing(sequence):\n        return True\n    else: # if the first remove of the right integer doesn't work then we need to remove the left integer and see whether it fixes the issue \n        \n        original_sequence.pop(position_to_pop-1)\n        \n        if is_sequence_increasing(original_sequence):\n            return True\n    \n    return False\n\ndef is_sequence_increasing(sequence_to_check):\n    \n    for index, num in enumerate(sequence_to_check):\n        if index != len(sequence_to_check) -1:\n            if sequence_to_check[index+1] <= num:\n                return False\n    return True\n"], ["from dataclasses import make_dataclass\nClas = make_dataclass('A', \n                      ['d'], \n                      namespace={\n                                 '__post_init__': lambda self: self.__dict__.update(self.d)\n                      })\nd = {'a':1, 'b': 2}\ninstance = Clas(d)\ninstance.a\n"], [], [], ["import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1, weight = None):\n        \"\"\"if smoothing == 0, it's one-hot method\n           if 0 < smoothing < 1, it's smooth method\n        \"\"\"\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.weight = weight\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        assert 0 <= self.smoothing < 1\n        pred = pred.log_softmax(dim=self.dim)\n\n        if self.weight is not None:\n            pred = pred * self.weight.unsqueeze(0)   \n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n", "class SmoothCrossEntropyLoss(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    def k_one_hot(self, targets:torch.Tensor, n_classes:int, smoothing=0.0):\n        with torch.no_grad():\n            targets = torch.empty(size=(targets.size(0), n_classes),\n                                  device=targets.device) \\\n                                  .fill_(smoothing /(n_classes-1)) \\\n                                  .scatter_(1, targets.data.unsqueeze(1), 1.-smoothing)\n        return targets\n\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n        if self.reduction == 'sum' else loss\n\n    def forward(self, inputs, targets):\n        assert 0 <= self.smoothing < 1\n\n        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n        log_preds = F.log_softmax(inputs, -1)\n\n        if self.weight is not None:\n            log_preds = log_preds * self.weight.unsqueeze(0)\n\n        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))\n", "import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\n\n\nif __name__==\"__main__\":\n    # 1. Devin Yang\n    crit = LabelSmoothingLoss(classes=5, smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\n    # 2. Shital Shah\n    crit = SmoothCrossEntropyLoss(smoothing=0.5)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\ntensor(1.4178)\ntensor(1.4178)\n", "class LabelSmoothingLoss(torch.nn.Module):\n    def __init__(self, smoothing: float = 0.1, \n                 reduction=\"mean\", weight=None):\n        super(LabelSmoothingLoss, self).__init__()\n        self.smoothing   = smoothing\n        self.reduction = reduction\n        self.weight    = weight\n\n    def reduce_loss(self, loss):\n        return loss.mean() if self.reduction == 'mean' else loss.sum() \\\n         if self.reduction == 'sum' else loss\n\n    def linear_combination(self, x, y):\n        return self.smoothing * x + (1 - self.smoothing) * y\n\n    def forward(self, preds, target):\n        assert 0 <= self.smoothing < 1\n\n        if self.weight is not None:\n            self.weight = self.weight.to(preds.device)\n\n        n = preds.size(-1)\n        log_preds = F.log_softmax(preds, dim=-1)\n        loss = self.reduce_loss(-log_preds.sum(dim=-1))\n        nll = F.nll_loss(\n            log_preds, target, reduction=self.reduction, weight=self.weight\n        )\n        return self.linear_combination(loss / n, nll)\n", "class LabelSmoothing(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n", "if __name__==\"__main__\":\n    # Wangleiofficial\n    crit = LabelSmoothingLoss(smoothing=0.3, reduction=\"mean\")\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\n    # NVIDIA\n    crit = LabelSmoothing(smoothing=0.3)\n    predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n                                 [0, 0.9, 0.2, 0.2, 1], \n                                 [1, 0.2, 0.7, 0.9, 1]])\n    v = crit(Variable(predict),\n             Variable(torch.LongTensor([2, 1, 0])))\n    print(v)\n\ntensor(1.3883)\ntensor(1.3883)\n", "torch.nn.CrossEntropyLoss(weight=None, size_average=None, \n                          ignore_index=- 100, reduce=None, \n                          reduction='mean', label_smoothing=0.0)\n"], [], ["x, y, z, n = 2, 3, 4, 5\nprint([(i, j, k) for i in range(x + 1) for j in range(y + 1)\n    for k in range(z + 1) if i + j + k != n])\n", "[(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3), (0, 0, 4), (0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 1, 3), (0, 2, 0), (0, 2, 1), (0, 2, 2), (0, 2, 4), (0, 3, 0), (0, 3, 1), (0, 3, 3), (0, 3, 4), (1, 0, 0), (1, 0, 1), (1, 0, 2), (1, 0, 3), (1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 1, 4), (1, 2, 0), (1, 2, 1), (1, 2, 3), (1, 2, 4), (1, 3, 0), (1, 3, 2), (1, 3, 3), (1, 3, 4), (2, 0, 0), (2, 0, 1), (2, 0, 2), (2, 0, 4), (2, 1, 0), (2, 1, 1), (2, 1, 3), (2, 1, 4), (2, 2, 0), (2, 2, 2), (2, 2, 3), (2, 2, 4), (2, 3, 1), (2, 3, 2), (2, 3, 3), (2, 3, 4)]\n"], [], [], [], ["from pyarrow.parquet import ParquetFile\nimport pyarrow as pa \n\npf = ParquetFile('file_name.pq') \nfirst_ten_rows = next(pf.iter_batches(batch_size = 10)) \ndf = pa.Table.from_batches([first_ten_rows]).to_pandas() \n"], ["$ pipenv install\nWarning: Python >= 3.5 was not found on your system...\nNeither 'pyenv' nor 'asdf' could be found to install Python.\nYou can specify specific versions of Python with:\n$ pipenv --python path/to/python\n\n$ pipenv --python `which python3` install\nCreating a virtualenv for this project...\n"], [], [], ["update-alternatives --config python3\nThere are 3 choices for the alternative python3 (providing /usr/bin/python3).\n\n  Selection    Path                Priority   Status\n------------------------------------------------------------\n* 0            /usr/bin/python3.9   3         auto mode\n  1            /usr/bin/python3.6   1         manual mode\n  2            /usr/bin/python3.8   2         manual mode\n  3            /usr/bin/python3.9   3         manual mode\n\nPress <enter> to keep the current choice[*], or type selection number: 1\n"], [], ["import os; \nprint(os.listdir())\n"], ["   \n# Opening JSON file\nf = open('dbl.json',)\n   \n# returns JSON object as \n# a dictionary\ndata = json.load(f)\n   \n# Iterating through the json\n# list\nfor i in data:\n    print(i)\n   \n# Closing file\nf.close()```\n\n\n\n"], ["file = open(\"dbl.json\")\n"], ["$ curl https://dl.google.com/\n\nOutput: <a href=\"https://www.google.com/chrome\"> Found  </a>\n"], [], ["nums[nums[i]-1], nums[i]\n", "nums[i], nums[nums[i]-1]\n", ">>> print(nums)\n>>> [1, 2, 4, 3]\n>>> nums[i], nums[i-1] = nums[i-1], nums[i]\n>>> print(nums)\n>>> [1, 4, 2, 3]\n"], [], [], ["conda install -n notebook_env nb_conda_kernels\n"], ["cd /path_to_your_code_dir/\n", "cd /repos/\n"], [" public int countSeq(int[] arr) {\nint len = arr.length;\nif (len < 2) {\n  return 0;\n}\n\nint s = 0;\nint e = 1;\nint sign = arr[e] - arr[s];\nint count = 0;\n\nwhile (e < len) {\n  while (e < len && arr[e] - arr[e-1] != 0 && isSameSign(arr[e] - arr[e-1], sign)) {\n    sign = -1 * sign;\n    e++;\n  }\n  // the biggest continue subsequence starting from s ends at e-1;\n  int size = e - s;\n  count = count + (size * (size - 1)/2); // basically doing C(size,2)\n  s = e - 1;\n  e = s + 1;\n}\n\nreturn count;\n"], ["if file_extension == 'xlsx':\n    df = pd.read_excel(file.read(), engine='openpyxl')\nelif file_extension == 'xls':\n    df = pd.read_excel(file.read())\nelif file_extension == 'csv':\n    df = pd.read_csv(file.read())\n"], ["from django.db import models\nfrom pydantic import BaseModel\n\nclass CustomList(BaseModel):\n    data: list[dict]\n"], [], ["brew install mysql\n"], [], [], [], [], [], ["import numpy as np; np.random.seed(42)\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport pandas as pd\n\ndf = pd.DataFrame({\"xaxs\" : np.random.randint(50000,250000, size=20),\n                   \"yaxs\" : np.random.randint(7,15, size=20),\n                   \"col\"  : np.random.choice(list(\"ABC\"), size=20)})\n    \nfig, ax = plt.subplots(figsize=(8, 5))    \npalette = sns.color_palette(\"bright\", 6)\nsns.scatterplot(ax=ax, x=\"xaxs\", y=\"yaxs\", hue=\"col\", data=df, \n                marker='o', s=100, palette=\"magma\")\nax.legend(bbox_to_anchor=(1, 1), ncol=1)\nax.set(xlim = (50000,250000))\n\nax.xaxis.set_major_formatter(ticker.EngFormatter())\n\nplt.show()\n"], ["import json5\n\ndata = '''{\n\"Fridge\": [\n    [\"apples\"],\n    [\"chips\",\"cake\",\"10\"]    // This comment here is causing error\n],\n\"car\": [\n    [\"engine\",\"tires\",\"fuel\"],\n    ]\n}'''\n\nprint(json5.loads(data))\n", "{'Fridge': [['apples'], ['chips', 'cake', '10']], 'car': [['engine', 'tires', 'fuel']]}\n"], ["import json\n\njsondata = \"\"\nwith open('contents.json', 'r') as jsonfile:\n    for line in jsonfile:\n        jsondata += line.split(\"//\")[0]\n\nobjec = json.loads(jsondata)\n\nlist_o = objec['Fridge']\n\nfor i in (list_o):\n    print(i)\n", "['apples']\n['chips', 'cake', '10']\n", "objec = json.loads(jsondata)\n", "import commentjson  # python3 -m pip install commentjson\nobjec = commentjson.loads(jsondata)\n"], [], [" path1='C:\\\\Program Files\\\\Capitaria MT5 Terminal\\\\terminal64.exe'\n\n path2='C:\\\\Program Files\\\\Admiral Markets MT5\\\\terminal64.exe'\n\n            \n"], [], [], [], [], [], [">>> df[[\"foo\",\"bar\"]] = df.apply(lambda r: [\"foobar\",\"baz\"], axis=1)\n\"None of [Index(['foo', 'bar'], dtype='object')] are in the [columns]\"\n", "df[[\"foo\",\"bar\"]] = df.apply(lambda r: [\"foobar\",\"baz\"], axis=1, result_type=\"expand\")\n"], ["from pydantic import BaseModel, Field as PydanticField\nfrom bson import ObjectId\n\nclass PyObjectId(ObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n    @classmethod\n    def validate(cls, v):\n        if not ObjectId.is_valid(v):\n            raise ValueError(\"Invalid objectid\")\n        return ObjectId(v)\n    @classmethod\n    def __modify_schema__(cls, field_schema):\n        field_schema.update(type=\"string\")\n", "from models.PyObjectId import PyObjectId\nfrom pydantic import BaseModel, Field as PydanticField\nfrom bson import ObjectId\nclass Users(BaseModel):\n    id: PyObjectId = PydanticField(default_factory=PyObjectId, alias=\"_id\")\n    class Config:\n        allow_population_by_field_name = True\n        arbitrary_types_allowed = True #required for the _id \n        json_encoders = {ObjectId: str}\n"], [], ["!gdown --id 'id of the file'\n"], ["nums = [10, 20, 40, 30]\n", ">>> nums[i], nums[nums[i]-1] = nums[nums[i]-1], nums[i]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n"], ["~$datasheet.xlsx\n", "Excel file format cannot be determined, you must specify an engine manually.\n"], [], [], [], [">>> import dis\n>>> dis.dis(\"nums[i], nums[nums[i]-1] = nums[nums[i]-1], nums[i]\")\n  1           0 LOAD_NAME                0 (nums)\n              2 LOAD_NAME                0 (nums)\n              4 LOAD_NAME                1 (i)\n\n              6 BINARY_SUBSCR\n              8 LOAD_CONST               0 (1)\n             10 BINARY_SUBTRACT\n             12 BINARY_SUBSCR\n             14 LOAD_NAME                0 (nums)\n             16 LOAD_NAME                1 (i)\n             18 BINARY_SUBSCR\n\n             20 ROT_TWO\n\n             22 LOAD_NAME                0 (nums)\n             24 LOAD_NAME                1 (i)\n             26 STORE_SUBSCR\n\n             28 LOAD_NAME                0 (nums)\n             30 LOAD_NAME                0 (nums)\n             32 LOAD_NAME                1 (i)\n             34 BINARY_SUBSCR\n             36 LOAD_CONST               0 (1)\n             38 BINARY_SUBTRACT\n             40 STORE_SUBSCR\n\n             42 LOAD_CONST               1 (None)\n             44 RETURN_VALUE\n", "stack = []\nstack.append(nums[nums[i]-1])\nstack.append(nums[i])\nstack.reverse()\nnums[i] = stack.pop()\nnums[nums[i]-1] = stack.pop()\n", ">>> dis.dis(\"nums[nums[i]-1], nums[i] = nums[i], nums[nums[i]-1]\")\n  1           0 LOAD_NAME                0 (nums)\n              2 LOAD_NAME                1 (i)\n              4 BINARY_SUBSCR\n\n              6 LOAD_NAME                0 (nums)\n              8 LOAD_NAME                0 (nums)\n             10 LOAD_NAME                1 (i)\n             12 BINARY_SUBSCR\n             14 LOAD_CONST               0 (1)\n             16 BINARY_SUBTRACT\n             18 BINARY_SUBSCR\n\n             20 ROT_TWO\n\n             22 LOAD_NAME                0 (nums)\n             24 LOAD_NAME                0 (nums)\n             26 LOAD_NAME                1 (i)\n             28 BINARY_SUBSCR\n             30 LOAD_CONST               0 (1)\n             32 BINARY_SUBTRACT\n             34 STORE_SUBSCR\n\n             36 LOAD_NAME                0 (nums)\n             38 LOAD_NAME                1 (i)\n             40 STORE_SUBSCR\n\n             42 LOAD_CONST               1 (None)\n             44 RETURN_VALUE\n"], ["with open(\"data.html\", \"w\") as file:\n    file.write(data.data)\n"], ["MyEnum = collections.namedtuple(\n    \"MyEnum\", [\"state1\", \"state2\"]\n)(\n    state1=\"state1\", \n    state2=\"state2\"\n)\n"], ["class Variable:\n    def __init__(self, name, value):\n        self._name = name\n        self._value = value\n\n    @property\n    def value(self):\n        print(self._name, 'get', self._value)\n        return self._value\n\n    @value.setter\n    def value(self):\n        print(self._name, 'set', self._value)\n        self._value = value\n\na = Variable('a', 1)\nb = Variable('b', 2)\n\na.value, b.value = b.value, a.value\n", "b get 2\na get 1\na set 2\nb set 1\n"], [], ["t = nums[nums[i]-1], nums[i]  # t = (3,4)\nnums[i] = t[0] # nums = [1,2,3,3]\nn = nums[i]-1 # n = 2\nnums[n] = t[1] # nums = [1,2,4,3]\n", "t = nums[i], nums[nums[i]-1]  # t = (4,3)\nn = nums[i]-1 # n = 3\nnums[n] = t[0] # nums = [1,2,4,4]\nnums[i] = t[0] # nums = [1,2,3,4]\n"], [" click==7.1.2\n colorama==0.4.4\n Flask==1.1.4\n Flask-Script==2.0.6\n itsdangerous==1.1.0\n Jinja2==2.11.3\n MarkupSafe==2.0.1\n Werkzeug==1.0.1\n", " from flask import Flask\n app = Flask(__name__)\n\n if __name__ == \"__main__\":\n     app.run(debug=True)\n", " from flask_script import Manager\n from app import app\n\n manager = Manager(app)\n\n @manager.command\n def hello():\n     print('test')\n\n if __name__ == \"__main__\":\n     manager.run()\n"], ["nums[2], nums[nums[2]-1] = nums[nums[2]-1], nums[2]\n", "nums[2], nums[nums[2]-1] = nums[nums[2]-1], nums[2]\n\nnums[2], nums[nums[2]-1] = nums[3], nums[2]\n\nnums[2], nums[nums[2]-1] = 3, 4\n", "nums[2] = 3\nnums[nums[2]-1] = 4\n\nnums[2] = 3\nnums[3-1] = 4\n\nnums[2] = 3\nnums[2] = 4\n", "print(nums)\n# [1, 2, 4, 3]\n", "nums[nums[2]-1], nums[2] = nums[2], nums[nums[2]-1]\n\nnums[nums[2]-1], nums[2] = nums[2], nums[3]\n\nnums[nums[2]-1], nums[2] = 4, 3\n\nnums[nums[2]-1] = 4\nnums[2] = 3\n\nnums[4-1] = 4\nnums[2] = 3\n\nnums[3] = 4\nnums[2] = 3\nprint(nums)\n# [1, 2, 3, 4]\n"], ["nums[i], nums[nums[i]-1] = nums[nums[i]-1], nums[i]\n", "tmp = nums[nums[i]-1], nums[i]\nnums[i] = tmp[0]\nnums[nums[i] - 1] = tmp[1]\n", "nums[nums[i]-1], nums[i] = nums[i], nums[nums[i]-1]\n", "tmp = nums[i], nums[nums[i]-1]\nnums[nums[i] - 1] = tmp[0]\nnums[i] = tmp[1]\n"], ["nums[i], nums[nums[i]-1] =\n", "nums[nums[i]-1], nums[i] =\n"], ["pip uninstall mysqlclient\npip uninstall django\npip install django\npip install mysqlclient\n"], ["[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\npython-ldap = {path = \"./dependencies/python_ldap-3.1.0-cp37-cp37m-win_amd64.whl\"}\nrequests = \"~=2.0\"\nmysqlclient = \"~=1.0\"\n\n[dev-packages]\n\n[requires]\npython_version = \"^3.7\"\n"], [], ["[requires]\npython_version = \"3.6\"\n", "[requires]\npython_version = \"*\"\n"], ["pip uninstall pyzmq\npip install pyzmq\n"], ["user_list = []\nfor user in users:\n  user_list.append(User(**user))\n", "user_list = [User(**user) for user in users]\n"], [], ["pydoc ...\nThe Ellipsis Object\n*******************\n\nThis object is commonly used by slicing (see Slicings).  It supports\nno special operations.  There is exactly one ellipsis object, named\n\"Ellipsis\" (a built-in name).  \"type(Ellipsis)()\" produces the\n\"Ellipsis\" singleton.\n\nIt is written as \"Ellipsis\" or \"...\".\n\nRelated help topics: SLICINGS\n"], []]