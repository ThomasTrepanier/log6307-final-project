[[" #remove white space at both ends:\n#'buying', 'maint','doors','persons','lug_boot','safety','class'\ndf.buying=df.buying.astype(str).str.strip()\ndf.maint=df.maint.astype(str).str.strip()\ndf.persons=df.persons.astype(str).str.strip()\ndf.lug_boot=df.lug_boot.astype(str).str.strip()\ndf.safety=df.safety.astype(str).str.strip()\ndf.class_account=df.class_account.astype(str).str.strip()\n"], [], ["list_numbers = [{1:'one'},{2:'two'},{3:'three'}]\n\nfor int_rep,str_rep in [(int_rep,str_rep) for x in list_numbers for (int_rep,str_rep) in x.items()]:\n    print(f\"{int_rep} is written as {str_rep}\")\n"], ["def calculate_distance(lat1,lon1,lat2,lon2):\n  # approximate radius of earth in km\n  R = 6373.0\n\n  lat1 = radians(lat1)\n  lon1 = radians(lon1)\n  lat2 = radians(lat2)\n  lon2 = radians(lon2)\n\n  dlon = lon2 - lon1\n  dlat = lat2 - lat1\n\n  a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n  c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n  distance = R * c\n\n  return distance\n", "df['distance'] = [calculate_distance(row.lat1, row.lon1, row.lat2, row.lon2) for row in df.itertuples() ]\n"], ["import math\nfrom math import sin, cos, sqrt, atan2, radians\n\ndef get_distance(in_lat1, in_lon1, in_lat2, in_lon2):\n    # approximate radius of earth in km\n    R = 6373.0\n\n    lat1 = radians(in_lat1)\n    lon1 = radians(in_lon1)\n    lat2 = radians(in_lat2)\n    lon2 = radians(in_lon2)\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    distance = R * c\n\n    return distance\n", "df['Distance'] = df.apply(lambda x: get_distance(x['Departure_lat'], x['Departure_lon'], x['Arrival_lat'], x['Arrival_lon']), axis=1)\n", "        City  Departure_lat  Departure_lon  Arrival_lat  Arrival_lon\n0  CityName1      25.229676      36.012229    51.406374    20.925168\n", "        City  Departure_lat  Departure_lon  Arrival_lat  Arrival_lon    Distance\n0  CityName1      25.229676      36.012229    51.406374    20.925168  3181.11039\n"], ["import pandas as pd\nimport numpy as np\n\nrow = pd.Series({\n    \"lat1\": 25.2296756,\n    \"lon1\": 36.0122287,\n    \"lat2\": 51.406374,\n    \"lon2\": 20.9251681\n})\ndf = pd.concat([row]*5, axis=1).T.apply(np.radians)\n\ndf[\"dlon\"] = df.lon2 - df.lon1\ndf[\"dlat\"] = df.lat2 - df.lat1\n\nR = 6373\na = np.sin(df.dlat / 2)**2 + np.cos(df.lat1) * np.cos(df.lat2) * np.sin(df.dlon / 2)**2\nc = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\ndf[\"distance\"] = R*c\n", "       lat1      lon1     lat2      lon2      dlon     dlat    distance\n0  0.440341  0.628532  0.89721  0.365213 -0.263319  0.45687  3181.11039\n1  0.440341  0.628532  0.89721  0.365213 -0.263319  0.45687  3181.11039\n2  0.440341  0.628532  0.89721  0.365213 -0.263319  0.45687  3181.11039\n3  0.440341  0.628532  0.89721  0.365213 -0.263319  0.45687  3181.11039\n4  0.440341  0.628532  0.89721  0.365213 -0.263319  0.45687  3181.11039\n"], [], ["for number in list_numbers:\n   (int_rep,str_rep), = number.items()\n   print(f\"{int_rep} is written as {str_rep}\")\n"], [], ["from collections import ChainMap\n\nlist_numbers = [{1: 'one'}, {2: 'two'}, {3: 'three'}]\n\nfor int_rep, str_rep in ChainMap(*list_numbers).items():\n    print(f\"{int_rep} is written as {str_rep}\")\n"], ["[print(f\"{int_rep} is written as {str_rep}\")  for d in list_numbers for int_rep, str_rep in d.items()]\n"], ["li = 'Text 1. Text 2? Text 3!'\ni = ['.', '?', '!']\nt = li\nfor y in i:\n    t = t.replace(y, '')\nprint(t)\n", "Text 1 Text 2 Text 3\n"], ["li = 'Text 1. Text 2? Text 3!'\ni = ['.', '?', '!']\nfor y in i:\n    li = li.replace(y, '')\n", "li = 'Text 1. Text 2? Text 3!'\ni = ['.', '?', '!']\nli = re.sub(\"[\" + \"\".join(i) + \"]\", '', li)\n"], ["li = 'Text 1. Text 2? Text 3!'\nt = ''.join(list(filter(lambda x: x not in '.?!', li)))\nprint(t)\n"], ["li = str('Text 1. Text 2? Text 3!')\ni = ['.', '?', '!']\n\nfor y in i:\n    li = li.replace(y,'')\nprint(li)\n"], ["li = 'Text 1. Text 2? Text 3!'\ni = ['.', '?', '!']\n\nfor y in i:\n    li = li.replace(y, '')\n\nprint(li)\n"], ["list1=['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\nlist2=['y','tor','e','eps','ay',None,'le','n']\ns=''\nnew=''\nfor i in list1:\n    for j in list2[::-1]:\n        if i==None:\n            i=''\n        elif j==None:\n            j=''\n        new=i+j\n        s=s+new+' '\n        list2.pop(-1)\n        break\nprint(s)\n"], ["Python: Language Server\nDefine Type of the Language Server\nSELECT: Pylance\n", "import numpy as np\nimport pandas as pd\n"], ["\"python.languageServer\": \"Microsoft\"\n", "\"python.languageServer\": \"Pylance\"\n"], ["Starting development server at http://127.0.0.1:8000/\nQuit the server with \nCTRL-BREAK.\nNot Found: /Quit\n[27/May/2021 22:36:07] \"GET /Quit HTTP/1.1\" 404 2066\n[27/May/2021 22:36:59] \"GET / HTTP/1.1\" 200 5873\n[27/May/2021 22:37:04] \"GET / HTTP/1.1\" 200 5873\n"], ["start = time.time()\n#result = numpy.empty([5000, 5000])\nw, h = 5000, 5000;\nresult = [[0 for x in range(w)] for y in range(h)]\nfor i in range(5000):\n    for j in range(5000):\n        result[i][j] = (i * j) % 10\nend = time.time()\nprint(end - start) # 4.4 s\n"], [], ["oldList = list(range(1,10))\nnewList = []\nfor i in oldList:\n    if (i%2 == 0) and (i%4 != 0):\n        try:\n            newList.append(i+1)\n            newList.append(i+2)\n        except IndexError:\n            break\n", ">>> newList\n[3, 4, 7, 8]\n"], [], ["def convert(x,d):\n    return int(x) / 10**d\n\nprint(convert(11098,2))\n"], ["def convert(number,points):\n    decimal = pow(10,points) # power function\n\n    return number / decimal\n\nnum = convert(11098,3)\nprint(num)\n", "11.098\n", "num = convert(11098,5)\nprint(num)\n# Output: 0.11098\n"], [], ["def convert(value, decimals):\n     value = str(value) # convert value to string-type for indexing\n     d1 = value[:-decimals] # d1 comes before the decimal\n     d2 = value[-decimals:] # d2 comes after the decimal\n     output = f\"{d1}.{d2}\" # join d1 and d2 with . using f-string\n     return output\n\nprint(convert(11098, 2))\n\n110.98\n"], ["x = ['apple','orange','banana','kiwi']\n\nenter = str(input('enter your fruit: '))\n\ndef exists():\n  print('this fruit exists')\n\nenter = enter.lower()\n\nfruitExist = False\n\nfor fruit in x:\n   if fruit.lower() == enter:\n     fruitExist = True\n     break\n\nif fruitExist :\n    exists()\nelse:\n   print(\"this fruit doesn't exist\")\n"], ["x = ['apple', 'orange', 'banana', 'kiwi']\nenter = input('enter your fruit: ')\n\ndef exists():\n    print('this fruit exists')\n\nfor fruit in x:\n    if fruit == enter:\n        exists()\n        break\nelse:  # else must be under for and is executed if the loop has been executed completely, without interruption (break)\n    print(\"this fruit doesn't exist\")\n"], ["x = ['apple','orange','banana','kiwi']\n\nenter = input('enter your fruit: ')\n\nif enter in x:\n    print('this fruit exists')\nelse:\n   print(\"this fruit doesn't exist\")\n", "x = ['apple','orange','banana','kiwi']\n\nenter = input('enter your fruit: ')\nerror=0\nfor e in x:\n    if enter in x:\n        print('this fruit exists')\n        break\n    else:\n        error=1\nif error==1:\n    print(\"this fruit doesn't exist\")\n", "x = ['apple','orange','banana','kiwi']\n\nenter = input('enter your fruit: ')\nif not [print(\"found\") for f in x if enter==f]:print(\"not found\")\n"], ["for fruit in x:\n   if fruit == str(enter):\n     exists()\n     break\n else:\n   print('this fruit doesn't exist')\n", "x = ['apple','orange','banana','kiwi']\n", "if enter in x:\n    exists()\n    break\nelse:\n    print('this fruit doesn't exist')\n"], [], ["price= int(input(\"Enter the price: \"))\n\nprice_list=[]\n\nwhile price != 0:\n    if price< 0:\n       print(\"Wrong entry\")\n       break\n    price_list.append(price)\n    price=int(input())            \n    price_sum= sum(price_list)\nif price > 0:\n    print(f\"Avg price is: {price_sum / len(price_list)}\")\n"], ["price= int(input(\"Enter the price: \"))\n\nprice_list=[]\n\nwhile price!= 0:\n    price_list.append(price)\n    if price< 0:\n       print(\"Wrong entry\")\n       break\n    price=int(input())\n\n    price_sum= sum(price_list)\nelse:\n    print(f\"Avg price is: {price_sum / len(price_list)}\")\n"], ["price= int(input(\"Enter the price: \"))\n\nprice_list=[]\n\nwhile price!= 0:\n    \n    if price< 0:\n       print(\"Wrong entry\")\n       break\n    price_list.append(price)\n    price=int(input())\nif price_list:\n    price_sum= sum(price_list)\n    print(f\"Avg price is: {price_sum / len(price_list)}\")\n"], ["price= int(input(\"Enter the price: \"))\nok = True\nprice_list=[]\n\nwhile price!= 0:\n    price_list.append(price)\n    if price< 0:\n       print(\"Wrong entry\")\n       ok = False\n       break\n    price=int(input())\nif ok:\n    price_sum= sum(price_list)\n    print(f\"Avg price is: {price_sum / len(price_list)}\")\n"], ["if price < 0:\n    break\n"], ["x=float(\"nan\")\ns_nan = str(x)\nif s_nan == \"nan\":\n   # What you need to do...\n   print('x is not a number')\n"], ["import json\nf = open('simplejson2.json')\ndata = json.load(f)\ntotal_len = len(data)\nn = 0\ncount = 0\nwhile n < total_len:\n    if data[n][\"status\"] == \"failed\":\n        count += 1\n        \n    if n == total_len-1:\n        break\n    n += 1\n    \nprint(\"No of failed : %s\" % count) \n\n\n"], ["import json\nfrom collections import Counter\n\nwith open('simplejson2.json', 'r') as f:\n    str1=f.read()\n\ndata=json.loads(str1)\n\nc = Counter(k[:] for d in data for k, v in d.items() if k.startswith('status') and v.startswith('failed'))\n#c now has the count. Below it will check if count is 0 or not and print.\nif c['status']>0:\n    print(\"There are\",c['status'],\"failed cases\")\nelse:\n    print(\"test case passed sucessfully\")\n"], ["import json\ns = '''[\n  {\n    \"status\": \"passed\",\n    \"name\": \"Whiskers\",\n    \"species\" : \"cat\",\n    \"foods\": {\n      \"likes\": [\"celery\", \"strawberries\"],\n      \"dislikes\": [\"carrots\"]\n    }\n  },\n  {\n    \"status\": \"failed\",\n    \"name\": \"Woof\",\n    \"species\" : \"dog\",\n    \"foods\": {\n      \"likes\": [\"dog food\"],\n      \"dislikes\": [\"cat food\"]\n    }\n  },\n  {\n    \"status\": \"failed\",\n    \"name\": \"Fluffy\",\n    \"species\" : \"cat\",\n    \"foods\": {\n      \"likes\": [\"canned food\"],\n      \"dislikes\": [\"dry food\"]\n    }\n  }\n]'''\n\nd = json.loads(s)\ncount = 0\n\nfor i in d:\n    if i['status'] == 'failed':\n       count += 1\n\nprint(f'Failed: {count}')\n", "Output:\n\n2\n"], ["import json\nfrom collections import Counter\n\n\ndata = [\n  {\n    \"status\": \"passed\",\n    \"name\": \"Whiskers\",\n    \"species\" : \"cat\",\n    \"foods\": {\n      \"likes\": [\"celery\", \"strawberries\"],\n      \"dislikes\": [\"carrots\"]\n    }\n  },\n  {\n    \"status\": \"failed\",\n    \"name\": \"Woof\",\n    \"species\" : \"dog\",\n    \"foods\": {\n      \"likes\": [\"dog food\"],\n      \"dislikes\": [\"cat food\"]\n    }\n  },\n  {\n    \"status\": \"failed\",\n    \"name\": \"Fluffy\",\n    \"species\" : \"cat\",\n    \"foods\": {\n      \"likes\": [\"canned food\"],\n      \"dislikes\": [\"dry food\"]\n    }\n  }\n]\n\nc = Counter(v[:] for d in data for k, v in d.items() if v==\"failed\")\nprint(json.dumps(c)) \n\nthe output:\n{\"failed\": 2}\n"], [], ["def inner_fib(acc, num):\n    # acc is a list of two values \n    return [(acc[0]+acc[1]) % 997, (acc[0]+2*acc[1]) % 997]\n\ndef fib(n):\n  return reduce(inner_fib, \n                range(2, n+1), # start from 2 since n=1 is covered in the base case\n                [1,2])        # [1,2] is the base case\n\n"], ["data = \"\"\"\nCome to the\nRiver\nOf my\nSoulful\nSentiments\nMeandering silently\nYearning for release.\nHasten\nEarnestly\nAs my love flows by\nRushing through the flood-gates\nTo your heart.\n\"\"\"\n\ntext = data[1];\ni = 1\nwhile i < len(data):\n  if i+1 == len(data):\n    break;\n  if data[i] == '\\n':\n    text = text + data[i+1].lower()\n  i = i + 1\nprint(text)\n"], [], ["data = \"\"\"\nCome to the\nRiver\nOf my\nSoulful\nSentiments\nMeandering silently\nYearning for release.\nHasten\nEarnestly\nAs my love flows by\nRushing through the flood-gates\nTo your heart.\n\"\"\"\n\n# Get each line into a list by splitting data on every newline\n# ['', 'Come to the', 'River', 'Of my', ... 'To your heart.', '']\ndata_list = data.split(\"\\n\")\n\n# Get a list of the first letter of every list element\n# use only those elements with length > 0\n# ['C', 'R', 'O', 'S', 'S', 'M', 'Y', 'H', 'E', 'A', 'R', 'T']\nletters_list = [x[0] for x in data_list if len(x)>0]\n\n# Join it to make one word\n# 'CROSSMYHEART'\nmy_word = ''.join(letters_list)\n\n# Capitalise only the first letter\nmy_word = my_word.capitalize()\n\n# Print out the answer. \n# 'Crossmyheart'\nprint(my_word)\n"], ["x = ''.join([i[0] for i in data.splitlines() if len(i) > 0])\n"], ["data = [\"Come to the\", \"River\", \"Of my\", \"Soulful\", \"Sentiments\", \"Meandering silently\", \"Yearning for release\", \"Hasten\", \"Earnestly\", \"As my love flows by\", \"Rushing through the flood-gates\", \"To your heart\"]\n\nnewString = \"\"\nfor word in data:\n    letter = word[0]\n    newString = newString + letter\n    print(newString[-1], end=\" \")\n"], ["import pandas as pd\ndf = pd.DataFrame({'signal': [1,0,0,0,1,1,1,0,0,1,1,1,1,0,0]})\n\ndf.loc[(np.sign(df['signal']).diff().ne(0))]\n", "    signal\n0   1\n1   0\n4   1\n7   0\n9   1\n13  0\n"], [], ["def chars_and_nums(text):\n    if not text:\n        return iter(), iter()\n    return filter(str.isdigit, text), filter(str.isalpha, text)\n", "def chars_and_nums_efficient(text):\n    if not text:\n        return [], []\n    digits, chars = [], []\n    for c in text:\n        if c.isdigit():\n            digits.append(c)\n        elif c.isalpha():\n            chars.append(c)\n    return digits, chars\n"], ["from itertools import groupby\n\nparts = [\"\".join(g) for _, g in groupby(string, key=str.isdigit)]\n"], ["\nstring = \"Hello world001\"\nend = []\nlast_change = string[-1].isdigit()\ntemp = []\nfor x in range(len(string)-1, -1, -1):\n    char = string[x]\n    changed = char.isdigit()\n    if changed==last_change:\n        temp.append(char)\n        continue\n    temp.reverse()\n    end.append(\"\".join(temp))\n    temp = [char]\n    last_change = changed\n\nif temp:\n    temp.reverse()\n    end.append(\"\".join(temp))\n\nend.reverse()\nprint(end)\n", "string_part = string[:x]\n", "number = end[-1]\n# or \"\".join(temp) if you do not overwrite it\n", "for char in string: ...\n"], ["import re \nstring = 'Hello, welcome to my world001'\nm=re.search(r'^(.*?)(\\d+)$', string)\n\n>>> m.groups()\n('Hello, welcome to my world', '001')\n", ">>> re.split(r'(?<=\\D)(?=\\d+$)', string)\n['Hello, welcome to my world', '001']\n", "for i,(c1,c2) in enumerate(zip(string, string[1:]),1):\n    if c2.isdigit(): break\n\ns1,s2=(string[0:i],string[i:])\n\n>>> (s1,s2)\n('Hello, welcome to my world', '001')\n"], [], [], ["!pip install --upgrade scikit-learn\n!pip install --upgrade imblearn\n"], ["old = [('1', 'Server'), ('2', 'Computer')]\n\ninvoicenr = ['44', '44']\n\nnew = [(a, *b) for a, b in zip(invoicenr, old)]\n", "[('44', '1', 'Server'), ('44', '2', 'Computer')]\n"], ["data=[('1', 'Server'), ('2', 'Computer')]\n\nres=[('44',) + x for x in data]\n", "[('44', '1', 'Server'), ('44', '2', 'Computer')]\n"], ["A = [('1', 'Server'), ('2', 'Computer')]\nB = ['44','44']\nC= [tuple([b] + list(a)) for b,a in zip(B, A)]\nprint(C)\n"], ["x1 = [('1', 'Server'), ('2', 'Computer')]\nx2 = ('44',)\n\n[x2 + x for x in x1]\n"], ["new_list = [(invoicenr,) + item for item in old_list]\n"], ["newBigList = []\nFor singleTuple in myListTuple:\n    newList = list(singleTuple)\n    newList.append(\"someValue\")\n    newTuple = tuple(newList)\n    newBigList.append(newTuple)\n"], [], ["import random\n\nk = 50  # number to exclude\nn = 100  # range: choices should be in {0,...,49,51,...,99}\nrnd = random.randrange(0, n - 1)  # in {0,...,98}\nrnd += int(rnd >= k)\n"], [], [], ["import ast\n\nlist_names = ['!sdf?', '!sdfds?', '!afsafsa?', '!afasf?']\n\nmodified_list_names = ast.literal_eval(str(list_names).replace(\"!\",\".\").replace(\"?\",\".\"))\n\nprint(modified_list_names)\n"], [], [], [], [], [], ["def takecommand():\nr = sr.Recognizer()\nwith sr.Microphone() as source:\n    r.adjust_for_ambient_noise(source, 1)\n    print(\"Listening....\")\n    speak(\"listening\")\n    audio = r.listen(source)\n\n\n    try:\n        print(\"Recognizing...\")\n        query = r.recognize_google(audio, language='en-in')\n        print(f\"You said: {query}\\n \")\n\n    except sr.UnknownValueError:\n        speak(\"Could not hear that, Try saying again\")\n\n    except sr.RequestError:\n        speak(\"Make Sure that you have a good Internet connection\")\n    return query\n"], [], ["import os\nprint(os.path.exists('test_case_transformation.txt'))\n", "cd /tmp; python test_case_transformation.py                                                                       \nTrue\n\ncd /; python /tmp/test_case_transformation.py                                                                            \nFalse\n"], ["    from pathlib import Path\n\n    parents = Path(__file__).resolve().parents. // get all the parents of \n    current file location\n    root_path = str(parents[0])\n\n    //Create file\n    with Path(root_path, \"file_name.txt\").open(mode=\"w\") as fp:\n       fp.write(\"some data\")\n\n    //read file\n    with Path(root_path, \"file_name.txt\").open(mode=\"r\") as fp:\n      for line in fp:\n         ---do something-----\n"], ["with open(\"test_case_transformation.txt\") as f:\n    print(f.read())\n", "with open(\"/path/to/file/test_case_transformation.txt\") as f:\n    print(f.read()) \n", "with open(r\"C:\\path\\to\\file\\test_case_transformation.txt\") as f:\n    print(f.read())\n"], ["import os\nwith open(os.getcwd() +\"\\\\test_case_transformation.txt\") as f:\n"], ["with open(\"file.txt\") as f:\n    ...\n"], ["pip install pipwin\n"], [], ["def check_ab(str):\n    if (len(str)==0):\n        return True\n    \n    if (str[0] == 'a'):\n        \n        if (len(str[1:])>1 and str[1:3] == 'bb'):\n            return check_ab(str[3:])\n        \n        else:\n            return check_ab(str[1:])\n        \n    else:\n        return False\n    \n\nstr = input()\nif (check_ab(str)):\n    print(\"true\")\nelse:\n    print(\"false\")\n"], ["from glob import glob\n\nfullpath = f'C:\\Users\\siddhn\\Desktop\\phone[1-6].csv'\ndfs = [pd.read_csv(file) for file in glob(fullpath)]\n\nprint(dfs[0])\n"], ["import more_itertools\noldlist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[*more_itertools.interleave(oldlist[2::4], oldlist[3::4])]\n# [3, 4, 7, 8]\n"], ["from sys import getsizeof\nimport math\ndef chunkify_list(L, max_size_kb):\n    chunk_size_elements = int(math.ceil(len(L)/int(math.ceil(getsizeof(L)/(1024*max_size_kb)))))\n    return [L[x: x+chunk_size_elements] for x in range(0, len(L), chunk_size_elements)]\n"], [], ["import pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randint(1,100000, (1739, 26)))\ndf.columns = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n\nN = 7  # 150 in your case\nidx = np.argsort(df.values, 1)[:, 0:N]\n\npd.concat([pd.DataFrame(np.take_along_axis(df.to_numpy(), idx, axis=1), index=df.index),\n           pd.DataFrame(df.columns.to_numpy(), index=df.index)],\n           keys=['Value', 'Columns'], axis=1)\n", "      Value                                           Columns                  \n          0      1      2      3      4      5      6       0  1  2  3  4  5  6\n0      5193   7752   8445  19947  20610  21441  21759       C  K  U  V  I  G  P\n1       432   3607  16278  17138  19434  26104  33879       R  J  W  C  B  D  G\n2        16   1047   1845   9553  12314  13784  19432       K  S  E  F  M  O  U\n3       244   5272  10836  13682  29237  33230  34448       K  Q  A  S  X  W  G\n4      9765  11275  13160  22808  30870  33484  42760       K  T  L  U  C  D  M\n5      2034   2179   4980   7184  14826  15238  22807       Z  H  F  Q  L  R  X\n...\n"], [">>> [val for j, val in enumerate(old_list) if j % 4 in (2, 3)]\n\n[3, 4, 7, 8]\n"], ["points = x.keys()\ndf['POINT']=points\n"], ["pd.DataFrame.from_dict(x, 'index', columns=[\"X\", \"Y\", \"Z\"])\n", "     X    Y    Z\nA  3.4  4.2  5.6\nB  5.6  7.8  2.3\n"], ["df_data = [[key] + values for key, values in data_dict.items()]\ndf = pd.DataFrame(df_data, columns=[\"POINT\", \"X\", \"Y\", \"Z\"])\n"], ["df = pd.DataFrame(x).T.reset_index()\ndf.columns = ['POINT', 'X', 'Y',  'Z']\n", "print(df)\n\n  POINT    X    Y    Z\n0     A  3.4  4.2  5.6\n1     B  5.6  7.8  2.3\n"], ["x = { 'A': [3.4, 4.2, 5.6],\n      'B': [5.6, 7.8, 2.3]\n    }\ndf = pd.DataFrame(x).T.rename(columns= {0: 'X', 1: 'Y', 2: 'Z'})\n", "     X    Y    Z\nA  3.4  4.2  5.6\nB  5.6  7.8  2.3\n"], ["a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nb = [a[i]  for i in range(len(a)) if i%4 in (2,3)]\n\n# Output: b = [3, 4, 7, 8]\n"], ["first = oldlist[2::4]\nsecond = oldlist[3::4]\npairs = [(x, y) for x, y in zip(first, second)]\n", "newlist = [x for p in pairs for x in p]\n", "newlist = [z for p in [(x, y) for x, y in zip(oldlist[2::4], oldlist[3::4])] for z in p]\n"], [], [], [], ["from functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef fibonacci(n, p):\n    \"Calculate Fibonacci(n) modulo p\"\n    if n < 3:\n        return [0, 1, 1][n]\n    if n % 2 == 0:\n        m = n // 2\n        v1 = fibonacci(m - 1, p)\n        v2 = fibonacci(m, p)\n        return (2*v1 + v2) * v2 % p\n    else:\n        m = (n + 1) // 2\n        v1 = fibonacci(m, p) ** 2\n        v2 = fibonacci(m - 1, p) ** 2\n        return (v1 + v2) % p\n\n\nprint(fibonacci(500, 997))\n#=> 836\nprint(fibonacci(1000, 997))\n#=> 996\n", "# Recursion tree:\n500\n  249\n    124\n      61\n        30\n          14\n            6\n              2\n              3\n                1\n                2\n            7\n              4\n          15\n            8\n        31\n          16\n      62\n    125\n      63\n        32\n  250\n", "500\n  499\n    498\n      ...\n        ...\n           1\n"], [], ["from itertools import accumulate\nimport operator as op\n\nip_lst = [1,2,3,4,5]\nprint(list(accumulate(ip_lst, func=op.mul)))\n\n"], ["!make\n", "GPU=1\nCUDNN=1\nOPENCV=1\n"], [], ["import random\n\nrandom_range = range(0,1000)\nexclude = [5, 55, 555]\n\ndef generate(n:int, ran: range, exclude:list):\n    exclusive_range = range(ran.start, ran.stop-len(exclude))\n    randoms = []\n    for i in range(n):\n        r = random.choice(exclusive_range)\n        if r in exclude:\n            r = exclusive_range.stop + exclude.index(r)\n        randoms.append(r)\n    return randoms\n\nx = generate(1000, random_range, exclude)\n"], ["def random_numbers_except(a, b, exclusions):\n    while True:\n        while (choice := random.randint(a, b)) in exclusions:\n            pass\n        yield choice\n\nnumbers = [number for _, number in zip(range(5), random_numbers_except(0, 10, [2, 5, 7]))\n"], ["numbers = list(range(0, 11))\nnumbers.remove(k)\nresult = random.choices(numbers, k=5)\n"], ["import more_itertools\n\nlst = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\n\nprint(tuple(more_itertools.split_at(lst, lambda x: x == 'k')))\n", "(['a'], ['b', 'c'], ['d', 'e', 'g'])\n"], ["for i in range(1, 7):\n    locals()[f'df{i}'] = pd.read_csv(fr'C:\\Users\\siddhn\\Desktop\\phone{i}.csv')\n\nprint(df1)\nprint(df2)\n"], ["var = []\nfor i in range(1, 7):\n    var.append(i)\n\nprint(var[0])\nprint(var[2])\n"], ["df_list = [pd.read_csv(r'C:\\Users\\siddhn\\Desktop\\phone'+str(i)+'.csv', engine = 'python') for i in range(1, 7)]\ndf_list[1]\n"], ["for i in range(1, 7):\n    'df'+i = pd.read_csv(r'C:\\Users\\siddhn\\Desktop\\phone'+str(i)+'.csv', engine = 'python')\n", "for i in range(7):\n   df.append(pd.read_csv(r'C:\\Users\\siddhn\\Desktop\\phone'+str(i)+'.csv', engine = 'python')\n"], ["import numpy \nimport qiskit as qc \nfrom qiskit import QuantumCircuit, execute, Aer\nimport matplotlib\nfrom qiskit.visualization import plot_state_city\nimport matplotlib.pyplot as plt\n\ncirc = qc.QuantumCircuit(3)\n\ncirc.h(0)\ncirc.cx(0,1)\ncirc.cx(0,2)\ncirc.draw(output='mpl')\n\nbackend = Aer.get_backend('statevector_simulator')\njob = execute(circ, backend)\nresult = job.result()\noutputstate = result.get_statevector(circ, decimals=3)\nprint(outputstate)\nplot_state_city(outputstate)\n\nplt.show() \n"], ["a = 0.1\nb = 0.2\nc = a + b \nanswer = round(c,3)   # here 3 represents the number of digits you want to round\nprint(answer)    \n"], ["#for two decimal places use 2f, for 3 use 3f\nval = 0.1 + 0.2\nprint(f\"val = {val:.2f}\")\n\n#output | val = 0.30\n\n#or else \nprint(round(val,2))\n\n#output | val = 0.30\n"], ["a = 0.1 + 0.2\na = round(a, 2)\nprint(a)\nOutput: 0.3\n"], [], [], ["from decimal import Decimal\ndef exact_add(*nbs):\n    return float(sum([Decimal(str(nb)) for nb in nbs]))\n\nexact_add(0.1, 0.2)\n# > 0.3\n"], [], [], [], ["Process finished with exit code -1073741571 (0xC00000FD)\n", "[...]\nfa 000000e9`6da1b680 00007fff`fb698a6e     python37!PyArg_UnpackStack+0x371\nfb 000000e9`6da1b740 00007fff`fb68b841     python37!PyEval_EvalFrameDefault+0x73e\nfc 000000e9`6da1b980 00007fff`fb698a6e     python37!PyArg_UnpackStack+0x371\nfd 000000e9`6da1ba40 00007fff`fb68b841     python37!PyEval_EvalFrameDefault+0x73e\nfe 000000e9`6da1bc80 00007fff`fb698a6e     python37!PyArg_UnpackStack+0x371\nff 000000e9`6da1bd40 00007fff`fb68b841     python37!PyEval_EvalFrameDefault+0x73e\n\n2:011> ? 000000e9`6da1bd40 - 000000e9`6da1ba40 \nEvaluate expression: 768 = 00000000`00000300\n", "[...]\nif __name__=='__main__':\n    sys.setrecursionlimit(60000)\n    print(fib(50000)[0])\n", "0:000> .exr -1\nExceptionAddress: 00961015 (RecursionCpp!fib+0x00000015)\nExceptionCode: c00000fd (Stack overflow)\n[...]\n0:000> k\n[...]\nfc 00604f90 00961045     RecursionCpp!fib+0x45 [C:\\...\\RecursionCpp.cpp @ 7] \nfd 00604fb0 00961045     RecursionCpp!fib+0x45 [C:\\...\\RecursionCpp.cpp @ 7] \nfe 00604fd0 00961045     RecursionCpp!fib+0x45 [C:\\...\\RecursionCpp.cpp @ 7] \nff 00604ff0 00961045     RecursionCpp!fib+0x45 [C:\\...\\RecursionCpp.cpp @ 7] \n\n0:000> ? 00604ff0 - 00604fd0\nEvaluate expression: 32 = 00000020\n"], [], ["userID = 1\n\nprint(\"Modify: \")\nprint(\"1. Username\")\nprint(\"2. Password\")\nprint(\"3. Contact number\")\nprint(\"4. Location\")\nprint()\nprint(\"Enter 'main' to back to main screen\")\nselect = int(input())-1\nchange = input(\"Change to: \")\n\nif select != 'main:':\n    if int(select) <= 4:\n        # save to individual file\n        with open(\"user_\" + str(userID) + \".txt\", 'r+') as file:\n            data = file.readlines()\n        for index, line in enumerate(data):\n            if index == select:\n                data[index] = line.replace(line, f\"{change}\\n\")\n                break\n        with open(\"user_\" + str(userID) + \".txt\", 'r+') as file:\n            file.writelines(data)\n        print(\"Changed to:\", change)\n        print(\"Change successful\")\n        print(\"\\n Bring you back to <customer main menu>\")\n"], ["with open(path_to_text_file,'r') as txt:\n    text=txt.readlines()\n    text[3]='phone number\\n'\n\n    with open(path_to_text_file,'w') as txt:\n        txt.writelines(text)\n"], ["userID = 1\n\nprint(\"Modify: \")\nprint(\"1. Username\")\nprint(\"2. Password\")\nprint(\"3. Contact number\")\nprint(\"4. Location\")\nprint()\nprint(\"Enter 'main' to back to main screen\")\nselect = int(input())\nchange = input(\"Change to: \")\n\nif select != 'main:':\n    if int(select) <= 4:\n        #save to individual file\n        with open(\"user_\"+str(userID)+\".txt\",'r') as f:\n            lines = f.readlines()\n        # fhand = open(\"user_\"+str(userID)+\".txt\",'r+') \n        \n        lineNo = 0 #line number\n        for line in lines: \n            lineNo = lineNo + 1\n            if lineNo == select -1:\n                lines[lineNo] = line.replace(line, f'{change}\\n')\n                with open(\"user_\"+str(userID)+\".txt\",'w') as f:\n                    f.write(\"\".join(lines)) #replace the content\n                print()\n                print(\"Changed to:\", change)\n                print(\"Change successful\")\n                print(\"\\n Bring you back to <customer main menu>\")\n                break\n \n        # fhand.close()\n"], [], [], ["data = [1, 0, 0, 0, 1, 1, 1, 0, 2, 3, 1, 0, 0, 1, 1, 1, 0]\n\ndef calc(data):\n    sum = 0\n    new = []\n    for i in range(len(data)):\n        if data[i] == 0:\n            new.append(sum)\n            if i == len(data) - 1:\n                new.append(0)\n            sum = 0\n        else:\n            sum = sum = sum + data[i]\n            new.append(0)\n            if i == len(data) - 1:\n                new.append(sum)\n    if new[0] == 0:\n        del new[0]\n    return new\n"], ["def slice_when(predicate, iterable):\n    i, x, size = 0, 0, len(iterable)\n    while i < size-1:\n        if predicate(iterable[i], iterable[i+1]):\n            yield iterable[x:i+1]\n            x = i + 1\n        i += 1\n    yield iterable[x:size]\n", "data = [1, 0, 0, 0, 1, 1, 1, 0, 2, 3, 1, 0, 0, 1, 1, 1, 0]\n\ntest_method = slice_when(lambda _, y: y==0, data)\nlist(test_method)\n#=> [[1], [0], [0], [0, 1, 1, 1], [0, 2, 3, 1], [0], [0, 1, 1, 1], [0]]\n", "res = []\nfor e in slice_when(lambda _, y: y==0, data):\n    res += [0]*(len(e)-1) + [sum(e)]\n    \nres\n#=> [1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0]\n"], ["def arrange_list(arr):\n    value = 0\n    for x in range(len(arr)-1):\n        if arr[x] != 0:\n            value += arr[x]\n        else:\n            value = 0\n        if arr[x+1] != 0:\n            arr[x] = 0\n        else:\n            arr[x] = value\n            value = 0\n    if value !=0:\n        arr[-1] = value + arr[-1]\n    return arr\n"], ["data = [1, 0, 0, 0, 1, 1, 1, 0, 2, 3, 1, 0, 0, 1, 1, 1, 0]\nk = len(data)-1\nfor i in range(0,k):\n    if(data[i]==0):\n        continue\n    if(data[i]!=0 and data[i+1]!=0):\n        data[i+1]=data[i]+data[i+1]\n        data[i]=0\nprint(data)\n"], ["data = [1, 0, 0, 0, 1, 1, 1, 0, 2, 3, 1, 0, 0, 1, 1, 1, 0]\n\nx = 0\nr = []\nfor n in data:\n    if n == 0:\n        if x:\n            r.append(x)\n            x = 0\n        r.append(n)\n    else:\n        x += n\n        \nprint(r)\n[1, 0, 0, 0, 3, 0, 6, 0, 0, 3, 0]\n"], ["data=[1, 0, 0, 0, 1, 1, 1, 0, 2, 3, 1, 0, 0, 1, 1, 1, 0]\nk = 1\nwhile True:\n    \n    if(data[k-1] == 0):\n        pass\n    elif(data[k] == 0):\n        pass\n    elif(data[k] != 0):\n        data[k] = data[k] + data[k-1]\n        print(data[k])\n        data[k-1] = 0\n    k = k+1\n    if(k == len(data)):\n        break\nprint(data)\n", "3\n5\n6\n2\n3\n[1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0]\n"], ["ans = []\ntmp = 0\ndata = [1, 0, 0, 0, 1, 1, 1, 0]\nfor curr in data:\n    if curr != 0:\n        tmp += curr\n    elif tmp == 0:\n        ans.append(tmp)\n    else:\n        ans.append(tmp)\n        ans.append(0)\n        tmp = 0\nprint(ans)\n"], ["def convert(A):\n    B = [0] * len(A)\n    partial_sum = 0\n    for i in range(len(A) - 1):\n        partial_sum += A[i]\n        if A[i + 1] == 0:\n            B[i] = partial_sum\n            partial_sum = 0\n            \n    if A[-1] != 0:\n        B[-1] = partial_sum + A[-1]\n        \n    return B\n"], ["def fib_acc(n, a, b):\n    if n == 1:\n        return (a, b)\n    return lambda: fib_acc(n - 1, (a+b) % 997, (a+2*b) % 997)\n\ndef fib(n):\n    x = fib_acc(n, 1, 2)\n    while callable(x):\n        x = x()\n    return x\n\nif __name__=='__main__':\n    print(fib(50000)[0])\n"], ["import sys\n\nsys.setrecursionlimit(new_limit)\n"], ["[print(col) for col in df.select_dtypes(include = object).columns]\nName\nJob\n"], ["import numpy as np\n# Get columns whose data type is object\nfilteredColumns = df.dtypes[df.dtypes == np.object]\n# List of columns whose data type is object\nlistOfColumnNames = list(filteredColumns.index)\nprint(listOfColumnNames)\n"], ["df.columns[df.dtypes == object].to_list()\n", ">> ['Name', 'Job']\n"], ["df.select_dtypes('object').columns.to_list()\n", "['Name', 'Job']\n", "for column in df:\n    if df[column].dtype == 'object':\n        print(column)\n\n"], ["[x for x in df.columns if df[x].dtype == object]\n", "['Name', 'Job']\n"], ["df.dtypes[df.dtypes == 'object'].index.values\n", "array(['Name', 'Job'], dtype=object)\n", "[print(val) for val in df.dtypes[df.dtypes == 'object'].index]\nName\nJob\n"], ["for column in df.columns:\n    if type(column) == 'object':\n        print(column)\n"], [">>>num = \"8005551212\"\n>>>req = \"{}{}{}-{}{}{}-{}{}{}{}\".format(*num)\n>>>print(req)\n\n '800-555-1212'\n"], [], ["n = '8005551212'\n\n# Simple Solution\n'-'.join([n[:3],n[3:6],n[6:]])\n# Out[29]: '800-555-1212'\n\n# General Solution\nix = [0,3,6]\n'-'.join([n[i:j] for i,j in zip(ix, ix[1:]+[None])])\n# Out[21]: '800-555-1212'\n"], [], ["s_num = str(num) # num being the phone number integer\nf'{s_num[:3]}-{s_num[3:6]}-{s_num[6:]}' # this is the result\n"], ["import itertools \n\ndef countways(l, target): \n    data = []  \n    for length in range(1, target+1):  \n        data.extend([x for x in itertools.combinations_with_replacement(l, length) if sum(x) == target]) \n    return len(data)\n"], [], ["pip install tensorflow-estimator==1.15.0\n"], ["from tensorflow.python.framework.random_seed import set_random_seed\n"], ["my_list = [1,2,2,0]\n\nresult = list(set(filter(lambda x: my_list.count(x) > 1 , my_list)))\n\n# Result =>  [2]\n"], [], ["def find_duplicates(ls):\n    if type(ls) != list:\n        return -1   \n    non_dupe = []\n    i = 0\n    while i < len(ls):\n        if ls[i] < 1:\n            return -1\n        elif ls[i] in non_dupe:\n            return ls[i]\n        else:\n            non_dupe.append(i)\n        i += 1\n    return 0\n"], [], ["def find_duplicates(ls):\n    if type(ls) is not list:\n        return -1\n    duplicated = 0\n    i = 0\n    while i < len(ls):\n        if ls[i] < 1:\n            return -1\n        if ls.count(ls[i]) > 1 and duplicated == 0\n            duplicated = ls[i]\n        i += 1\n    return duplicated\n"], ["df = pd.DataFrame({\n    'ID': range(1, 4),\n    'col1': [10, 5, 10],\n    'col2': [15, 10, 15],\n    'col3': [10, 15, 15],\n    'total': [35, 30, 40]\n})\n\ncols = ['col1', 'col2', 'col3', 'total']\nfor col in cols:\n    df[col] = (df[col]/df['total']*100).round(2).astype(str) + ' %'\n", "    ID  col1    col2    col3    total\n0   1   28.57 % 42.86 % 28.57 % 100.0 %\n1   2   16.67 % 33.33 % 50.0 %  100.0 %\n2   3   25.0 %  37.5 %  37.5 %  100.0 %\n"], [], [">>> df.iloc[:, 1:] = df.iloc[:, 1:].div(df['total'], axis=0).mul(100).round(2).astype(str).add(' %')\n>>> df \n   ID     col1     col2     col3    total\n0   1  28.57 %  42.86 %  28.57 %  100.0 %\n1   2  16.67 %  33.33 %   50.0 %  100.0 %\n2   3   25.0 %   37.5 %   37.5 %  100.0 %\n"], ["|    |   ID |     col1 |     col2 |     col3 |   total |\n|---:|-----:|---------:|---------:|---------:|--------:|\n|  0 |    1 | 0.285714 | 0.428571 | 0.285714 |       1 |\n|  1 |    2 | 0.166667 | 0.333333 | 0.5      |       1 |\n|  2 |    3 | 0.25     | 0.375    | 0.375    |       1 |\n"], ["import pandas as pd \n\ndf = pd.DataFrame({\n    'ID': range(1, 4),\n    'col1': [10, 5, 10],\n    'col2': [15, 10, 15],\n    'col3': [10, 15, 15],\n    'total': [35, 30, 40]\n})\n\ndf['col1'] = (df['col1']/df['total']).mul(100).round(2).astype(str).add('%')\ndf['col2'] = (df['col2']/df['total']).mul(100).round(2).astype(str).add('%')\ndf['col3'] = (df['col3']/df['total']).mul(100).round(2).astype(str).add('%')\ndf['total'] = (df['total']/df['total']).mul(100).round(2).astype(str).add('%')\n\n\nprint(df)\n\n", "   ID    col1    col2    col3   total\n0   1  28.57%  42.86%  28.57%  100.0%\n1   2  16.67%  33.33%   50.0%  100.0%\n2   3   25.0%   37.5%   37.5%  100.0%\n"], ["tags = page_soup.find_all(id=True)\nfor tag in tags:\n    print(tag.name,tag['id'],sep='->')\n", " tags = page_soup.find_all()\n    for tag in tags:\n        if 'id' in tag.attrs:\n            print(tag.name,tag['id'],sep='->')\n", "ids =[tag['id'] for tag in page_soup.find_all(id=True)]\n"], ["with sr.Microphone() as source:\n    print(\"SAY SOMETHING\")\n    audio = r.listen(source,timeout=3, phrase_time_limit=3)\n    print(\"TIME OVER\")\ntry:\n    print(\"TEXTE : \"+r.recognize_google(audio, language=\"fr-FR\"))\nexcept sr.WaitTimeoutError:\n    print(\"ERROR\")\n"], ["def ways(l, target):\n    dp =[ [] for i in range(target+1) ]\n    l.sort()\n    n=len(l)\n    for i in range(n):\n        for j in range(l[i], target+1):    \n            if j==l[i]:\n                dp[j].append([l[i]])\n            else:\n                if dp[j-l[i]]:   \n                    for u in dp[j-l[i]]:       \n                        dp[j].append(u+[l[i]])       \n    return dp[-1]\n\nif __name__ == \"__main__\":\n     l = [1,2,3] \n     target = 5\n     print(len(ways(l, target)))\n     l = [5, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n     target = 30\n     print(len(ways(l, target)))\n"], ["from itertools import combinations_with_replacement as cwr\n\ndef countways(l, target):\n    return len([1 for i in range(target) for j in cwr(l, i + 1) if sum(j) == target])\n\nprint(countways([1, 2, 3], 5))\n", "5\n", "from itertools import combinations_with_replacement as cwr\n\ndef countways(l, target):\n    for i in range(target):\n        for j in cwr(l, i + 1):\n            if sum(j) == target:\n                print(j)\n\ncountways([1, 2, 3], 5)\n", "(2, 3)\n(1, 1, 3)\n(1, 2, 2)\n(1, 1, 1, 2)\n(1, 1, 1, 1, 1)\n", "def countways(l, target):\n    l = [i for i in l if i <= target]\n    return len([1 for i in range(target // max(l) - 1,\n                                 target // min(l)) for j in cwr(l, i + 1) if sum(j) == target])\n"], ["dict_obj = {'a': 1, \"b\": 2, \"c\": 3}\n", "dict_obj = {'a' = 1, \"b\" = 2, \"c\" = 3}\n"], ["soup = BeautifulSoup(markup=page, 'html.parser')\ntest = [r['id'] for r in soup.find_all(name=\"div\", attrs={\"id\":\"12346\"}) if r.get('id') is not None]\n", "['12345', '12346']\n", "test = [r['id'] for r in soup.find_all(name=\"div\", attrs={\"id\":True})]\n"], ["(df=='?').sum()\nOut[182]: \ncolA    2\ncolB    1\ncolC    1\ndtype: int64\n"], [], [], ["list1 = ['\"22.23.24.25\"', \"'11.22.33.44'\", \"11.22.33.44\"]\n\nnew_list = \"\\n\".join(list1).replace(\"\\\"\",\"\").replace(\"'\",\"\").split(\"\\n\")\n\nprint(new_list) # ['22.23.24.25', '11.22.33.44', '11.22.33.44']\n"], ["# try \nresult = True # return\n# finally \nresult = False # return (reassign value)\nprint(result) # Output: False\n\n# try\nx = 100 \nresult = x # return\n# finally\nx = 90 \nresult = x # return (reassign value)\nprint(result) # Output: 90\n\n# try\nx = 100\nresult = x # return\n# finally\nx = 90 # no return so result not updated\nprint(result) # Output: 100\nprint(x) # Output: 90 (x is changed actually)\n\n# try\nx = [100]\nresult = x # return the list (result refer to a list and list is mutable)\n# finally\nx[0] = 90 # changing the list in-place so it affects the result\nprint(result) # Output: [90]\n\n# try\nx = [100]\nresult = x[0] # return the integer (result refer to the integer)\n# finally\n# changing the list in-place which have no effect to result unless reassign value by return x[0]\nx[0] = 90\nprint(result) # Output: 100\nprint(x) # Output: [90] (x is changed actually)\n"], [], [], ["import pandas as pd\n\nsearch_list = ('STEEL','IRON','GOLD','SILVER')\n\ndf = pd.DataFrame({'a':[123,456,789],'b':['blah blah Steel','blah blah blah','blah blah Gold']})\n\ndf.assign(c = df['b'].apply(lambda x: [j for j in x.split() if j.upper() in search_list]))\n"], ["l1=['\"10.40.30.40.50\"']\nfor x in l1:\n    s=str(l1[0])\n    for n in s:\n        value=s[1:len(s)-1]\nprint(value)\n"], ["list1 = ['\"22.23.24.25\"']\nfor i in range(len(list1)):\n    x=list1[i].replace(\"\\\"\",\"\")\n"], ["list1 = ['\"22.23.24.25\"', \"'11.22.33.44'\"]\n\nlist1 = [x.replace('\"', '').replace(\"'\", '') for x in list1]\n\nprint(list1)\n['22.23.24.25', '11.22.33.44']\n"], ["a=[i.replace(\"\\\"\",\"\") for i in list1]\n", "['22.23.24.25']\n"], ["def compare_sign(s):\n    if len(s) == 2:\n        return s.iat[1] if s.iat[0] != s.iat[1] else 0\n    else:\n        return 0  # this case covers the first value in the rolling window series\n\ndef cross(df, field):\n    return pd.Series([compare_sign(w) for w in df[field].apply(np.sign).rolling(2)])\n"], ["a = ['a','a','b','a']\n\ncounts = {}\nfor element in a:\n    counts[element] = counts.get(element, 0) + 1\n\nfor element in a:\n    if counts[element] == 1:\n        print(element)\n", "b\n"], [], [], ["def booll(x):\n    if x == 0:\n        return True\n    else:\n        return False\n"], ["def booll(x):\n    if x == 0:\n        yield True\n    yield False\n"], [], [], [], ["df.date = df[\"date\"].str.extract('\\[(.*?)]', expand=True).replace(\"'\",\"\", regex=True)\n", "    date\n0   2014-02-19\n1   2013-02-28\n2   2018-04-15\n", "df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   date    3 non-null      object\ndtypes: object(1)\nmemory usage: 152.0+ bytes\n", "df.date = df[\"date\"].str.extract('\\[(.*?)]', expand=True).replace(\"'\",\"\", regex=True)\ndf.date = pd.to_datetime(df.date)\ndf\n", "    date\n0   2014-02-19\n1   2013-02-28\n2   2018-04-15\n", "df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   date    3 non-null      datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 152.0 bytes\n", "df = pd.DataFrame({\n    'date':[\"DatetimeIndex(['2014-02-19'], dtype='datetime64[ns]', freq=None)\",\n\"DatetimeIndex(['2013-02-28'], dtype='datetime64[ns]', freq=None)\",\n\"DatetimeIndex(['2018-04-15'], dtype='datetime64[ns]', freq=None)\"]\n})\n\ndf\n"], ["import pandas as pd\nimport datetime as dt\nrng = pd.date_range('2015-02-24', periods=5, freq='T')\ndf = pd.DataFrame({ 'date': rng })\n    \ndf['date'] = df['date'].dt.strftime(\"%Y-%m-%d\")\n", "datelist = df['date'].tolist()\n"], ["df['date'] = df['date'].str[0]\n", "df = df.explode('date')\n", "df[\"date\"] = df[\"date\"].str.extract(r\"(\\d{4}-\\d{2}-\\d{2})\", expand=False)\n", "df[\"date\"] = df[\"date\"].str.findall(r\"(\\d{4}-\\d{2}-\\d{2})\")\ndf = df.explode('date')\n"], ["df['new_column'] = df['date'].apply(lambda x: x[0].strftime(\"%Y-%m-%d\"))\n"], ["def fundate(x):\n    return x.strftime(\"%Y-%m-%d\")\n"], ["function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"], ["pip install --no-use-pep517 opencv-python\n"], ["from tensorflow.random import set_seed \n"], ["%cd /your_path/\n!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile\n!sed -i 's/GPU=0/GPU=1/g' Makefile\n!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile\n!sed -i \"s/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= ${ARCH_VALUE}/g\" Makefile\n!make\n"], [], ["import pandas as pd\ndf = pd.read_csv('Tax_Calculation.csv')\nmin = df['Direct_Tax'].min()\nmax = df['Direct_Tax'].max()\n", "import numpy as np\nbins = np.linspace(min,max, 5)\n", "df['bins_dist'] = pd.cut(df['Direct_Tax'], bins=bins, labels=[ExSmall, Small, Medium, Large], include_lowest=True)\n", "df['bin_freq'] = pd.qcut(df['Direct_Tax'], q=4, precision=1, labels=[ExSmall, Small, Medium, Large])\n"], [], ["l=[1,2,3,4,5,4,3,2,1,2,3,4,3,2,4,2,1,2]\nn=int(input(\"The size of window on either side \"))\nfor i in range(n,len(l)-n):\n    if max(l[i-n:i]+l[i+1:i+n+1])<l[i]:\n        print(l[i],' at index = ',i)\n"], ["# Python program to check if two\n# to get unique values from list\n# using set\n\n# function to get unique values\ndef unique(list1):\n \n    # insert the list to the set\n    list_set = set(list1)\n    # convert the set to the list\n    unique_list = (list(list_set))\n    for x in unique_list:\n        print x,\n \n\n# driver code\nlist1 = [10, 20, 10, 30, 40, 40]\nprint(\"the unique values from 1st list is\")\nunique(list1)\n\n\nlist2 =[1, 2, 1, 1, 3, 4, 3, 3, 5]\nprint(\"\\nthe unique values from 2nd list is\")\nunique(list2)\n", "# function to get unique values\ndef unique(list1):\n    x = np.array(list1)\n    print(np.unique(x))\n \n\n# driver code\nlist1 = [10, 20, 10, 30, 40, 40]\nprint(\"the unique values from 1st list is\")\nunique(list1)\n\n\nlist2 =[1, 2, 1, 1, 3, 4, 3, 3, 5]\nprint(\"\\nthe unique values from 2nd list is\")\nunique(list2)\n"], ["my_list = ['a','a','b','a']\nmy_dict = {}\nonly_one = []\n\n#Step 1\nfor element in my_list:\n  if element not in my_dict:\n    my_dict[element] = 1\n  else:\n    my_dict[element] += 1\n\n#Step 2 and Step 3\nfor key, value in my_dict.items():\n  if value == 1:\n    only_one.append(key)\n\n#Step 4\nfor unique in only_one:\n  print(unique)\n", "b\n", "my_dict = {'a': 3, 'b': 1}\nonly_one = ['b']\n"], [], [">>> data = ['a','a','b','a']\n>>> from collections import Counter\n>>> [k for k,v in Counter(data).items() if v == 1]\n['b']\n"], ["# downgrade pip to 20.3.4\npip install --user pip==20.3.4\n# better: 'pip<21', but it must be quoted!\n\n# install virtualenv if not installed already\npip install --user virtualenv\n\n# create a new venv for old python\nvirtualenv -p /usr/bin/python2.7 /tmp/venv27\n\n# update pip back to the recent version\npip install --user --upgrade pip\n", "sh-5.0$ pip --version\npip 21.1.1 from /home/vpfb/.local/lib/python3.9/site-packages/pip (python 3.9)\n\nsh-5.0$ pwd\n/tmp/venv27/bin\n\nsh-5.0$ source ./activate\n\n(venv27) sh-5.0$ pip --version\npip 20.3.4 from /tmp/venv27/lib/python2.7/site-packages/pip (python 2.7)\n"], [], [], [], [], ["def merge_list(list1, list2):\nmerged_data = \"\"\nlist3 = list1   \nfor i in range(1, 2*len(list2) + 1,2):    #Converting both lists in single list\n    list3.insert(i, list2[-1])    # Adding last elements of list2 at alternate to positions elements of list3\n    list2.pop()  #Removing the last element from list2\nlist3 = [\"\" if i is None else i for i in list3]   # Handling NoneType condition. If there is \"None\", convertted to \"\"\nfor i in range(0,len(list3),2):\n    word=\"\".join(list3[i:i+2])   #joining the elements in set of two\n    merged_data=merged_data+word+\" \"   #converting above word into one\nreturn merged_data\n"], ["import numpy as np\ndelta = pd.to_datetime('2021-05-21 06:00:00') - pd.to_datetime('2021-05-21 06:02:00')\n\n\nprint(delta)\nTimedelta('-1 days +23:58:00')\n\n#minutes\nprint(delta / np.timedelta64(1,'m')\n-2.0\n#seconds\ndelta / np.timedelta64(1,'s')\n-120.0\n"], [], ["# using pure python\nfrom datetime import datetime\ndatetime(2021,5,21,6,0,0) - datetime(2021,5,21,6,2,0)\ndatetime.timedelta(days=-1, seconds=86280)\n", "from datetime import timedelta\nd = timedelta(microseconds=-1)\n(d.days, d.seconds, d.microseconds)\n(-1, 86399, 999999)\n"], [], ["(pd.to_datetime('2021-05-21 06:00:00') - pd.to_datetime('2021-05-21 06:02:00')).total_seconds()\nOut[9]: -120.0\n"], ["day, month, year = birthday.split(\"/\")\n", "day = birthday[:birthday.find(\"/\")]\nmonth = birthday[birthday.find(\"/\")+1:birthday.rfind(\"/\")]\nyear = birthday[birthday.rfind(\"/\")+1:]\n"], ["var1,var2 = True,True\nif all((var1, var2)):\n    print('all true')\n", "all true\n", "var1,var2 = True,True\ntrues = [var1,var2]\nif trues.count(trues[0]) == len(trues):\n    print('all true')\n"], [], ["if all((var1, var2, var3, var4)):\n    print(\"This function is correct\")\n"], [], ["if var1 and var2:\n print(\"This function is correct\")\nelse:\n print(\"This function doesn't work\")\n"], ["while True:\n    command = input(\"Command: \")\n    if command == \"quit\":\n        break\n    ...\n"], [], ["while command != \"quit\":\n    command = input(\"Command: \")\n    if command == \"start\":\n        print(\"Car ready to go\")\n    elif command == \"stop\":\n        print(\"Car stopped\")\n    else:\n        print(\"I don't understand that\")\nelse:\n    print(\"Game exited\")\n", "while command != \"quit\":\n    command = input(\"Command: \")\n    if command == \"start\":\n        print(\"Car ready to go\")\n    elif command == \"stop\":\n        print(\"Car stopped\")\n    elif command != \"quit\":\n        print(\"I don't understand that\")\nelse:\n    print(\"Game exited\")\n"], [], ["else:\n    print(\"Game exited\")\n", "I don't understand that\nGame exited\n"], [], ["while command != \"quit\":\n    command = input(\"Command: \")\n    ...\n"], ["def map_randoms(df):\n    df['col_rand'] = np.random.randint(0,2, size=len(df))\n    return df\n\nddf = ddf.map_partitions(map_randoms)\nddf.persist()\n"], ["> conda create -n tf_env tensorflow=2\n"], [], ["def returnMaxFrequency(ar):   \n    freqDict = {x:0 for x in ar}\n    for val in ar:\n        freqDict[val] = freqDict[val] + 1\n    maxFreq = max(freqDict.values())\n    return maxFreq\n"], ["([1, 999, 3], [4, 999])\n"], ["options = Options()\noptions.add_argument(\"start-maximized\")\noptions.add_argument(\"disable-infobars\")\noptions.add_argument(\"--disable-extensions\")\noptions.add_argument('--no-sandbox')\noptions.add_argument('--disable-application-cache')\noptions.add_argument('--disable-gpu')\noptions.add_argument(\"--disable-dev-shm-usage\")\n"], ["df['col'] = df['col'].astype('str')\n"], [], [], ["name = input('File: ')\n\nwith open(name, 'r') as f:\n    lines = f.readlines()\n\nline_nr = 0\nfor line in lines:\n    line_nr += 1\n    numbers = line.strip('\\n').split(' ')\n    sum_numbers = 0\n    total_numbers = len(numbers)\n    \n    for number in numbers:\n        int_number = int(number)\n        sum_numbers += int_number\n        \n\n    \n\n    print(f'The average of line {line_nr} is:', sum_numbers/total_numbers)\n", "The average of line 1 is: 44.4\nThe average of line 2 is: 42.0\nThe average of line 3 is: 57.8\nThe average of line 4 is: 97.6\nThe average of line 5 is: 63.0\n"], [], ["file1 = open('numbers.txt', 'r')\nLines = file1.readlines()\ncount = 0\n\nfor line in Lines:\n    count = 0\n    sum_number = 0\n    for i in line.split(' '):\n        count += 1\n        sum_number += int(i)\n    print(\"avg: {}\".format(sum_number / count))\n", "avg: 44.4\navg: 42.0\navg: 57.8\navg: 97.6\navg: 63.0\n"], ["with open(input('File: ')) as f:\n    for line_no, line in enumerate(f):                     # enumerate provides data and line number for each line\n        numbers = [int(i) for i in line.rstrip().split()]  # convert each line to numbers\n        avg = sum(numbers)/len(numbers)                    # using definition of average\n        print(f'Avg. of line {line_no} is {avg}')          # print line number and average\n        \n"], ["with open('file.txt') as f:\n    array = [[int(x) for x in line.split()] for line in f]\n\nnp.mean(array, axis=1)\n"], ["In [132]: import pandas as pd\n\nIn [109]: df1 = pd.DataFrame(mylist1)\n\nIn [110]: df2 = pd.DataFrame(mylist2)\n\nIn [117]: res = pd.merge(df1, df2, on=0)\n\nIn [121]: res['mean'] = res.mean(axis=1)\n\nIn [125]: res.drop(['1_x', '1_y'], 1, inplace=True)\n\nIn [131]: res.values.tolist()\nOut[131]: [['egg', 0.45], ['chocolate', 0.5]]\n"], ["def two_string(a, b):\n  for i in range(min(len(a),len(b))):\n    if a[i] == b[i]:\n      print(f\"Match of character '{a[i]}' found at index {i}\")\n\ntwo_string('The Holy Grail', 'Life of Brian')\n# Output: \n# Match of character 'o' found at index 5\n# Match of character 'a' found at index 11\n"], ["$> for tup in two_string('The Holy Grail', 'Life of Brian'):\n--   print(tup)\n-- \n('o', 5)\n('a', 11)\n"], ["i in b\n"], ["def two_string(a: str, b: str) -> dict:\n    return {idx: match[0] for idx, match in enumerate(zip(a.lower(),b.lower())) if match[0] == match[1]}\n\ntwo_string('The Holy Grail', 'Life of Brian')\n\n# {5: 'o', 11: 'a'}\n"], ["def two_string(a, b):\n    res = []\n    i = 0\n    for c in a:\n        if c in b and i == b.index(c):\n            res.append((c,i))\n        i += 1\n    print(res)\n", "[('o', 5), ('a', 11)]\n"], ["def two_string(a, b):\n    for i, ca in enumerate(a):\n        # Prevent index out of bounds\n        if i < len(b) and b[i] == ca:\n            print(i, ca)\n\n\ntwo_string('The Holy Grail', 'Life of Brian')\n", "5 o\n11 a\n"], ["def two_string(a,b):\n    for i, (ca, cb) in enumerate(zip(a,b)):\n        if ca==cb:\n            print(ca, i)\n"], ["dask_df['col'] = pd.Series(list or array)\n"], ["mydict = {'1':22, '2':33, '3':44}\nmylist = [23,24,25]\nfor key, value in mydict.items():\n    mydict[key] = [value, mylist[int(key)-1]]\n"], ["[c for c in dfif df[c].eq(\"CSS\").any()]\n\n"], [], [], ["import os\nimport hashlib\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n\ndef split_tfrecord(tfrecord_path, n_splits):\n    dataset = tf.data.TFRecordDataset(tfrecord_path)\n    outfiles=[]\n    for n_split in range(n_splits):\n        output_tfrecord_dir = f\"{os.path.splitext(tfrecord_path)[0]}\"\n        if not os.path.exists(output_tfrecord_dir):\n            os.makedirs(output_tfrecord_dir)\n        output_tfrecord_path=os.path.join(output_tfrecord_dir, f\"{n_split:03d}.tfrecord\")\n        out_f = tf.io.TFRecordWriter(output_tfrecord_path)\n        outfiles.append(out_f)\n\n    for record in tqdm(dataset):\n        sample = tf.train.Example()\n        record = record.numpy()\n        sample.ParseFromString(record)\n\n        idx = int(hashlib.sha1(record).hexdigest(),16) % n_splits\n        outfiles[idx].write(example.SerializeToString())\n\n    for file in outfiles:\n        file.close()\n"], ["rows = []\nwith open(\"my_file\", \"r\") as f:\n    for row in f.readlines():\n        value_pairs = row.strip().split(\" \")\n        print(value_pairs)\n        values = {pair.split(\":\")[0]: pair.split(\":\")[1] for pair in value_pairs}\n        print(values[\"psnr_y\"])\n        rows.append(values)\n\nprint(rows)\n"], ["import re\n\nwith open('textfile.txt') as f:\n    a = f.readlines()\n    pattern = r'psnr_y:([\\d.]+)'\n    for line in a:\n        print(re.search(pattern, line)[1])\n", "import re\n\na_list = []\n\nwith open('textfile.txt') as f:\n    a = f.readlines()\n    pattern = r'psnr_y:([\\d.]+)'\n    for line in a:\n        a_list.append(re.search(pattern, line)[1])\n"], [], ["import fileinput\nimport re\n\nfor line in fileinput.input():\n    row = dict([s.split(':') for s in re.findall('[\\S]+:[\\S]+', line)])\n    print(row['psnr_y'])\n", "python script_name.py < /path/to/your/dataset.txt\n"], ["r'psnr_y:([\\d.]+)'\n"], ["id = soup.find_all(id=True)\nprint(id)\n", "for ID in soup.find_all('div', id=True):  \n    print(ID.get('id'))\n"], ["s = input()\ns = s.lower()\nfor i in s:\n    for x in ['a','e','i','o','u']:\n        if i == x:\n            s = s.replace(i,'')\nnew_s = ''\nfor i in s:\n    new_s += '.'+ i\nprint(new_s)\n"], [], [], ["mydic = {'1':22, '2':33, '3':44}\nmylist = [23,24,25]\n\nmydic = {k: [v1, v2] for (k,v1), v2 in zip(mydic.items(),mylist)}\n"], ["count = 0\nfor x in mydic.items():\n    mydic[x[0]] = [mydic[x[0]]]\n    mydic[x[0]].append(mylist[count])\n    count += 1\nprint(mydic)\n"], ["mydic = {'1':22, '2':33, '3':44}\nmylist = [23, 24, 25]\n\nmykeys = list(mydic.keys())\nfor idx in range(len(mylist)):\n    mydic[mykeys[idx]] = [mydic[mykeys[idx]], mylist[idx]]\n\nprint(mydic)\n# {'1': [22, 23], '2': [33, 24], '3': [44, 25]}\n\n"], ["for i,(k,v) in enumerate(mydic.items()):\n    mydic[k]=[v,mylist[i]]\n"], ["pd.Series(np.count_nonzero(df.to_numpy()=='?', axis=0), index=df.columns)\n# pd.Series((df.values == '?').sum(0), index=df.columns)\n\ncolA    2\ncolB    1\ncolC    1\ndtype: int64\n", "big_df = pd.DataFrame(df.to_numpy().repeat(200_000,axis=0))\nbig_df.shape\n(1000000, 3)\n"], ["import sys\n!{sys.executable} -m pip install -U pandas-profiling[notebook]\n!jupyter nbextension enable --py widgetsnbextension\n\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\ntrain = pd.read_csv(\"train.csv\")\n", "report = ProfileReport(train)\n\nprint(repr(report.report))\n\nreport.to_widgets()\n"], [], [], ["tuple = (3, 2)\nfor i in tuple:\n    i -= 1\nprint(tuple)\n", "list = [3,2]\nfor i in list:\n    i -= 1\nprint(list)\n", "list = [3, 2]\nfor idx, item in enumerate(list):\n    list[idx] = item - 1\nprint(list)\n"], ["my_tuple = (3, 2)\nnew_list = []\nfor i in my_tuple:\n    i -= 1\n    new_list.append(i)\nnew_tuple = tuple(new_list)\nprint(new_tuple)\n"], ["a = [3, 2]\nfor i in a:\n    i -= 1\nprint(a)\n", "[3, 2]\n", "for index in range(len(a)):\n    i = a[index]\n    i -= 1\nprint(a)\n", "a = [3, 2]\nfor index in range(len(a)):\n    a[index] -= 1\n\nprint(a)\n", "a = [3, 2]\na = [i - 1 for i in a]\nprint(a)\n"], ["tup = (3, 2)\nfor i in tup:\n    i -= 1\nprint(tup)\n", "(3, 2)\n", "tup = (3,2)\ntup = tuple([i - 1 for i in tup])\nprint(tup)\n", "(2, 1)\n"], ["my_tuple = (1,2,3)\nmy_tuple [0] = 3\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\nTypeError: 'tuple' object does not support item assignment\n\n"], ["float('nan') == math.nan                   # FALSE\n", "float('nan').hex() == math.nan.hex()       # TRUE\n", "float('nan').hex() == float('nan').hex()   # TRUE\n", "float('nan').hex() == numpy.nan.hex()   # TRUE\n", "df.eval('A == \"NaN\"')\n", "df.applymap(lambda x: 'NaN' if x.hex() == float('NaN').hex() else x).eval('A == \"NaN\"')\n"], ["df = pd.read_csv(r'/path/to/csv')\n", "df = df.astype(str).apply(lambda x: x.str.replace(r'[^@.]', 'x'), axis=1)\n", "sed -E 's/(\\w+)@(\\w+)/xxx@xxx/' /path/to/file.csv > /path/to/new_file.csv\n"], ["import re\n\ndef hideEmail(email):\n    #hide email\n    text = re.sub(r'[^@.]', 'x', email)\n    return text \n\nwith open('file.csv', 'r') as r:\n    r = map(hideEmail, r.readlines())\n\nwith open('file2.csv', 'w') as f:\n    for line in r:\n        f.write(line + '\\n')\n"], ["dataset = pd.read_csv('D:\\seminar\\\\totaldata.csv')  \ndataset.head()\n", "dataset = pd.read_csv(r'D:\\seminar\\totaldata.csv')  \ndataset.head()\n"], ["from numpy.random import seed\nseed(1)\nfrom tensorflow import random\nrandom.set_seed(1)\n"], [], ["import pandas as pd #import pandas\ndf = pd.read_csv('enter_file_path_here') #read the data\n\ndf['col'] = df['col'].apply(lambda x: hideEmail(x))\n#if you want to make it back to a csv:\ndf.to_csv('name.csv')\n"], ["def hideEmail(email):\n    #hide email\n    text = re.sub(r'[^@.]', 'x', email)\n    return text\n\n\nwith open('path/to/csvfile', 'r') as file:\n     lines = [l.strip().split(';') for l in file.readlines()]\n\nmodifiedlines = []       # to store lines after email field is modified \n\nfor i in lines[1:]:         # iterating from index 1 as index 0 is header\n    i[3] = hideEmail(i[3])       # as email field is at index 3\n    modifiedlines.append(';'.join(i))     # appending modified line\n\nwith open('path/to/csvfile', 'w') as file:\n     file.writelines(modifiedlines)            # writing the lines back to file\n"], [], [], [], [], [], [], [], ["!pip install pandas-profiling==2.7.1 \n"], ["df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',\n             header=None,\n             names=[list of column names])\n", "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',\n             header=None,\n             names=[list of column names], dtype = str)\n", "mapping = {'small': 1, 'med':2, 'big': 3}\ndf.replace({'lug_boot': mapping})\n"], [], ["# `splits` is a list of the number of records you want in each output file\ndef split_files(filename: str, splits: List[int]) -> None:\n    dataset: tf.data.Dataset = tf.data.TFRecordDataset(filename)\n    rec_counter: int = 0\n\n    # An extra iteration over the data to get the size\n    total_records: int = len([r for r in dataset])\n    print(f\"Found {total_records} records in source file.\")\n\n    if sum(splits) != total_records:\n        raise ValueError(f\"Sum of splits {sum(splits)} does not equal \"\n                         f\"total number of records {total_records}\")\n\n    rec_iter:Iterator = iter(dataset)\n    split: int\n    for split_idx, split in enumerate(splits):\n        outfile: str = filename + f\".{split_idx}-{split}\"\n        with tf.io.TFRecordWriter(outfile) as writer:\n            for out_idx in range(split):\n                rec: tf.Tensor = next(rec_iter, None)\n                rec_counter +=1\n                writer.write(rec.numpy())\n        print(f\"Finished writing {split} records to file {split_idx}\")\n"], [], ["from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nn_bins = 10\ndseries, return_bins = pd.qcut(train['price'], n_bins, retbins=True)\n\nn_bins=n_bins+2\nreturn_bins[0]=return_bins[0]*.99\nreturn_bins[-1]=return_bins[-1]*0.99\nreturn_bins_lst=[r for r  in return_bins]\nreturn_bins_lst.insert(0,return_bins[0]*1000)\nreturn_bins_lst.append(return_bins[-1]*1000)\nreturn_bins=np.array(return_bins_lst)\n \ntrain['label']=label_encoder.fit_transform(pd.cut(train['price'], return_bins, \nlabels=range(n_bins)))\ntest['label']=label_encoder.transform(pd.cut(test['price'], return_bins, \nlabels=range(n_bins)))\n"], ["import speech_recognition as sr\n\nrObject = sr.Recognizer() \naudio = '' \nwith sr.Microphone() as source: \n    print(\"Speak...\")   \n    audio = rObject.listen(source, phrase_time_limit = 0) \n    print(\"Stop.\")\n    try: \n        text = rObject.recognize_google(audio, language ='fr-FR') \n        print(\"You : \"+ text)  \n    except: \n        speak(\"Could not understand your audio...PLease try again !\") \n"], ["def checkAB(s):\n    if len(s) == 0:\n        return 1\n    if s[0] == 'a':\n        if len(s[1:]) > 1 and s[1:3] == 'bb':\n            return checkAB(s[3:])\n        else:\n            return checkAB(s[1:])\n    else:\n        return 0\n\ns = input()\nif checkAB(s):\n    print('true')\nelse:\n    print('false')\n"], ["def checkAB(s):\n\n    # Check for the validity of the input\n    if len(s) == 0 or s[0] != 'a':\n        return False\n\n    def loop(val):\n        # Done processing, all is well\n        if len(val) == 0:  \n            return True\n\n        # If the first char is a, recurse using the rest of the string\n        if val[0] == 'a':\n            return loop(val[1:])\n\n        # If there are 3 chars, it can not be bbb\n        if len(val) > 2 and val[0] == 'b' and val[1] == 'b' and val[2] == 'b':\n            return False\n\n        # If there is a single value left, and that is b it is not ok\n        if len(val) == 1 and val[0] == 'b':\n            return False\n\n        # If we have bb, recurse using the rest of the string\n        if val[0] == 'b' and val[1] == 'b':\n            return loop(val[2:])\n\n        # In any other case\n        else:\n            return False\n    return loop(s)\n", "\\Aa+(?:bba+)*(?:bb)?\\Z\n"], ["def state2(lst):\n    if lst==[]:\n        return True\n    poped = lst.pop(0)\n    if poped == \"a\":\n        return state2(lst)\n    else:\n        if lst == [] or lst[0] == 'a':\n            return False\n        if lst[0]==\"b\":\n            lst.pop(0) # removed \"BB\"\n            return state1(lst)\n\ndef state1(lst):\n    if lst==[]:\n        return True\n    if lst[0]==\"a\":\n        lst.pop(0)\n        return state2(lst)\n    return False\n", "def checkString(inp):\n    # print(list(inp))\n    return state1(list(inp))\n\ntestcases = [\"abb\",\"a\",\"ab\",\"abba\", \"\",\"abbabbb\",\"abbabbaaabbaaabbabb\"]\nfor i in testcases:\n    print(checkString(i))\n"], [], [], ["pip install -U pylint\n"], [], ["pyinstaller --windowed my_code.py\n"], [], ["in polls/urls.py\n", "from django.urls import path\nfrom django.conf.urls import url\nfrom . import views\n\nurlpatterns = [\n    path('', views.index, name='index'),\n]\n"], [">>> import numpy as np\n>>> np.cumprod([1, 2, 3, 4, 5])\narray([  1,   2,   6,  24, 120])\n", ">>> list(np.cumprod([1, 2, 3, 4, 5]))\n[1, 2, 6, 24, 120]\n"], [], ["a = [1, 2, 2, 3, 3, 4, 5, 6]\n    \ndef get_unique_N(iterable, N):\n    \"\"\"Yields (in order) the first N unique elements of iterable. \n    Might yield less if data too short.\"\"\"\n    seen = set()\n    for e in iterable:\n        if e in seen:\n            continue\n        seen.add(e)\n        yield e\n        if len(seen) == N:\n            return\n            \nk = get_unique_N([1, 2, 2, 3, 3, 4, 5, 6], 4)\nprint(list(k))\n    \n", "[1, 2, 3, 4]\n"], ["18:37   Commit failed with error\n            0 file committed, 1 file failed to commit: Update pre-commit hooks\n            env: python3.7: No such file or directory\n", "zsh: command not found: python3.7\n", "#!/usr/bin/env python3\n"], [], ["pyinstaller --windowed <script name>.py\n"], ["class Foo:\n    def __init__(self):\n        return 3\n\nf = Foo()\n", "$ mypy tmp.py\nSuccess: no issues found in 1 source file\n", "$ python tmp.py\nTraceback (most recent call last):\n  File \"tmp.py\", line 5, in <module>\n    f = Foo()\nTypeError: __init__() should return None, not 'int'\n", "$ mypy tmp.py\ntmp.py:3: error: No return value expected\nFound 1 error in 1 file (checked 1 source file)\n", "$ mypy tmp.py\ntmp.py:2: error: The return type of \"__init__\" must be None\nFound 1 error in 1 file (checked 1 source file)\n", "def __init__(self, x: int):\n     return x\n"], [], [], [], [], [], [], ["pip install tensorflow==2.0 --user\n"], [], ["import string\n\n# Python Dictionary\n# I manually created these word relationship - primary_word:synonyms\nword_relationship = {\"father\": ['dad', 'daddy', 'old man', 'pa', 'pappy', 'papa', 'pop'],\n          \"mother\": [\"mamma\", \"momma\", \"mama\", \"mammy\", \"mummy\", \"mommy\", \"mom\", \"mum\"]}\n\n# This input text is from various poems about mothers and fathers\ninput_text = 'The hand that rocks the cradle also makes the house a home. It is the prayers of the mother ' \\\n         'that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of ' \\\n         'her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She ' \\\n         'has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the ' \\\n         'greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend, ' \\\n         'This to me you have always been. Through the good times and the bad, Your understanding I have had.'\n\n# converts the input text to lowercase and splits the words based on empty space.\nwordlist = input_text.lower().split()\n\n# remove all punctuation from the wordlist\nremove_punctuation = [''.join(ch for ch in s if ch not in string.punctuation) \nfor s in wordlist]\n\n# list for word frequencies\nwordfreq = []\n\n# count the frequencies of a word\nfor w in remove_punctuation:\nwordfreq.append(remove_punctuation.count(w))\n\nword_frequencies = (dict(zip(remove_punctuation, wordfreq)))\n\nword_matches = []\n\n# loop through the dictionaries\nfor word, frequency in word_frequencies.items():\n   for keyword, synonym in word_relationship.items():\n      match = [x for x in synonym if word == x]\n      if word == keyword or match:\n        match = ' '.join(map(str, match))\n        # append the keywords (mother), synonyms(mom) and frequencies to a list\n        word_matches.append([keyword, match, frequency])\n\n# used to hold the final keyword and frequencies\nfinal_results = {}\n\n# list comprehension to obtain the primary keyword and its frequencies\nsynonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]\n\n# iterate synonym_matches and output total frequency count for a specific keyword\nfor item in synonym_matches:\n  if item[0] not in final_results.keys():\n    frequency_count = 0\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n  else:\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n\n \nprint(final_results)\n# output\n{'mother': 3, 'father': 2}\n", "from nltk.corpus import wordnet\n\nsynonyms = []\nword = 'mother'\nfor synonym in wordnet.synsets(word):\n   for item in synonym.lemmas():\n      if word != synonym.name() and len(synonym.lemma_names()) > 1:\n        synonyms.append(item.name())\n\nprint(synonyms)\n['mother', 'female_parent', 'mother', 'fuss', 'overprotect', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth']\n", "from PyDictionary import PyDictionary\ndictionary_mother = PyDictionary('mother')\n\nprint(dictionary_mother.getSynonyms())\n# output \n[{'mother': ['mother-in-law', 'female parent', 'supermom', 'mum', 'parent', 'mom', 'momma', 'para I', 'mama', 'mummy', 'quadripara', 'mommy', 'quintipara', 'ma', 'puerpera', 'surrogate mother', 'mater', 'primipara', 'mammy', 'mamma']}]\n\ndictionary_mum = PyDictionary('mum')\n\nprint(dictionary_mum.getSynonyms())\n# output \n[{'mum': ['incommunicative', 'silent', 'uncommunicative']}]\n", "https://www.thesaurus.com/browse/mother\nsynonyms: mom, parent, ancestor, creator, mommy, origin, predecessor, progenitor, source, child-bearer, forebearer, procreator\n", "from string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom PyDictionary import PyDictionary\n\ninput_text = \"\"\"The hand that rocks the cradle also makes the house a home. It is the prayers of the mother\n         that keeps the family strong. When I think about my mum, I just cannot help but smile; The beauty of\n         her loving heart, the easy grace in her style. I will always need my mom, regardless of my age. She\n         has made me laugh, made me cry. Her love will never fade. If I could write a story, It would be the\n         greatest ever told. I would write about my daddy, For he had a heart of gold. For my father, my friend,\n         This to me you have always been. Through the good times and the bad, Your understanding I have had.\"\"\"\n\n\ndef normalize_textual_information(text):\n   # split text into tokens by white space\n   token = text.split()\n\n   # remove punctuation from each token\n   table = str.maketrans('', '', punctuation)\n   token = [word.translate(table) for word in token]\n\n   # remove any tokens that are not alphabetic\n   token = [word.lower() for word in token if word.isalpha()]\n\n   # filter out English stop words\n   stop_words = set(stopwords.words('english'))\n\n   # you could add additional stops like this\n   stop_words.add('cannot')\n   stop_words.add('could')\n   stop_words.add('would')\n\n   token = [word for word in token if word not in stop_words]\n\n   # filter out any short tokens\n   token = [word for word in token if len(word) > 1]\n   return token\n\n\ndef generate_word_frequencies(words):\n   # list to hold word frequencies\n   word_frequencies = []\n\n   # loop through the tokens and generate a word count for each token\n   for word in words:\n      word_frequencies.append(words.count(word))\n\n   # aggregates the words and word_frequencies into tuples and coverts them into a dictionary\n   word_frequencies = (dict(zip(words, word_frequencies)))\n\n   # sort the frequency of the words from low to high\n   sorted_frequencies = {key: value for key, value in \n   sorted(word_frequencies.items(), key=lambda item: item[1])}\n\n return sorted_frequencies\n\n\ndef get_synonyms_internet(word):\n   dictionary = PyDictionary(word)\n   synonym = dictionary.getSynonyms()\n   return synonym\n\n \nwords = normalize_textual_information(input_text)\n\nall_synsets_1 = {}\nfor word in words:\n  for synonym in wordnet.synsets(word):\n    if word != synonym.name() and len(synonym.lemma_names()) > 1:\n      for item in synonym.lemmas():\n        if word != item.name():\n          all_synsets_1.setdefault(word, []).append(str(item.name()).lower())\n\nall_synsets_2 = {}\nfor word in words:\n  word_synonyms = get_synonyms_internet(word)\n  for synonym in word_synonyms:\n    if word != synonym and synonym is not None:\n      all_synsets_2.update(synonym)\n\n word_relationship = {**all_synsets_1, **all_synsets_2}\n\n frequencies = generate_word_frequencies(words)\n word_matches = []\n word_set = {}\n duplication_check = set()\n\n for word, frequency in frequencies.items():\n    for keyword, synonym in word_relationship.items():\n       match = [x for x in synonym if word == x]\n       if word == keyword or match:\n         match = ' '.join(map(str, match))\n         if match not in word_set or match not in duplication_check or word not in duplication_check:\n            duplication_check.add(word)\n            duplication_check.add(match)\n            word_matches.append([keyword, match, frequency])\n\n # used to hold the final keyword and frequencies\n final_results = {}\n\n # list comprehension to obtain the primary keyword and its frequencies\n synonym_matches = [(keyword[0], keyword[2]) for keyword in word_matches]\n\n # iterate synonym_matches and output total frequency count for a specific keyword\n for item in synonym_matches:\n    if item[0] not in final_results.keys():\n      frequency_count = 0\n      frequency_count = frequency_count + item[1]\n      final_results[item[0]] = frequency_count\n else:\n    frequency_count = frequency_count + item[1]\n    final_results[item[0]] = frequency_count\n\n# do something with the final results\n"], [], ["pip install tensorflow==2.0 --user\n"], ["new_dict = {}\nfor k, v in my_dict.items():\n    vv = new_dict.setdefault(v, [])\n    vv.append(k)\n"], ["$ sudo apt-get install python-opencv\n"], [], [], [], ["import os\nos.getcwd()\n", "companies = pd.read_csv(\"companies.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n", "companies = pd.read_csv(\"C:\\\\Users\\\\ersar\\\\Desktop\\\\Project\\\\Company\\\\companies.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n"], ["class User(db.Model):\n    \n    __tablename__ = 'users'\n\n    id = db.Column(db.Integer, primary_key=True)\n    email = db.Column(db.String(120), unique=True, index=True)\n    username = db.Column(db.String(20), unique=True, index=True)\n    hash_passwrd = db.Column(db.String(200))\n\n\n\n    def __init__(self, email, username, password):\n        self.email = email\n        self.username = username\n        self.hash_passwrd = generate_password_hash(password)\n\n\nadams = user('adams@email.com', 'adams', 'adams@1')\nben= user('ben@email.com', 'ben', 'ben@1')\npoolo= user('poolo@email.com', 'poolo', 'poolo@1')\n", "db.session.add_all([adams, ben, poolo])\ndb.session.commit()\n"], [], ["class A:\n    def __init__(self, a):\n        self.a = a\n\nclass B(A):\n\n    def printme(self):\n        print(self.a)\n\nb = B(10)\nb.printme()\n", "10\n"], ["class ParentClass:\n\n    def __init__(self, parent_arg):\n        self.parent_arg = parent_arg\n\n\nclass ChildClass(ParentClass):\n\n    def child_method(self):\n        print(self.parent_arg)\n\n\nc = ChildClass(parent_arg=\"Test\")\nc.child_method() # Output: Test\n", "class ChildClass(ParentClass):\n\n    def __init__(self, child_arg):\n        self.child_arg = child_arg\n\n    def child_method(self):\n        print(self.child_arg) # Output: Child Test\n\n        # This will throw an error since this attribute is created by the parent constructor\n        print(self.parent_arg)\n\nc = ChildClass(child_arg=\"Child Test\")\nc.child_method()\n", "def __init__(self, child_arg, parent_arg):\n    self.child_arg = child_arg\n    super().__init__(parent_arg)\n\nprint(self.parent_arg) # This will then work\n"], ["pip3 install scikit-build\npip3 install cython\n", "pip3 install opencv-python\n"], ["{\"word\": \"ma\", \"key\": \"ma_1\", \"pos\": \"noun\", \"synonyms\": [\"mamma\", \"momma\", \"mama\", \"mammy\", \"mummy\", \"mommy\", \"mom\", \"mum\"]}\n"], ["# assuming series is called 'data'\nsign = data > 0\nsign_change = (sign != sign.shift(1))  # or -1, depending if you want True before the sign change\n"], [], ["from sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\n\nimport spacy\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load the large english model\nnlp = spacy.load(\"en_core_web_lg\")\n\ntokens = nlp(\"dog cat banana apple teaching teacher mom mother mama mommy berlin paris\")\n\n# Generate word embedding vectors\nvectors = np.array([token.vector for token in tokens])\nvectors.shape\n# (12, 300)\n", "pca_vecs = PCA(n_components=3).fit_transform(vectors)\npca_vecs.shape\n# (12, 3)\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection='3d')\nxs, ys, zs = pca_vecs[:, 0], pca_vecs[:, 1], pca_vecs[:, 2]\n_ = ax.scatter(xs, ys, zs)\n\nfor x, y, z, lable in zip(xs, ys, zs, tokens):\n    ax.text(x+0.3, y, z, str(lable))\n", "model = DBSCAN(eps=5, min_samples=1)\nmodel.fit(vectors)\n\nfor word, cluster in zip(tokens, model.labels_):\n    print(word, '->', cluster)\n", "dog -> 0\ncat -> 0\nbanana -> 1\napple -> 2\nteaching -> 3\nteacher -> 3\nmom -> 4\nmother -> 4\nmama -> 4\nmommy -> 4\nberlin -> 5\nparis -> 6\n"], ["import re\n\ndata    = 'KujhKyjiubBMNBHJGJhbvgqsauijuetystareFGcvb'\n\nmatches = re.compile('[^aeiou]', re.I).finditer(data)\nfinal   = f\".{'.'.join([m.group().lower() for m in matches])}\"\n\nprint(final)\n\n#.k.j.h.k.y.j.b.b.m.n.b.h.j.g.j.h.b.v.g.q.s.j.t.y.s.t.r.f.g.c.v.b\n"], ["st = input()\nst=st.lower()\nst=st.replace('a','')\nst=st.replace('e','')\nst=st.replace('o','')\nst=st.replace('i','')\nst=st.replace('u','')\nprint(st)\nst_new = ''\nfor c in st:\n    st_new += '.' + c\nprint(st_new)\n"], ["st = input()\nst=st.lower()\nst=st.replace('a','')\nst=st.replace('e','')\nst=st.replace('o','')\nst=st.replace('i','')\nst=st.replace('u','')\nprint(st)\nst_new = ''\nfor c in st:\n    st_new += '.' + c\nprint(st_new)\n", "for c in 'aeiou':\n    st = st.replace(c, '')\n"], [], ["# st = 'bcb'\nst = '.' + '.'.join(st)\n# '.b.c.b'\n"], [], ["#@title Load the Universal Sentence Encoder's TF Hub module\nfrom absl import logging\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\n\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\nmodel = hub.load(module_url)\nprint (\"module %s loaded\" % module_url)\ndef embed(input):\n  return model(input)\n\ndef plot_similarity(labels, features, rotation):\n  corr = np.inner(features, features)\n  sns.set(font_scale=1.2)\n  g = sns.heatmap(\n      corr,\n      xticklabels=labels,\n      yticklabels=labels,\n      vmin=0,\n      vmax=1,\n      cmap=\"YlOrRd\")\n  g.set_xticklabels(labels, rotation=rotation)\n  g.set_title(\"Semantic Textual Similarity\")\n\ndef run_and_plot(messages_):\n  message_embeddings_ = embed(messages_)\n  plot_similarity(messages_, message_embeddings_, 90)\n\nmessages = [\n    \"Mother\",\n    \"Mom\",\n    \"Mama\",\n    \"Dog\",\n    \"Cat\"\n]\n\nrun_and_plot(messages)\n"], ["# Load a pretrained word2vec model\nimport gensim.downloader as api\nmodel = api.load('word2vec-google-news-300')\n\nvectors = [model.get_vector(w) for w in words]\nfor i, w in enumerate(vectors):\n   first_best_match = model.cosine_similarities(vectors[i], vectors).argsort()[::-1][1]\n   second_best_match = model.cosine_similarities(vectors[i], vectors).argsort()[::-1][2]\n   \n   print (f\"{words[i]} + {words[first_best_match]}\")\n   print (f\"{words[i]} + {words[second_best_match]}\")  \n", "mom + mother\nmom + teacher\nmother + mom\nmother + teacher\nlondon + mom\nlondon + life\nlife + mother\nlife + mom\nteach + teacher\nteach + mom\nteacher + teach\nteacher + mother\n"], ["dictionary.getSynonyms()\n"], [], ["import uuid \n# Some basic setup:\n# Setup detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common libraries\nimport numpy as np\nimport os, json, cv2, random\n#from google.colab.patches import cv2_imshow\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.structures import BoxMode\n\nfrom detectron2.utils.visualizer import ColorMode\n\n\nROOT_DIR = \"./\"\nipdir = ROOT_DIR + \"nswtable_input/image/\"\nopdir = ROOT_DIR + \"results_nswtable/\"\n\n\n\ndef predict(im, item):\n    fileName=item\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=balloon_metadata, \n                   scale=0.8, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n    )\n    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    #print(outputs[\"instances\"].pred_boxes.tensor.numpy())\n    path = \"/root/images/\"\n    path1=\"/root/tblImg/\"\n    cv2.imwrite(path1 + fileName + \".png\", v.get_image()[:, :, ::-1])\n    boxes = {}\n    \n    file = os.path.join(path,fileName)\n    try:\n        f=os.makedirs(file,exist_ok=True)\n        print(\"Directory '%s' created \" % file)\n    except OSError as error:\n        print(\"cannot create\"%directory)\n    i=1\n    coords=[]\n    for coordinates in outputs[\"instances\"].to(\"cpu\").pred_boxes:\n        \n        coordinates_array = []\n        for k in coordinates:\n            coordinates_array.append(int(k))\n        boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n        coords.append(coordinates_array)\n    \n    for k,v in boxes.items():\n\n        crop_img = im[v[1]:v[3], v[0]:v[2], :]\n        #print(v[1],v[3], v[0],v[2])\n        #cv2_imshow(crop_img)\n        crop_width,crop_height=crop_img.shape[0],crop_img.shape[1]\n        if crop_width>crop_height:\n            img_rot=cv2.rotate(crop_img,cv2.ROTATE_90_CLOCKWISE)\n            \n        #------for naming the images------#v[1]=y,v[3]=y+h, v[0]=x,v[2]=x+w\n            margin = 0\n        \n            ymin = max(v[1]-margin,0)\n            ymax =v[3]+margin\n            xmin = max(v[0] - margin,0)\n            xmax = v[2]+margin\n            #print(ymin,ymax,xmin,xmax)\n            cv2.imwrite(file+'/'+str(i)+'_'+str(xmin)+'_'+str(ymin)+'_'+str(xmin)+'_'+str(ymax)+'_'+str(xmax)+'_'+str(ymin)+'_'+str(xmax)+'_'+str(ymax)+ '.png', img_rot)\n            i=i+1\n\n    \n    return outputs\n\n\ndirs = os.listdir(ipdir)\n\n\nfor item in dirs:\n    if os.path.isfile(ipdir+item):\n        im = cv2.imread(ipdir+item)\n        print(item)\n        f, e = os.path.splitext(ipdir+item)\n        #width,height = im.shape[1],im.shape[0]\n        item = item[:-4]\n        predict(im, item)\n", "https://ibb.co/0Q16Gyv\nhttps://ibb.co/7KRVp4M\nhttps://ibb.co/NTjwJ6F\n        \n    \n"], [], [], ["Mode                LastWriteTime         Length Name\n----                -------------         ------ ----\n-a----        8/23/2020   6:41 PM            185 kernel.json\n-a----        1/28/2020   2:18 AM           1084 logo-32x32.png\n-a----        1/28/2020   2:18 AM           2180 logo-64x64.png\n", "{\n \"argv\": [\n  \"C:\\\\Users\\\\harish\\\\Anaconda3\\\\python.exe\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"myenv\",\n \"language\": \"python\"\n}\n"], [], [], ["d = dict(spam=1, eggs=2)\n"], [], ["dmap = dict({0 : 'Mon', 1 : 'Tue', 2 : 'Wed', 3 : 'Thu', 4 : 'Fri', 5 : 'Sat', 6 : 'Sun'})\n"], [], ["\nimport numpy as np\nimport imutils\nimport glob\nimport cv2\n\ntemplate = cv2.imread(\"apA8L.png\")\ntemplate = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\ntemplate = cv2.Canny(template, 50, 200)\n(h, w) = template.shape[:2]\n\nfor imagePath in glob.glob(\"img2\" + \"/*.jpg\"):\n    image = cv2.imread(imagePath)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    found = None\n\n    for scale in np.linspace(0.2, 1.0, 20)[::-1]:\n        resized = imutils.resize(gray, width=int(gray.shape[1] * scale))\n        r = gray.shape[1] / float(resized.shape[1])\n\n        if resized.shape[0] < h or resized.shape[1] < w:\n            break\n\n        edged = cv2.Canny(resized, 50, 200)\n        result = cv2.matchTemplate(edged, template, cv2.TM_CCOEFF)\n        (_, maxVal, _, maxLoc) = cv2.minMaxLoc(result)\n\n        if found is None or maxVal > found[0]:\n            found = (maxVal, maxLoc, r)\n\n    (_, maxLoc, r) = found\n    (startX, startY) = (int(maxLoc[0] * r), int(maxLoc[1] * r))\n    (endX, endY) = (int((maxLoc[0] + w) * r), int((maxLoc[1] + h) * r))\n\n    cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n    cv2.imwrite(\"out.png\", image)\n    print(\"Table coordinates: ({}, {}, {}, {})\".format(startX, startY, endX, endY))\n", "Table Coordinates: (352, 1915, 753, 2445)\n"], ["def get_edges(img):\n  gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n  bitwised_img = cv2.bitwise_not(gray_img)\n  guassian_img = cv2.GaussianBlur(bitwised_img, (5, 5), 0)\n  bilateral_img = cv2.bilateralFilter(guassian_img, 11, 17, 17)\n  t, thresh_bin = cv2.threshold(bilateral_img, 0, 255, cv2.THRESH_OTSU)\n  canny = cv2.Canny(thresh_bin, 0.5 * t, t)\n  dilated = cv2.dilate(canny,\n                       cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3)))\n  medianed = cv2.medianBlur(dilated, 3)\n  # Edges will be discontinous so dialtion will be make them contionuous\n  return medianed\n\n", "def validate_contour(img, cc):\n  \"\"\"Check if the contour is a good predictor of photo location. \n Here you need to address some realtionship to find the table contour\"\"\"\n  x, y, w, h = cv2.boundingRect(cc)\n  if (170000<area <200000) and 500<h<600 and 300<w<400:\n    return True\n  return False\n  \ndef get_contours(img):\n  contours, hierarchy = cv2.findContours(edges, 1,\n                                         2)\n  # filter contours that are too large or small\n  # print('not_filtered_contours_contours',contours)\n  contours = [cc for cc in contours if validate_contour(img, cc)]\n  return contours\n"], [], [], ["sudo -i\npip3 uninstall discord.py\npip3 install discord.py\npython3 -m pip install -U discord.py\n", "(sleep 30; python3 /home/pi/applications/myBot.py &) &\n"], [], [], ["conda create -n test python=3.7.6\nconda activate test\npip install scikit-learn==0.23.1\npip install imbalanced-learn==0.7.0\n"], ["pip install pipwin\n\npipwin install pyaudio\n"], ["discord.py==1.3.4\n"], ["colA    2\ncolB    1\ncolC    1\ndtype: int64\n", "  colA colB colC\n0    ?  NaN    ?\n1  NaN  NaN  NaN\n2  NaN    ?  NaN\n3  NaN  NaN  NaN\n4    ?  NaN  NaN\n"], [], ["df.eq('?').sum()\nOut[182]: \ncolA    2\ncolB    1\ncolC    1\ndtype: int64\n"], ["pip uninstall discord.py\npip install discord.py\npy -3 -m pip install -U discord.py\n", "pip uninstall discord.py\npip install discord.py\npython3 -m pip install -U discord.py\n"], ["--- channel.py.old  2017-02-27 15:02:23.000000000 -0800\n+++ channel.py  2020-07-22 02:44:03.000000000 -0700\n@@ -27,13 +27,28 @@\n from . import utils\n from .permissions import Permissions, PermissionOverwrite\n from .enums import ChannelType\n-from collections import namedtuple\n from .mixins import Hashable\n from .role import Role\n from .user import User\n from .member import Member\n \n-Overwrites = namedtuple('Overwrites', 'id allow deny type')\n+class Overwrites:\n+    __slots__ = ('id', 'allow', 'deny', 'type')\n+\n+    def __init__(self, **kwargs):\n+        self.id = kwargs.pop('id')\n+        self.allow = kwargs.pop('allow', 0)\n+        self.deny = kwargs.pop('deny', 0)\n+        self.type = kwargs.pop('type')\n+\n+    def _asdict(self):\n+        return {\n+            'id': self.id,\n+            'allow': self.allow,\n+            'deny': self.deny,\n+            'type': self.type,\n+        }\n+\n \n class Channel(Hashable):\n     \"\"\"Represents a Discord server channel.\n"], ["python3 -m pip install -U discord.py\n"], [], [], ["from dask.array import from_array as fa\ndf.compute()['Name of you column'] = fa(the_list_you_want_to_assign_as_column)\n"], [], ["\nIn [1]: def num_return():\n   ...:   try:\n   ...:     x=[100]\n   ...:     return x\n   ...:   finally:\n   ...:     x[0] = 90\n   ...:\n\nIn [2]: num_return()\nOut[2]: [90]\n\nIn [3]: def num_return():\n   ...:   try:\n   ...:     x=[100]\n   ...:     return x[0]\n   ...:   finally:\n   ...:     x[0] = 90\n   ...:\n\nIn [4]: num_return()\nOut[4]: 100\n"], [], [], [], ["df['Maintanance_price'].replace(to_replace = ['low', 'med','high','vhigh'], value =[1,2,3,4], inplace=True)\ndf.head()\n"], ["new_dict = {v: [x for x, y in my_dict.items() if y == v] for k, v in my_dict.items() }\n"], ["def func(my_dict):\n    new_dict = {}\n    new_dict['name'] = {}\n    new_dict['last_name'] = {}\n    for i in my_dict.keys():\n        if i%2 == 1 :\n            new_dict['name'].append(i)\n        else :\n            new_dict['last_name'].append(i)\n    return new_dict\n"], ["my_dict = {1: \"name\",\n           2: \"last_name\",\n           3: \"name\",\n           4: \"last_name\",\n           5: \"name\",\n           6: \"last_name\"}\n\ndef my_function(my_dict):\n\n    new_dict = {v:[k for k in my_dict.keys() if my_dict[k] == v] for v in my_dict.values()}\n\n    return new_dict\n\nprint(my_function(my_dict))\n", "{'name': [1, 3, 5], 'last_name': [2, 4, 6]}\n"], [], ["new_dict.fromkeys(set(my_dict.values()), [])\n", "new_dict = { k: [] for k in my_dict.values() }\n"], [], [], [], ["from functools import reduce\n\nlst = [1, 2, 3, 4, 5]\n\nprint(reduce(lambda x, y: x + [x[-1] * y], lst, [lst.pop(0)]))\n", "lst = [1, 2, 3, 4, 5]\n\nseq = [a.append(a[-1] * b) or a.pop(0) for a in [[lst.pop(0)]] for b in [*lst, 1]]\n\nprint(seq)\n"], ["from math import factorial\n\ninput_list = [1, 2, 3, 4, 5]\nl2r = [factorial(i) for i in input_list]\n\nprint(l2r)\n", "[1, 2, 6, 24, 120]\n"], ["import math\n\norig = [1, 2, 3, 4, 5]\nprint([math.prod(orig[:pos]) for pos in range(1, len(orig) + 1)])\n", "[1, 2, 6, 24, 120]\n", "pos   values    prod\n===  =========  ====\n 1   1             1\n 2   1,2           2\n 3   1,2,3         6\n 4   1,2,3,4      24\n 5   1,2,3,4,5   120\n", "def listToListOfProds(orig):\n    curr = 1\n    newList = []\n    for item in orig:\n        curr *= item\n        newList.append(curr)\n    return newList\n\nprint(listToListOfProds([1, 2, 3, 4, 5]))\n"], ["[1, 2, 6, 24, 120]\n", "from itertools import accumulate\n\nout = [*accumulate(lst, lambda a, b: a*b)]\nprint(out)\n"], ["conda activate <yourEnv>\npython -i\n"], ["   train['Some_feature']=train.Some_feature.astype(object)\n"], ["# start templated\nINSTALL_PYTHON = 'PATH/TO/YOUR/ENV/EXECUTABLE'\n", "# start templated\nINSTALL_PYTHON = 'PATH/TO/YOUR/ENV/EXECUTABLE'\nos.environ['PATH'] = f'{os.path.dirname(INSTALL_PYTHON)}{os.pathsep}{os.environ[\"PATH\"]}'\n"], [], [], ["import numpy as np \nnp.sign(data).diff().ne(0)\n"], [], ["mylist1 = [[\"lemon\", 0.1], [\"egg\", 0.1], [\"muffin\", 0.3], [\"chocolate\", 0.5]]\nmylist2 = [[\"chocolate\", 0.5], [\"milk\", 0.2], [\"carrot\", 0.8], [\"egg\", 0.8]]\n\nrecipe_1 = dict(mylist1)  # {'lemon': 0.1, 'egg': 0.1, 'muffin': 0.3, 'chocolate': 0.5}\nrecipe_2 = dict(mylist2)  # {'chocolate': 0.5, 'milk': 0.2, 'carrot': 0.8, 'egg': 0.8}\n\ncommon_keys = recipe_1.keys() & recipe_2.keys()  # {'chocolate', 'egg'}\n\nmyoutput = [[item, np.mean((recipe_1[item], recipe_2[item]))] for item in common_keys]\nmyoutput = [[item, (recipe_1[item] + recipe_2[item]) / 2] for item in common_keys]\n"], ["from collections import defaultdict\nfrom statistics import mean\n\nmylist1 = [[\"lemon\", 0.1], [\"egg\", 0.1], [\"muffin\", 0.3], [\"chocolate\", 0.5]]\nmylist2 = [[\"chocolate\", 0.5], [\"milk\", 0.2], [\"carrot\", 0.8], [\"egg\", 0.8]]\n\nd = defaultdict(list)\nfor lst in (mylist1, mylist2):\n    for k, v in lst:\n        d[k].append(v)\n\nresult = [[k, mean(v)] for k, v in d.items()]\n\nprint(result)\n# [['lemon', 0.1], ['egg', 0.45], ['muffin', 0.3], ['chocolate', 0.5], ['milk', 0.2], ['carrot', 0.8]]\n", "result = [[k, mean(v)] for k, v in d.items() if len(v) > 1]\n\nprint(result)\n# [['egg', 0.45], ['chocolate', 0.5]]\n", "mylist1 = [[\"lemon\", 0.1], [\"egg\", 0.1], [\"muffin\", 0.3], [\"chocolate\", 0.5]]\nmylist2 = [[\"chocolate\", 0.5], [\"milk\", 0.2], [\"carrot\", 0.8], [\"egg\", 0.8]]\n\nd1, d2 = dict(mylist1), dict(mylist2)\n\nresult = [[k, (d1[k] + d2[k]) / 2] for k in d1.keys() & d2.keys()]\n\nprint(result)\n# [['egg', 0.45], ['chocolate', 0.5]]\n"], ["intersect = set([a[0] for a in mylist1]).intersection([a[0] for a in mylist2])\nd1=dict(mylist1)\nd2=dict(mylist2)\n{i:(d1[i]+d2[i])/2 for i in intersect}\n"], ["mylist1 = [[\"lemon\", 0.1], [\"egg\", 0.1], [\"muffin\", 0.3], [\"chocolate\", 0.5]]\nmylist2 = [[\"chocolate\", 0.5], [\"milk\", 0.2], [\"carrot\", 0.8], [\"egg\", 0.8]]\n\ndict1 = dict(mylist1)\ndict2 = dict(mylist2)\nres = [[key, (dict1.get(key)+dict2.get(key))/2] for key in set(dict1.keys()).intersection(set(dict2.keys()))]\nprint(res)\n", ">> [['chocolate', 0.5], ['egg', 0.45]]\n"], ["In []:\nd = {}\nfor lst in (mylist1, mylist2):\n    for i, v in lst:\n        d.setdefault(i, []).append(v)   # alternative use collections.defaultdict\n\n[(k, sum(v)/len(v)) for k, v in d.items()]\n\nOut[]:\n[('lemon', 0.1), ('egg', 0.45), ('muffin', 0.3), ('chocolate', 0.5), ('milk', 0.2), ('carrot', 0.8)]\n", "In []:\n[(k, sum(v)/len(v)) for k, v in d.items() if len(v) > 1]\n\nOut[]:\n[('egg', 0.45), ('chocolate', 0.5)]\n"], ["d_list1 = dict(mylist1)\nd_list2 = dict(mylist2)\n\n[[k, (v+d_list2[k])/2] for k, v in d_list1.items() if k in d_list2]\n#[['egg', 0.45], ['chocolate', 0.5]]\n"], ["[['chocolate', 0.5], ['egg', 0.45]]\n"], [], [], [], ["conda install -c conda-forge/label/cf202003 pandas-profiling\n"], [], ["import tensorflow as tf\ntf.random.set_seed() \n"], [], ["(setenv \"WORKON_HOME\" \"~/anaconda3/envs\") ; /anaconda3 || /miniconda || wathever path your conda installation is located at\n(pyvenv-mode 1)\n"], ["def split_by_size(items, max_size, get_size=len):\n    buffer = []\n    buffer_size = 0\n    for item in items:\n        item_size = get_size(item)\n        if buffer_size + item_size <= max_size:\n            buffer.append(item)\n            buffer_size += item_size\n        else:\n            yield buffer\n            buffer = [item]\n            buffer_size = item_size\n    if buffer_size > 0:\n        yield buffer\n", "import random\n\n\nk = 10\nn = 15\nmax_size = 10\n\nrandom.seed(0)\nitems = [b'x' * random.randint(1, 2 * k // 3) for _ in range(n)]\nprint(items)\n# [b'xxxx', b'xxxx', b'x', b'xxx', b'xxxxx', b'xxxx', b'xxxx', b'xxx', b'xxxx', b'xxx', b'xxxxx', b'xx', b'xxxxx', b'xx', b'xxx']\n\nprint(list(split_by_size(items, k)))\n# [[b'xxxx', b'xxxx', b'x'], [b'xxx', b'xxxxx'], [b'xxxx', b'xxxx'], [b'xxx', b'xxxx', b'xxx'], [b'xxxxx', b'xx'], [b'xxxxx', b'xx', b'xxx']]\n", "def chunks_by_size(items, max_size, get_size=len):\n    result = []\n    size = max_size + 1\n    for item in items:\n        item_size = get_size(item)\n        size += item_size\n        if size > max_size:\n            result.append([])\n            size = item_size\n        result[-1].append(item)\n    return result\n", "def chunks_by_size_reduce(items, size, get_size=len):\n    return functools.reduce(\n        lambda a, b, size=size:\n            a[-1].append(b) or a\n            if a and sum(get_size(x) for x in a[-1]) + get_size(b) <= size\n            else a.append([b]) or a, items, [])\n", "%timeit list(split_by_size(items * 100000, k + 1))\n# 10 loops, best of 3: 281 ms per loop\n%timeit list(split_by_size_cy(items * 100000, k + 1))\n# 10 loops, best of 3: 181 ms per loop\n%timeit list(split_by_size_nb(items * 100000, k + 1))\n# 100 loops, best of 3: 5.17 ms per loop\n%timeit chunks_by_size(items * 100000, k + 1)\n# 10 loops, best of 3: 318 ms per loop\n%timeit chunks_by_size_reduce(items * 100000, k + 1)\n# 1 loop, best of 3: 1.18 s per loop\n"], ["l = [b'abc', b'def', b'ghi', b'jklm', b'nopqrstuv', b'wx', b'yz']\n\nreduce(lambda a, b, size=7: a[-1].append(b) or a if a and sum(len(x) for x in a[-1]) + len(b) <= size else a.append([b]) or a, l, [])\n", "[[b'abc', b'def'], [b'ghi', b'jklm'], [b'nopqrstuv'], [b'wx', b'yz']]\n"], ["[[b'foo', b'bar'], [b'baz']]\n"], ["tf.executing_eagerly()\n", "tf.enable_eager_execution()\n"], [], [], [], [], ["l1=['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\nl2=['y','tor','e','eps','ay',None,'le','n']\na=l2[::-1]\nl3=[]\nfor i in range(len(l1)):\n    if(l1[i] is None or a[i] is None):\n        l3.append(l1[i])`enter code here`\n    else:\n        l3.append(l1[i]+a[i])\n\n\nprint(\" \".join(l3))\n"], ["from selenium import webdriver\n\nprofile = webdriver.FirefoxProfile() \nprofile.add_extension(extension='extension_name.xpi')\ndriver = webdriver.Firefox(firefox_profile=profile, executable_path=r'C:\\path\\to\\geckodriver.exe') \n"], [], ["- task: UsePythonVersion@0\n  displayName: 'Use Python 3.6'\n  inputs:\n    versionSpec: 3.6 # Functions V2 supports Python 3.6 as of today\n\n- bash: |\n    python -m venv worker_venv\n    source worker_venv/bin/activate\n    pip install -r requirements.txt\n  workingDirectory: $(workingDirectory)\n  displayName: 'Install application dependencies'\n", "- master\n\nvariables:\n  # Azure Resource Manager connection created during pipeline creation\n  azureSubscription: '<subscription-id>'\n\n  # Function app name\n  functionAppName: '<built-function-app-name>'\n\n  # Agent VM image name\n  vmImageName: 'ubuntu-latest'\n\n  # Working Directory\n  workingDirectory: '$(System.DefaultWorkingDirectory)/__app__'\n\nstages:\n- stage: Build\n  displayName: Build stage\n\n  jobs:\n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: $(vmImageName)\n\n    steps:\n    - bash: |\n        if [ -f extensions.csproj ]\n        then\n            dotnet build extensions.csproj --runtime ubuntu.16.04-x64 --output ./bin\n        fi\n      workingDirectory: $(workingDirectory)\n      displayName: 'Build extensions'\n\n    - task: UsePythonVersion@0\n      displayName: 'Use Python 3.7'\n      inputs:\n        versionSpec: 3.7 # Functions V2 supports Python 3.6 as of today\n\n    - bash: |\n        pip install --upgrade pip\n        pip install --target=\"./.python_packages/lib/site-packages\" -r ./requirements.txt\n      workingDirectory: $(workingDirectory)\n      displayName: 'Install application dependencies'\n\n    - task: ArchiveFiles@2\n      displayName: 'Archive files'\n      inputs:\n        rootFolderOrFile: '$(workingDirectory)'\n        includeRootFolder: false\n        archiveType: zip\n        archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip\n        replaceExistingArchive: true\n\n    - publish: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip\n      artifact: drop\n\n- stage: Deploy\n  displayName: Deploy stage\n  dependsOn: Build\n  condition: succeeded()\n\n  jobs:\n  - deployment: Deploy\n    displayName: Deploy\n    environment: 'production'\n    pool:\n      vmImage: $(vmImageName)\n\n    strategy:\n      runOnce:\n        deploy:\n\n          steps:\n          - task: AzureFunctionApp@1\n            displayName: 'Azure functions app deploy'\n            inputs:\n              azureSubscription: '$(azureSubscription)'\n              appType: functionAppLinux\n              appName: $(functionAppName)\n              package: '$(Pipeline.Workspace)/drop/$(Build.BuildId).zip'\n"], ["df[\"AndHeathSolRadFact\"] = np.select(\n    [\n    (df['Month'].between(8,12)),\n    (df['Month'].between(1,2) & df['CloudCover']>30)\n    ],  #list of conditions\n    [1, 1],     #list of results\n    default=0).all()    #default if no match\n"], [], ["v = np.array([7, 2, 0, 4, 4, 6, 6, 9, 5, 5])\np = get_peaks(v)\nprint(v)        # [7 2 0 4 4 6 6 9 5 5]\nprint(p)        # [0 2 7 9] (peak indexes)\nprint(v[p])     # [7 0 9 5] (peak elements)\n", "v = np.array([8, 2, 1, 0, 1, 2, 2, 5, 9, 3])\np = get_peaks(v)\nprint(v)        # [8 2 1 0 1 2 2 5 9 3]\nprint(p)        # [0 3 8 9] (peak indexes)\nprint(v[p])     # [8 0 9 3] (peak elements)\n", "v = np.array([9, 8, 8, 8, 0, 8, 9, 9, 9, 6])\np = get_peaks(v)\nprint(v)        # [9 8 8 8 0 8 9 9 9 6]\nprint(p)        # [0 4 6 9] (peak indexes)\nprint(v[p])     # [9 0 9 6] (peak elements)\n"], ["import tensorflow as tf\n\nraw_dataset = tf.data.TFRecordDataset(\"input_file.tfrecord\")\n\nshards = 10\n\nfor i in range(shards):\n    writer = tf.data.experimental.TFRecordWriter(f\"output_file-part-{i}.tfrecord\")\n    writer.write(raw_dataset.shard(shards, i))\n"], ["INSTALLED_APPS = [\n    'myapp.apps.MyappConfig',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n"], ["import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\ntf.enable_eager_execution()\n"], ["from tensorflow.python.keras.models import Sequential\n"], ["pip3.6 install --target .python_packages/lib/site-packages -r requirements.txt\n", "python3.6 -m venv worker_venv\nsource worker_venv/bin/activate\npip3.6 install setuptools\npip3.6 install -r requirements.txt\n"], ["python -m venv /path/to/my/dir\nsource /path/to/my/dir/bin/activate \ncd /path/to/my/dir/bin/activate\npip install -r requirements.txt\ndeactivate\n"], ["import azure.functions as func\nimport requests\n"], [], [], ["SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH%\n"], ["from tensorflow import set_random_seed\n", "set_random_seed(x)\n", "import tensorflow\ntensorflow.random.set_seed(x)\n"], ["import tensorflow as tf    \ntf.set_random_seed(1234)\n"], [], ["max(ar.count(i) for i in ar)\n", "max(map(ar.count,ar))\n"], ["from collections import Counter\nwinner = Counter(ar).most_common(1)[0]\n", "# Python program to find the maximum repeating number \n\n# Returns maximum repeating element in arr[0..n-1]. \n# The array elements are in range from 0 to k-1 \ndef maxRepeating(arr, n,  k): \n\n    # Iterate though input array, for every element \n    # arr[i], increment arr[arr[i]%k] by k \n    for i in range(0,  n): \n        arr[arr[i]%k] += k \n\n    # Find index of the maximum repeating element \n    max = arr[0] \n    result = 0\n    for i in range(1, n): \n\n        if arr[i] > max: \n            max = arr[i] \n            result = i \n\n    # Uncomment this code to get the original array back \n    #for i in range(0, n): \n    #    arr[i] = arr[i]%k \n\n    # Return index of the maximum element \n    return result \n"], ["max(set(ar), key=ar.count) \n"], ["from collections import Counter\n\ndef returnMaxFrequency(ar):\n    return max(Counter(t).values())\n"], ["def returnMaxFrequency(ar):   \n    freqDict = {x:ar.count(x) for x in set(ar)}   \n    maxFreq = max(freqDict.values())\n    return maxFreq\n"], [">>>import tensorflow as tf\ntensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\n\n>>>print(tf.__version__)\n2.0.0\n\n>>>from tensorflow.python.client import device_lib\n>>>print(device_lib.list_local_devices())\n\ntensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\ntensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.531\nGPU libraries are statically linked, skip dlopen check.\nAdding visible gpu devices: 0\nDevice interconnect StreamExecutor with strength 1 edge matrix:\n     0\n0:   N\nCreated TensorFlow device (/device:GPU:0 with 1340 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4456898788177247918\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 1406107238\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 3224787151756357043\nphysical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n]\n"], ["import tensorflow as tf\n\ntf.compat.v1.disable_eager_execution()\n"], [], ["conda update tensorflow\n", "pip install --upgrade Tensorflow\n"], ["In [41]: v = np.arange(5000)\n\nIn [42]: %timeit np.outer(v, v) % 10\n1 loop, best of 3: 544 ms per loop\n", "In [37]: def f(n):\n    ...:     v = np.arange(n)\n    ...:     a = np.outer(v, v)\n    ...:     a %= 10\n    ...:     return a\n    ...:\n\nIn [39]: %timeit f(5000)\n1 loop, best of 3: 437 ms per loop\n", "In [50]: def f(n):\n    ...:  v = np.arange(n, dtype=np.int32)\n    ...:  a = np.outer(v, v)\n    ...:  a %= 10\n    ...:  return a\n    ...:\n\nIn [51]: %timeit f(5000)\n10 loops, best of 3: 126 ms per loop\n", "In [69]: v = np.arange(5000, dtype=np.int32)\n\nIn [70]: vt = v[np.newaxis].T\n\nIn [71]: %timeit ne.evaluate('v * vt % 10')\n10 loops, best of 3: 25.3 ms per loop\n"], ["import numpy as np\nimport numba as nb\nimport numexpr as ne\n\n@nb.njit(parallel=True)\ndef func_1(num):\n    result = np.empty((num, num),dtype=np.int32)\n    for i in nb.prange(result.shape[0]):\n        for j in range(result.shape[1]):\n            result[i, j] = (i * j) % 10\n    return result\n"], [], ["temp = numpy.arange(5000)\nresult = numpy.outer(temp, temp) % 10\n# or result = temp * temp[:, None] % 10\n"], [], ["import dask.dataframe as dd\nimport dask.multiprocessing\nimport dask.threaded\nimport pandas as pd\nimport numpy as np\n\n# Dataframes implement the Pandas API\nimport dask.dataframe as dd\n\nfrom timeit import default_timer as timer\nstart = timer()\nddf = dd.read_csv(r'C:\\Users\\i5-Desktop\\Downloads\\Weathergrids.csv')\n#print(ddf.describe(include='all'))\n\n#Wrangle the dates so we can interrogate them\nddf['DateTime'] = dd.to_datetime(ddf['Date'], format='%Y-%d-%m %H:%M')\nddf['Month'] = ddf['DateTime'].dt.month\n\n#Grass Fuel Moisture Content\nddf['Grass_FMC'] = (97.7+4.06*ddf['RH'])/(ddf['Temperature']+6)-0.00854*ddf['RH']+3000/ddf['Curing']-30\n\n#Convert to a Pandas DataFrame because dask was being slow with the select logic below\ndf = ddf.compute() \ndel [ddf]\n\n#ddf[\"AndHeathSolRadFact\"] = np.select(\n#Solar Radiation Factor - this seems to take 32 seconds. Why?\ndf[\"AndHeathSolRadFact\"] = np.select(\n    [\n    (df['Month'].between(8,12)),\n    (df['Month'].between(1,2) & df['CloudCover']>30)\n    ],  #list of conditions\n    [1, 1],     #list of results\n    default=0)    #default if no match\n\n#Convert back to a Dask dataframe because we want that juicy parallelism\nddf2 = dd.from_pandas(df,npartitions=4)\ndel [df]\n\nprint(ddf2.head())\n#print(ddf.tail())\nend = timer()\nprint(end - start)\n\n#Clean up remaining dataframes\ndel [[ddf2]]\n"], [], [], [], ["C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64\n"], ["pip3 uninstall torch\npip3 uninstall torchvision\n", "pip3 install torch==1.1.0\npip3 install torchvision==0.3.0\n"], ["<title>AccessibleComputing</title>\n", "<page>\n    <title>Anarchism</title>\n    <ns>0</ns>\n    <id>12</id>\n    ... \n</page>\n", "from lxml import etree\nfrom lxml.etree import tostring\n\ntree = etree.parse('data/enwiki-20190620-pages-articles-multistream.xml')\nroot = tree.getroot()\n# iterate through all the titles\nfor title in root.findall(\".//title\", namespaces=root.nsmap):\n    print(tostring(title))\n    print(title.text)\n\n", "b'<title xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">AccessibleComputing</title>\\n    '\nAccessibleComputing\nb'<title xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">Anarchism</title>\\n    '\nAnarchism\n\n", "nsmap = root.nsmap\nnsmap['x'] = root.nsmap[None]\nnsmap.pop(None)\n# iterate through all the pages\nfor page in root.findall(\".//x:page\", namespaces=nsmap):\n    print(page)\n    print(repr(page.text)) # which prints '\\n    '\n    print('number of children: %i' % len(page.getchildren()))\n\n", "<Element {http://www.mediawiki.org/xml/export-0.10/}page at 0x7ff75cc610c8>\n'\\n    '\nnumber of children: 5\n<Element {http://www.mediawiki.org/xml/export-0.10/}page at 0x7ff75cc71bc8>\n'\\n    '\nnumber of children: 5\n"], ["pip3 install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-win_amd64.whl\n", "pip3 install https://download.pytorch.org/whl/cpu/torchvision-0.3.0-cp37-cp37m-win_amd64.whl\n"], ["pip install torchvision --no-deps\n"], ["import untangle\n\ndoc = untangle.parse('data/enwiki-20190620-pages-articles-multistream.xml')\nfor page in doc.mediawiki.page:\n    print(page.title.cdata)\n    for text in page.revision.text:\n        print(text.cdata)\n"], ["elif tname == 'page':\n", "elif tname == 'text':\n"], [], ["python-3.7.3\n"], [], ["import os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 'g_we0w^3!=y@rk40#m1+$luf!wc8d%x#*mekm#*z@g-vv^zowo'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.genres',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'djangoproject.urls'\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'djangoproject.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.2/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.2/ref/settings/#auth-password- \nvalidators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': \n'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': \n'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': \n'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': \n'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.2/howto/static-files/\n\nSTATIC_URL = '/static/'\n"], ["import pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nn_bins = 5\ndf = pd.DataFrame(data=norm.rvs(loc=500, scale=50, size=100),\n                  columns=['PRICE'])\ny = label_encoder.fit_transform(pd.cut(df['PRICE'], n_bins, retbins=True)[0])\nrfc = RandomForestClassifier(n_estimators=100, verbose=2)\nrfc.fit(df[['PRICE']], y)\n", "array([1, 2, 2, 0, 2, 2, 0, 1, 3, 1, 1, 2, 1, 4, 4, 2, 3, 1, 1, 3, 2, 3,\n       2, 2, 2, 0, 2, 2, 4, 1, 3, 2, 1, 3, 3, 2, 1, 4, 3, 1, 1, 4, 2, 3,\n       3, 2, 1, 1, 3, 4, 3, 3, 3, 2, 1, 2, 3, 1, 3, 1, 2, 0, 1, 1, 2, 4,\n       1, 2, 2, 2, 0, 1, 0, 3, 3, 4, 2, 3, 3, 2, 3, 1, 3, 4, 2, 2, 2, 0,\n       0, 0, 2, 2, 0, 4, 2, 3, 2, 2, 2, 2])\n"], ["df['CLASS'] = np.where( df.PRICE > 1000, 1, 0) # Classify price above 1000 or less\n", "from sklearn import linear_model\nreg = linear_model()\nreg.fit(df[features], df['CLASS'])\n"], [], [], [], [], ["div_tags = soup.find_all('div')\nids = []\nfor div in div_tags:\n     ID = div.get('id')\n     if ID is not None:\n         ids.append(ID)\n"], [], ["def gen_unique_zero_sum_list(num_members):\n    ret = []\n    for i in range(int(num_members) - 1):\n        candidate = random.randint(-100, 100)\n        while candidate in ret:\n            candidate = random.randint(-100, 100)\n        ret.append(candidate)\n    ret.append(-sum(ret))\n    return ret\n", "def gen_unique_zero_sum_list(num_members, min=-100, max=100):\n    if int(num_members) < 1:\n        raise ValueError\n    ret = []\n    # populate as many as we can randomly\n    for i in range(0, int(num_members) - 2, 1):\n        candidate = random.randint(min, max)\n        while candidate in ret:\n            candidate = random.randint(min, max)\n        ret.append(candidate)\n    if int(num_members) > 1:\n        while len(ret) < int(num_members):\n            # at this point we could get a forced duplicate\n            candidate = random.randint(min, max)\n            while candidate in ret:\n                candidate = random.randint(min, max)\n            final = -(sum(ret) + candidate)\n            if final in ret or final == candidate:\n                # we would have a duplicate, force two new numbers\n                continue\n            ret.append(candidate)\n            ret.append(final)\n    else:\n        # this will always be zero, by definition\n        ret.append(-sum(ret))\n    return ret\n"], ["import random\n\n\ndef foo(n, low, high):\n    \"\"\"Helper function for recursive function\"\"\"\n\n    def rbar(arr):\n        \"\"\"Recursive function to create a 'random' list of n integers\n        between low and high (exclusive) such that the sum of the list is 0\n        and the entries of the list are unique.\"\"\"\n\n        # if one spot left check if -sum(arr) in arr\n        if len(arr) == n-1:\n            if -sum(arr) not in arr and -sum(arr) in range(low, high):\n                    return arr + [-sum(arr)]\n\n        # if more than one spot then generate the next possible values\n        # and try to get to a full list of size n with those\n        else:\n            # loop through shuffled options (randomness here)\n            options = [x for x in range(low, high) if x not in arr]\n\n            for opt in random.sample(options, len(options)):\n                # if recursively going through function produces a list then return it\n                if rbar(arr + [opt]) is not None:\n                    return rbar(arr + [opt])\n\n            # if nothing worked then return None\n            return\n\n\n    if n==0:\n        return []\n    elif n==1 and 0 in range(low, high):\n        return [0]\n    elif n==1 and 0 not in range(low, high):\n        return None\n    else:\n        return rbar([])\n\n\nk = foo(4, -10, 10)\n\nif k is not None:\n    print(\"List: {}, Sum:{}\".format(k, sum(k)))\nelse:\n    print(None)\n"], ["import random\nN=4\nl = random.sample(range(-10, 10), k=3)\nprint(l + [-sum(l)])\n", "[-5, 1, -8, 12]\n", "print(sum([-5, 1, -8, 12]))\n", "0\n"], [], ["df.columns[df.stack().str.contains('%').any(level=1)]\n\nIndex(['C', 'D'], dtype='object')\n", "[c for c in df if df[c].str.contains('%').any()]\n\n['C', 'D']\n", "[*filter(lambda c: df[c].str.contains('%').any(), df)]\n\n['C', 'D']\n", "from numpy.core.defchararray import find\n\ndf.columns[(find(df.to_numpy().astype(str), '%') >= 0).any(0)]\n\nIndex(['C', 'D'], dtype='object')\n"], ["df.melt().loc[lambda x :x.value.str.contains('%'),'variable'].unique()\nOut[556]: array(['C', 'D'], dtype=object)\n"], ["df.loc[:,(df != df.replace('%', '', regex=True)).any()]\n     C    D\n0   10  10%\n1  20%  20%\n2  30%   30\n\ndf.columns[(df != df.replace('%', '', regex=True)).any()]\n# Index(['C', 'D'], dtype='object')\n"], ["c = df.columns[df.select_dtypes(object).applymap(lambda x: '%' in str(x)).any()].tolist()\nprint (c)\n['C', 'D']\n", "f = lambda x: x.str.contains('%', na=False)\nc = df.columns[df.select_dtypes(object).apply(f).any()].tolist()\nprint (c)\n['C', 'D']\n"], ["df.columns[df.apply(lambda x: x.str.contains(\"\\%\")).any()]\n"], ["list1=['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\nlist2=['y','tor','e','eps','ay',None,'le','n']\na=list2.remove(None)\nlist2.insert(5,\"\")\nlist3 = [ str(x[0]) + x[1]   for x in zip(list1, list2[::-1]) ]\nprint ' '.join(list3)\n"], ["import datetime\n\nbirthday = input('Enter your birthday in dd/mm/yyyy format')\nday, month, year = list(map(int, birthday.split(\"/\")))\nbirthdate = datetime.date(year, month, day)\n\nprint(f\"Birthday is on {birthdate.strftime('%d/%m/%Y')}\")\n"], ["a = [1,2,2,3,3,4,5,6]\n\nfrom collections import defaultdict\ndef function(lis,n):\n    dic = defaultdict(int)\n\n    sol=set()\n\n    for i in lis:\n            try:\n                if dic[i]:\n                    pass\n                else:\n                    sol.add(i)\n                    dic[i]=1\n                    if len(sol)>=n:\n                        break\n            except KeyError:\n                pass\n\n    return list(sol)\n\nprint(function(a,3))\n", "[1, 2, 3]\n"], ["print ('\"quotation marks\"')\n\"quotation marks\"\n", "print (\"\\\"quotation marks\\\"\")\n\"quotation marks\" \n", "print (\"\"\" \"quotation marks\" \"\"\")\n\"quotation marks\" \n"], ["print('\"\"\"\"')\n", "print(\"\\\"\\\"\\\"\\\"\")\n", "print('\"'*4)\n"], ["print(\"\\\"\")\n"], [], ["Import keras.<something>.<something>\n", "Import tensorflow.keras.<something>.<something>\n"], ["b = \"30/8/1985\"\n\nfirst_sep, last_sep = b.find(\"/\"), b.rfind(\"/\")\nday = b[:first_sep]\nmonth = b[first_sep+1:last_sep]\nyear = b[last_sep+1:]\n\nprint(\"Day: \", day)\nprint(\"Month: \", month)\nprint(\"Year: \", year)\n", "Day:  30\nMonth:  8\nYear:  1985\n"], ["from datetime import datetime\n\nbirthday = input(\"Enter your date of birth: \")\n\nbday = datetime.strptime(birthday, '%d/%m/%Y')\n\nprint(f'Day: {bday.day}')\nprint(f'Month: {bday.month}')\nprint(f'Year: {bday.year}')\n"], ["birthday = input(\"Enter your date of birth: \",)\nmatch = re.search(r'\\d{4}-\\d{2}-\\d{2}', birthday)\ndate = datetime.strptime(match.group(), '%Y-%m-%d').date()\n"], ["birthday = input(\"Enter your date of birth: \",)\nbirthday_list = birthday.split(\"/\")\nprint(\"Day: \",birthday_list[0])\nprint(\"Month: \", birthday_list[1])\nprint(\"Year: \", birthday_list[2])\n"], ["import math\nimport numpy\n\nmath.isnan(math.nan)\n# True\nmath.isnan(numpy.nan)\n# True\nmath.isnan(float(\"nan\"))\n# True\n", "math.nan == numpy.nan or math.nan is numpy.nan\n# False\nmath.nan == float(\"nan\") or math.nan is float(\"nan\")\n# False\nnumpy.nan == float(\"nan\") or numpy.nan is float(\"nan\")\n# False\n"], [], [">>> a = float('nan')\n>>> b = float('nan')\n>>> a is b\nFalse\n", ">>> a = 1.\n>>> b = 1.\n>>> a is b\nFalse\n", ">>> a = 1\n>>> b = 1\n>>> a is b\nTrue\n", ">>> a=1.\n>>> b=1.\n>>> c=float('nan')\n>>> d=float('nan')\n>>> e=1\n>>> f=1\n>>> id(a)\n139622774035752\n>>> id(b)\n139622774035872\n>>> id(c)\n139622774035824\n>>> id(d)\n139622774035800\n>>> id(e)\n139622781650528\n>>> id(f)\n139622781650528\n"], [">>> a = [1,2,3]\n>>> b = a\n>>> id(a)\n140302781856200\n>>> id(b)\n140302781856200\n>>> a == b\nTrue\n>>> a is b\nTrue\n>>> c = [1,2,3]\n>>> id(c)\n140302781864904\n>>> a == c\nTrue\n>>> a is c\nFalse\n"], ["from django.urls import path\nfrom django.views.generic import RedirectView\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('projects/', include('projects.urls')),\n    path('', RedirectView.as_view(url='/projects/')),\n]\n"], ["from django.contrib import admin\nfrom django.urls import path, include\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('', include('projects.urls'))\n]\n"], [], [], ["import pandas as pd\n\nurl = \"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\"\ntable = pd.read_html(url)[1]\n", "print (table.to_string())\n                                   0                   1                                  2                  3        4                                                  5\n0                               Name            Industry                             Sector       Headquarters  Founded                                              Notes\n1                  Airfast Indonesia   Consumer services                           Airlines          Tangerang     1971                                    Private airline\n2                       Angkasa Pura         Industrials            Transportation services            Jakarta     1962                               State-owned airports\n3                Astra International       Conglomerates                                  -            Jakarta     1957    Automotive, financials, industrials, technology\n4                  Bank Central Asia          Financials                              Banks            Jakarta     1957                                               Bank\n5                       Bank Danamon          Financials                              Banks            Jakarta     1956                                               Bank\n6                       Bank Mandiri          Financials                              Banks            Jakarta     1998                                               Bank\n7              Bank Negara Indonesia          Financials                              Banks            Jakarta     1946                                               Bank\n8              Bank Rakyat Indonesia          Financials                              Banks            Jakarta     1895                                 Micro-finance bank\n9                     Bumi Resources     Basic materials                     General mining            Jakarta     1973                                             Mining\n10                            Djarum      Consumer goods                            Tobacco  Kudus and Jakarta     1951                                            Tobacco\n11   Dragon Computer & Communication          Technology                  Computer hardware            Jakarta     1980                                  Computer hardware\n12             Elex Media Komputindo   Consumer services                         Publishing            Jakarta     1985                                          Publisher\n13                            Femina   Consumer services                              Media            Jakarta     1972                                    Weekly magazine\n14                  Garuda Indonesia   Consumer services                   Travel & leisure          Tangerang     1949                                State-owned airline\n15                      Gudang Garam      Consumer goods                            Tobacco             Kediri     1958                                            Tobacco\n16                      Gunung Agung   Consumer services                Specialty retailers            Jakarta     1953                                         Bookstores\n17       Indocement Tunggal Prakarsa         Industrials      Building materials & fixtures            Jakarta     1985         Cement, part of HeidelbergCement (Germany)\n18                          Indofood      Consumer goods                      Food products            Jakarta     1968                                    Food production\n19              Indonesian Aerospace         Industrials                          Aerospace            Bandung     1976                        State-owned aircraft design\n20    Indonesian Bureau of Logistics      Consumer goods                      Food products            Jakarta     1967                                  Food distribution\n21                           Indosat  Telecommunications      Fixed line telecommunications            Jakarta     1967                         Telecommunications network\n22               Infomedia Nusantara   Consumer services                         Publishing            Jakarta     1975                                Directory publisher\n23      Jalur Nugraha Ekakurir (JNE)         Industrials                  Delivery services            Jakarta     1990                                  Express logistics\n24                       Kalbe Farma         Health care                    Pharmaceuticals            Jakarta     1966                                    Pharmaceuticals\n25              Kereta Api Indonesia         Industrials                          Railroads            Bandung     1945                                State-owned railway\n26                       Kimia Farma         Health care                    Pharmaceuticals            Jakarta     1971                                 State-owned pharma\n27             Kompas Gramedia Group   Consumer services                     Media agencies            Jakarta     1965                                      Media holding\n28                    Krakatau Steel     Basic materials                       Iron & steel            Cilegon     1970                                  State-owned steel\n29                          Lion Air   Consumer services                           Airlines            Jakarta     2000                                   Low-cost airline\n30                       Lippo Group          Financials  Real estate holding & development            Jakarta     1950                                        Development\n31                          Matahari   Consumer services                Broadline retailers          Tangerang     1982                                  Department stores\n32                       MedcoEnergi           Oil & gas           Exploration & production            Jakarta     1980                                Energy, oil and gas\n33             Media Nusantara Citra   Consumer services       Broadcasting & entertainment            Jakarta     1997                                              Media\n34                   Panin Sekuritas          Financials                Investment services            Jakarta     1989                                             Broker\n35                         Pegadaian          Financials                   Consumer finance            Jakarta     1901                     State-owned financial services\n36                             Pelni         Industrials              Marine transportation            Jakarta     1952                                           Shipping\n37                     Pos Indonesia         Industrials                  Delivery services            Bandung     1995                         State-owned postal service\n38                         Pertamina           Oil & gas               Integrated oil & gas            Jakarta     1957                    State-owned oil and natural gas\n39             Perusahaan Gas Negara           Oil & gas           Exploration & production            Jakarta     1965                                                Gas\n40             Perusahaan Gas Negara           Utilities                   Gas distribution            Jakarta     1965             State-owned natural gas transportation\n41         Perusahaan Listrik Negara           Utilities           Conventional electricity            Jakarta     1945                State-owned electrical distribution\n42  Phillip Securities Indonesia, PT          Financials                Investment services            Jakarta     1989                                 Financial services\n43                            Pindad         Industrials                            Defense            Bandung     1808                                State-owned defense\n44                PT Lapindo Brantas           Oil & gas           Exploration & production            Jakarta     1996                                        Oil and gas\n45   PT Metro Supermarket Realty Tbk   Consumer services       Food retailers & wholesalers            Jakarta     1955                                       Supermarkets\n46                       Salim Group       Conglomerates                                  -            Jakarta     1972            Industrials, financials, consumer goods\n47                         Sampoerna      Consumer goods                            Tobacco           Surabaya     1913                                            Tobacco\n48                   Semen Indonesia         Industrials      Building materials & fixtures             Gresik     1957                                             Cement\n49                          Susi Air   Consumer services                           Airlines        Pangandaran     2004                                    Charter airline\n50                  Telkom Indonesia  Telecommunications      Fixed line telecommunications            Bandung     1856                         Telecommunication services\n51                         Telkomsel  Telecommunications          Mobile telecommunications            Jakarta     1995           Mobile network, part of Telkom Indonesia\n52                        Trans Corp       Conglomerates                                  -            Jakarta     2006  Media, consumer services, real estate, part of...\n53                Unilever Indonesia      Consumer goods                  Personal products            Jakarta     1933  Personal care products, part of Unilever (Neth...\n54                   United Tractors         Industrials       Commercial vehicles & trucks            Jakarta     1972                                    Heavy equipment\n55                           Waskita         Industrials                 Heavy construction            Jakarta     1961                           State-owned construction\n"], ["import pandas as pd\n\nurl = \"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\"\ntable = pd.read_html(url,header=0)\nprint(table[1])\n"], ["import requests\nfrom bs4 import BeautifulSoup as bs\nURL = \"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\"\nhtml = requests.get(URL).text\nsoup = bs(html, 'html.parser')\nta=soup.find(\"table\",{\"class\":\"wikitable sortable\"})\nprint(ta)\n", "ta=soup.find_all(\"table\",{\"class\":\"wikitable sortable\"})\n"], ["import requests\nfrom bs4 import BeautifulSoup as bs\n\nurl = \"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\"\nhtml = requests.get(url).text\nsoup = bs(html, 'html.parser')\n\nfor data in soup.find_all('table', {\"class\":\"wikitable\"}):\n    for td in data.find_all('td'):\n        for link in td.find_all('a'):\n            print (link.text)\n"], ["import requests\nfrom bs4 import BeautifulSoup\n\npage = requests.get(\"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\")\nsoup = BeautifulSoup(page.content, 'html.parser')\nta = soup.find_all('table',class_=\"wikitable\")\n\nprint(ta)\n", "[<table class=\"wikitable sortable\">\n<tbody><tr>\n<th>Rank\n</th>\n<th>Image\n</th>\n<th>Name\n</th>\n<th>2016 Revenues (USD $M)\n</th>\n<th>Employees\n</th>\n<th>Notes\n.\n.\n.\n"], ["import requests\nfrom bs4 import BeautifulSoup as bs\nURL = \"https://en.wikipedia.org/wiki/List_of_companies_of_Indonesia\"\nhtml = requests.get(url).text\nsoup = bs(html, 'html.parser')\nta=soup.find_all('table',{'class':'wikitable'})\nprint(ta)\n"], ["1) start a console/terminal\n2) switch to the conda environment `activate py37` \n    (or with virtualenv: `source .py37dev/bin/activate`)\n3) start Emacs from that same shell that has the modified environment variables.  \n    On a Mac its: `/Applications/Emacs.app/Contents/MacOS/Emacs` \n    (I use a installed version of Emacs on the Mac because the one that \n    comes with Mac is ancient).  \n    On Linux and Windows the path to EMacs will be different but the idea is the same.\n4) start a shell inside Emacs and you should see the shell looks the way it does \n    in your conda shell (or virtualenv shell)\n"], ["a = [1, 2, 2, 3, 3, 4, 5, 6]\n", "def unique_elements(lst, number_of_elements=None):\n    return list(dict.fromkeys(lst))[:number_of_elements]\n", "print(unique_elements(a, 2))\n"], ["1. If Firefox is open, close Firefox.\n2. Press Windows +R on the keyboard. A Run dialog will open.\n3. In the Run dialog box, type in firefox.exe -P\nNote: You can use -P or -ProfileManager(either one should work).\n4. Click OK.\n5. Create a new profile and sets its location to the RAM Drive.\n", "ProfilesIni profile = new ProfilesIni();\nFirefoxProfile myprofile = profile.getProfile(\"automation_profile\");\nWebDriver driver = new FirefoxDriver(myprofile);\n"], [], ["search_list = ['STEEL','IRON','GOLD','SILVER']\npat = r'\\b|\\b'.join(search_list)\npat2 = r'({})'.format('|'.join(search_list))\n\ndf_new= df.loc[df.b.str.contains(pat,case=False,na=False)].reset_index(drop=True)\ndf_new['new_col']=df_new.b.str.upper().str.extract(pat2)\nprint(df_new)\n\n     a                  b new_col\n0  123  'Blah Blah Steel'   STEEL\n1  789   'Blah Blah Gold'    GOLD\n"], ["search_list = ['STEEL','IRON','GOLD','SILVER']\n\ndef process(x):\n    for s in search_list:\n        if s in x['b'].upper(): print(\"'\"+ s +\"'\");return \"'\"+ s +\"'\"\n    return ''\n\ndf['c']= df.apply(lambda x: process(x),axis=1)\ndf = df.drop(df[df['c'] == ''].index).reset_index(drop=True)\n\nprint(df)\n", "     a                 b        c\n0  123  'Blah Blah Steel  'STEEL'\n1  789  'Blah Blah Gold'   'GOLD'\n"], ["search_list = ['STEEL','IRON','GOLD','SILVER']\n\ndf['c'] = df.b.str.extract('({0})'.format('|'.join(search_list)), flags=re.IGNORECASE)\nresult = df[~pd.isna(df.c)]\n\nprint(result)\n", "              a       b      c\n123 'Blah  Blah  Steel'  Steel\n789 'Blah  Blah   Gold'   Gold\n", "df['c'] = df.b.str.extract('(?i)({0})'.format('|'.join(search_list)))\n"], ["search_list = set(['STEEL','IRON','GOLD','SILVER'])\ndf['c'] = df['b'].apply(lambda x: set.intersection(set(x.upper().split(' ')), search_list))\n", "     a                b        c\n0  123  Blah Blah Steel  {STEEL}\n1  456   Blah Blah Blah       {}\n2  789   Blah Blah Gold   {GOLD}\n", "     a                b        c\n0  123  Blah Blah Steel  {STEEL}\n2  789   Blah Blah Gold   {GOLD}\n"], ["s=pd.DataFrame(df.b.str.upper().str.strip(\"'\").str.split(' ').tolist())\ns.where(s.isin(search_list),'').sum(1)\nOut[492]: \n0    STEEL\n1         \n2     GOLD\ndtype: object\ndf['New']=s.where(s.isin(search_list),'').sum(1)\ndf\nOut[494]: \n     a                  b    New\n0  123  'Blah Blah Steel'  STEEL\n1  456   'Blah Blah Blah'       \n2  789   'Blah Blah Gold'   GOLD\n"], ["def get_word(my_string):\n    for word in search_list:\n         if word.lower() in my_string.lower():\n               return word\n    return None\n\nnew_df[\"c\"]= new_df[\"b\"].apply(get_word)\n", "new_df[\"c\"]= new_df[\"b\"].apply(lambda my_string: [word for word in search_list if word.lower() in my_string.lower()][0])\n", "new_df[\"c\"]= new_df[\"b\"].apply(lambda my_string: next(word for word in search_list if word.lower() in my_string.lower())\n"], ["for index, row in df.iterrows(): # your loop\n    new_row = sorted(row.values)[:150]\n    # new_row should be a list with length 150.\n", "import numpy\nimport pandas\nimport random\n\n# generate dummy data\nl = list(range(1600))\nrandom.shuffle(l)\na = numpy.array(l)\na = a.reshape(40, 40) # columns x rows\ndummy_df = pandas.DataFrame(a)\n\n# dummy_df.shape = (40, 40)\n\nsmallest = []\nfor idx, row in dummy_df.iterrows():\n    smallest.append(sorted(row.values)[:10])\n\nnew_df = pandas.DataFrame(numpy.array(smallest))\n# new_df.shape = (40, 10)\n"], ["import numpy as np\nrow = np.array([1, 6, 2, 12, 7, 8, 9, 11, 15, 26])\nk = 5\nidx = np.argpartition(row, k)[:k]\n\nprint(idx)\nprint(row[idx])\n\n-->\n[1 0 2 4 5]\n[6 1 2 7 8]\n", "import numpy as np\ndata = np.array([\n    [1, 6, 2, 12, 7, 8, 9, 11, 15, 26],\n    [1, 65, 2, 12, 7, 8, 9, 11, 15, 26],\n    [16, 6, 2, 12, 7, 8, 9, 11, 15, 26]])\nk = 5\nidx = np.argpartition(data, k)[:,:k]\n\nprint(idx)\n\n-->\n[[1 0 2 4 5]\n [2 0 4 5 6]\n [4 2 1 5 6]]\n"], ["import pandas as pd\nimport numpy as np\nimport heapq\n\ndf = pd.DataFrame(np.random.randn(1000, 1000))\n\n# Find the 150 smallest values in each row\nsmallest = df.apply(lambda x: heapq.nsmallest(150, x), axis=1)\n", "smallest_df = pd.DataFrame(smallest.values.tolist())\n", "smallest_df.head()\n\n"], ["df = df.transpose()\nfor col in df.columns:\n    min_values = df[col].sort_values()[0:150]\n    # now calc slope/area\n"], ["import tensorflow as tf\n\ndef split_tfrecord(tfrecord_path, split_size):\n    with tf.Graph().as_default(), tf.Session() as sess:\n        ds = tf.data.TFRecordDataset(tfrecord_path).batch(split_size)\n        batch = ds.make_one_shot_iterator().get_next()\n        part_num = 0\n        while True:\n            try:\n                records = sess.run(batch)\n                part_path = tfrecord_path + '.{:03d}'.format(part_num)\n                with tf.python_io.TFRecordWriter(part_path) as writer:\n                    for record in records:\n                        writer.write(record)\n                part_num += 1\n            except tf.errors.OutOfRangeError: break\n", "split_tfrecord(my_records.tfrecord, 100)\n"], ["from itertools import chain\n\nmylist = [['Ac,Cr,Dr'], \n          ['Ac,Ad,Sc'], \n          ['Ac,Bi,Dr'], \n          ['Ad,Dr,Sc'], \n          ['An,Dr,Fa'],   \n          ['Bi,Co,Dr'], \n          ['Dr,Mu'], \n          ['Ac,Co,My'], \n          ['Co,Dr'], \n          ['Ac,Ad,Sc'], \n          ['An,Ac,Ad']\n         ]\n\nflat_list = list(chain.from_iterable(mylist))\n\nunique_list = set(','.join(flat_list).split(','))\n"], ["from collections import Counter\nfrom itertools import chain\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    mylist = [\n        ['Ac,Cr,Dr'],\n        ['Ac,Ad,Sc'],\n        ['Ac,Bi,Dr'],\n        ['Ad,Dr,Sc'],\n        ['An,Dr,Fa'],\n        ['Bi,Co,Dr'],\n        ['Dr,Mu'],\n        ['Ac,Co,My'],\n        ['Co,Dr'],\n        ['Ac,Ad,Sc'],\n        ['An,Ac,Ad'],\n     ]\n\n    # if lists contain only one element\n    occurrence_count = Counter(chain(*map(lambda x: x[0].split(','), mylist)))\n\n    items = list(occurrence_count.keys())  # items, with no repetitions\n    all_items = list(occurrence_count.elements())  # all items\n    ac_occurrences = occurrence_count['Ac']  # occurrences of 'Ac'\n\n    print(f\"Unique items: {items}\")\n    print(f\"All list elements: {all_items}\")\n    print(f\"Occurrences of 'Ac': {ac_occurrences}\")\n", "Unique items: ['Ac', 'Cr', 'Dr', 'Ad', 'Sc', 'Bi', 'An', 'Fa', 'Co', 'Mu', 'My']\nAll list elements: ['Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Cr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Ad', 'Ad', 'Ad', 'Ad', 'Sc', 'Sc', 'Sc', 'Bi', 'Bi', 'An', 'An', 'Fa', 'Co', 'Co', 'Co', 'Mu', 'My']\nOccurrences of 'Ac': 6\n", "from collections import Counter\nfrom itertools import chain\nfrom functools import partial\n\nif __name__ == '__main__':\n\n    mylist_complex = [\n        ['Ac,Cr,Dr', 'Ac,Ad,Sc'],\n        ['Ac,Ad,Sc', 'Ac,Bi,Dr'],\n        ['Ac,Bi,Dr', 'Ad,Dr,Sc'],\n        ['Ad,Dr,Sc', 'An,Dr,Fa'],\n        ['An,Dr,Fa', 'Bi,Co,Dr'],\n        ['Bi,Co,Dr', 'Dr,Mu'],\n        ['Dr,Mu', 'Ac,Co,My'],\n        ['Ac,Co,My', 'Co,Dr'],\n        ['Co,Dr', 'Ac,Ad,Sc'],\n        ['Ac,Ad,Sc', 'An,Ac,Ad'],\n        ['An,Ac,Ad', 'Ac,Cr,Dr'],\n    ]\n\n    # if lists contain more than one element\n    occurrence_count_complex = Counter(chain(*map(lambda x: chain(*map(partial(str.split, sep=','), x)), mylist_complex)))\n\n    items = list(occurrence_count_complex.keys())  # items, with no repetitions\n    all_items = list(occurrence_count_complex.elements())  # all items\n    ac_occurrences = occurrence_count_complex['Ac']  # occurrences of 'Ac'\n\n    print(f\"Unique items: {items}\")\n    print(f\"All list elements: {all_items}\")\n    print(f\"Occurrences of 'Ac': {ac_occurrences}\")\n", "Unique items: ['Ac', 'Cr', 'Dr', 'Ad', 'Sc', 'Bi', 'An', 'Fa', 'Co', 'Mu', 'My']\nAll list elements: ['Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Ac', 'Cr', 'Cr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Dr', 'Ad', 'Ad', 'Ad', 'Ad', 'Ad', 'Ad', 'Ad', 'Ad', 'Sc', 'Sc', 'Sc', 'Sc', 'Sc', 'Sc', 'Bi', 'Bi', 'Bi', 'Bi', 'An', 'An', 'An', 'An', 'Fa', 'Fa', 'Co', 'Co', 'Co', 'Co', 'Co', 'Co', 'Mu', 'Mu', 'My', 'My']\nOccurrences of 'Ac': 12\n"], ["from collections import Counter\ncount = Counter()\nmylist = [['Ac,Cr,Dr'],\n ['Ac,Ad,Sc'],\n ['Ac,Bi,Dr'],\n ['Ad,Dr,Sc'],\n ['An,Dr,Fa'],\n ['Bi,Co,Dr'],\n ['Dr,Mu'],\n ['Ac,Co,My'],\n ['Co,Dr'],\n ['Ac,Ad,Sc'],\n ['An,Ac,Ad'],\n]\n\nfor arr in mylist:\n    count.update(arr[0].split(','))\n\nprint(count) # dictionary of symbols: counts\nprint(list(count.keys())) # list of all unique elements\n"], ["    mylist = [['Ac,Cr,Dr'],\n    ['Ac,Ad,Sc'],\n    ['Ac,Bi,Dr'],\n    ['Ad,Dr,Sc'],\n    ['An,Dr,Fa'],\n    ['Bi,Co,Dr'],\n    ['Dr,Mu'],\n    ['Ac,Co,My'],\n    ['Co,Dr'],\n    ['Ac,Ad,Sc'],\n    ['An,Ac,Ad'],\n    ]\n\n    uniquedict = {}\n\n    for sublist in mylist:\n        for item in sublist[0].split(','):\n            if item in uniquedict.keys():\n                uniquedict[item] += 1\n            else:\n                uniquedict[item] = 1\n\n    print(uniquedict)\n    print(list(uniquedict.keys()))\n"], [], ["import chardet    \nrawdata = open('D:\\\\seminar\\\\totaldata.csv', 'rb').read()\nresult = chardet.detect(rawdata)\ncharenc = result['encoding']\nprint(charenc)\n", "pd.read_csv('D:\\\\seminar\\\\totaldata.csv',encoding = 'encoding you found')\n", "pd.read_csv(r'D:\\seminar\\totaldata.csv',encoding = 'encoding you found')\n"], ["\" \".join([list1[x]+[y if y is not None else '' for y in list2 ][::-1][x] for x in range(len(list1)-1)]\n"], ["def fetch_index(list2, item_index):\n    x = list2[::-1]\n    return x[item_index]\n\ndef merge_list(list1, list2):\n    list_3 = []\n    #write your logic here\n    for l1 in list1:\n        l2 = fetch_index(list2, list1.index(l1))\n        if l1 is None and l2 is None:\n            pass\n        elif l1 is None:\n            list_3.append(l2)\n        elif l2 is None:\n            list_3.append(l1)\n        else:\n            list_3.append(l1+l2)\n    return(list_3)\n\nlist1=['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\nlist2=['y','tor','e','eps','ay',None,'le','n']\nx = merge_list(list1,list2)\nprint ' '.join(i for i in x)\n"], ["def merge_list(lst1,lst2):\n    s = ''\n    for x, y in zip(lst1, lst2[::-1]):\n        if y and x:\n            s += x + y\n        elif x:\n            s += x\n        elif y:\n            s += y\n        s += ' '\n    return s[:-1]\n\nlist1 = ['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\nlist2 = ['y','tor','e','eps','ay',None,'le','n']\nmerged_data = merge_list(list1,list2)\n\nprint(merged_data)\n# An apple a day keeps the doctor away\n", "def merge_list(lst1,lst2):\n    return ' '.join(x + y if x and y else x if x else y for x, y in zip(lst1, lst2[::-1]))\n"], ["\" \".join(str(x) for x in [list1[i]+list2[len(list2)-1-i] if list2[len(list2)-1-i] != None else list1[i] for i in range(len(list1))])\n    'An apple a day keeps the doctor away'\n"], ["[\"\".join(row) for row in zip(list1, reversed(list2))]\n", "['An', 'apple', 'a', 'day', 'keeps', 'the', 'doctor', 'away']\n"], [">>> list1=['A', 'app','a', 'd', 'ke', 'th', 'doc', 'awa']\n>>> list2=['y','tor','e','eps','ay',None,'le','n']\n>>> ' '.join([l1 + l2 if l1 and l2 else l1 if l1 and not l2 else l2 for l1, l2 in zip(list1, reversed(list2)) if l1 and l2])\n'An apple a day keeps the doctor away'\n"], ["data_array = [1, 2, 5, 4, 6, 9]\n# Delete the first and the last element of the data array. \nreduced_array = [ data_array[i] for i in range(1, len(data_array)-1) ]\n# Find the maximum value of the modified array \npeak_value = max(reduced_array)\n# Print out the maximum value and its index in the data array. \nprint 'The peak value is: ' + str(peak_value)\nprint 'And its position is: ' + str(data_array.index(peak_value))\n", "The peak value is: 6\nAnd its position is: 4\n"], ["def peaks(ar):\n    i, up = 0, False\n    for j in range(1, len(ar)):\n        prev, val = ar[j-1], ar[j]\n        if up and val < prev:\n            yield prev, i\n            up = False\n        if val > prev:\n            i, up = j, True\n\n>>> list(peaks([0,1,2,5,1,0]))\n[(5, 3)]\n>>> list(peaks([0,1,2,5,1,2,0]))\n[(5, 3), (2, 5)]\n>>> list(peaks([0,1,2,5,1,2,0,3]))\n[(5, 3), (2, 5)]\n>>> list(peaks([1,2,2,2,1]))\n[(2, 1)]\n>>> list(peaks([1,2,2,2,3]))\n[]\n"], ["from itertools import groupby\n\ndef process_data(data):\n    return [list(val for num in group) for val, group in groupby(data)]\n\n\ndef peaks(arr):\n    #print(arr)\n    posPeaks = {\n    \"pos\": [],\n    \"peaks\": [],\n    }\n    startFound = False\n    n = 0\n    while startFound == False:\n        if arr[n][0] == arr[n+1][0]:\n            n += 1\n        else:\n            startFound = True\n\n    endFound = False\n    m = len(arr) - 1\n    while endFound == False:\n        if arr[m][0] == arr[m-1][0]:\n            m -= 1\n        else:\n            endFound = True\n\n    for i in range(n+1, m):\n        if arr[i][0] == arr[i-1][0]:\n            None\n        elif arr[i][0] >= arr[i-1][0] and arr[i][0] >= arr[i+1][0]:\n            pos = sum([len(arr[idx]) for idx in range(i)])\n            posPeaks[\"pos\"].append(pos) #.append(i)\n            posPeaks[\"peaks\"].append(arr[i][0])\n    return posPeaks\n\n\n\nprint(peaks(process_data([0, 1, 2, 5, 1, 0])))\nprint(peaks(process_data([1, 2, 2, 2, 1])))\nprint(peaks(process_data([1, 2, 2, 2, 3])))\n", "{'pos': [3], 'peaks': [5]}\n{'pos': [1], 'peaks': [2]}\n{'pos': [], 'peaks': []}\n"], ["from itertools import groupby\n\n\ndef peaks(data):\n    start = 0\n    sequence = []\n    for key, group in groupby(data):\n        sequence.append((key, start))\n        start += sum(1 for _ in group)\n\n    for (b, bi), (m, mi), (a, ai) in zip(sequence, sequence[1:], sequence[2:]):\n        if b < m and a < m:\n            yield m, mi\n\n\nprint(list(peaks([0, 1, 2, 5, 1, 0])))\nprint(list(peaks([1, 2, 2, 2, 1])))\nprint(list(peaks([1, 2, 2, 2, 3])))\n", "[(5, 3)]\n[(2, 1)]\n[]\n"], [">>> a = [1, 2, 2, 3, 3, 4, 5, 6]\n>>> list(set(a))[:5]\n[1, 2, 3, 4, 5]\n"], ["import itertools as it\n\n\na = [1, 2, 2, 3, 3, 4, 5, 6]\n", "[k for k, _ in it.groupby(a)][:5]\n# [1, 2, 3, 4, 5]\n", "list(dict.fromkeys(a))[:5]\n# [1, 2, 3, 4, 5]\n"], ["from itertools import groupby, islice\n\ndef first_unique(data, upper):\n    return islice((key for (key, _) in groupby(data)), 0, upper)\n\na = [1, 2, 2, 3, 3, 4, 5, 6]\n\nprint(list(first_unique(a, 5)))\n"], ["def unique_everseen_limit(iterable, limit=5):\n    seen = set()\n    seen_add = seen.add\n    for element in iterable:\n        if element not in seen:\n            seen_add(element)\n            yield element\n        if len(seen) == limit:\n            break\n\na = [1,2,2,3,3,4,5,6]\n\nres = list(unique_everseen_limit(a))  # [1, 2, 3, 4, 5]\n", "from itertools import islice\n\ndef unique_everseen(iterable):\n    seen = set()\n    seen_add = seen.add\n    for element in iterable:\n        if element not in seen:\n            seen_add(element)\n            yield element\n\nres = list(islice(unique_everseen(a), 5))  # [1, 2, 3, 4, 5]\n", "from itertools import islice\nfrom more_itertools import unique_everseen\nfrom toolz import unique\n\nres = list(islice(unique_everseen(a), 5))  # [1, 2, 3, 4, 5]\nres = list(islice(unique(a), 5))           # [1, 2, 3, 4, 5]\n"], ["a = [1,2,2,3,3,4,5,6]\nres = []\nfor x in a:\n    if x not in res:  # yes, not optimal, but doesnt need additional dict\n        res.append(x)\n        if len(res) == 5:\n            break\nprint(res)\n"], ["from collections import OrderedDict\n\n\ndef nub(iterable):\n    \"\"\"Returns unique elements preserving order.\"\"\"\n    return OrderedDict.fromkeys(iterable).keys()\n", "from itertools import islice\n\n\ndef iterate(itr, upper=5):\n    return islice(nub(itr), upper)\n", "def iterate(itr, upper=5):\n    return list(nub(itr))[:upper]\n", "def nub(iterable):\n    seen = set()\n    add_seen = seen.add\n    for element in iterable:\n        if element in seen:\n            continue\n        yield element\n        add_seen(element)\n"], ["In [95]: from itertools import takewhile\n\nIn [96]: seen = set()\n\nIn [97]: set(takewhile(lambda x: seen.add(x) or len(seen) <= 4, a))\nOut[97]: {1, 2, 3, 4}\n"], ["N = 3\na = [1, 2, 2, 3, 3, 3, 4]\nd = {x: True for x in a}\nlist(d.keys())[:N]\n"], ["sorted(set(a), key=list(a).index)[:5]\nOut[136]: [1, 2, 3, 4, 5]\n"], ["lst = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\n\nnew_lst = []\nsublist = []\nfor element in lst:\n    if element != 'k':\n        sublist.append(element)\n    else:\n        new_lst.append(sublist)\n        sublist = []\n\nif sublist: # add the last sublist\n    new_lst.append(sublist)\n\nresult = tuple(new_lst) \nprint(result)\n# (['a'], ['b', 'c'], ['d', 'e', 'g'])\n", "from itertools import groupby\n\nlst = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\nresult = tuple(list(gp) for is_k, gp in groupby(lst, \"k\".__eq__) if not is_k)\n\nprint(result)\n# (['a'], ['b', 'c'], ['d', 'e', 'g'])\n"], [">>> l = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\n>>> t = []\n>>> for s in ''.join(l).split('k'):\n...     t.append(list(s))\n...\n>>> t\n[['a'], ['b', 'c'], ['d', 'e', 'g']]\n>>> t = tuple(t)\n>>> t\n(['a'], ['b', 'c'], ['d', 'e', 'g'])\n", ">>> def list_to_tuple(l):\n...     t = []\n...     for s in l:\n...             t.append(list(s))\n...     return tuple(t)\n...\n>>> l = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\n>>> l = ''.join(l).split('k')\n>>> l = list_to_tuple(l)\n>>> l\n(['a'], ['b', 'c'], ['d', 'e', 'g'])\n"], ["def isplit_list(lst, v):\n    while True:\n        try:\n            end = lst.index(v)\n        except ValueError:\n            break\n\n        yield lst[:end]\n        lst = lst[end+1:]\n\n    if len(lst):\n        yield lst\n\n\nlst = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g', 'k']\n\nresults = tuple(isplit_list(lst, 'k'))\n"], ["tuple(list(i) for i in ''.join(lst).split('k'))\n", "(['a'], ['b', 'c'], ['d', 'e', 'g'])\n"], ["smallerlist = [l.split(',') for l in ','.join(lst).split('k')]\nprint(smallerlist)\n", "[['a', ''], ['', 'b', 'c', ''], ['', 'd', 'e', 'g']]\n", "smallerlist = [' '.join(l).split() for l in smallerlist]\nprint(smallerlist)\n", "[['a'], ['b', 'c'], ['d', 'e', 'g']]  \n"], ["import re\n\nlst = ['a', 'k', 'b', 'c', 'k', 'd', 'e', 'g']\n\ntuple(map(list, re.split('k',''.join(lst))))\n\n(['a'], ['b', 'c'], ['d', 'e', 'g'])\n"], ["df[\"Buying_Price\"]=df[\"Buying_Price\"].replace({\"vhigh\":4})\n", "import pandas as pd\nimport numpy as np\n\na = np.array([1])\ndf = pd.DataFrame({\"Maintanance_price\": a})\ndf[\"Maintanance_price\"] = df[\"Maintanance_price\"].replace({\"a\":1})\n\nprint(df)\n", "TypeError: Cannot compare types 'ndarray(dtype=int64)' and 'str'\n"]]